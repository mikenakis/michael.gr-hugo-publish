<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Testing on Michael's Blog</title><link>https://blog2.michael.gr/tags/testing/</link><description>Recent content in Testing on Michael's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Michael Belivanakis (a.k.a. Mike Nakis)</copyright><lastBuildDate>Thu, 23 Oct 2025 20:16:22 +0200</lastBuildDate><atom:link href="https://blog2.michael.gr/tags/testing/index.xml" rel="self" type="application/rss+xml"/><item><title>The confusion about the term Unit Testing</title><link>https://blog2.michael.gr/post/2025-04-the-confusion-about-term-unit-testing/</link><pubDate>Fri, 04 Apr 2025 14:53:02 +0000</pubDate><guid>https://blog2.michael.gr/post/2025-04-the-confusion-about-term-unit-testing/</guid><description>&lt;p&gt;&lt;img src="https://blog2.michael.gr/post/2025-04-the-confusion-about-term-unit-testing/images/crash-test-dummy-penseur.jpg"
width="2560"
height="1600"
srcset="https://blog2.michael.gr/post/2025-04-the-confusion-about-term-unit-testing/images/crash-test-dummy-penseur_hu_bcad233453122025.jpg 480w, https://blog2.michael.gr/post/2025-04-the-confusion-about-term-unit-testing/images/crash-test-dummy-penseur_hu_81009ce0ef2f8adf.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="160"
data-flex-basis="384px"
&gt;
&lt;/p&gt;
&lt;p&gt;Virtually everyone claims to be doing Unit Testing, but there is a surprising amount of disagreement as to how unit testing is defined. Let us see what the authorities on the subject have to say about it. What follows is mainly quotations from reputable sources, with some minimal commentary by me.&lt;/p&gt;
&lt;h3 id="wikipedia"&gt;Wikipedia
&lt;/h3&gt;&lt;p&gt;Let us begin by checking &lt;a class="external"
href="https://en.wikipedia.org/wiki/Unit_testing" target="_blank"
&gt;the Wikipedia entry for Unit Testing&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Unit testing, a.k.a. component or module testing, is a form of software testing by which isolated source code is tested to validate expected
behavior. Unit testing describes tests that are run at the unit-level to contrast testing at the integration or system level.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Further down in the history section, Wikipedia lists some of the earliest known efforts of what we would today call unit testing, where the common theme is testing separately smaller parts of large software systems before integrating them together.&lt;/p&gt;
&lt;p&gt;I am in full agreement with Wikipedia's definition, but Wikipedia is everyone's favorite source to cite if it agrees with their preconceptions, or proclaim untrustworthy if it does not, so can we find any other definition that corroborates the above?&lt;/p&gt;
&lt;h3 id="ieee"&gt;IEEE
&lt;/h3&gt;&lt;p&gt;In the &lt;em&gt;&lt;strong&gt;Definitions&lt;/strong&gt;&lt;/em&gt; section of &lt;a class="external"
href="https://ieeexplore.ieee.org/document/27763" target="_blank"
&gt;IEEE 1008-1987 Standard for Software Unit Testing&lt;/a&gt; we read:&lt;/p&gt;
&lt;p&gt;[Warning! wooden language ahead!]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;test unit3:&lt;/strong&gt; A set of one or more computer program modules together with associated control data, (for example, tables), usage procedures, and operating procedures that satisfy the following conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;All modules are from a single computer program&lt;/li&gt;
&lt;li&gt;At least one of the new or changed modules in the set has not completed the unit test&lt;sup&gt;4&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;The set of modules together with its associated data and procedures are the sole object of a testing process&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;And the footnotes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; A test unit may occur at any level of the design hierarchy from a single module to a complete program. Therefore, a test unit may be a module, a few modules, or a complete computer program along with associated data and procedures.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;4&lt;/sup&gt; A test unit may contain one or more modules that have already been unit tested.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As we can see, IEEE's definition says nothing about isolation; instead, it considers an entire set of modules, of which only one might need testing, as a unit.&lt;/p&gt;
&lt;p&gt;So, we have found a source that contradicts Wikipedia. It is a tie. Now we need to find a third opinion, to form a majority.&lt;/p&gt;
&lt;h3 id="kent-beck"&gt;Kent Beck
&lt;/h3&gt;&lt;p&gt;Surely, Kent Beck, the inventor of Test-Driven Development and author of JUnit must have defined the term, right? Well, as it turns out, no.&lt;/p&gt;
&lt;p&gt;In his original &lt;a class="external"
href="https://web.archive.org/web/20150315073817/http://www.xprogramming.com/testfram.htm" target="_blank"
&gt;&amp;quot;Simple Smalltalk Testing: With Patterns&amp;quot; paper&lt;/a&gt; the closest he gets to providing a definition is this sentence:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I recommend that developers write their own unit tests, one per class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Can &amp;quot;one test per class&amp;quot; be regarded as a definition of the term? I do not think so. I do not think it even makes sense as a statement, with modern programming languages and tooling.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;Test Driven Development by Example&lt;/em&gt; (2002) the closest that Kent Beck gets to providing a definition is this sentence:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The problem with driving development with small scale tests (I call them &amp;quot;unit tests&amp;quot;, but they don't match the accepted definition of unit tests very well) is that you run the risk [...]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, Kent Beck seems to regard unit tests as small-scale tests, which is not really a definition, and he acknowledges that there exists some other, accepted definition, but he does not say what that definition is. Perhaps Kent Beck thinks of a unit test as &lt;em&gt;a unit of testing&lt;/em&gt;, as in &lt;em&gt;a unit of information&lt;/em&gt; or &lt;em&gt;a unit of improvement&lt;/em&gt;, but we cannot be sure.&lt;/p&gt;
&lt;p&gt;Although Kent Beck makes no other attempt to define the term, in the same book he does mention a couple of times that a unit test should be concerned with the externally visible behavior of a unit, not with its implementation.&lt;/p&gt;
&lt;p&gt;As a result, it should come as no surprise to hear that Kent Beck does not use mocks. In the video &lt;em&gt;Thoughtworks Hangouts: Is TDD dead?&lt;/em&gt; (&lt;a class="external"
href="https://www.youtube.com/watch?v=z9quxZsLcfo" target="_blank"
&gt;youtube&lt;/a&gt;, &lt;a class="external"
href="https://martinfowler.com/articles/is-tdd-dead/" target="_blank"
&gt;text digest&lt;/a&gt;) at 21':10'' Kent Beck states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;My personal practice is I mock almost nothing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="martin-fowler"&gt;Martin Fowler
&lt;/h3&gt;&lt;p&gt;One often-cited author who is known for defining terms and elucidating concepts is Martin Fowler. So, what does he have to say about unit testing?&lt;/p&gt;
&lt;p&gt;&lt;a class="external"
href="https://martinfowler.com/bliki/UnitTest.html" target="_blank"
&gt;Martin Fowler's page on &lt;em&gt;&lt;strong&gt;Unit Test&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt; begins by acknowledging that it is an ill-defined term, and that the only characteristics of unit testing that people seem to agree on are that they are supposed to be a) small-scale, b) written by the programmers themselves, and c) fast. Then, Martin Fowler proceeds to talk about two schools of thought that understand the term differently:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &amp;quot;classicist&amp;quot; school of thought, which favors &amp;quot;sociable&amp;quot; unit tests, places emphasis on testing the behavior of a component, allowing the component to interact with its collaborators and assuming that the collaborators are working correctly. Martin Fowler places himself in this school of thought.&lt;/li&gt;
&lt;li&gt;The &amp;quot;mockist&amp;quot; school of thought, which favors &amp;quot;solitary&amp;quot; unit tests, insists on testing each component in isolation from its collaborators, and therefore requires that every collaborator must be replaced with a &amp;quot;test double&amp;quot; for the purpose of testing. Martin Fowler states that he respects this school of thought, but he does not belong to it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Okay, so this did not lead us to a single definition of unit testing, but at least it helped us further define two competing definitions.&lt;/p&gt;
&lt;p&gt;It is also worth noting that Martin Fowler does not use mocks, either. In the video &lt;em&gt;Thoughtworks Hangouts: Is TDD dead?&lt;/em&gt; (&lt;a class="external"
href="https://www.youtube.com/watch?v=z9quxZsLcfo" target="_blank"
&gt;youtube&lt;/a&gt;, &lt;a class="external"
href="https://martinfowler.com/articles/is-tdd-dead/" target="_blank"
&gt;text digest&lt;/a&gt;) at 23':56'' Martin Fowler adds:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I'm with Kent, I hardly ever use mocks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="robert-c-martin-uncle-bob"&gt;Robert C. Martin (Uncle Bob)
&lt;/h3&gt;&lt;p&gt;Among industry speakers, one of the most recognizable names is Robert C. Martin, a.k.a. Uncle Bob, author of the highly acclaimed book &lt;em&gt;&lt;strong&gt;Clean Code&lt;/strong&gt;&lt;/em&gt;. In his blog, under &lt;a class="external"
href="https://blog.cleancoder.com/uncle-bob/2017/05/05/TestDefinitions.html" target="_blank"
&gt;First-Class Tests&lt;/a&gt; he writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Unit Test: A test written by a programmer for the purpose of ensuring that the production code does what the programmer expects it to do.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is not very useful. According to this definition, a unit test could be virtually anything.&lt;/p&gt;
&lt;p&gt;Further down Uncle Bob gives a separate definition for integration tests, so maybe he regards the two as different, which would imply that he regards unit tests as testing units in isolation, but we cannot really be sure.&lt;/p&gt;
&lt;p&gt;To confuse things, further down he mentions mocks only in the context of what he calls functional tests, so maybe he thinks of mocks as not belonging to unit tests, (which then begs the question how the unit tests can achieve isolation,) but we cannot be sure about that, either.&lt;/p&gt;
&lt;p&gt;One thing we can be sure of is that Uncle Bob is also not particularly in favor of mocks. On that same page we read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I, for example, seldom use a mocking tool. When I need a mock (or, rather, a Test Double) I write it myself.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that Uncle Bob finds it important enough to state his preference for a test double rather than a mock. That is probably because what he writes himself is fakes, not mocks. (Both fakes and mocks are different kinds of test doubles, see &lt;a class="external"
href="https://martinfowler.com/bliki/TestDouble.html" target="_blank"
&gt;Martin Fowler: Test Double&lt;/a&gt; and &lt;a class="external"
href="https://martinfowler.com/articles/mocksArentStubs.html" target="_blank"
&gt;Martin Fowler: Mocks Aren't Stubs&lt;/a&gt;.)&lt;/p&gt;
&lt;h3 id="ian-cooper"&gt;Ian Cooper
&lt;/h3&gt;&lt;p&gt;An interestingly conflicting opinion comes from Ian Cooper, an outspoken TDD advocate.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;&lt;a class="external"
href="https://www.infoq.com/presentations/tdd-original/" target="_blank"
&gt;TDD, Where Did It All Go Wrong? (&lt;/a&gt;&lt;a class="external"
href="https://www.infoq.com/presentations/tdd-original/" target="_blank"
&gt;InfoQ&lt;/a&gt;&lt;a class="external"
href="https://www.infoq.com/presentations/tdd-original/" target="_blank"
&gt;2017)&lt;/a&gt;&lt;/em&gt; Ian Cooper states that in TDD a unit test is defined as a test that runs in isolation from other tests, not a test that isolates the unit under test from other units. In other words, the unit of isolation is the test, not the unit under test.&lt;/p&gt;
&lt;p&gt;Ian Cooper obviously acknowledges that the prevailing understanding of unit tests is that they isolate the unit under test from other units, and he introduces a dissenting understanding, as if TDD is so radical that it justifies redefining long established terms. This is at best a refreshingly different take on the subject, and at worst a completely unfounded mental acrobatic.&lt;/p&gt;
&lt;p&gt;The notion that the term &amp;quot;unit&amp;quot; in unit testing refers to the test rather than the component-under-test is inadmissible at the very least because it does not rhyme with integration testing and end-to-end testing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integration testing is about running our tests on integrations of system components, not about running tests somehow integrated with each other;&lt;/li&gt;
&lt;li&gt;End-to-end testing is about running our tests on our entire system as a whole, not about somehow stringing all of our tests together.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;therefore:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Unit testing is about running our tests on individual components of our system, not about running the tests individually. (Although I grant you that having isolation between individual tests is also a good idea, when possible.)&lt;/p&gt;
&lt;p&gt;It is worth noting that Ian Cooper also belongs to the ranks of those who do not approve of mocks. In the same talk, at 49':45'' he says:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;I argue quite heavily against mocks because they are over-specified.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="glenford-myers"&gt;Glenford Myers
&lt;/h3&gt;&lt;p&gt;So far we have had only a moderate amount of luck in finding a majority opinion to define unit testing. Let us try to locate the original source of the term, shall we?&lt;/p&gt;
&lt;p&gt;I do not know for sure that the first recorded use of the term is in the 1979 classic &lt;em&gt;The Art of Software Testing&lt;/em&gt; by Glenford Myers, but the book is so old that it seems reasonable to suppose so.&lt;/p&gt;
&lt;p&gt;The original 1979 edition (ISBN 9780471043287, 0471043281) is not easy to obtain, so I cannot ascertain this, but I strongly suspect that the term &amp;quot;unit&amp;quot; did not appear in it; instead, it was likely added in the 2nd edition, revised by other authors and published in 2004. Nonetheless, I think it is safe to assume that when back in 1979 Glenford Myers was writing of &amp;quot;module testing&amp;quot; what he meant was precisely that which we now call unit testing.&lt;/p&gt;
&lt;p&gt;In chapter 5 &amp;quot;Module (Unit) Testing&amp;quot; of the 2nd edition we read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Module testing (or unit testing) is a process of testing the individual subprograms, subroutines, or procedures in a program. That is, rather than initially testing the program as a whole, testing is first focused on the smaller building blocks of the program.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Later in the same chapter the author acknowledges this form of testing to be white-box testing:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Module testing is largely white-box oriented.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Further down, he even lays down the foundations of what later came to be known as mocks:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[...] since module B calls module E, something must be present to receive control when B calls E. A stub module, a special module given the name &amp;quot;E&amp;quot; that must be coded to simulate the function of module E, accomplishes this.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, this definition is in line with Wikipedia's definition; we finally have a majority.&lt;/p&gt;
&lt;h3 id="conclusion"&gt;Conclusion
&lt;/h3&gt;&lt;p&gt;Although not unanimous, the prevailing opinion seems to be that the term unit refers to the component under test, and it is specifically called a unit because it is supposed to be tested in isolation from its collaborators, in contrast to integration testing and end-to-end testing where components are allowed to interact with their collaborators.&lt;/p&gt;
&lt;p&gt;This prevailing opinion comes from:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wikipedia&lt;/li&gt;
&lt;li&gt;Glenford Myers&lt;/li&gt;
&lt;li&gt;the mockist school of thought mentioned by Martin Fowler&lt;/li&gt;
&lt;li&gt;hints about a popular understanding of unit testing outside of TDD, which Ian Cooper tries to redefine in the context of TDD.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A lot of the confusion seems to stem from the fact that testing a component in isolation requires mocking its collaborators, but almost all of the people cited in this research realize that the use of mocks is misguided, so they either refrain from accurately defining the term, or try to give alternative definitions of the term, or speak of different schools of thought, in an attempt to legitimize violations of the requirement for isolation, so that they can still call what they do unit testing, even though it really is not.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Cover image: Created by michael.gr using ChatGPT, and then retouched to remove imperfections. The prompt used was: &amp;quot;Please give me an image of a crash test dummy in the style of The Thinker, by Auguste Rodin.&amp;quot;&lt;/p&gt;</description></item><item><title>Testana: A better way of running tests</title><link>https://blog2.michael.gr/post/2024-10-testana/</link><pubDate>Sat, 26 Oct 2024 10:58:59 +0000</pubDate><guid>https://blog2.michael.gr/post/2024-10-testana/</guid><description>&lt;p&gt;&lt;img src="https://blog2.michael.gr/post/2024-10-testana/media/testana-logo.svg"
loading="lazy"
&gt;
&lt;/p&gt;
&lt;h3 id="abstract"&gt;Abstract
&lt;/h3&gt;&lt;p&gt;A software testing tool is presented, which uses &lt;em&gt;&lt;strong&gt;dependency analysis&lt;/strong&gt;&lt;/em&gt; to greatly optimize the process of running tests.&lt;/p&gt;
&lt;p&gt;(Useful pre-reading: &lt;a
href="https://blog2.michael.gr/post/2022-11-about-these-papers/"
&gt;About these papers&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id="what-is-testana"&gt;What is Testana?
&lt;/h3&gt;&lt;p&gt;Testana is a console application that you launch when you want to run your tests. So far, I have created two implementations of Testana:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A Java implementation, supporting JUnit 4 annotations in Maven-based projects.&lt;/li&gt;
&lt;li&gt;A C# implementation, supporting MSTest attributes in MSBuild solutions.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="what-does-testana-achieve-that-existing-tools-do-not"&gt;What does Testana achieve that existing tools do not?
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Testana &lt;em&gt;&lt;strong&gt;runs only the subset of test modules that actually need to run&lt;/strong&gt;&lt;/em&gt;, based on the last successful run time of each test module, and whether it, or any of its dependencies, have changed.&lt;/li&gt;
&lt;li&gt;Testana &lt;em&gt;&lt;strong&gt;always considers all test modules in your entire code base as candidates for running&lt;/strong&gt;&lt;/em&gt;, so you never have to manually select a subset of the tests to run in the interest of saving time.&lt;/li&gt;
&lt;li&gt;Testana &lt;em&gt;&lt;strong&gt;runs test modules by order of dependency&lt;/strong&gt;&lt;/em&gt;, meaning that tests of modules that have no dependencies run first, tests of modules that depend on those run next, and so on.&lt;/li&gt;
&lt;li&gt;Testana &lt;em&gt;&lt;strong&gt;runs test methods in Natural Method Order,&lt;/strong&gt;&lt;/em&gt; which is the order in which the methods appear in the source file. (This is the norm in C#, but not in Java, where extra measures are necessary to accomplish.)&lt;/li&gt;
&lt;li&gt;Testana &lt;em&gt;&lt;strong&gt;runs test methods in ascending order of inheritance&lt;/strong&gt;&lt;/em&gt;, meaning that test methods in the base-most test class run first, and test methods in derived test classes run afterwards.&lt;/li&gt;
&lt;li&gt;Testana &lt;em&gt;&lt;strong&gt;discovers and reports mistakes&lt;/strong&gt;&lt;/em&gt; in the formulation of test methods, instead of ignoring the mistakes, which is what most other test frameworks do. (Silent failure.)&lt;/li&gt;
&lt;li&gt;Testana &lt;em&gt;&lt;strong&gt;does not catch any exceptions when debugging&lt;/strong&gt;&lt;/em&gt;, thus allowing your debugger to stop on the source line that threw the exception. (Testana will catch and report exceptions when not debugging, as the case is when running on a continuous build server.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="how-does-testana-work"&gt;How does Testana work?
&lt;/h3&gt;&lt;p&gt;Testana begins by constructing the dependency graph of your software system. Since this process is expensive, Testana cashes the dependency graph in a file, and recalculates it only when the structure of the system changes. The cache is stored in a text file, which is located at the root of the source tree, and is meant to be excluded from source control.&lt;/p&gt;
&lt;p&gt;Then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Testana locates the modules that depend on nothing else within the system, and runs the tests of those modules.&lt;/li&gt;
&lt;li&gt;Once these tests are done, Testana finds modules that depend only on modules that have already been tested, and runs their tests.&lt;/li&gt;
&lt;li&gt;Testana keeps repeating the previous step, until all tests have been run.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Testana keeps a diary where it records the last successful run time of each test module. This diary is also stored in a text file, which is also located
at the root of the source tree, and is also meant to be excluded from source control.&lt;/p&gt;
&lt;p&gt;Next time Testana runs, it considers the last successful run time of each test module, versus the last modification time of that module and its dependencies. Testana then refrains from running the test module if neither it, nor any of its dependencies, have changed.&lt;/p&gt;
&lt;h3 id="why-should-i-care-about-running-only-the-tests-that-need-to-run"&gt;Why should I care about running only the tests that need to run?
&lt;/h3&gt;&lt;p&gt;The usual situation with large code bases is that tests take an unreasonably long time to run, so developers tend to take shortcuts in running them. One approach some developers take is that they simply commit code without running any tests, leaving it up to the continuous build server to run the tests and notify them of any test failures. This has multiple disadvantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It causes repeated interruptions in the workflow, due to the slow turnaround of the continuous build, which is often of the order of an hour, sometimes longer, and even in the fastest cases, always longer than a normal person's attention span. (This is so by definition; if it was not, then there would be no problem with quickly running all tests locally before committing.)&lt;/li&gt;
&lt;li&gt;The failed tests require additional commits to fix, and each commit requires a meaningful commit message, which increases the overall level of bureaucracy in the development process.&lt;/li&gt;
&lt;li&gt;The commit history becomes bloated with commits that were done in vain and should never be checked out because they contain bugs that are fixed in later commits.&lt;/li&gt;
&lt;li&gt;Untested commits that contain bugs are regularly being made to branches in the repository; these bugs stay there while the continuous build does its thing; eventually the tests fail, the developers take notice, and commit fixes. This whole process takes time, during which other unsuspecting developers might pull from those branches, thus receiving the bugs. Kind of like &lt;em&gt;Continuous Infection&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Testana solves the above problems by figuring out which tests need to run based on what has changed, and only running those tests. This cuts down the time it takes to run tests to a tiny fraction of what it is when blindly running all tests, which means that running the tests now becomes piece of cake and can usually be done real quick before committing, as it should.&lt;/p&gt;
&lt;p&gt;Also, running the tests real quick right after each pull from source control now becomes feasible, so a developer can avoid starting to work on source code on which the tests are failing. (How often have you found yourself in a situation where you pull from source control, change something, run the tests, the tests fail, and you are now wondering whether they fail due to the changes you just made, or due to changes you pulled from the repository?)&lt;/p&gt;
&lt;h3 id="why-should-i-care-about-considering-all-test-modules-in-my-entire-code-base-as-candidates-for-running"&gt;Why should I care about considering all test modules in my entire code base as candidates for running?
&lt;/h3&gt;&lt;p&gt;Another approach taken by some developers, in the interest of saving time, is manually choosing which tests to run, based on their knowledge of what may have been affected by the changes they just made.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One simple reason why this is problematic is that it requires cognitive effort to figure out which tests might need running, and manual work to launch them individually; it is not as easy as pressing a single button that stands for &amp;quot;run whatever tests need to run in response to the changes I just made.&amp;quot;&lt;/li&gt;
&lt;li&gt;A far bigger problem is that in manually selecting the tests to run, the developer is making assumptions about the dependencies of the code that they have modified. In complex systems, dependency graphs can be difficult to grasp, and as systems evolve, the dependencies keep changing. This often leads to situations where no single developer in the house has a complete grasp of the dependency graph of the entire system. Unfortunately, unknown or not-fully-understood dependencies are a major source of bugs, and yet by hand-selecting what to test based on our assumptions about the dependencies, it is precisely the not-fully-understood dependencies that are likely to not be tested. This is a recipe for disaster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Testana solves the above problems by always considering all test modules as candidates for running. It does not hurt to do that, because the tests that do not actually need to run will not be run by Testana anyway.&lt;/p&gt;
&lt;h3 id="why-should-i-care-about-running-test-modules-in-order-of-dependency"&gt;Why should I care about running test modules in order of dependency?
&lt;/h3&gt;&lt;p&gt;Existing test frameworks do not do anything intelligent in the direction of automatically figuring out some order of test execution that has any purpose or merit. The order tends to be arbitrary, and not configurable. In the best case it is alphabetic, but this is still problematic, because our criteria for naming test modules usually have nothing to do with the order in which we would like to see them executing.&lt;/p&gt;
&lt;p&gt;For example, it is very common for a code base to contain a module called &amp;quot;Utilities&amp;quot;, which most other modules depend on; Since it is a highly dependent-upon module, it should be tested first, but since its name begins with a &amp;quot;U&amp;quot;, it tends to be tested last.&lt;/p&gt;
&lt;p&gt;Testana executes test modules in order of dependency. This means that modules with no dependencies are tested first, modules that depend upon them are tested next, and so on until everything has been tested. Thus, the first test failure is guaranteed to point at the most fundamental problem; there is no need to look further down in case some other test failure indicates a more fundamental problem. Subsequently, Testana stops executing tests after the first failure, so it saves even more time.&lt;/p&gt;
&lt;p&gt;For more information about this way of testing, see &lt;a
href="https://blog2.michael.gr/post/2022-10-incremental-integration-testing/"
&gt;Incremental Integration Testing&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="why-should-i-care-about-running-test-methods-in-natural-order"&gt;Why should I care about running test methods in natural order?
&lt;/h3&gt;&lt;p&gt;Test frameworks in the C# world tend to run test methods in natural order, which is great, but in the Java world, the JUnit framework runs test methods in random order, which is at best useless, and arguably treacherous.&lt;/p&gt;
&lt;p&gt;One reason for wanting the test methods to run in the order in which they appear in the source file is because we usually test fundamental operations of our software before we test operations that depend upon them. (Note: it is the operations of the components under test that depend upon each other, not the tests themselves that depend upon each other!) So, if a fundamental operation fails, we want that to be the very first error that gets reported.&lt;/p&gt;
&lt;p&gt;Tests of operations that rely upon an operation whose test has failed might as well be skipped, because they can all be expected to fail. Reporting those failures before the failure of the more fundamental operation is an act of sabotage against the developer, because it is sending us looking for problems in places where there are no problems to be found, and it is making it more difficult for us to locate the real problem, which typically lies in the test that failed first &lt;em&gt;&lt;strong&gt;in the source file&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To give an example, suppose I am developing some kind of data store with insert and find functionality, and I am writing tests to make sure this functionality works. The find-item-in-store test necessarily involves insertion before finding, so I am likely to precede it with an insert-item-to-store test. In such a scenario, it is counter-productive to be told that my find-item-in-store test failed, sending me to troubleshoot the find function, and only later to be told that my insert-item-to-store test failed, which obviously means that it was in fact the insert function that needed troubleshooting; if insert-item-to-store fails, it is game over; no other operation on this store can possibly succeed, so there is no point in running any other tests on it, just as there is no point in beating a dead horse.&lt;/p&gt;
&lt;p&gt;Finally, another very simple, very straightforward, and very important reason for wanting the test methods to be executed in natural order is because seeing the test methods listed in any other order is &lt;em&gt;&lt;strong&gt;brainfuck&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A related rant can be found here: &lt;a
href="https://blog2.michael.gr/post/2018-04-random-order-of-tests/"
&gt;On JUnit's random order of test method execution&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="why-should-i-care-for-running-test-methods-in-ascending-order-of-inheritance"&gt;Why should I care for running test methods in ascending order of inheritance?
&lt;/h3&gt;&lt;p&gt;This feature of Testana might be irrelevant to you if you never use inheritance in test classes, but I do, and I consider it very important. I also consider the typical behavior of existing test frameworks on this matter very annoying, because they tend to do the exact opposite of what is useful.&lt;/p&gt;
&lt;p&gt;Inheritance in test classes can help to achieve great code coverage while reducing the total amount of test code. Suppose you have a collection hierarchy to test: you have an ArrayList class and a HashSet class, and you also have their corresponding test classes: ArrayListTest and HashSetTest. Now, both ArrayList and HashSet inherit from Collection, which means that lots of tests are going to be identical between ArrayListTest and HashSetTest. One way to eliminate duplication is to have a CollectionTest abstract base class, which tests only Collection methods, and then have both ArrayListTest and HashSetTest inherit from CollectionTest and provide additional tests for functionality that is specific to ArrayList and HashSet respectively. Under this scenario, when ArrayListTest or HashSetTest runs, we want the methods of CollectionTest to be executed first, because they are testing the fundamental (more general) functionality.&lt;/p&gt;
&lt;p&gt;To make the example more specific, CollectionTest is likely to add an item to the collection and then check whether the collection contains the item. If this test fails, there is absolutely no point in proceeding with tests of ArrayListTest which will, for example, add multiple items to the collection and check to make sure that IndexOf() returns the right results.&lt;/p&gt;
&lt;p&gt;Again, existing test frameworks tend to handle this in a way which is exactly the opposite of what we would want: they execute the descendant (more specialized) methods first, and the ancestor (more general) methods last.&lt;/p&gt;
&lt;p&gt;Testana corrects this by executing ancestor methods first, descendant methods last.&lt;/p&gt;
&lt;h3 id="what-additional-error-checking-does-testana-perform"&gt;What additional error checking does Testana perform?
&lt;/h3&gt;&lt;p&gt;While running tests, Testana will warn the programmer if it discovers any method that has been declared as a test method but fails to meet the requirements for a test method.&lt;/p&gt;
&lt;p&gt;Usually, test frameworks require that a test method must be a public instance method, must accept no parameters, and must return nothing; however, when these frameworks encounter a method that is declared as a test and yet fails to meet those requirements, (for example, a test method declared static,) they fail to report the mistake.&lt;/p&gt;
&lt;p&gt;Testana does not fail to report such mistakes.&lt;/p&gt;
&lt;h3 id="can-testana-be-fooled-by-inversion-of-control"&gt;Can Testana be fooled by Inversion of Control?
&lt;/h3&gt;&lt;p&gt;No. In a scenario where class A receives and invokes interface I without having a dependency on class B which implements I, the test of A still has to instantiate both A and B in order to supply A with the I interface of B, so the test depends on both A and B, which means that Testana will run the test if there is a change in either A or B.&lt;/p&gt;
&lt;h3 id="can-testana-be-fooled-by-the-use-of-mocks"&gt;Can Testana be fooled by the use of mocks?
&lt;/h3&gt;&lt;p&gt;Yes, Testana can be fooled by mocks, because that is what mocks do: they make a mockery out of the software testing process. In a scenario where class A receives and invokes interface I without having a dependency on class B which implements I, and the test of A also refrains from depending on B by just mocking I, Testana will of course not run the test of A when there is a change in B. This, however, should not be a problem, because you should not be using mocks anyway; for more information, see &lt;a
href="https://blog2.michael.gr/post/2023-01-14-mocking/"
&gt;If you are using mock objects you are doing it wrong&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="can-testana-be-fooled-by-the-use-of-fakes"&gt;Can Testana be fooled by the use of fakes?
&lt;/h3&gt;&lt;p&gt;No, as long as you do your testing properly. A test that utilizes a fake will be run by Testana only when there is a change in the fake, not when there is a change in the real thing; however, you should have a separate test which ensures that the behavior of the fake is identical to the behavior of the real thing in all aspects that matter. This test will be run by Testana when you modify either the fake, or the real thing, or both. Thus:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you make a breaking change to the real thing, then your tests will show you that you need to make the corresponding change to the fake; the change in the fake will in turn cause Testana to run the tests that utilize the fake.&lt;/li&gt;
&lt;li&gt;If you make a non-breaking change to the real thing, then the fake will remain unchanged, and this is what gives you the luxury of not having to re-run tests utilizing the fake when you make a change that only affects the real thing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information, see &lt;a
href="https://blog2.michael.gr/post/2022-10-testing-with-fakes/"
&gt;Testing with Fakes instead of Mocks&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="what-about-undiscoverable-dependencies-due-to-weak-typing-the-use-of-rest-etc"&gt;What about undiscoverable dependencies due to weak typing, the use of REST, etc?
&lt;/h3&gt;&lt;p&gt;The following &amp;quot;hip&amp;quot; and &amp;quot;trendy&amp;quot; practices of the modern day are not supported by Testana, and there is no plan to ever support them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Squandering dependencies via &lt;em&gt;&lt;strong&gt;weak typing&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Obscuring dependencies via &lt;em&gt;&lt;strong&gt;duck-typing&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Denaturing dependencies via &lt;em&gt;&lt;strong&gt;stringly-typing.&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Disavowing dependencies via &lt;em&gt;&lt;strong&gt;configuration files&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Abnegating dependencies via &lt;em&gt;&lt;strong&gt;non-programmatic interfaces&lt;/strong&gt;&lt;/em&gt; such as REST.&lt;/li&gt;
&lt;li&gt;Fragmenting dependencies via &lt;em&gt;&lt;strong&gt;cross-language invocations&lt;/strong&gt;&lt;/em&gt; (following the &lt;em&gt;&lt;strong&gt;polyglot craze&lt;/strong&gt;&lt;/em&gt;.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Seriously, stop all this fuckery and use a single, &lt;em&gt;&lt;strong&gt;real&lt;/strong&gt;&lt;/em&gt; programming language, (that is, a programming language with &lt;em&gt;&lt;strong&gt;strong typing&lt;/strong&gt;&lt;/em&gt;,) encode your dependencies via the type system, and everything will be fine. For more information, see &lt;a
href="https://blog2.michael.gr/post/2017-05-on-scripting-languages/"
&gt;On Scripting Languages&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="how-compatible-is-testana-with-what-i-already-have"&gt;How compatible is Testana with what I already have?
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;The Java implementation of Testana:
&lt;ul&gt;
&lt;li&gt;Works with maven projects (pom.xml files.)&lt;/li&gt;
&lt;li&gt;Supports JUnit 4.
&lt;ul&gt;
&lt;li&gt;Supports only the basic, minimum viable subset of JUnit 4 functionality, namely the @Test, @Before, @After, and @Ignore annotations, without any parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The C# implementation of Testana:
&lt;ul&gt;
&lt;li&gt;Works with MSBuild projects (.sln and .csproj files)&lt;/li&gt;
&lt;li&gt;Supports MSTest.
&lt;ul&gt;
&lt;li&gt;Supports only the basic, minimum viable subset of MSTest functionality, namely the [TestClass], [TestMethod], [ClassInitialize], [ClassCleanup], and [Ignore] attributes, without any parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Support for more languages, more project formats, more test frameworks, and more functionality may be added in the future.&lt;/p&gt;
&lt;h3 id="how-is-it-like-using-testana"&gt;How is it like using Testana?
&lt;/h3&gt;&lt;p&gt;You run Testana every time you want to run your tests. You launch it at the root of your source tree, without any command-line arguments, and its default behavior is to figure out everything by itself and do the right thing.&lt;/p&gt;
&lt;p&gt;Note that the first time you run Testana, there may be a noticeable delay while information is being collected; the information is cached, so this delay will not be there next time you run Testana.&lt;/p&gt;
&lt;p&gt;The first time you run Testana, it will run all tests.&lt;/p&gt;
&lt;p&gt;If you immediately re-run Testana, it will not run any tests, because nothing will have changed.&lt;/p&gt;
&lt;p&gt;If you touch one of your source files, build your project, and re-run Testana, it will only run tests that either directly or indirectly depend on the
changed file. If you run Testana with --help it will give you a rundown of the command-line arguments it supports.&lt;/p&gt;
&lt;h3 id="where-can-i-find-testana"&gt;Where can I find Testana?
&lt;/h3&gt;&lt;p&gt;The Java implementation of Testana is here:
&lt;a class="external"
href="https://github.com/mikenakis/Public/tree/master/testana" target="_blank"
&gt;https://github.com/mikenakis/Public/tree/master/testana&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The C# implementation of Testana is coming soon. (As soon as I turn it into an independent solution, because currently it is a project within a larger solution.)&lt;/p&gt;
&lt;h3 id="notes"&gt;Notes
&lt;/h3&gt;&lt;p&gt;In episode 167 of the Software Engineering Podcast (&lt;a class="external"
href="https://se-radio.net/2010/09/episode-167-the-history-of-junit-and-the-future-of-testing-with-kent-beck/" target="_blank"
&gt;SE Radio 167: The History of JUnit and the Future of Testing with Kent Beck&lt;/a&gt;) at about 40':00'' Kent Beck says that recently failed tests have the highest probability of failing again in the near future, so he suggests using this statistical fact at as a heuristic for picking which tests to run first. Testana optimizes the testing process deterministically, so there is no need to resort to heuristics.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Cover image: The Testana logo, &lt;em&gt;profile of a crash test dummy&lt;/em&gt; by michael.gr. Based on &lt;a class="external"
href="https://thenounproject.com/term/crash-test-dummy/401583/" target="_blank"
&gt;original work by Wes Breazell&lt;/a&gt; and &lt;a class="external"
href="https://thenounproject.com/term/woman/129498/" target="_blank"
&gt;Alexander Skowalsky&lt;/a&gt;. Used under &lt;a class="external"
href="https://creativecommons.org/licenses/by/3.0/us/" target="_blank"
&gt;CC BY License.&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Artificial Code Coverage</title><link>https://blog2.michael.gr/post/2024-03-codecoverage/</link><pubDate>Tue, 26 Mar 2024 15:01:55 +0000</pubDate><guid>https://blog2.michael.gr/post/2024-03-codecoverage/</guid><description>&lt;p&gt;&lt;img src="https://blog2.michael.gr/post/2024-03-codecoverage/images/patrick-robert-doyle-UrHNIeIjoE4-unsplash.jpg"
width="3258"
height="1890"
srcset="https://blog2.michael.gr/post/2024-03-codecoverage/images/patrick-robert-doyle-UrHNIeIjoE4-unsplash_hu_d19487727715a135.jpg 480w, https://blog2.michael.gr/post/2024-03-codecoverage/images/patrick-robert-doyle-UrHNIeIjoE4-unsplash_hu_15abe3b3c0c4821e.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
&gt;
&lt;/p&gt;
&lt;h3 id="abstract"&gt;Abstract
&lt;/h3&gt;&lt;p&gt;In this paper I put forth the proposition that contrary to popular belief, 100% code coverage can be a very advantageous thing to have, and I discuss a technique for achieving it without excessive effort.&lt;/p&gt;
&lt;p&gt;(Useful pre-reading: &lt;a
href="https://blog2.michael.gr/post/2022-11-about-these-papers/"
&gt;About these papers&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id="the-problem"&gt;The problem
&lt;/h3&gt;&lt;p&gt;Conventional wisdom says that 100% code coverage is unnecessary, or even undesirable, because achieving it requires an exceedingly large amount of effort &lt;em&gt;&lt;strong&gt;not&lt;/strong&gt;&lt;/em&gt; for the purpose of asserting correctness, but instead for the sole purpose of achieving coverage. In other words, it is often said that 100% code coverage has no business value.&lt;/p&gt;
&lt;p&gt;Let me tell you why this is wrong, and why 100% code coverage can indeed be a very good thing to have.&lt;/p&gt;
&lt;p&gt;If you don't have 100% code coverage, then by definition, you have some lower percentage, like 87.2%, or 94.5%. The remaining 12.8%, or 5.5% is uncovered. I call this &lt;em&gt;&lt;strong&gt;the worrisome percentage.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As you keep working on your code base, the worrisome percentage fluctuates:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;one day you might add a test for some code that was previously uncovered, so the worrisome percentage decreases;&lt;/li&gt;
&lt;li&gt;another day you may add some code with no tests, so the percentage increases;&lt;/li&gt;
&lt;li&gt;yet another day you may add some more code along with tests, so even though the number of uncovered lines has not changed, it now represents a smaller percentage;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;... and it goes on like that.&lt;/p&gt;
&lt;p&gt;If the worrisome percentage is high, then you know for sure that you are doing a bad job, but if it is low, it does not mean that you are doing a good job, because some very important functionality may be left uncovered, and you just do not know. To make matters worse, modern programming languages offer constructs that achieve great terseness of code, meaning that a few uncovered lines may represent a considerable amount of uncovered functionality.&lt;/p&gt;
&lt;p&gt;So, each time you look at the worrisome percentage, you have to wonder what is in there: are all the important lines covered? are the uncovered lines okay to be left uncovered?&lt;/p&gt;
&lt;p&gt;In order to answer this question, you have to go over every single line of code in the worrisome percentage, and examine it to determine whether it is okay that it is being left uncovered. What you find is, more often than not, the usual suspects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some &lt;code&gt;ToString()&lt;/code&gt; function which is only used for diagnostics;&lt;/li&gt;
&lt;li&gt;Some &lt;code&gt;Equals()&lt;/code&gt; and &lt;code&gt;HashCode()&lt;/code&gt; functions of some value type which does not currently happen to be used as a key in a hash-map;&lt;/li&gt;
&lt;li&gt;Some &lt;code&gt;default&lt;/code&gt; &lt;code&gt;switch&lt;/code&gt; clause which can never be reached, and if it was to ever be reached it would throw;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;... etc.&lt;/p&gt;
&lt;p&gt;So, your curiosity is satisfied, your worries are allayed, and you go back to your usual software development tasks.&lt;/p&gt;
&lt;p&gt;A couple of weeks later, the worrisome percentage has changed again, prompting the same question: what is being left uncovered now?&lt;/p&gt;
&lt;p&gt;Each time you need to have this question answered, you have to re-examine every single line of code in the worrisome percentage. As you do this, you discover that in the vast majority of cases, the lines that you are examining now are the exact same lines that you were examining the previous time you were going through this exercise. After a while, this starts getting tedious. Eventually, you quit looking. Sooner or later, everyone in the shop quits looking.&lt;/p&gt;
&lt;p&gt;The worrisome percentage has now become &lt;em&gt;&lt;strong&gt;terra incognita&lt;/strong&gt;&lt;/em&gt;: literally anything could be in there; nobody knows, and nobody wants to know, because finding out is such a dreary chore.&lt;/p&gt;
&lt;p&gt;That is not a particularly nice situation to be in.&lt;/p&gt;
&lt;h3 id="the-solution"&gt;The solution
&lt;/h3&gt;&lt;p&gt;So, here is a radical proposition: If you always keep your code coverage at 100%, then the worrisome percentage is always zero, so there is nothing to worry about!&lt;/p&gt;
&lt;p&gt;When the worrisome percentage is never zero, then no matter how it fluctuates, it never represents an appreciable change in the situation: it always goes from some non-zero number to some other non-zero number, meaning that we used to have some code uncovered, and we still have some code uncovered. No matter what happens, there is no actionable item.&lt;/p&gt;
&lt;p&gt;On the other hand, if the worrisome percentage is normally zero, then each time it rises above zero it represents a definite change in the situation: you used to have everything covered, and now you have something uncovered. This signifies a clear call to action: the code that is now being left uncovered needs to be examined, and dealt with.&lt;/p&gt;
&lt;p&gt;By dealing with uncovered code as soon as it gets introduced, you bring the worrisome percentage back to zero, thus achieving two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You ensure that next time the worrisome percentage becomes non-zero, it will represent a new call to action.&lt;/li&gt;
&lt;li&gt;You never find yourself in the unpleasant situation of re-examining code that has been examined before; so, the examination does not feel like a dreary chore.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The conventional understanding of how to deal with uncovered code is to write a test for it, and that is why achieving 100% code coverage is regarded as onerous; however, there exist alternatives that are much easier. For any given piece of uncovered code, you have three options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Option #1: Write a test for the code.&lt;/p&gt;
&lt;p&gt;This is of course the highest quality option, but it does not always represent the best value for money, and it is not even always possible. You only need to do it if the code is important enough to warrant testing, and you can only do it if the code is in fact testable. If you write a test, you can still minimize the effort of doing so, by utilizing certain techniques that I talk about in other posts, such as &lt;a
href="https://blog2.michael.gr/post/2024-04-audit-testing/"
&gt;Audit Testing&lt;/a&gt;, &lt;a
href="https://blog2.michael.gr/post/2022-10-testing-with-fakes/"
&gt;Testing with Fakes instead of Mocks&lt;/a&gt;, and &lt;a
href="https://blog2.michael.gr/post/2022-10-incremental-integration-testing/"
&gt;Incremental Integration Testing&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Option #2: Exclude the code from code coverage.&lt;/p&gt;
&lt;p&gt;Code that is not testable, or not important enough to warrant testing, can be moved into a separate module which does not participate in coverage analysis. Alternatively, if your code coverage analysis tool supports it, you may be able to exclude individual methods without having to move them to another module. In the DotNet world, this can be accomplished by marking a method with &lt;a class="external"
href="https://learn.microsoft.com/en-us/dotnet/api/system.diagnostics.codeanalysis.excludefromcodecoverageattribute" target="_blank"
&gt;the &lt;code&gt;ExcludeFromCodeCoverage&lt;/code&gt; attribute&lt;/a&gt;, found in the &lt;code&gt;System.Diagnostics.CodeAnalysis&lt;/code&gt; namespace. In the Java world, IntelliJ IDEA offers a setting for specifying what annotation we want to use for marking methods to be excluded from code coverage, so you can use any annotation you like. (See &lt;a
href="https://blog2.michael.gr/post/2022-12-intellij-idea-can-now-exclude-methods/"
&gt;IntelliJ IDEA can now exclude methods from code coverage&lt;/a&gt;.) Various different code coverage analyzers support additional ways of excluding code from coverage.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Option #3: Artificially cover the code.&lt;/p&gt;
&lt;p&gt;With the previous two options you should be able to bring the worrisome percentage down to a very small number, like 1 or 2 percent. What remains is code which should really be excluded from coverage, but it cannot, due to limitations in available tooling: although code coverage analyzers generally allow excluding entire functions from coverage analysis, they generally do not offer any means of excluding individual lines of code, such as the unreachable &lt;code&gt;default&lt;/code&gt; clause of some &lt;code&gt;switch&lt;/code&gt; statement. You can try moving that line into a separate function, and excluding that function, but you cannot exclude the call to that function, so the problem remains.&lt;/p&gt;
&lt;p&gt;The solution in these cases is to cause the uncovered code to be invoked during testing, &lt;em&gt;&lt;strong&gt;not&lt;/strong&gt;&lt;/em&gt; in order to test it, but simply in order to have it covered. This might sound like cheating, but it is not, because the stated objective was not to test the code, it was to exclude it from coverage. You would have excluded that line from coverage if the tooling supported doing so, but since it does not, the next best thing, (and the only option you are left with,) is to artificially include it in the code coverage.&lt;/p&gt;
&lt;p&gt;Here is a (hopefully exhaustive) list of all the different reasons due to which code might be left uncovered, and what to do in each case:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The code should really be covered, but you forgot to write tests for it, or you have plans to write tests in the future.&lt;/p&gt;
&lt;p&gt;Go with Option #1: write tests for it. Not in the future, &lt;em&gt;&lt;strong&gt;now&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The code is not used and there is no plan to use it.&lt;/p&gt;
&lt;p&gt;This is presumably code which exists for historical reasons, or for reference, or because it took some effort to write it and you do not want to admit that the effort was a waste by throwing away the code.&lt;/p&gt;
&lt;p&gt;Go with Option #2 and exclude it from coverage.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The code is only used for diagnostics.&lt;/p&gt;
&lt;p&gt;The prime example of this is &lt;code&gt;ToString()&lt;/code&gt; methods that are not normally invoked in a production environment, but give informative
descriptions of our objects while debugging.&lt;/p&gt;
&lt;p&gt;Go with Option #2: Exclude such methods from coverage.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The code is not normally reachable, but it is there in case something unexpected happens.&lt;/p&gt;
&lt;p&gt;The prime example of this is C# &lt;code&gt;switch&lt;/code&gt; statements that cover all possible cases and yet also contain a &lt;code&gt;default&lt;/code&gt; clause just in case an unexpected value somehow manages to creep in.&lt;/p&gt;
&lt;p&gt;Go with Option #3: Artificially cover such code. This may require a bit of refactoring to make it easier to cause the problematic &lt;code&gt;switch&lt;/code&gt; statement to be invoked with an invalid value. The code most likely throws, so catch the exception and swallow it. You can also assert that the expected exception was thrown, in which case it becomes more like Option #1: a test.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The code is reachable but not currently being reached.&lt;/p&gt;
&lt;p&gt;This is code which is necessary for completeness, and it just so happens that it is not currently being used, but nothing prevents it from being used at any moment. A prime example of this is the &lt;code&gt;Equals()&lt;/code&gt; and &lt;code&gt;HashCode()&lt;/code&gt; functions of value types: without those functions, a value type is incomplete; however, if the value type does not currently happen to be used as a key in a hash-map, then those functions are almost certainly unused.&lt;/p&gt;
&lt;p&gt;In this case, you can go with any of the three options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can go with Option #1 and write a proper test.&lt;/li&gt;
&lt;li&gt;You can go with Option #2 and exclude the code.&lt;/li&gt;
&lt;li&gt;You can go with Option #3 and artificially cover the code.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The code is not important enough to have a test for it.&lt;/p&gt;
&lt;p&gt;Say you have a function which takes a tree data structure and converts it to text using &lt;a class="external"
href="https://en.wikipedia.org/wiki/Box-drawing_character" target="_blank"
&gt;&lt;em&gt;box-drawing characters&lt;/em&gt;&lt;/a&gt; so as to be able to print it nicely as a tree on the console. Since the function receives text and emits text, it is certainly testable, but is it really worth testing? If it ever draws something wrongly, you will probably notice, and if you do not notice, then maybe it did not matter anyway.&lt;/p&gt;
&lt;p&gt;In this case you can go either with Option #2 and exclude such functions, or Option #3 and artificially cover them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The code is literally or practically untestable.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If your application has a Graphical User Interface (GUI), you can write automated tests for all of your application logic, but the only practical way to ascertain the correctness of the GUI is to have human eyes staring at the screen. (There exist tools for testing GUIs, but I assess them as &lt;em&gt;woefully impractical and acutely ineffective&lt;/em&gt;.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If your application controls some hardware, you may have a hardware abstraction layer with two implementations, one which emulates the hardware, and one which interacts with the actual hardware. The emulator will enable you to test all of your application logic without having the actual hardware in place; however, the implementation which interacts with the actual hardware is practically untestable by software alone.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you have a piece of code that queries the endianness of the hardware architecture and operates slightly differently depending on it, the only path you can truly cover is the one for the endianness of the hardware architecture you are actually using. (You can fake the endianness query, and pretend that your hardware has the opposite endianness, but you still have no guarantees as to whether the bit-juggling that you do in that path is right for the opposite endianness.)&lt;/p&gt;
&lt;p&gt;In all of the above cases, and in all similar cases, we have no option but #2: exclude the code from coverage.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id="conclusion"&gt;Conclusion
&lt;/h4&gt;&lt;p&gt;If testing has business value, then 100% code coverage has business value, too.&lt;/p&gt;
&lt;p&gt;A code coverage percentage of 100% is very useful, not for bragging, but for maintaining certainty that everything that ought to be tested is in fact being tested.&lt;/p&gt;
&lt;p&gt;Achieving a code coverage percentage of 100% does require some effort, but with techniques such as Artificial Coverage the effort can be reduced to manageable levels.&lt;/p&gt;
&lt;p&gt;Ideally, Artificial Coverage should never be necessary, but it is a practical workaround for the inability of coverage tools to exclude individual lines of code from analysis.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Cover image by &lt;a class="external"
href="https://unsplash.com/@teapowered" target="_blank"
&gt;Patrick Robert Doyle from Unsplash&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Audit Testing</title><link>https://blog2.michael.gr/post/2024-04-audit-testing/</link><pubDate>Fri, 09 Feb 2024 14:55:40 +0000</pubDate><guid>https://blog2.michael.gr/post/2024-04-audit-testing/</guid><description>&lt;p&gt;&lt;img src="https://blog2.michael.gr/post/2024-04-audit-testing/images/audit-testing.svg"
loading="lazy"
&gt;
&lt;/p&gt;
&lt;h4 id="abstract"&gt;Abstract
&lt;/h4&gt;&lt;p&gt;An automated software testing technique is presented which spares us from having to stipulate our expectations in test code, and from having to go fixing test code each time our expectations change.&lt;/p&gt;
&lt;p&gt;(Useful pre-reading: &lt;a
href="https://blog2.michael.gr/post/2022-11-about-these-papers/"
&gt;About these papers&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id="the-problem"&gt;The Problem
&lt;/h3&gt;&lt;p&gt;The most common scenario in automated software testing is ensuring that given specific input, a component-under-test produces expected output. The conventional way of achieving this is by feeding the component-under-test with a set of predetermined parameters, obtaining the output of the component-under-test, comparing the output against an instance of known-good output which has been hard-coded within the test, and failing the test if the two are not equal.&lt;/p&gt;
&lt;p&gt;This approach works, but it is inefficient, because during the development and evolution of a software system we often make changes to the production code fully anticipating the output of certain components to change. Unfortunately, each time we do this, the tests fail, because they are still expecting the old output. So, each change in the production code must be followed by a round of fixing tests to make them pass.&lt;/p&gt;
&lt;p&gt;Note that under Test-Driven Development things are not any better: first we modify the tests to start expecting the new output, then we observe them fail, then we modify the components to produce the new output, then we watch the tests pass. We still have to stipulate our expectations in test code, and we still have to change test code each time our expectations change, which is inefficient.&lt;/p&gt;
&lt;p&gt;This imposes a considerable burden on the software development process. As a matter of fact, it often happens that programmers refrain from making needed changes to their software because they dread the prospect of having to fix all the tests that will break as a result of those changes.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Audit Testing&lt;/strong&gt;&lt;/em&gt; is a technique for automated software testing which aims to correct all this.&lt;/p&gt;
&lt;h3 id="the-solution"&gt;The Solution
&lt;/h3&gt;&lt;p&gt;Under Audit Testing, the assertions that verify the correctness of the output of the component-under-test are abolished, and replaced with code that simply saves the output to a text file. This file is known as the &lt;em&gt;&lt;strong&gt;Audit File&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The test may still fail if the component-under-test encounters an error while producing output, in which case we follow a conventional test-fix-repeat workflow, but if the component-under-test manages to produce output, then the output is saved in the Audit File and the test completes successfully &lt;em&gt;&lt;strong&gt;without examining it.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The trick is that the Audit File is saved right next to the source code file of the test, which means that it is kept under Version Control. In the most common case, each test run produces the exact same audit output as the previous run, so nothing changes, meaning that all is good. If a test run produces different audit output from a previous test run, then the tooling alerts the developer to that effect, and the Version Control System additionally indicates that the Audit File has been modified and is in need of committing. Thus, the developer cannot fail to notice that the audit output has changed.&lt;/p&gt;
&lt;p&gt;The developer can then utilize the &amp;quot;Compare with unmodified&amp;quot; feature of the Version Control System to see the differences between the audit output that was produced by the modified code, and the audit output of the last known-good test run. By visually inspecting these differences, the developer can decide whether they are as expected or not, according to the changes they made in the code.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the observed differences are not as expected, then the developer needs to keep working on their code until they are.&lt;/li&gt;
&lt;li&gt;If the observed differences are as expected, then the developer can simply commit the new code, along with the new Audit File, &lt;em&gt;&lt;strong&gt;and they are done.&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This way, we eliminate the following burdens:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Having to hard-code into the tests the output expected from the component-under-test.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Having to assert, in each test, that the output of the component-under-test matches the expected output.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Having to go fixing test code each time there is a (fully expected) change in the output of the component-under-test.&lt;/p&gt;
&lt;p&gt;The eliminated burdens are traded for the following much simpler responsibilities:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The output of the component-under-test must be converted to text and written to an audit file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When the version control system shows that an audit file changed after a test run, the differences must be reviewed, and a decision must be made as to whether they are as expected or not.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tests and production code must be written with some noise reduction concerns in mind. (More on that further down.)&lt;/p&gt;
&lt;p&gt;This represents a considerable optimization of the software development process.&lt;/p&gt;
&lt;p&gt;Note that the arrangement is also convenient for the reviewer, who can see both the changes in the code and the resulting changes in the Audit Files.&lt;/p&gt;
&lt;p&gt;As an added safety measure, the continuous build pipeline can deliberately fail the tests if an unclean working copy is detected after running the tests, because that would mean that the tests produced different results from what was expected, or that someone failed to commit some updated audit file.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="noise-reduction"&gt;Noise reduction
&lt;/h3&gt;&lt;p&gt;For Audit Testing to work effectively, all audit output must be completely free of noise. By noise we mean:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two test runs of the exact same code producing different audit output.&lt;/li&gt;
&lt;li&gt;A single change in the code producing wildly different audit output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, if a test emits the username of the current user into the audit output, then the audit file generated by that test will be different for every user that runs it, even if the user does not modify any code.&lt;/p&gt;
&lt;p&gt;Noise is undesirable, because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Needlessly modified audit files are a false cause of alarm.&lt;/li&gt;
&lt;li&gt;Examining changes in audit files only to discover that they are due to noise is a waste of time.&lt;/li&gt;
&lt;li&gt;A change that might be important to notice can be lost in the noise.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Noise in audit files is most commonly caused by various sources of non-determinism, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wall-clock time.&lt;/p&gt;
&lt;p&gt;As the saying goes, the arrow of time is always moving forward. This means that the &amp;quot;current&amp;quot; time coordinate is always different from test run to test run, and this in turn means that if any wall-clock timestamps find their way into the audit output, the resulting audit file will always be different from the previous run. So, for example, if your software generates a log, and you were thinking of using the log as your audit output, then you will have to either remove the timestamps from the log, or fake them. Faking the clock for the purpose of testing is a well-known best practice anyway, regardless of audit testing. To accomplish this, create a &amp;quot;Clock&amp;quot; interface, and propagate it to every place in your software that needs to know the current time. Create two implementations of that interface: one for production, which queries the actual wall-clock time from the operating environment, and one for testing, which starts from some fixed, known origin and increments by a fixed amount each time it is queried.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Random number generation.&lt;/p&gt;
&lt;p&gt;Random number generators are usually pseudo-random, and we tend to make them practically random by seeding them with the wall-clock time. This can be easily fixed for the purpose of testing by seeding them with a known fixed value instead. Some pseudo-random generators seed themselves with the wall-clock time without allowing us to override this behavior; this is deplorable. Such generators must be faked in their entirety for the purpose of testing. This extends to any other constructs that employ random number generation, such as GUIDs/UUIDs: they must also be faked when testing, using deterministic generators.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multi-threading.&lt;/p&gt;
&lt;p&gt;Multiple threads running in parallel tend to exhibit unpredictable timing irregularities, and result in a chaotically changing order of events. If these threads affect audit output, then the ordering of the content of the audit file will be changing on every test run. For this reason, multi-threading must either be completely avoided when testing, or additional mechanisms (queuing, sorting, etc.) must be employed to guarantee a consistent ordering of audit output.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Floating-point number imprecision.&lt;/p&gt;
&lt;p&gt;Floating-point calculations can produce slightly different results depending on whether optimizations are enabled or not. To ensure that the audit file is unaffected, any floating point values emitted to the audit file must be rounded to as few digits as necessary. At the very least, they must be rounded to one digit less than their full precision.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Other external factors.&lt;/p&gt;
&lt;p&gt;User names, computer names, file creation times, IP addresses resolved from DNS, etc must either be prevented from finding their way into the audit output, or they must be faked when running tests. Fake your file-system; fake The Internet if necessary. For more information about faking stuff, see &lt;a
href="https://blog2.michael.gr/post/2022-10-testing-with-fakes/"
&gt;Testing with Fakes instead of Mocks&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In short, anything that would cause &lt;a class="external"
href="https://ell.stackexchange.com/a/299395/129530" target="_blank"
&gt;flakiness&lt;/a&gt; in software tests will cause noisiness in Audit Testing.&lt;/p&gt;
&lt;p&gt;Additionally, the content of audit files can be affected by some constructs that are fully deterministic in their nature. These constructs will never result in changed audit files without any changes in the code, but may produce drastically different audit files as a result of only minute changes in the code. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Hash Table Rehashing.&lt;/p&gt;
&lt;p&gt;A hash table may decide to re-hash itself as a result of a single key addition, if that addition happens to cause some internal load factor
threshold to be exceeded. Exactly when and how this happens depends on the implementation of the hash table and we usually have no control over it. After re-hashing, the order in which the hash table enumerates its keys is drastically different, and if the keys are emitted to audit output, then the audit file will be drastically different. To avoid this, replace plain hash tables with hash tables that retain the order of key insertion.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Insufficient Sorting Keys.&lt;/p&gt;
&lt;p&gt;When sorting data, the order of items with identical keys is undefined. It is still deterministic, but the addition or removal of a single item can cause all items with the same sorting key to be arbitrarily rearranged. To avoid this, always use a full set of sorting keys when sorting data, so as to give every item a specific unique order. Introduce additional sorting keys if necessary, even if you would not normally have a use for them.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Noise reduction aims to ensure that we will never see changes in the audit files unless there have been changes in the code, and that for every unique change in the code we will see a specific expected set of changes in the audit output, instead of a large number of irrelevant changes. This ensures that the single change that matters will not be lost in the noise, and makes it easier to determine that the modifications we made to the code have exactly the intended consequences and not any unintended consequences.&lt;/p&gt;
&lt;p&gt;Note that in some cases, noise reduction can be implemented in the tests rather than in the production code. For example, instead of replacing a plain hash table with an ordered hash table in production code, our test can obtain the contents of the plain hash table and sort them before writing them to the audit file. However, this may not be possible in cases where the hash table is several transformations away from the auditing. Thus, replacing a plain hash table with an ordered hash table may sometimes be necessary in production code.&lt;/p&gt;
&lt;p&gt;Noise reduction in production code can be either always enabled, or only enabled during testing. The most performant choice is to only have it enabled during testing, but the safest choice is to have it always enabled.&lt;/p&gt;
&lt;h3 id="failure-testing"&gt;Failure Testing
&lt;/h3&gt;&lt;p&gt;Failure Testing is the practice of deliberately supplying the component-under-test with invalid input and ensuring that the component-under-test detects the error and throws an appropriate exception. Such scenarios can leverage Audit Testing by simply catching exceptions and serializing them, as text, into the audit output.&lt;/p&gt;
&lt;h3 id="applicability"&gt;Applicability
&lt;/h3&gt;&lt;p&gt;Audit Testing is most readily useful when the Component Under Test produces results as text, or results that are directly translatable to text. With a bit of effort, any kind of output can be converted to text, so Audit Testing is universally applicable.&lt;/p&gt;
&lt;h3 id="must-audit-files-be-committed"&gt;Must Audit Files be committed?
&lt;/h3&gt;&lt;p&gt;It is in theory possible to refrain from storing Audit Files in the source code repository, but doing so would have the following disadvantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It would deprive the code reviewer from the convenience of being able to see not only the changes in the code, but also the differences that these changes have introduced in the audit output of the test.&lt;/li&gt;
&lt;li&gt;It would require the developer to always remember to immediately run the tests each time they pull from the source code repository, so as to have the unmodified Audit Files produced locally, before proceeding to make modifications to the code which would further modify the Audit Files.&lt;/li&gt;
&lt;li&gt;It would make it more difficult for the developer to take notice when the Audit Files change.&lt;/li&gt;
&lt;li&gt;It would make it more difficult for the developer to see diffs between the modified Audit Files and the unmodified ones.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course all of this could be taken care of with some extra tooling. What remains to be seen is whether the effort of developing such tooling can be justified by the mere benefit of not having to store Audit Files in the source code repository.&lt;/p&gt;
&lt;h4 id="conclusion"&gt;Conclusion
&lt;/h4&gt;&lt;p&gt;Audit Testing is a universally applicable technique for automated software testing which can significantly reduce the effort of writing and maintaining tests by sparing us from having to stipulate our expectations in test code, and from having to go fixing test code each time our expectations change.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Cover image: &amp;quot;Audit Testing&amp;quot; by michael.gr.&lt;/p&gt;</description></item><item><title>If you are using mock objects you are doing it wrong</title><link>https://blog2.michael.gr/post/2023-01-14-mocking/</link><pubDate>Sat, 14 Jan 2023 14:13:37 +0000</pubDate><guid>https://blog2.michael.gr/post/2023-01-14-mocking/</guid><description>&lt;p&gt;&lt;img src="https://blog2.michael.gr/post/2023-01-14-mocking/images/mocking.svg"
loading="lazy"
&gt;
&lt;/p&gt;
&lt;h4 id="abstract"&gt;Abstract:
&lt;/h4&gt;&lt;p&gt;The practice of using Mock Objects in automated software testing is examined from a critical point of view and found to be highly problematic. Opinions of some well known industry speakers are cited. The supposed benefits of Mock Objects are shown to be either no real benefits, or achievable via alternative means.&lt;/p&gt;
&lt;p&gt;(Useful pre-reading: &lt;a
href="https://blog2.michael.gr/post/2022-11-about-these-papers/"
&gt;About these papers&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id="introduction"&gt;Introduction
&lt;/h3&gt;&lt;p&gt;The automated software testing technique which is predominant in the industry today is Unit Testing. The goal of Unit Testing is to achieve defect localization, and to this effect it requires each component to be tested in strict isolation from its collaborators.&lt;/p&gt;
&lt;p&gt;Testing components in isolation from each other poses certain challenges:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;While being tested, the component-under-test makes invocations to collaborator interfaces; since the collaborator components are not present, some kind of substitute must be there to implement the collaborator interfaces and receive those invocations.&lt;/li&gt;
&lt;li&gt;For each invocation that the component-under-test makes to a collaborator, it expects to receive back some result; therefore, the substitute receiving the invocation must be capable of generating a result that matches the result that would be generated by the real collaborator.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The technique which is predominant in the industry today for providing the component-under-test with substitutes of its collaborators is &lt;em&gt;&lt;strong&gt;Mock Objects&lt;/strong&gt;&lt;/em&gt;, or just mocks.&lt;/p&gt;
&lt;h3 id="how-do-mocks-work"&gt;How do mocks work?
&lt;/h3&gt;&lt;p&gt;Mocks are based on the premise that the real work done by collaborators in a production environment is irrelevant during testing, and all that the component-under-test really needs from them is the results that they return when invoked. A test exercises the component-under-test in a specific way, therefore the component-under-test is expected to invoke its collaborators in ways which are known in advance; thus, regardless of how the real collaborators would work, the mocks which replace them do not need to contain any functionality; all they need to do is to yield the same results that the real collaborators would have returned, which are also known in advance.&lt;/p&gt;
&lt;p&gt;To this effect, each test dynamically creates and configures as many mocks as necessary to substitute each one of the collaborators of the component-under-test, with the help of some mocking framework. These frameworks are so popular that there exists a proliferation of them: JMock, EasyMock, Mockito, NMock, Moq, JustMock, and the list goes on.&lt;/p&gt;
&lt;p&gt;A mock object is configured to expose the same interface as the real collaborator that it substitutes, and to expect specific methods of this interface to be invoked, with specific argument values, sometimes even in a specific order of invocation. If anything goes wrong, such as an unexpected method being invoked, or a parameter having an unexpected value, the mock fails the test. A very common practice is to also fail the test if an expected method is &lt;em&gt;not&lt;/em&gt; invoked.&lt;/p&gt;
&lt;p&gt;For each one of the expected methods, the mock is configured to yield a prefabricated result which is intended to match the result that the real collaborator would have produced if it was being used, and if it was working exactly according to its specification.&lt;/p&gt;
&lt;p&gt;Or at least, that is the intention.&lt;/p&gt;
&lt;h4 id="drawbacks-of-mocks"&gt;Drawbacks of Mocks
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Complex and laborious&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;In each test it is not enough to invoke the component-under-test to perform a computation and check the results; we also have to configure a mock for each one of the collaborators of the component, to anticipate every single call that the component will be making to them while performing the computation, and for each call to fabricate a result which matches the result that the real collaborator would have returned from that call.&lt;/li&gt;
&lt;li&gt;Luckily, mocking frameworks lessen the amount of code necessary to accomplish this, but no matter how terse the mocking code is, the fact still remains that it constitutes substantial additional functionality which represents considerable additional complexity.&lt;/li&gt;
&lt;li&gt;One of the well-known caveats of software testing is that a test failure does not necessarily indicate a defect in the production code; it always indicates a defect either in the production code or in the test itself, and the only way to know is to troubleshoot. Thus, the more code we put in tests, and the more complex this code is, the more time we end up wasting in chasing and fixing bugs in the tests themselves rather than in the code that they are meant to test.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Over-specified&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;By anticipating every single call that the component-under-test makes to its collaborators, we are claiming to have detailed knowledge of the inner workings of the component-under-test, and we are concerned not only with what it accomplishes, but also with every little detail about how it goes on about accomplishing it. Essentially, we are implementing all of our application logic twice: once with production code expressing the logic in imperative mode, and once more with testing code expressing the same logic in expectational mode. In both cases, we write copious amounts of code describing what should happen in excruciatingly meticulous detail.&lt;/li&gt;
&lt;li&gt;Note that over-specification might not even be a goal in and of itself in some cases, but with mocking it is unavoidable in all cases: Each request that the component-under-test sends to its collaborators could conceivably be ignored, but the component-under-test still needs to receive some meaningful result in response to that request, so as to continue functioning during the remainder of the test; unfortunately, the only way that mocks can fabricate individual responses is by anticipating individual requests, even if the intention of the test is not to verify whether the requests are being made.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Presumptuous&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;When using mocks we are claiming to not only have detailed knowledge of the calls that the component-under-test makes to its collaborators, but also detailed knowledge of the results that would be returned by the real collaborators in a production environment.&lt;/li&gt;
&lt;li&gt;Furthermore, the results returned by a collaborator depend on the state that the collaborator is in, which in turn depends on previous calls made to it, but a mock is by its nature incapable of emulating state, so when using mocks we are also claiming to have knowledge of the state transitions that the real collaborators undergo in a production environment, and of the effect that these state transitions have on the results that they return.&lt;/li&gt;
&lt;li&gt;Such exorbitant presumptuousness might be okay if we are building high-criticality software, where each collaborator is likely to have requirements and specification that are well-defined and unlikely to change; however, in all other software, which is regular, commercial, non-high-criticality software, things are a lot less strict: not only the requirements and specifications change all the time, but also, by established practice, both the requirements, and the specification, and even the documentation, tend to be the code itself, and the code changes every time a new commit is made to the source code repository. Thus, the only way to know exactly how a collaborator behaves tends to be to actually invoke it and see what it does, while the mechanism which ensures that it does what it is supposed to do is the tests of that collaborator itself, which are unrelated to the tests of components that invoke it.&lt;/li&gt;
&lt;li&gt;As a result of all this, the practice of mocking often places us in the all too familiar situation where our Unit Tests all pass with flying colors, but our Integration Tests miserably fail because the behavior of the real collaborators turns out to be different from what the mocks assumed it would be.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fragile&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;By its nature, a mock object has no option but to fail the test if the interactions between the component under test and its collaborators deviate from what it expects. However, these interactions may legitimately change as software evolves. This may happen due to the application of a bug-fix, due to refactoring, or simply because as we write new code we invariably have to also modify existing code to interact with the new code that we are adding. Thus, when using mocks, every time we change the behavior of production code, we also have to fix tests to expect the new behavior. (Not only do we have to write all of our application logic twice, we also have to perform all of its maintenance twice.)&lt;/li&gt;
&lt;li&gt;The original promise of Automated Software Testing was to enable us to continuously evolve our software without fear of breaking it. The idea is that whenever you modify the production code, you can re-run the tests to ensure that everything still works. When using mocks this does not work, because every time you change the slightest thing in the production code, the tests break. As a result, many programmers are hesitant to make needed changes to production code because of all the changes in testing code that would be required. The understanding is growing within the software engineering community that mock objects actually hinder software development instead of facilitating it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-reusable&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Mocks exercise the implementation of a component rather than its interface. Thus, when using mocks, it is impossible to reuse the same testing code to validate multiple different components that implement the same public interface but employ different collaborators. For example:
&lt;ul&gt;
&lt;li&gt;It is impossible to completely rewrite the component and reuse the old tests to make sure that the new implementation works exactly as the old one did.&lt;/li&gt;
&lt;li&gt;It is impossible to use a single test suite to exercise both a real component and its fake.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unenlightening&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Ideally, a set of tests for a certain component should act as sample code demonstrating usage scenarios of that component. A programmer who is not familiar with a particular component should be able to read the tests of that component and gain a fairly good idea of what it can do, what it cannot do, and how to write production code that interacts with it.&lt;/li&gt;
&lt;li&gt;Unfortunately, when using mocks, the tests are full of cryptic mock-related jabber, which obscures the actual usage of the component-under-test, and so the enlightening bits are lost in the noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="what-do-others-say"&gt;What do others say?
&lt;/h4&gt;&lt;p&gt;I am certainly not the only one to voice dissatisfaction with mocks. People have been noticing that although automated software testing is intended to facilitate refactoring by ensuring that the code still works after each change that we make, the use of mocks often hinders refactoring, because the tests are so tied to the implementation that you cannot change anything without breaking the tests.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the video &lt;em&gt;Thoughtworks - TW Hangouts: Is TDD dead?&lt;/em&gt; (&lt;a class="external"
href="https://www.youtube.com/watch?v=z9quxZsLcfo" target="_blank"
&gt;youtube&lt;/a&gt;, &lt;a class="external"
href="https://martinfowler.com/articles/is-tdd-dead/" target="_blank"
&gt;text digest&lt;/a&gt;) at 21':10'' Kent Beck states &amp;quot;My personal practice is I mock almost nothing.&amp;quot;&lt;/li&gt;
&lt;li&gt;In the same video, at 23':56'' Martin Fowler adds &amp;quot;I'm with Kent, I hardly ever use mocks.&amp;quot;&lt;/li&gt;
&lt;li&gt;In the &lt;em&gt;Fragile Test&lt;/em&gt; section of his book &lt;em&gt;xUnit Test Patterns: Refactoring Test Code&lt;/em&gt; (&lt;a class="external"
href="https://xunitpatterns.com/" target="_blank"
&gt;xunitpatterns.com&lt;/a&gt;) author Gerard Meszaros admits that &amp;quot;extensive use of Mock Objects causes overcoupled tests.&amp;quot;&lt;/li&gt;
&lt;li&gt;In his presentation &lt;em&gt;TDD, where did it all go wrong?&lt;/em&gt; (&lt;a class="external"
href="https://www.infoq.com/presentations/tdd-original/" target="_blank"
&gt;InfoQ&lt;/a&gt;, &lt;a class="external"
href="https://www.youtube.com/watch?v=EZ05e7EMOLM" target="_blank"
&gt;YouTube&lt;/a&gt;) at 49':32'' Ian Cooper states &amp;quot;I argue quite heavily against mocks because they are overspecified.&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that in an attempt to avoid sounding too blasphemous, these people refrain from suggesting that mocks should be abolished; however, it is evident that 3 out of 4 of them are strongly against mocks, and we do not need to read much between the lines to figure out that they would probably be calling for the complete abolition of mocks if they had a viable and universally applicable alternative to propose.&lt;/p&gt;
&lt;h4 id="so-if-not-mocking-then-what"&gt;So, if not mocking, then what?
&lt;/h4&gt;&lt;p&gt;Mocking has been such a great hit with the software industry because it achieves multiple different goals at once. Here is a list of the supposed benefits of mocking, and for each one of them an explanation of why it is not really a benefit, or how it can be achieved without mocking:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mocking achieves defect localization by eliminating collaborators from the picture and allowing components to be tested in strict isolation from each other.
&lt;ul&gt;
&lt;li&gt;Defect localization is useful, but it is not an absolute necessity, and it does not have to be done to absolute perfection as mocking aims to do; we can achieve more than good enough defect localization by testing each component in integration with its collaborators, simply by arranging the order in which tests are executed to ensure that by the time a component gets tested, all of its collaborators have already passed their tests. See &lt;a
href="https://blog2.michael.gr/post/2022-10-incremental-integration-testing/"
&gt;Incremental Integration Testing&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mocking allows a component to be tested without the performance overhead of instantiating and invoking its real collaborators.
&lt;ul&gt;
&lt;li&gt;The performance overhead of instantiating and invoking the real collaborators is not always prohibitive, or even noticeable, so in many cases it is perfectly fine to test a component in integration with its real collaborators. See &lt;a
href="https://blog2.michael.gr/post/2022-10-incremental-integration-testing/"
&gt;Incremental Integration Testing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;In the limited number of cases where the performance overhead is indeed prohibitive, it can be avoided with the use of Fakes instead of Mocks. See &lt;a
href="https://blog2.michael.gr/post/2022-10-testing-with-fakes/"
&gt;Testing with Fakes instead of Mocks&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mocking allows us to examine invocations being made by the component-under-test to its collaborators, to ensure that they are issued exactly as expected.
&lt;ul&gt;
&lt;li&gt;In most cases, examining the invocations made by the component-under-test to its collaborators is in fact bad practice, because it constitutes white-box testing. The only reason why this is being widely practiced in the industry is because mocking does not work otherwise, so in this regard mocking contains a certain element of a self-serving paradigm.&lt;/li&gt;
&lt;li&gt;In those rare cases where examining the invocations is in fact necessary, it is still bad practice to do so programmatically, because it results in tests that are over-specified and fragile.&lt;/li&gt;
&lt;li&gt;What we can do instead is to record the interactions during each test run, visually compare the latest recording with that of the last known good run, and decide whether the differences match our expectations; if they do not match, then we must keep working on our code; but if they do match, then we are done without the need to go fixing any tests. See &lt;a
href="https://blog2.michael.gr/post/2024-04-audit-testing/"
&gt;Audit Testing&lt;/a&gt; and &lt;a
href="https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/"
&gt;Collaboration Monitoring&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mocking allows us to fabricate the results returned from a collaborator to the component-under-test, so as to guarantee that they are free from defects that could be caused by bugs in the implementation of the real collaborator.
&lt;ul&gt;
&lt;li&gt;Fabricating the results that would have been returned by a real collaborator is in fact bad practice, because it will not magically make any bugs go away, (in this sense it can be likened to ostrich policy,) and because as I have already explained, it is highly presumptuous. The definitive authority on what results are returned by a certain collaborator is the real implementation of that collaborator, or a fake thereof, which in turn necessitates integration testing. See &lt;a
href="https://blog2.michael.gr/post/2022-10-incremental-integration-testing/"
&gt;Incremental Integration Testing&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mocking allows us to verify the correctness of components that generate their output by means of forwarding results to collaborators rather than by returning results from invocations.
&lt;ul&gt;
&lt;li&gt;Even in this case, &lt;em&gt;Collaboration Monitoring&lt;/em&gt; can be used instead of mocking, to verify that the results are generated as expected without having to programmatically describe what the results should be and without having to go fixing tests each time we modify the component under test and deliberately change something about the results it generates. See &lt;a
href="https://blog2.michael.gr/post/2024-04-audit-testing/"
&gt;Audit Testing&lt;/a&gt; and &lt;a
href="https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/"
&gt;Collaboration Monitoring&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mocking allows us to start testing a component while one or more of its collaborators are not ready yet for integration because they are still in development, and no fakes of them are available either.
&lt;ul&gt;
&lt;li&gt;This is true, but once the collaborators (or fakes thereof) become available, it is best to integrate them in the tests, and to unceremoniously throw away the mocks. See &lt;a
href="https://blog2.michael.gr/post/2022-10-incremental-integration-testing/"
&gt;Incremental Integration Testing&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mocking allows us to develop a component without depending on factors that we have no control over, such as the time of delivery of collaborators, the quality of their implementation, and the quality of their testing. With the use of Mocks we can claim that our component is complete and fully tested, based on nothing but the specification of its collaborators, and we can claim that it should work fine in integration with its collaborators when they happen to be delivered, and if they happen to work according to spec.
&lt;ul&gt;
&lt;li&gt;True, but this implies a very bureaucratic way of working, and utter lack of trust towards the developers of the collaborators; it is best if it never comes to that.&lt;/li&gt;
&lt;li&gt;We can still avoid the use of mocks by creating fakes of the collaborators ourselves. See &lt;a
href="https://blog2.michael.gr/post/2022-10-testing-with-fakes/"
&gt;Testing with Fakes instead of Mocks&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To summarize, mocks can always be replaced with one or more of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fakes (see &lt;a
href="https://blog2.michael.gr/post/2022-10-testing-with-fakes/"
&gt;Testing with Fakes instead of Mocks&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Incremental Integration Testing (see &lt;a
href="https://blog2.michael.gr/post/2022-10-incremental-integration-testing/"
&gt;Incremental Integration Testing&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Audit Testing (see &lt;a
href="https://blog2.michael.gr/post/2024-04-audit-testing/"
&gt;Audit Testing&lt;/a&gt;) and Collaboration Monitoring (see &lt;a
href="https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/"
&gt;Collaboration Monitoring&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="conclusion"&gt;Conclusion
&lt;/h4&gt;&lt;p&gt;As we have shown, the practice of using Mock Objects in automated software testing is laborious, over-specified, presumptuous, and leads to tests that are fragile and non-reusable, while each of the alleged benefits of using mocks is either not a real benefit, or can be realized by other means, which we have named.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center"&gt;&lt;img src="https://blog2.michael.gr/post/2023-01-14-mocking/images/grumpy-cat-mock-objects.jpg"
width="797"
height="1024"
srcset="https://blog2.michael.gr/post/2023-01-14-mocking/images/grumpy-cat-mock-objects_hu_8156248a341d61af.jpg 480w, https://blog2.michael.gr/post/2023-01-14-mocking/images/grumpy-cat-mock-objects_hu_5b4ff396469f2342.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="77"
data-flex-basis="186px"
&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center"&gt;Mandatory grumpy cat meme - &amp;quot;Mock objects - they are horrible&amp;quot;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;Cover image: &amp;quot;Mocking&amp;quot; by michael.gr, based on &lt;a class="external"
href="https://thenounproject.com/icon/mock-2657532/" target="_blank"
&gt;'mock' by 'Iconbox' from the noun project.&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Collaboration Monitoring</title><link>https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/</link><pubDate>Fri, 06 Jan 2023 13:03:22 +0000</pubDate><guid>https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/</guid><description>&lt;p&gt;&lt;img src="https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/images/collaboration-monitoring.png"
width="1024"
height="512"
srcset="https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/images/collaboration-monitoring_hu_3c58b5508de82250.png 480w, https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/images/collaboration-monitoring_hu_1f58869c4f30abf2.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
&gt;
&lt;/p&gt;
&lt;h3 id="abstract"&gt;Abstract
&lt;/h3&gt;&lt;p&gt;An automated software testing technique is presented which solves the fragile test problem of white-box testing by allowing us to ensure that the component-under-test interacts with its collaborators according to our expectations without having to stipulate our expectations as test code, without having the tests fail each time our expectations change, and without having to go fixing test code each time this happens.&lt;/p&gt;
&lt;p&gt;(Useful pre-reading: &lt;a
href="https://blog2.michael.gr/post/2022-11-about-these-papers/"
&gt;About these papers&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id="summary"&gt;Summary
&lt;/h3&gt;&lt;p&gt;In automated software testing it is sometimes necessary to ensure not only that given specific input, the component-under-test produces correct output, (&lt;a class="external"
href="https://en.wikipedia.org/wiki/Black-box_testing" target="_blank"
&gt;Black-Box Testing&lt;/a&gt;,) but also that while doing so, it interacts with its collaborators in certain expected ways. (&lt;a class="external"
href="https://en.wikipedia.org/wiki/White-box_testing" target="_blank"
&gt;White-Box Testing&lt;/a&gt;.) The prevailing technique for achieving white-box testing (&lt;a class="external"
href="https://en.wikipedia.org/wiki/Mock_object" target="_blank"
&gt;Mock Objects&lt;/a&gt;) requires copious amounts of additional code in the tests to describe the interaction that are expected to happen, and fails the tests if the actual interactions deviate from the expected ones.&lt;/p&gt;
&lt;p&gt;Unfortunately, the interactions often change due to various reasons, for example applying a bug fix, performing refactoring, or modifying existing code in order to accommodate the addition of new code intended to introduce new functionality; so, tests keep breaking all the time, (the &lt;a class="external"
href="https://xunitpatterns.com/Fragile%20Test.html" target="_blank"
&gt;Fragile Test&lt;/a&gt; problem,) requiring constant maintenance, which imposes a heavy burden on the Software Development process.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Collaboration Monitoring&lt;/strong&gt;&lt;/em&gt; is a technique for white-box testing where during a test run we record detailed information about the interactions between collaborators, we compare the recording against that of a previous test run, and we visually examine the differences to determine whether the changes observed in the interactions are as expected according to the changes that were made in the code. Thus, no code has to be written to describe in advance how collaborators are expected to interact, and no tests have to be fixed each time the expectations change.&lt;/p&gt;
&lt;h3 id="the-problem"&gt;The problem
&lt;/h3&gt;&lt;p&gt;Most software testing as conventionally practiced all over the world today consists of two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Result Validation:&lt;/strong&gt;&lt;/em&gt; ascertaining that given specific input, the component-under-test produces specific expected output.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Collaboration Validation:&lt;/strong&gt;&lt;/em&gt; ensuring that while performing a certain computation, the component-under-test interacts with its collaborators in specific expected ways.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As I argue elsewhere, in the vast majority of cases, Collaboration Validation is ill-advised, because it constitutes white-box testing; however, there are some cases where it is necessary, for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In high-criticality software, which is all about safety, not only the requirements must be met, but also nothing must be left to chance. Thus, the cost of white-box testing is justified, and the goal is in fact to ensure that the component-under-test not only produces correct results, but also that while doing so, it interacts with its collaborators as expected.&lt;/li&gt;
&lt;li&gt;In reactive programming, the component-under-test does not produce output by returning results from function calls; instead, it produces output by forwarding results to collaborators. Thus, even if all we want to do is to ascertain the correctness of the component's output, we have to examine how it interacts with its collaborators, because that is the only way to observe its output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The prevalent mechanism by which the Software Industry achieves Collaboration Validation today is Mock Objects. As I argue elsewhere, (see &lt;a
href="https://blog2.michael.gr/post/2023-01-14-mocking/"
&gt;If you are using mock objects you are doing it wrong&lt;/a&gt;) the use of mocks is generally ill-advised due to various reasons, but with respect to Collaboration Validation in specific, the problem with mocks is that their use is extremely laborious:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When we write a test for a certain component, it is counter-productive to have to stipulate in code exactly how we expect it to interact with its collaborators.&lt;/li&gt;
&lt;li&gt;When we revise the implementation of a component, the component may now legitimately start interacting with its collaborators in a different way; when this happens, it is counter-productive to have the tests fail, and to have to go fix them so that they stop expecting the old interactions and start expecting the new interactions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The original promise of automated software testing was to allow us to modify code without the fear of breaking it, but with the use of mocks the slightest modification to the code causes the tests to fail, so the code always looks broken, and the tests always require fixing.&lt;/p&gt;
&lt;p&gt;This is particularly problematic in light of the fact that there is nothing about the concept of Collaboration Validation which requires that the interactions between collaborators must be stipulated in advance, nor that the tests must fail each time the interactions change; all that is required is that we must be able to tell whether the interactions between collaborators are as expected or not. Thus, Collaboration Validation does not necessitate the use of mocks; it could conceivably be achieved by some entirely different means.&lt;/p&gt;
&lt;h3 id="the-solution"&gt;The Solution
&lt;/h3&gt;&lt;p&gt;If we want to ensure that given specific input, a component produces expected results, we do of course have to write some test code to exercise the component as a black-box. If we also want to ensure that the component-under-test interacts with its collaborators in specific ways while it is being exercised, this would be white-box testing, so it would be best if it does not have to also be written in code. To achieve this without code, all we need is the ability to somehow capture the interactions so that we can visually examine them and decide whether they are in agreement with our expectations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If they are not as expected, then we have to keep working on the production code and/or the black-box testing code.&lt;/li&gt;
&lt;li&gt;If they are as expected, then we are done: we can commit our code, and call it a day, without having to modify any white-box tests!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The trick is to do so in a convenient, iterative, and fail-safe way, meaning that the following must hold true:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When a change in the code causes a change in the interactions, there should be some kind of indication telling us that the interactions have now changed, and this indication should be so clear that we cannot possibly miss it.&lt;/li&gt;
&lt;li&gt;Each time we modify some code and run the tests, we want to be able to see what has changed in the interactions as a result of only those modifications, so that we do not have to pore through long lists of irrelevant interactions, and so that no information gets lost in the noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To achieve this, I use a technique that I call Collaboration Monitoring.&lt;/p&gt;
&lt;p&gt;Collaboration Monitoring is based on another testing technique that I call Audit Testing, so it might be a good idea to read the related paper before proceeding: &lt;a
href="https://blog2.michael.gr/post/2024-04-audit-testing/"
&gt;Audit Testing&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let us assume that we have a component that we want to test, which invokes interface T as part of its job. In order to test the component, we have to wire it with a collaborator that implements T. For this, we can use either the real collaborator that would be wired in the production environment, or a Fake thereof. Regardless of what we choose, we have a very simple picture which looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/images/collaboration-monitor-1.png"
width="422"
height="74"
srcset="https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/images/collaboration-monitor-1_hu_8cbeb5070d7d33.png 480w, https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/images/collaboration-monitor-1_hu_422fd14dddc93b76.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="570"
data-flex-basis="1368px"
&gt;
&lt;/p&gt;
&lt;p&gt;Note that with this setup we can exercise the component-under-test as a black-box, but we cannot yet observe how it interacts with its collaborator.&lt;/p&gt;
&lt;p&gt;In order to observe how the component-under-test interacts with its collaborator, we interject between the two of them a new component, called a &lt;em&gt;&lt;strong&gt;Collaboration Monitor&lt;/strong&gt;&lt;/em&gt;, which is a decorator of T. The purpose of this Collaboration Monitor is to record into a text file information about each function call that passes through it. The text file is called a &lt;em&gt;&lt;strong&gt;Snoop File&lt;/strong&gt;&lt;/em&gt;, and it is a special form of &lt;em&gt;&lt;strong&gt;Audit File&lt;/strong&gt;&lt;/em&gt;. (See &lt;a
href="https://blog2.michael.gr/post/2024-04-audit-testing/"
&gt;Audit Testing&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Thus, we now have the following picture:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/images/collaboration-monitor-2.png"
width="568"
height="193"
srcset="https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/images/collaboration-monitor-2_hu_6876382b85ecd6f0.png 480w, https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/images/collaboration-monitor-2_hu_88f51d25c12dfe11.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="294"
data-flex-basis="706px"
&gt;
&lt;/p&gt;
&lt;p&gt;The information that the Collaboration Monitor saves for each function call includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The name of the function.&lt;/li&gt;
&lt;li&gt;A serialization of the value of each parameter that was passed to the function.&lt;/li&gt;
&lt;li&gt;A serialization of the return value of the function.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As per Audit Testing, the Snoop File is saved in the source code tree, right next to the source code file of the test that generated it, and gets committed into the Source Code Repository / Version Control System along with the source code. For example, if we have &lt;code&gt;SuchAndSuchTest.java&lt;/code&gt;, then after running the tests for the first time we will find a &lt;code&gt;SuchAndSuchTest.snoop&lt;/code&gt; file right next to it. We can examine this file to ensure that the component-under-test interacted with the collaborator exactly as expected.&lt;/p&gt;
&lt;p&gt;As we continue developing our system, the modifications that we make to the code will sometimes have no effect on how collaborators interact with each other, and sometimes will cause the collaborators to start interacting differently. Thus, as we continue running our tests while developing our system, we will be observing the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For as long as the collaborations continue in exactly the same way, the contents of the Snoop Files remain unchanged, despite the fact that the files are re-generated on each test run.&lt;/li&gt;
&lt;li&gt;As soon as some collaborations change, the contents of some Snoop Files will change.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As per Audit Testing, we can then leverage our Version Control System and our Integrated Development Environment to take care of the rest of the workflow, as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When we make a revision in the production code or in the testing code, and as a result of this revision the interactions between the component-under-test and its collaborators are now even slightly different, we will not fail to take notice because our Version Control System will show the corresponding Snoop File as modified and in need of committing.&lt;/li&gt;
&lt;li&gt;By asking our Integrated Development Environment to show us a &amp;quot;diff&amp;quot; between the current snoop file and the unmodified version, we can see precisely what has changed without having to pore through the entire snoop file.&lt;/li&gt;
&lt;li&gt;If the observed interactions are not exactly what we expected them to be according to the revisions we just made, we keep working on our revision.&lt;/li&gt;
&lt;li&gt;When we are confident that the differences in the interactions are exactly as expected according to the changes that we made to the code, we commit our revision, along with the Snoop Files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="what-about-code-review"&gt;What about code review?
&lt;/h3&gt;&lt;p&gt;As per Audit Testing, the reviewer is able to see both the changes in the code, and the corresponding changes in the Snoop Files, and vouch for them, or not, as the case might be.&lt;/p&gt;
&lt;h3 id="requirements"&gt;Requirements
&lt;/h3&gt;&lt;p&gt;For Collaboration Monitoring to work, snoop files must be free from non-deterministic noise, and it is best if they are also free from deterministic noise. For more information about these types of noise and what you can do about them, see &lt;a
href="https://blog2.michael.gr/post/2024-04-audit-testing/"
&gt;Audit Testing&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="automation"&gt;Automation
&lt;/h3&gt;&lt;p&gt;When using languages like Java and C# which support reflection and intermediate code generation, we do not have to write Collaboration Monitors by hand; we can instead create a facility which will be automatically generating them for us on demand, at runtime. Such a facility can be very easily written with the help of Intertwine (see &lt;a
href="https://blog2.michael.gr/post/2022-12-intertwine/"
&gt;Intertwine&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Using Intertwine, we can create a Collaboration Monitor for any interface T. Such a Collaboration Monitor works as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Contains an Entwiner of T so that it can expose interface T without any hand-written code implementing interface T. The Entwiner delegates to an instance of &lt;code&gt;AnyCall&lt;/code&gt;, which expresses each invocation in a general-purpose form.&lt;/li&gt;
&lt;li&gt;Contains an implementation of &lt;code&gt;AnyCall&lt;/code&gt; which serializes all necessary information about the invocation into the Snoop File.&lt;/li&gt;
&lt;li&gt;Contains an untwiner of T, so that it can convert each invocation from &lt;code&gt;AnyCall&lt;/code&gt; back to an instance of T, without any hand-written code for invoking interface T.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="comparison-of-workflows"&gt;Comparison of Workflows
&lt;/h4&gt;&lt;p&gt;Here is a step-by-step comparison of the software development process when using mocks, and when using collaboration monitoring.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Workflow using Mock Objects:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Modify the production code and/or the black-box part of the tests.&lt;/li&gt;
&lt;li&gt;Run the tests.
&lt;ul&gt;
&lt;li&gt;If the tests pass:
&lt;ul&gt;
&lt;li&gt;Done.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If the tests fail:
&lt;ul&gt;
&lt;li&gt;Troubleshoot why this is happening.
&lt;ul&gt;
&lt;li&gt;If either the production code or the black-box part of the tests is
wrong:
&lt;ul&gt;
&lt;li&gt;Go to step 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If the white-box part of the tests is wrong:
&lt;ul&gt;
&lt;li&gt;Modify the white-box part of the tests (the mocking code) to stop expecting the old interactions and start expecting the new interactions.&lt;/li&gt;
&lt;li&gt;Go to step 2.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Workflow using Collaboration Monitoring:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Modify the production code and/or the tests.&lt;/li&gt;
&lt;li&gt;Run the tests.
&lt;ul&gt;
&lt;li&gt;If the tests pass:
&lt;ul&gt;
&lt;li&gt;If the interactions have remained unchanged:
&lt;ul&gt;
&lt;li&gt;Done.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If the interactions have changed:
&lt;ul&gt;
&lt;li&gt;Visually inspect the changes.
&lt;ul&gt;
&lt;li&gt;If the interactions agree with our expectations:
&lt;ul&gt;
&lt;li&gt;Done.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If the interactions differ from our expectations:
&lt;ul&gt;
&lt;li&gt;Go to step 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If the tests fail:
&lt;ul&gt;
&lt;li&gt;Go to step 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id="conclusion"&gt;Conclusion
&lt;/h4&gt;&lt;p&gt;Collaboration Monitoring is an adaptation of Audit Testing which allows the developer to write black-box tests which only exercise the public interface of the component-under-test, while remaining confident that the component interacts with its collaborators inside the black box according to their expectations, without having to write white-box testing code to stipulate the expectations, and without having to modify white-box testing code each time the expectations change.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Cover image: &amp;quot;Collaboration Monitoring&amp;quot; by michael.gr based on original work &lt;a class="external"
href="https://thenounproject.com/icon/monitoring-4861371/" target="_blank"
&gt;'monitoring' by Arif Arisandi&lt;/a&gt; and &lt;a class="external"
href="https://thenounproject.com/icon/gears-1705750/" target="_blank"
&gt;'Gears' by Free Fair &amp;amp; Healthy&lt;/a&gt; from the Noun Project.&lt;/p&gt;</description></item><item><title>Testing with Fakes instead of Mocks</title><link>https://blog2.michael.gr/post/2022-10-testing-with-fakes/</link><pubDate>Fri, 30 Dec 2022 14:01:26 +0000</pubDate><guid>https://blog2.michael.gr/post/2022-10-testing-with-fakes/</guid><description>&lt;p&gt;&lt;img src="https://blog2.michael.gr/post/2022-10-testing-with-fakes/media/fake.png"
width="2000"
height="1000"
srcset="https://blog2.michael.gr/post/2022-10-testing-with-fakes/media/fake_hu_94f7aa407fc2e88a.png 480w, https://blog2.michael.gr/post/2022-10-testing-with-fakes/media/fake_hu_fbb249274ecaf5fb.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
&gt;
&lt;/p&gt;
&lt;h3 id="abstract"&gt;Abstract
&lt;/h3&gt;&lt;p&gt;What are &lt;em&gt;fakes&lt;/em&gt;, what are their benefits, and why they are incontestably preferable over &lt;em&gt;mocks&lt;/em&gt;. Also, how to create fakes if needed.&lt;/p&gt;
&lt;h4 id="introduction"&gt;Introduction
&lt;/h4&gt;&lt;p&gt;(Useful pre-reading: &lt;a
href="https://blog2.michael.gr/post/2022-11-about-these-papers/"
&gt;About these papers&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;When testing a component it is often necessary to refrain from connecting it with the real collaborators that it would be connected with in a production environment, and instead to connect it with special substitutes of its collaborators, also known as &lt;em&gt;test doubles&lt;/em&gt;, which are more suitable for testing than the real ones.&lt;/p&gt;
&lt;p&gt;One book that names and describes various kinds of test doubles is &lt;em&gt;xUnit Test Patterns: Refactoring Test Code&lt;/em&gt; by Gerard Meszaros, (&lt;a class="external"
href="https://xunitpatterns.com/" target="_blank"
&gt;xunitpatterns.com&lt;/a&gt;) though I first read about them from &lt;a class="external"
href="https://martinfowler.com/bliki/TestDouble.html" target="_blank"
&gt;martinfowler.com - TestDouble&lt;/a&gt;, which refers to Meszaros as the original source.&lt;/p&gt;
&lt;p&gt;There exist a few different kinds of test doubles; by far the most commonly used kind is mocks, which, as I explain elsewhere, are a very bad idea and should be avoided like COVID-19. (See &lt;a
href="https://blog2.michael.gr/post/2023-01-14-mocking/"
&gt;If you are using mock objects you are doing it wrong&lt;/a&gt;.) Another kind of test double, which does not suffer from the disadvantages of mocks, is &lt;em&gt;&lt;strong&gt;Fake Objects&lt;/strong&gt;&lt;/em&gt;, or simply &lt;em&gt;&lt;strong&gt;fakes&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id="what-are-fakes"&gt;What are fakes
&lt;/h3&gt;&lt;p&gt;In just one word, a fake is an emulator.&lt;/p&gt;
&lt;p&gt;In a bit more detail, a fake is a component that fully implements the interface of the real component that it substitutes, or at any rate the subset of that interface that we have a use for; it maintains state which is equivalent to the state of the real component, and based on this state it provides the full functionality of the real component, or a very convincing illusion thereof; to achieve this, it makes some compromises which either do not matter during testing, or are actually desirable during testing. Examples of such compromises are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Having limited capacity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Not being scalable.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Not being distributed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Not remembering any state from run to run.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pretending to interact, but not actually interacting, with the physical world.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generating fake data that would be unusable in a real production scenario.&lt;/p&gt;
&lt;p&gt;A fake can be more suitable for testing than the real thing in the following ways:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;By performing much better than the real thing; for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;by keeping state in-memory instead of persisting to the file-system.&lt;/li&gt;
&lt;li&gt;by working locally instead of over the network.&lt;/li&gt;
&lt;li&gt;by pretending that the time has come for the next timer to fire instead of having to wait for that timer to fire.&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;By being deterministic; for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;by fabricating time-stamps instead of querying the system clock.&lt;/li&gt;
&lt;li&gt;by fabricating entities such as GUIDs, that would otherwise introduce randomness.&lt;/li&gt;
&lt;li&gt;by utilizing a single thread, or forcing threads to work in a lock-step fashion.&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;By avoiding undesirable interactions with the real world; for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;by pretending that a mass e-mail was sent instead of actually sending it.&lt;/li&gt;
&lt;li&gt;by pretending that an application-modal message box popped up, and that the user picked one of the available choices, instead of allowing an actual modal message box to block the running of tests on the developer's computer, or, worse yet, on some continuous build server in some data center out there.&lt;/li&gt;
&lt;li&gt;by pretending that an industrial robot made a certain movement, instead of causing an actual robot to move on a factory floor.&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A few examples of frequently used fakes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Various in-memory file-system libraries exist for various platforms, which can be used in place of the actual file-systems on those platforms.&lt;/li&gt;
&lt;li&gt;HSQLDB and H2 for Java, in-memory DbContext for DotNet EntityFramework, etc. are in-memory database systems that can be used in place of actual Relational Database Management Systems when testing.&lt;/li&gt;
&lt;li&gt;EmbeddedKafka can be used in place of an actual pair of Kafka + Zookeeper instances.&lt;/li&gt;
&lt;li&gt;A pseudo-random number generator seeded with a known constant value acts as a fake of the same pseudo-random number generator seeded with a practically random value such as the current time coordinate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To recap:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fakes refrain from performing the actual operations that the real thing would perform, (e.g. when a file is created while using an in-memory file-system, no file gets created on disk,) but:&lt;/li&gt;
&lt;li&gt;They do go through all the motions, (e.g. attempting to create a file using an invalid filename will cause an error just as in a real file-system,) and:&lt;/li&gt;
&lt;li&gt;They do maintain the same state, (e.g. reading a file from an in-memory file-system will yield the exact same data that were previously written to
it,) so:&lt;/li&gt;
&lt;li&gt;They do fully behave as if the operations were actually performed as far as the component-under-test is concerned, while:&lt;/li&gt;
&lt;li&gt;The compromises that they make in order to achieve this are inconsequential or even desirable when testing. (e.g. during a test run it does not matter if files created during a previous test run do not exist anymore, and as a matter of fact it is better if they do not exist.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that the terminology is a bit unfortunate: fakes are not nearly as fake as mocks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mocks are the ultimate in fakery because:
&lt;ul&gt;
&lt;li&gt;They only respond to invocations that we prescribe in each test, based on our assumptions as to how the component-under-test would invoke the real thing.&lt;/li&gt;
&lt;li&gt;They maintain no state.&lt;/li&gt;
&lt;li&gt;They contain no functionality.&lt;/li&gt;
&lt;li&gt;They only return results that we prefabricate in each test, based on our assumptions as to how the real thing would respond.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fakes are not quite as fake as their name suggests, because:
&lt;ul&gt;
&lt;li&gt;They expose the same interface as the real thing.&lt;/li&gt;
&lt;li&gt;They maintain an equivalent state as the real thing.&lt;/li&gt;
&lt;li&gt;They implement equivalent functionality as the real thing.&lt;/li&gt;
&lt;li&gt;They return the exact same results as the real thing.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="benefits-of-fakes"&gt;Benefits of fakes
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;By using a fake instead of the real thing:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;We achieve better performance, so that our tests run quickly.&lt;/li&gt;
&lt;li&gt;We avoid non-determinism during testing, so our tests are repeatable.&lt;/li&gt;
&lt;li&gt;We avoid undesirable interactions with the real world, so nobody gets hurt.&lt;/li&gt;
&lt;li&gt;We have less code to write, since a fake is usually simpler to set up than the real thing.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;By using a fake instead of a mock:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;We save ourselves from having to write complicated mocking code in each test.&lt;/li&gt;
&lt;li&gt;We do not need to claim any knowledge as to how the component under test invokes its collaborators.&lt;/li&gt;
&lt;li&gt;We do not have to make assumptions about the state in which the collaborators are at any given moment.&lt;/li&gt;
&lt;li&gt;We do not have to make assumptions as to what results would be returned by each collaborator in each invocation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;In both cases:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;We are incorporating in our tests a collaborator which has already been tested and can be reasonably assumed to be free of defects. Thus, in the event of a test failure we can be fairly confident that the defect lies in the component-under-test, (or in the test itself,) but not in one of the collaborators, so we achieve defect localization, which is the aim of Unit Testing.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="creating-fakes-of-our-own-components"&gt;Creating fakes of our own components
&lt;/h3&gt;&lt;p&gt;In some cases we may want to create a fake ourselves, as a substitute of one of our own components. Not only will this allow other components to start their testing as early as possible without the need for mocks, but also, a non-negligible part of the effort invested in the creation of the fake will be reusable in the creation of the real thing, while the process of creating the fake is likely to yield valuable lessons which can guide the creation of the real thing. Thus, any effort that goes into creating a fake of a certain component represents a much better investment than the effort of creating a multitude of throw-away mocks for various isolated operations on that component.&lt;/p&gt;
&lt;p&gt;One might argue that keeping a fake side-by-side with the real thing may represent a considerable additional maintenance overhead, but in my experience the overhead of doing so is nowhere near the overhead of maintaining a proliferation of mocks for the real thing.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each time the implementation of the real thing changes without any change to its specification, (such as, for example, when applying some refactoring, or a bug fix,) some mocks must be modified, some must even be rewritten, while the fake usually does not have to be touched at all.&lt;/li&gt;
&lt;li&gt;When the specification of the real thing changes, the mocks have to be rewritten, and the fake has to be modified, but the beauty of the fake is that it is a self-contained module which implements a known abstraction, so it is easy to maintain, whereas every single snippet of mocking code is nothing but incidental complexity, and thus hard to maintain.&lt;/li&gt;
&lt;li&gt;In either case, a single change in the real thing will generally require a single corresponding change in the fake, whereas if we are using mocks we invariably have to go changing an arbitrary number of mocking snippets scattered throughout the tests.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Furthermore, the use of fakes instead of mocks promotes the creation of black-box tests instead of white-box tests. Once we get into the habit of writing all of our tests as black-box tests, new possibilities open up which greatly ease the development of fakes: we can now write a test for a certain module, and then reuse that test in order to test its fake. The test can be reused because it is a black-box test, so it does not care how the module works internally, therefore it can test the real thing just as well as the fake of the real thing. Once we run the test on the real thing, we run the same test on the fake, and if both pass, then from that moment on we can continue using the fake in place of the real thing in all other tests.&lt;/p&gt;
&lt;p&gt;The tests that exercise the real thing will be slow, but the real thing does not change very often, (if ever,) so here is where a testing tool like Testana shines: by using Testana we ensure that the tests exercising the real thing will only run in the rare event that the real thing actually changes. For more information about Testana, see &lt;a
href="https://blog2.michael.gr/post/2024-10-testana/"
&gt;Testana: A better way of running tests&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="creating-fakes-of-external-components"&gt;Creating fakes of external components
&lt;/h3&gt;&lt;p&gt;If we are using an external component for which no fake is available, we may wish to create a fake for it ourselves. First, we write a test suite which exercises the external component, not really looking for defects in it, but instead using its behavior as reference for writing the tests. Once we have built our test suite to specifically pass the behavior of the external component, we can reuse it against the fake, and if it also passes, then we have sufficient reasons to believe that the behavior of the fake matches the behavior of the external component. A similar technique is described by Martin Fowler in his &lt;a class="external"
href="https://martinfowler.com/bliki/ContractTest.html" target="_blank"
&gt;Contract Test&lt;/a&gt; post.&lt;/p&gt;
&lt;p&gt;In an ideal world where everyone would be practicing Black-Box testing, we should even be able to obtain from the creators of the external component the test suite that they have already built for testing their creation, and use it to test our fake.&lt;/p&gt;
&lt;p&gt;In an even more ideal world, anyone who develops a component for others to use would be shipping it together with its fake, so that nobody needs to get dirty with its test suite.&lt;/p&gt;
&lt;h3 id="conclusion"&gt;Conclusion
&lt;/h3&gt;&lt;p&gt;Despite widespread practices in the industry, fakes are the preferred alternative to mocks. Even though they might at first seem laborious, they are actually very convenient to use, and on the long run far less expensive than mocks.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Cover image: fake moustache by michael.gr based on &lt;a class="external"
href="https://thenounproject.com/icon/fake-mustache-31744/" target="_blank"
&gt;art by Claire Jones from the Noun Project&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>On Test-Driven Development (TDD)</title><link>https://blog2.michael.gr/post/2022-12-on-test-driven-development-tdd/</link><pubDate>Thu, 15 Dec 2022 17:16:45 +0000</pubDate><guid>https://blog2.michael.gr/post/2022-12-on-test-driven-development-tdd/</guid><description>&lt;p&gt;&lt;img src="https://blog2.michael.gr/post/2022-12-on-test-driven-development-tdd/media/tdd.svg"
loading="lazy"
&gt;
Let me get one thing out of the way first: I am open to Test-Driven Development (TDD). I am not currently practicing it, because when I gave it a try some time ago it did not seem to resonate with me, but I do not have any objections to it in principle, so I might give it another try in the future. Let us just say that it was not love at first sight, but then again some relationships do take some time to warm up.&lt;/p&gt;
&lt;p&gt;Having said that, let me now express a few reasons why I am skeptical of TDD. The previous paragraph should have established that I am not trashing TDD, I am just expressing some reservations.&lt;/p&gt;
&lt;p&gt;(Useful pre-reading: &lt;a
href="https://blog2.michael.gr/post/2022-11-about-these-papers/"
&gt;About these papers&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Reasons why I am skeptical of TDD:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The religion effect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Advocates of TDD say that it is the only proper way of developing software, and any other way is just plain wrong. If you do not like TDD, it is because you do not understand TDD. If you don't practice TDD, you are being unprofessional. In other words, TDD seems to have gained religion status. Its disciples are saying that their way is the one true virtuous way, and if you are not following it then you should repent and change your evil ways. As a civilization we have been there, tried that, it did not work well.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The &lt;em&gt;Life of Brian&lt;/em&gt; effect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;Thoughtworks - TW Hangouts: Is TDD dead?&lt;/em&gt; (&lt;a class="external"
href="https://www.youtube.com/watch?v=z9quxZsLcfo" target="_blank"
&gt;youtube&lt;/a&gt;) Kent Beck (the inventor of TDD) says starting at 13'19'' that there exist problems which are not amenable to solving via TDD, and when he comes across such problems, he does not use TDD. Martin Fowler adds that for him, the most important thing is to deliver properly tested code, and whether you write the tests before the production code or the other way around is secondary, and a matter of personal preference. Note how these statements constitute blasphemy among hard-core practitioners of TDD. It appears that the prophets do not endorse the creed as fervently as the adherents.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The stealth effect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Obviously you must never commit failing tests; this means that others should never be able to tell, by looking at the commit history, whether you wrote the tests first or the production code first. This in turn means that TDD is not observable, and therefore not enforceable, so perhaps we should not be too worried about something which is, by its nature, each developer's private business.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The envy effect&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Even though Test-Driven Development is a way of developing software which is based on a special way of doing testing, people seem to feel compelled to use the TDD buzzword with every opportunity, so the term is quite often used as nothing but a synonym for just plain testing. I come across articles which mention TDD in the title, but when you read the text you discover that absolutely nothing in there applies to writing the tests before the production code; therefore, the article was not about TDD, it was about testing at large. This in turn is an indication that TDD is being mentioned more often than it is actually being practiced.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Are these observations damning about TDD? No; I am just saying.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;P.S. (2025-08-08)&lt;/p&gt;
&lt;p&gt;So, I just discovered a debate between John Ousterhout and Robert Martin (Uncle Bob) about differences between John's book &amp;quot;A Philosophy of Software Design&amp;quot; and Bob's book &amp;quot;Clean Code&amp;quot;. The two men decided to hold the discussion mostly in the form of writing, and they posted it as a markdown document on GitHub.&lt;/p&gt;
&lt;p&gt;The debate is somewhat lengthy, and it is up to you to decide whether it is worth reading in its entirety, but here is a direct link into the TDD section of the debate:&lt;/p&gt;
&lt;p&gt;&lt;a class="external"
href="https://github.com/johnousterhout/aposd-vs-clean-code?tab=readme-ov-file#test-driven-development" target="_blank"
&gt;https://github.com/johnousterhout/aposd-vs-clean-code?tab=readme-ov-file#test-driven-development&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(This is, by the way, a very interesting format for holding a debate.)&lt;/p&gt;
&lt;p&gt;It is worth noting that there is no overlap between that debate and this post of mine about TDD; the two men did not touch upon the issues I go over in my post, and my post does not touch upon the issues that were debated by the two men.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Cover image: Conceptual illustration of Test-Driven Development, by michael.gr&lt;/p&gt;</description></item><item><title>Incremental Integration Testing</title><link>https://blog2.michael.gr/post/2022-10-incremental-integration-testing/</link><pubDate>Tue, 14 Dec 2021 09:07:09 +0000</pubDate><guid>https://blog2.michael.gr/post/2022-10-incremental-integration-testing/</guid><description>&lt;p&gt;&lt;img src="https://blog2.michael.gr/post/2022-10-incremental-integration-testing/media/incremental_integration_testing.svg"
loading="lazy"
&gt;
&lt;/p&gt;
&lt;h3 id="abstract"&gt;Abstract
&lt;/h3&gt;&lt;p&gt;A new method for &lt;em&gt;&lt;strong&gt;Automated Software Testing&lt;/strong&gt;&lt;/em&gt; is presented as an alternative to &lt;em&gt;&lt;strong&gt;Unit Testing&lt;/strong&gt;&lt;/em&gt;. The new method retains the benefit of Unit Testing, which is &lt;em&gt;&lt;strong&gt;Defect Localization&lt;/strong&gt;&lt;/em&gt;, but eliminates white-box testing and mocking, thus greatly lessening the effort of writing and maintaining tests.&lt;/p&gt;
&lt;p&gt;(Useful pre-reading: &lt;a
href="https://blog2.michael.gr/post/2022-11-about-these-papers/"
&gt;About these papers&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id="summary"&gt;Summary
&lt;/h3&gt;&lt;p&gt;Unit Testing aims to achieve Defect Localization by replacing the collaborators of the Component Under Test with Mocks. As we will show, the use of Mocks is laborious, complicated, over-specified, presumptuous, and constitutes testing against the implementation, not against the interface, thus leading to brittle tests that hinder refactoring rather than facilitating it.&lt;/p&gt;
&lt;p&gt;To avoid these problems, &lt;em&gt;&lt;strong&gt;Incremental Integration Testing&lt;/strong&gt;&lt;/em&gt; allows each component to be tested in integration with its collaborators, (or with Fakes thereof,) thus completely abolishing Mocks. Defect Localization is achieved by arranging the order in which tests are executed so that the collaborators of a component get tested before the component gets tested, and stopping as soon as a defect is encountered.&lt;/p&gt;
&lt;p&gt;Thus, when a test discovers a defect, we can be sufficiently confident that the defect lies in the component being tested, and not in any of its collaborators, because by that time, the collaborators have passed their tests.&lt;/p&gt;
&lt;h3 id="the-problem"&gt;The problem
&lt;/h3&gt;&lt;p&gt;The goal of automated software testing in general, regardless of what kind of testing it is, is to exercise a software system under various usage scenarios to ensure that it meets its requirements and that it is free from defects. The most simple and straightforward way to achieve this is to set up some input, invoke the system to perform a certain job, and then examine the output to ensure that it is what it is expected to be.&lt;/p&gt;
&lt;p&gt;Unfortunately, this approach only really works in the &amp;quot;sunny day&amp;quot; scenario: if no defects are discovered by the tests, then everything is fine; however, if defects are discovered, we are faced with a problem: the system consists of a large network of collaborating software components, and the test is telling us that there is a defect somewhere, but it is unclear in which component the problem lies. Even if we divide the system into subsystems and try to test each subsystem separately, each subsystem may still consist of many components, so the problem remains.&lt;/p&gt;
&lt;p&gt;What it ultimately boils down to is that each time we test a component, and a defect is discovered, it is unclear whether the defect lies in the component being tested, or in one or more of its collaborators.&lt;/p&gt;
&lt;p&gt;Ideally, we would like each test to be conducted in such a way as to detect defects specifically in the component that is being tested, instead of extraneous defects in its collaborators; in other words, we would like to achieve &lt;em&gt;Defect Localization&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id="the-existing-solution-unit-testing"&gt;The existing solution: Unit Testing
&lt;/h3&gt;&lt;p&gt;&lt;a class="external"
href="https://en.wikipedia.org/wiki/Unit_testing" target="_blank"
&gt;Unit Testing&lt;/a&gt; was invented specifically in order to achieve defect localization. It takes an extremely drastic approach: if the use of collaborators introduces uncertainties, one way to eliminate those uncertainties is to eliminate the collaborators. Thus, Unit Testing aims to test each component in strict isolation. Hence, its name.&lt;/p&gt;
&lt;p&gt;To achieve this remarkably ambitious goal, Unit Testing refrains from supplying the component under test with the actual collaborators that it would normally receive in a production environment; instead, it supplies the component under test with specially crafted &lt;em&gt;&lt;strong&gt;substitutes&lt;/strong&gt;&lt;/em&gt; of its collaborators, otherwise known as &lt;em&gt;&lt;strong&gt;test doubles&lt;/strong&gt;&lt;/em&gt;. There exist a few different kinds of substitutes, but by far the most widely used kind is &lt;em&gt;&lt;strong&gt;Mocks.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Each Mock must be hand-written for every individual test that is performed; it exposes the same interface as the real collaborator that it substitutes, and it expects specific methods of that interface to be invoked by the component-under-test, with specific argument values, sometimes even in a specific order of invocation. If anything goes wrong, such as an unexpected method being invoked, an expected method &lt;em&gt;not&lt;/em&gt; being invoked, or a parameter having an unexpected value, the Mock fails the test. When the component-under-test invokes one of the methods that the Mock expects to be invoked, the Mock does nothing of the sort that the real collaborator would do; instead, the Mock is hard-coded to yield a fabricated response which is intended to exactly match the response that the real collaborator would have produced if it was being used, and if it was working exactly according to its specification.&lt;/p&gt;
&lt;p&gt;Or at least, that is the intention.&lt;/p&gt;
&lt;h3 id="drawbacks-of-unit-testing"&gt;Drawbacks of Unit Testing
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Complex and laborious&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;In each test it is not enough to simply set up the input, invoke the component, and examine the output; we also have to anticipate every single call that the component will make to its collaborators, and for each call we have to set up a mock, expecting specific parameter values, and producing a specific response aiming to emulate the real collaborator under the same circumstances. Luckily, mocking frameworks lessen the amount of code necessary to accomplish this, but no matter how terse the mocking code is, the fact still remains that it implements a substantial amount of functionality which represents considerable complexity.&lt;/li&gt;
&lt;li&gt;One of the well-known caveats of software testing at large (regardless of what kind of testing it is) is that a test failure does not necessarily indicate a defect in the production code; it always indicates a defect either in the production code, or in the test itself. The only way to know is to troubleshoot. Thus, the more code we put in tests, and the more complex this code is, the more time we end up wasting in chasing and fixing bugs in the tests themselves rather than in the code that they are meant to test.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Over-specified&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Unit Testing is concerned not only with what a component accomplishes, but also with every little detail about how the component goes on about accomplishing it. This means that when we engage in Unit Testing we are essentially expressing all of our application logic twice: once with production code expressing the logic in imperative mode, and once more with testing code expressing the same logic in expectational mode. In both cases, we write copious amounts of code describing what should happen in excruciatingly meticulous detail.&lt;/li&gt;
&lt;li&gt;Note that with Unit Testing, over-specification might not even be goal in and of itself in some cases, but it is unavoidable in all cases. This is due to the elimination of the collaborators: the requests that the component under test sends to its collaborators could conceivably be routed into a black hole and ignored, but in order for the component under test to continue working so as to be tested, it still needs to receive a meaningful response to each request; thus, the test has to expect each request in order to produce each needed response, even if the intention of the test was not to know how, or even whether, the request is made.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Presumptuous&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Each Unit Test claims to have detailed knowledge of not only how the component-under-test invokes its collaborators, but also how each real collaborator would respond to each invocation in a production environment, which is a highly presumptuous thing to do.&lt;/li&gt;
&lt;li&gt;Such presumptuousness might be okay if we are building high-criticality software, where each collaborator is likely to have requirements and specification that are well-defined and unlikely to change; however, in all other software, which is regular, commercial, non-high-criticality software, things are a lot less strict: not only the requirements and specifications change all the time, but also quite often, the requirements, the specification, even the documentation, is the code itself, and the code changes every time a new commit is made to the source code repository. This might not be ideal, but it is pragmatic, and it is established practice. Thus, the only way to know exactly how a component behaves tends to be to actually invoke the latest version of that component and see how it responds, while the mechanism which ensures that these responses are what they are supposed to be is the tests of that component itself, which are unrelated to the tests of components that depend on it.&lt;/li&gt;
&lt;li&gt;As a result of this, Unit Testing often places us in the all too familiar situation where our Unit Tests all pass with flying colors, but our Integration Tests miserably fail because the behavior of the real collaborators turns out to be different from what the mocks assumed it would be.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fragile&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;During Unit Testing, if the interactions between the component under test and its collaborators deviate even slightly from our expectations, the test fails. However, these interactions may legitimately change as software evolves. This may happen due to the application of a bug-fix, due to refactoring, or due to the fact that whenever new code is added to implement new functionality, preexisting code must almost always be modified to accommodate the new code. With Unit Testing, every time we change the inner workings of production code, we have to go fixing all related tests to expect the new inner workings of that code.&lt;/li&gt;
&lt;li&gt;The original promise of Automated Software Testing was to enable us to continuously evolve software without fear of breaking it. The idea is that whenever you make a modification to the software, you can re-run the tests to ensure that everything still works as before. With Unit Testing this does not work, because every time you change the slightest thing in the production code you have to also change the tests, and you have to do this even for changes that are only internal. The understanding is growing within the software engineering community that Unit Testing with mocks actually hinders refactoring instead of facilitating it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-reusable&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Unit Testing exercises the implementation of a component rather than its interface. As such, the Unit Test of a certain component can only be used to test that component and nothing else. Thus, with Unit Testing the following things are impossible:
&lt;ul&gt;
&lt;li&gt;Completely rewrite a piece of production code and then reuse the old tests to make sure that the new implementation works exactly as the old one did.&lt;/li&gt;
&lt;li&gt;Reuse the same test to test multiple different components that implement the same interface.&lt;/li&gt;
&lt;li&gt;Use a single test to test multiple different implementations of a certain component, created by independently working development teams taking different approaches to solving the same problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The above disadvantages of Unit Testing are direct consequences of the fact that it is White-Box Testing by nature. What we need to be doing instead is Black-Box testing, which means that Unit Testing should be avoided, despite the entire Software Industry's addiction to it.&lt;/p&gt;
&lt;p&gt;Note that I am not the only one to voice dissatisfaction with Unit Testing with Mocks. People have been noticing that although tests are intended to facilitate refactoring by ensuring that the code still works after refactoring, tests often end up hindering refactoring, because they are so tied to the implementation that you can't refactor anything without breaking the tests. This problem has been identified by renowned personalities such as Martin Fowler and Ian Cooper, and even by Ken Beck, the inventor of Test-Driven Development (TDD).&lt;/p&gt;
&lt;p&gt;In the video &lt;em&gt;Thoughtworks - TW Hangouts: Is TDD dead?&lt;/em&gt; (&lt;a class="external"
href="https://www.youtube.com/watch?v=z9quxZsLcfo" target="_blank"
&gt;youtube&lt;/a&gt;) at 21':10'' Kent Beck says &amp;quot;My personal practice is I mock almost nothing&amp;quot; and at 23':56'' Martin Fowler says &amp;quot;I'm with Kent, I hardly ever use mocks&amp;quot;.&lt;/p&gt;
&lt;p&gt;In the &lt;em&gt;Fragile Test&lt;/em&gt; section of his book &lt;em&gt;xUnit Test Patterns: Refactoring Test Code&lt;/em&gt; (&lt;a class="external"
href="https://xunitpatterns.com/" target="_blank"
&gt;xunitpatterns.com&lt;/a&gt;) author Gerard Meszaros states that extensive use of Mock Objects causes overcoupled tests.&lt;/p&gt;
&lt;p&gt;In his presentation &lt;em&gt;TDD, where did it all go wrong?&lt;/em&gt; (&lt;a class="external"
href="https://www.infoq.com/presentations/tdd-original/" target="_blank"
&gt;InfoQ&lt;/a&gt;, &lt;a class="external"
href="https://www.youtube.com/watch?v=EZ05e7EMOLM" target="_blank"
&gt;YouTube&lt;/a&gt;) at 49':32'' Ian Cooper says &amp;quot;I argue quite heavily against mocks because they are overspecified.&amp;quot;&lt;/p&gt;
&lt;p&gt;Note that in an attempt to avoid sounding too blasphemous, none of these people calls for the complete abolition of mocks, they only warn against the excessive use of mocks. Furthermore, do not seem to be isolating the components under test, and yet they seem to have little, if anything, to say about any alternative means of achieving defect localization.&lt;/p&gt;
&lt;h3 id="a-new-solution-incremental-integration-testing"&gt;A new solution: Incremental Integration Testing
&lt;/h3&gt;&lt;p&gt;If we were to abandon Unit Testing with mocks, then one might ask what should we be doing instead. Obviously, we must somehow continue testing our software, and it would be nice if we can continue to be enjoying the benefits of defect localization.&lt;/p&gt;
&lt;p&gt;As it turns out, eliminating the collaborators is just one way of achieving defect localization; another, more pragmatic approach is as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Allow each component to be tested in integration with its collaborators, but only after each of the collaborators has undergone its own testing, and has successfully passed it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thus, any observed malfunction can be attributed with a high level of confidence to the component being tested, and not to any of its collaborators, because the collaborators have already been tested.&lt;/p&gt;
&lt;p&gt;I call this &lt;em&gt;&lt;strong&gt;Incremental Integration Testing&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;An alternative way of arriving at the idea of Incremental Integration Testing begins with the philosophical observation that strictly speaking, there is no such thing as a Unit Test; there always exist collaborators which by established practice we never mock and invariably integrate in Unit Tests without blinking an eye; these are, for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Many of the external libraries that we use.&lt;/li&gt;
&lt;li&gt;Most of the functionality provided by the Runtime Environment in which our software runs.&lt;/li&gt;
&lt;li&gt;Virtually all of the functionality provided by the Runtime Library of the language we are using.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nobody mocks standard collections such as array-lists, linked-lists, hash-sets, and hash-maps; very few people bother with mocking filesystems; nobody would mock a math library, a serialization library, and the like; even if one was so paranoid as to mock those, at the extreme end, nobody mocks the MUL and DIV instructions of the CPU; so clearly, there are always some things that we take for granted, and we allow ourselves the luxury of taking these things for granted because we believe that they have been sufficiently tested by their respective creators and can be reasonably assumed to be free of defects.&lt;/p&gt;
&lt;p&gt;So, why not also take our own creations for granted once we have tested them? Are we testing them sufficiently or not?&lt;/p&gt;
&lt;h3 id="prior-art"&gt;Prior Art
&lt;/h3&gt;&lt;p&gt;An internet search for &amp;quot;Incremental Integration Testing&amp;quot; does yield some results. An examination of those results reveals that they refer to some strategy for integration testing which is meant to be performed manually by human testers, constitutes an alternative to big-bang integration testing, and requires full Unit Testing of the traditional kind to have already taken place. I am hereby appropriating this term, so from now on it shall mean what I intend it to mean. If a context ever arises where disambiguation is needed, the terms &amp;quot;automated&amp;quot; vs. &amp;quot;manual&amp;quot; can be used.&lt;/p&gt;
&lt;p&gt;The first hints to Incremental Integration Testing can actually be found in the classic 1979 book &lt;em&gt;The Art of Software Testing&lt;/em&gt; by Glenford Myers. In chapter 5 &amp;quot;Module (Unit) Testing&amp;quot; the author plants the seeds of what later became white-box testing with mocks by writing:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[] since module B calls module E, something must be present to receive control when B calls E. A stub module, a special module given the name &amp;quot;E&amp;quot; that must be coded to simulate the function of module E, accomplishes this.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;then, the author proceeds to write:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The alternative approach is incremental testing. Rather than testing each module in isolation, the next module to be tested is first combined with the set of modules &lt;em&gt;&lt;strong&gt;that have already been tested.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(emphasis mine.)&lt;/p&gt;
&lt;p&gt;Back in 1979, Glen Myers envisioned these approaches to testing as being carried out by human testers, manually launching tests and receiving printouts of results to examine. He even envisioned employing multiple human testers to perform multiple tests in parallel. In the last several decades we have much better ways of doing all of that.&lt;/p&gt;
&lt;h3 id="implementing-the-solution-the-poor-mans-approach"&gt;Implementing the solution: the poor man's approach
&lt;/h3&gt;&lt;p&gt;As explained earlier, Incremental Integration Testing requires that when we test a component, all of its collaborators must have already been tested. Thus, Incremental Integration Testing necessitates exercising control over the order in which tests are executed.&lt;/p&gt;
&lt;p&gt;Most testing frameworks execute tests in alphanumeric order, so if we want to change the order of execution all we have to do is to appropriately name the tests, and the directories in which they reside.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;p&gt;Let us suppose that we have the following modules:&lt;/p&gt;
&lt;p&gt;com.acme.alpha_depends_on_bravo&lt;br&gt;
com.acme.bravo_depends_on_nothing&lt;br&gt;
com.acme.charlie_depends_on_alpha&lt;/p&gt;
&lt;p&gt;Note how the modules are listed alphanumerically, but they are not listed in order of dependency.&lt;/p&gt;
&lt;p&gt;Let us also suppose that we have one test suite for each module. By default, the names of the test suites follow the names of the modules that they test, so again, a listing of the test suites in alphanumeric order does not match the order of dependency of the modules that they test:&lt;/p&gt;
&lt;p&gt;com.acme.alpha_depends_on_bravo_&lt;strong&gt;tests&lt;/strong&gt;&lt;br&gt;
com.acme.bravo_depends_on_nothing_&lt;strong&gt;tests&lt;/strong&gt;&lt;br&gt;
com.acme.charlie_depends_on_alpha_&lt;strong&gt;tests&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To achieve Incremental Integration Testing, we add a suitably chosen prefix to the name of each test suite, as follows:&lt;/p&gt;
&lt;p&gt;com.acme.&lt;strong&gt;T02&lt;/strong&gt;_alpha_depends_on_bravo_tests&lt;br&gt;
com.acme.&lt;strong&gt;T01&lt;/strong&gt;_bravo_depends_on_nothing_tests&lt;br&gt;
com.acme.&lt;strong&gt;T03&lt;/strong&gt;_charlie_depends_on_alpha_tests&lt;/p&gt;
&lt;p&gt;Note how the prefixes have been chosen in such a way as to establish a new alphanumerical order for the tests. Thus, an alphanumeric listing of the test suites now lists them in order of dependency of the modules that they test:&lt;/p&gt;
&lt;p&gt;com.acme.T01_bravo_depends_on_nothing_tests&lt;br&gt;
com.acme.T02_alpha_depends_on_bravo_tests&lt;br&gt;
com.acme.T03_charlie_depends_on_alpha_tests&lt;/p&gt;
&lt;p&gt;At this point Java developers might object that this is impossible, because in Java, the tests always go in the same module as the production code, directory names must match package names, and test package names always match production package names. Well, I have news for you: they don't have to. The practice of doing things this way is very widespread in the Java world, but there are no rules that require it: the tests do not in fact have to be in the same module, nor in the same package as the production code. The only inviolable rule is that directory names must match package names, but you can call your test packages whatever you like, and your test directories accordingly.&lt;/p&gt;
&lt;p&gt;Java developers tend to place tests in the same module as the production code simply because the tools (maven) have a built-in provision for this, without ever questioning whether there is any actual benefit in doing so. Spoiler: there isn't. As a matter of fact, in the DotNet world there is no such provision, and nobody complains. Furthermore, Java developers tend to place tests in the same package as the production code for no purpose other than to make package-private entities of their production code accessible from their tests, but this is testing against the implementation, not against the interface, and therefore, as I have already explained, it is misguided.&lt;/p&gt;
&lt;p&gt;So, I know that this is a very hard thing to ask from most Java developers, but trust me, if you would only dare to take a tiny step off the beaten path, if you would for once do something in a certain way for reasons other than &amp;quot;everyone else does it this way&amp;quot;, you can very well do the renaming necessary to achieve Incremental Integration Testing.&lt;/p&gt;
&lt;p&gt;Now, admittedly, renaming tests in order to achieve a certain order of execution is not an ideal solution. It is awkward, it is thought-intensive since we have to figure out the right order of execution by ourselves, and it is error-prone because there is nothing to guarantee that we will get the order right. That's why I call it &amp;quot;the poor man's approach&amp;quot;. Let us now see how all of this could be automated.&lt;/p&gt;
&lt;h3 id="implementing-the-solution-the-automated-approach"&gt;Implementing the solution: the automated approach
&lt;/h3&gt;&lt;p&gt;Here is an algorithm to automate Incremental Integration Testing:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Begin by building a model of the dependency graph of the entire software system.&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;This requires system-wide static analysis to discover all components in our system, and all dependencies of each component. I did not say it was going to be easy.&lt;/li&gt;
&lt;li&gt;The graph should not include external dependencies, since they are presumed to have already been tested by their respective creators.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test each leaf node in the model.&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;A leaf node in the dependency graph is a node which has no dependencies; at this level, a Unit Test is indistinguishable from an Integration Test, because there are no dependencies to either integrate or mock.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;If any malfunction is discovered during step 2, then stop as soon as step 2 is complete.&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;If a certain component fails to pass its test, it is counter-productive to proceed with the tests of components that depend on it. Unit Testing seems to be completely oblivious to this little fact; Incremental Integration Testing fixes this.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Remove the leaf nodes from the model of the dependency graph.&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Thus removing the nodes that were previously tested in step 2, and obtaining a new, smaller graph, where a different set of nodes are now the leaf nodes.&lt;/li&gt;
&lt;li&gt;The dependencies of the new set of leaf nodes have already been successfully tested, so they are of no interest anymore: they are as good as external dependencies now.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Repeat starting from step 2, until there are no more nodes left in the model.&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Allowing each component to be tested in integration with its collaborators, since they have already been tested.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;No testing framework that I know of (JUnit, MSTest, etc.) is capable of doing the above; for this reason, I have developed a utility which I call &lt;em&gt;&lt;strong&gt;Testana&lt;/strong&gt;&lt;/em&gt;, that does exactly that.&lt;/p&gt;
&lt;p&gt;Testana will analyze a system to discover its structure, will analyze modules to discover dependencies and tests, and will run the tests in the right order so as to achieve Incremental Integration Testing. It will also do a few other nice things, like keep track of last successful test runs, and examine timestamps, so as to refrain from running tests whose dependencies have not changed since the last successful test run. For more information, see &lt;a
href="https://blog2.michael.gr/post/2024-10-testana/"
&gt;Testana: A better way of running tests&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="what-if-my-dependencies-are-not-discoverable"&gt;What if my dependencies are not discoverable?
&lt;/h3&gt;&lt;p&gt;Some very trendy practices of our modern day and age include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using scripting languages, where there is no notion of types, and therefore no way of discovering dependencies via static analysis.&lt;/li&gt;
&lt;li&gt;Breaking up systems into disparate source code repositories, so there is no single system on which to perform system-wide static analysis to discover dependencies.&lt;/li&gt;
&lt;li&gt;Incorporating multiple different programming languages in a single system, (following the polyglot craze,) thus hindering system-wide static analysis, since it now needs to be performed on multiple languages and across language barriers.&lt;/li&gt;
&lt;li&gt;Making modules interoperate not via normal programmatic interfaces, but instead via various byzantine mechanisms such as REST, whose modus operandi is binding by name, thus making dependencies undiscoverable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are following any of the above trendy practices, then you cannot programmatically discover dependencies, so you have no way of automating Incremental Integration Testing, so you will have to manually specify the order in which your tests will run, and you will have to keep maintaining this order manually.&lt;/p&gt;
&lt;p&gt;Sorry, but silly architectural choices do come with consequences.&lt;/p&gt;
&lt;h3 id="what-about-performance"&gt;What about performance?
&lt;/h3&gt;&lt;p&gt;One might argue that Incremental Integration Testing does not address one very important issue which is nicely taken care of by Unit Testing with Mocks, and that issue is performance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When collaborators are replaced with Mocks, the tests tend to be fast.&lt;/li&gt;
&lt;li&gt;When actual collaborators are integrated, such as file systems, relational database management systems, messaging queues, and what not, the tests can become very slow.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To address the performance issue I recommend the use of &lt;em&gt;&lt;strong&gt;Fakes&lt;/strong&gt;&lt;/em&gt;, not Mocks. For an explanation of what Fakes are, and why they are incontestably preferable over Mocks, please read &lt;a
href="https://blog2.michael.gr/post/2022-10-testing-with-fakes/"
&gt;Testing with Fakes instead of Mocks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;By supplying a component under test with a Fake instead of a Mock we benefit from great performance, while utilizing a collaborator which has already been tested by its creators and can be reasonably assumed to be free of defects. In doing so, we continue to avoid White-Box Testing and we keep defects localized.&lt;/p&gt;
&lt;p&gt;Furthermore, nothing prevents us from having our CI/CD server run the test of each component twice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Once in integration with Fakes&lt;/li&gt;
&lt;li&gt;Once in integration with the actual collaborators&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This will be slow, but CI/CD servers generally do not mind. The benefit of doing this is that it gives further guarantees that everything works as intended.&lt;/p&gt;
&lt;h3 id="benefits-of-incremental-integration-testing"&gt;Benefits of Incremental Integration Testing
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;It greatly reduces the effort of writing and maintaining tests, by eliminating the need for mocking code in each test.&lt;/li&gt;
&lt;li&gt;It allows our tests to engage in Black-Box Testing instead of White-Box Testing. For an in-depth discussion of what is wrong with White-Box Testing, see &lt;a
href="https://blog2.michael.gr/post/2021-12-white-box-vs-black-box-testing/"
&gt;White-Box vs. Black-Box Testing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;It makes tests more effective and accurate, by eliminating assumptions about the behavior of the real collaborators.&lt;/li&gt;
&lt;li&gt;It simplifies our testing operations by eliminating the need for two separate testing phases, one for Unit Testing and one for Integration Testing.&lt;/li&gt;
&lt;li&gt;It is unobtrusive, since it does not dictate how to construct the tests, it only dictates the order in which the tests should be executed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="arguments-and-counter-arguments"&gt;Arguments and counter-arguments
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Argument: Incremental Integration Testing assumes that a component which has been tested is free of defects.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A well-known caveat of software testing is that it cannot actually prove that software is free from defects, because it necessarily only checks for defects that we have anticipated and tested for. As Edsger W. Dijkstra famously put it, &amp;quot;program testing can be used to show the presence of bugs, but never to show their absence!&amp;quot;&lt;/p&gt;
&lt;p&gt;Counter-arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I am not claiming that once a component has been tested, it has been proven to be free from defects; all I am saying is that it can reasonably be assumed to be free from defects. Incremental Integration Testing is not meant to be a perfect solution; it is meant to be a pragmatic solution.&lt;/li&gt;
&lt;li&gt;The fact that testing cannot prove the absence of bugs does not mean that everything is futile in this vain world, and that we should abandon all hope in despair: testing might be imperfect, but it is what we can do, and it is in fact what we do, and practical, real-world observations show that it is quite effective.&lt;/li&gt;
&lt;li&gt;Most importantly: Any defects in an insufficiently tested component will not magically disappear if we mock that component in the tests of its dependents.
&lt;ul&gt;
&lt;li&gt;In this sense, the practice of mocking collaborators can arguably be likened to &lt;a class="external"
href="https://en.wikipedia.org/wiki/Ostrich_policy" target="_blank"
&gt;&lt;em&gt;Ostrich policy&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;On the contrary, continuing to integrate that component in subsequent tests gives us incrementally more opportunities to discover defects in it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Argument: Incremental Integration Testing fails to achieve complete defect localization.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If a certain component has defects which were not detected when it was being tested, these defects may cause tests of collaborators of that component to fail, in which case it will be unclear where the defect lies.&lt;/p&gt;
&lt;p&gt;Counter-arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is true that Incremental Integration Testing may fall short of achieving defect localization when collaborators have defects despite having already been tested. It is also true that Unit Testing with Mocks does not suffer from that problem when collaborators have defects; but then again, neither does it detect those defects. For that, it is necessary to always follow a round of Unit Testing with a round of Integration Testing. However, when the malfunction is finally observed during Integration Testing, we are facing the exact same problem that we would have faced if we had done a single round of Incremental Integration Testing instead: a malfunction is being observed which is not due to a defect in the root component of the integration, but instead due to a defect in some unknown collaborator. The difference is that Incremental Integration Testing gets us there faster.&lt;/li&gt;
&lt;li&gt;Let us not forget that the primary goal of software testing is to guarantee that software works as intended, and that defect localization is an important but nonetheless secondary goal. Incremental Integration Testing goes a long way towards achieving defect localization, but it may not achieve it perfectly, in favor of other conveniences, such as making it far more easy to write and maintain tests. So, it all boils down to whether Unit Testing represents overall more or less convenience than Incremental Integration Testing. I assert that Incremental Integration Testing is unquestionably far more convenient than Unit Testing.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Argument: Incremental Integration Testing only tests behavior; it does not check what is going on under the hood.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With Unit Testing, you can ensure that a certain module not only produces the right results, but also that it follows an expected sequence of steps to produce those results. With Incremental Integration Testing you cannot observe the steps, you can only check the results. Thus, the internal workings of a component might be slightly wrong, or less than ideal, and you would never know.&lt;/p&gt;
&lt;p&gt;Counter-arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is true, and this is why Incremental Integration Testing might be unsuitable for high-criticality software, where White-Box Testing is the explicit intention, since it is necessary to ensure not only that the software produces correct results, but also that its internals are working exactly according to plan. However, Incremental Integration Testing is not being proposed as a perfect solution, it is being proposed as a pragmatic solution: the vast majority of software being developed in the world is regular, commercial-grade, non-high-criticality software, where Black-Box Testing is appropriate and sufficient, since all that matters is that the requirements be met. Essentially, Incremental Integration Testing represents the realization that in the general case, tests which worry not only about the behavior, but also about the inner workings of a component, constitute over-engineering. For a more in-depth discussion about this, please read &lt;a
href="https://blog2.michael.gr/post/2021-12-white-box-vs-black-box-testing/"
&gt;White-Box vs. Black-Box Testing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;In order to make sure that everything is happening as expected under the hood, you do not have to stipulate in excruciating detail what should be happening, you do not have to fail the tests at the slightest sign of deviation from what was expected, and you do not have to go fixing tests each time the expectations change. Another way of ensuring the same thing is to simply:
&lt;ul&gt;
&lt;li&gt;Gain visibility into what is happening under the hood.&lt;/li&gt;
&lt;li&gt;Be notified when something different starts happening.&lt;/li&gt;
&lt;li&gt;Visually examine what is now different.&lt;/li&gt;
&lt;li&gt;Vouch for the differences being as expected.
For more details about this, see &lt;a
href="https://blog2.michael.gr/post/2023-01-06-collaboration-monitoring/"
&gt;Collaboration Monitoring&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Argument: Incremental Integration Testing prevents us from picking a single test and running it.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With Unit Testing, we can pick any individual test and run it. With Incremental Integration Testing, running an individual test of a certain component is meaningless unless we first run the tests of the collaborators of that component.&lt;/p&gt;
&lt;p&gt;Counter-arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Picking an individual test and running it is meaningless under all scenarios. It is usually done in the interest of saving time, but it is based on the assumption that we know what tests have been affected by the changes we just made to the source code. This is never a safe assumption to make.&lt;/li&gt;
&lt;li&gt;Instead of picking an individual test and running it, we need a way to automatically run all tests that have been affected by the changes we just made, which requires knowledge of the dependency graph of the system.&lt;/li&gt;
&lt;li&gt;If you are unsure as to exactly what you just changed, and exactly what depends on it, then consider using a tool like Testana, which figures all this out for you. See &lt;a
href="https://blog2.michael.gr/post/2024-10-testana/"
&gt;Testana: A better way of running tests&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Argument: Incremental Integration Testing requires additional tools.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Incremental Integration Testing is not supported by any of the popular testing frameworks, which means that in order to start practicing it, new tools are necessary. Obtaining such tools might be very difficult, if not impossible, and creating such tools might be difficult, because they would have to do advanced stuff like system-wide static analysis to discover the dependency graph of a system.&lt;/p&gt;
&lt;p&gt;Counter-arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;My intention is to show the way; if people see the way, the tools will come.&lt;/li&gt;
&lt;li&gt;I have already built such a tool which is compatible with some combinations of programming languages, build systems, and testing frameworks; see &lt;a
href="https://blog2.michael.gr/post/2024-10-testana/"
&gt;Testana: A better way of running tests&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Even in lack of tools, it is possible to start experimenting with Incremental Integration Testing today by following the poor-man's approach, which consists of simply naming the tests, and the directories in which they reside, in such a way that your existing testing framework will run them in the right order. This is described in the &amp;quot;poor man's approach&amp;quot; section of this paper.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="conclusion"&gt;Conclusion
&lt;/h3&gt;&lt;p&gt;Unit Testing was invented in order to achieve defect localization, but as we have shown, it constitutes White-Box Testing, so it is laborious, over-complicated, over-specified, and presumptuous. Furthermore, it is not even, strictly speaking, necessary. Incremental Integration Testing is a pragmatic approach which achieves almost the same degree of defect localization but without the use of mocks, and in so doing it greatly reduces the effort of developing and maintaining tests.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Cover image: &lt;em&gt;Incremental Integration Testing&lt;/em&gt; by michael.gr&lt;/p&gt;</description></item><item><title>White-Box vs. Black-Box Testing</title><link>https://blog2.michael.gr/post/2021-12-white-box-vs-black-box-testing/</link><pubDate>Wed, 22 Sep 2021 13:47:17 +0000</pubDate><guid>https://blog2.michael.gr/post/2021-12-white-box-vs-black-box-testing/</guid><description>&lt;p&gt;&lt;a class="external"
href="https://blogger.googleusercontent.com/img/a/AVvXsEjW7YbG075F9q253nTW0xSWiTzN9msG1BuZ2TgZO-mztpIBkgk9or5PoE2z-KhAx_WmfsT86Z5y6NFntAxir_gDF9PE3CCPaGQDmtx6ypfaffZirjHodfq1rM5SP8ONNI7AUmT3xHoijMhReRWmeHJZKjyPtZuyhHPAFJ6MrPUuPE7BXsGJ4gFWE36yyg=s1000" target="_blank"
&gt;&lt;img src="https://blog2.michael.gr/post/2021-12-white-box-vs-black-box-testing/media/tag-blogger.com,1999-blog-3494795920779884230.post-69667484864678488091.jpg"
width="640"
height="256"
srcset="https://blog2.michael.gr/post/2021-12-white-box-vs-black-box-testing/media/tag-blogger.com,1999-blog-3494795920779884230.post-69667484864678488091_hu_f50f246ab2524aaf.jpg 480w, https://blog2.michael.gr/post/2021-12-white-box-vs-black-box-testing/media/tag-blogger.com,1999-blog-3494795920779884230.post-69667484864678488091_hu_cde10918de1bcfd0.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="250"
data-flex-basis="600px"
&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I have something blasphemous to tell you.&lt;/p&gt;
&lt;p&gt;Unit Testing is wrong.&lt;/p&gt;
&lt;p&gt;There, I said it.&lt;/p&gt;
&lt;p&gt;I know I just insulted most people's sacred cow.&lt;/p&gt;
&lt;p&gt;Sorry, not sorry.&lt;/p&gt;
&lt;p&gt;I will explain, bear with me.&lt;/p&gt;
&lt;p&gt;(Useful pre-reading: &lt;a
href="https://blog2.michael.gr/post/2022-11-about-these-papers/"
&gt;About these papers&lt;/a&gt;)&lt;/p&gt;
&lt;h4 id="so-what-is-unit-testing-anyway"&gt;So, what is Unit Testing anyway?
&lt;/h4&gt;&lt;p&gt;Unit Testing, according to its definition, aims to examine a module in
isolation, to make sure that it behaves as expected without uncertainties
introduced by the behavior of other modules that it interacts with. These
other modules are known as dependencies. To achieve this, the test refrains
from connecting the module with its dependencies, and instead emulates the
behavior of the dependencies. That is what makes it a Unit Test, as opposed to
an Integration Test.&lt;/p&gt;
&lt;p&gt;The emulation of the dependencies is meant to be done in a very
straightforward and inexpensive way, because if it was complicated, then it
would introduce uncertainties of its own. So, if we were to imagine for a
moment that the math library is a dependency of the module under test, (just
for the sake of the example,) when the module under test asks for the cosine
of an angle, the Unit Test does not want to invoke the actual math library to perform the cosine
computation; instead, the Unit Test makes sure beforehand to supply the module
under test with such inputs that will cause the module to work with a known
angle of say 60 degrees, so the Unit Test can anticipate that the module will ask for the cosine of a 60 degree
angle, at which point the Unit Test will supply the module under test with a hard-coded value of 0.5,
which is known to be the cosine of 60 degrees. The Unit Test then proceeds to
make sure that the module does the right thing with that 0.5 and produces the
right results.&lt;/p&gt;
&lt;p&gt;In doing so, the Unit Test expects the module under test to interact with each of its
dependencies in a strictly predetermined way: a specific set of calls is
expected to be made, in a specific order, with specific arguments. Thus, the
unit test has knowledge of exactly how the module is implemented: not only the
outputs of the module must be according to spec, but also every single little
detail about the inner workings of the module must go exactly as expected.
Therefore, Unit Testing is white-box testing by nature.&lt;/p&gt;
&lt;h4 id="what-is-wrong-with-white-box-testing"&gt;What is wrong with White-Box Testing
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;White-box testing is not agnostic enough.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Just as users tend to test software in ways that the developer never
thought of, (the well known &amp;quot;works for me but always breaks in the hands
of the user&amp;quot; paradox,) software tests written by developers who maintain
an agnostic stance about the inner workings of the production code are
likely to test for things that were never considered by those who wrote
the production code.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;White-box testing is a laborious endeavor.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The amount of test code that has to be written and maintained often far
exceeds the amount of production code that is being tested.&lt;/li&gt;
&lt;li&gt;Each modification to the inner workings of production code requires corresponding modifications to the testing code, even if the interface and behavior of
the production code remains unchanged.&lt;/li&gt;
&lt;li&gt;With respect to procedural logic within the module under test, the Unit Test has to make sure
that every step of each workflow is followed, so the test essentially has to anticipate
every single decision that the module will make. This means that the test
duplicates all of the knowledge embodied within the module, and
essentially constitutes a repetition of all of the procedural logic of the
module, expressed in a different way. This problem has also been identified by others, and it is sometimes called the &amp;quot;over-specified tests problem&amp;quot;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;White-box testing suffers from &lt;em&gt;The Fragile Test Problem&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A bug fix in the production code more often than not causes tests to
break, which then have to be fixed. Note that this often happens even if we first write a test for the bug, which is expected to initially fail, and to start passing once the bug fix is applied: other previously existing tests will break. Unfortunately, it is often unclear to
what extent the tests are wrong, and to what extent the tests are right
but the production code suffers from other, dormant bugs, that keep
causing the tests to fail. When fixing tests as a result of bug fixes in
production, the general assumption is that the production is now correct,
therefore the test must be wrong, so the test is often hastily modified to make it pass
the existing production code. This often results in tests that &amp;quot;test
around&amp;quot; pre-existing bugs, meaning that the tests only pass if the bugs
are there.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;White-box tests are not re-usable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It should be possible to completely rewrite a piece of production code and
then reuse the old tests to make sure that the new code works exactly as
the old one did. This is impossible with white-box testing.&lt;/li&gt;
&lt;li&gt;It should be possible to write a test once and use it to test multiple
different implementations of a certain module, created by independently working
development teams taking different approaches to solving the same problem.
This is also impossible with white-box testing.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;White-box testing hinders refactoring.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quite often, refactorings which would affect the entire code base are unattainable because they would necessitate rewriting all unit tests, even if the refactorings themselves would have no effect on the behavior of the module, and would only require limited and harmless modifications to the production code, such as the case is when replacing one third-party library with another.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;White-box testing is highly presumptuous.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;White-box testing claims to have knowledge of exactly how the dependencies behave,
which may not be accurate. As an extreme example, the cosine of 60 is 0.5 only if that 60 is in degrees; if the cosine function of the actual math library used in production works with radians instead of degrees, then the result will
be something completely different, and the Unit Test will be achieving
nothing but ensuring that the module will only pass the test if it
severely malfunctions. In real-world scenarios the wrongful assumptions are much more subtle than a degrees vs radians discrepancy, making them a lot harder to detect and troubleshoot.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the preface of the book &lt;em&gt;&lt;strong&gt;The Art of Unit Testing&lt;/strong&gt;&lt;/em&gt; (Manning,
2009) by &lt;strong&gt;Roy Osherove&lt;/strong&gt;, the author admits to having participated in a
project which failed to a large part due to the tremendous development
burden imposed by badly designed unit tests which had to be maintained
throughout the duration of the development effort. The author does not go
into details about the design of those unit tests and what was so bad about
it, but I would dare to postulate that it was simply the fact that they were...
Unit Tests.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="is-white-box-testing-good-for-anything"&gt;Is white-box testing good for anything?
&lt;/h4&gt;&lt;p&gt;If you are sending humans to space, or developing any other high-criticality
system, then fine, go ahead and do white-box testing, as well as inside-out
testing, and upside-down testing, and anything else that you can think of,
because in high-criticality software, there is no amount of testing that
constitutes too much testing. However, the vast majority of software written
in the world today is not high criticality software, it is just plain normal,
garden variety, commercial software. Applying space-grade practices in the
development of commercial software does not make business sense, because
space-grade practices tend to be much more expensive than
commercial practices.&lt;/p&gt;
&lt;p&gt;In high criticality, it is all about safety; in commercial, it is all about
cost effectiveness.&lt;/p&gt;
&lt;p&gt;In high criticality, it is all about leaving nothing to chance; in commercial,
it is all about meeting the requirements.&lt;/p&gt;
&lt;h4 id="what-about-leaving-nothing-to-chance"&gt;What about leaving nothing to chance?
&lt;/h4&gt;&lt;p&gt;It is true that if you do black-box testing you
cannot be absolutely sure that absolutely everything goes absolutely as
intended. For example, you may be testing a module to ensure that given a
certain input, a certain record is written to the database.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you do white-box testing, you can ensure not only that the record has the correct content,
but also that the record is written once and only once.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you do black-box
testing, all you care is that at the end of the day, a record with the correct content can be found
in the database; there may be a bug which inadvertently
causes the record to be written twice, and you would not know.&lt;/p&gt;
&lt;p&gt;So, at this point some might argue that in promoting black-box testing I am
actually advocating imperfect software. Well, guess what: in the commercial sector, there is no such
thing as perfect software; there is only software that meets its requirements,
and software that does not. If the requirements are met, then some record being written twice is just a performance concern. Furthermore, it is a performance concern not only in the sense of the performance of the running software system, but also in the sense of the performance of your development process: By established practice, it is perfectly fine to knowingly allow a record to be written twice if eliminating this duplication would require too much development work to be worth it, so how is this any different from following an efficient development methodology which might allow that record to be written twice?&lt;/p&gt;
&lt;p&gt;This is in line with the observation that nobody aims to write software that is free from imperfections. Virtually
every single method that returns a collection in all of Java code written
since the dawn of time makes a safety copy of that collection; these safety
copies are almost always unnecessary, and yet people keep making them, because
they do not want to be concerned with what is safe and what is not safe on a
case by case basis; case-by-case addressing of safety concerns is the stuff
that bugs are made of. Software that is free of bugs is software that meets
the requirements, and that's all that counts.&lt;/p&gt;
&lt;p&gt;(Note: personally, I never make safety copies of collections; I use special
unmodifiable collection interfaces instead; but that's a different story.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="conclusion"&gt;Conclusion
&lt;/h4&gt;&lt;p&gt;In the book
&lt;em&gt;&lt;strong&gt;Design Patterns: Elements of Reusable Object-Oriented Software&lt;/strong&gt;&lt;/em&gt;
(Addison-Wesley, 1994) by &lt;em&gt;&lt;strong&gt;The Gang of Four&lt;/strong&gt;&lt;/em&gt; (Erich Gamma, Richard
Helm, Ralph Johnson, and John Vlissides) one of the principles listed is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Program against the interface, not against the implementation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Virtually all software engineers agree with this self-evident maxim, and
nobody in their right mind would take issue with it. To program against the
implementation rather than the interface is universally considered a misguided
practice.&lt;/p&gt;
&lt;p&gt;In the context of testing, the corollary to this maxim is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Test against the interface, not against the implementation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, do black-box testing, not white-box testing.&lt;/p&gt;
&lt;p&gt;This is not a unique idea of my own, others have had the same idea before, and have similar things to say. Ian Cooper in his &amp;quot;TDD, where did it all go wrong&amp;quot; talk states that in TDD a Unit Test is defined as a test that runs in isolation from other tests, not a test that isolates the unit under test from other units. In other words, the unit of isolation is the test, not the unit under test. Some excerpts from the talk are here: &lt;a class="external"
href="https://www.youtube.com/watch?v=HNjlJpuA5kQ" target="_blank"
&gt;Build Stuff '13: Ian Cooper - TDD, where did it all go wrong&lt;/a&gt; and the full talk is here: &lt;a class="external"
href="https://www.youtube.com/watch?v=EZ05e7EMOLM" target="_blank"
&gt;TDD, Where Did It All Go Wrong (Ian Cooper, 2017)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Other references:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class="external"
href="https://eng.libretexts.org/Bookshelves/Computer_Science/Programming_and_Computation_Fundamentals/Book%3A_Object-Oriented_Reengineering_Patterns_%28Demeyer_Ducasse_and_Nierstrasz%29/06%3A_Tests__Your_Life_Insurance/6.05%3A_Test_the_Interface_Not_the_Implementation" target="_blank"
&gt;Object-Oriented Reengineering Patterns (Demeyer, Ducasse, and Nierstrasz) - Tests - Your Life Insurance - 6.05 - Test the Interface Not the Implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="external"
href="https://web.archive.org/web/20180825215727/http://www.richardlord.net/blog/tdd/test-the-interface-not-the-implementation.html" target="_blank"
&gt;Richard Lord - Test the interface, not the implementation&lt;/a&gt; (via archive.org)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="if-not-unit-testing-then-what"&gt;If not Unit Testing, then what?
&lt;/h4&gt;&lt;p&gt;So, one might ask: if Unit Testing is wrong, then what should we be doing
instead? The original impetus behind the invention of Unit Testing still
remains: when we test a module we want to make sure that the observed behavior
is not affected by potential malfunction in its dependencies. How can we avoid that?&lt;/p&gt;
&lt;p&gt;The way I have been handling this in recent years is by means of a method that
I call &lt;em&gt;Incremental Integration Testing&lt;/em&gt;. You can read about it here: &lt;a
href="https://blog2.michael.gr/post/2022-10-incremental-integration-testing/"
&gt;Incremental Integration Testing&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>The case for software testing</title><link>https://blog2.michael.gr/post/2019-12-on-software-testing/</link><pubDate>Sun, 01 Dec 2019 20:48:53 +0000</pubDate><guid>https://blog2.michael.gr/post/2019-12-on-software-testing/</guid><description>&lt;p&gt;&lt;img src="https://blog2.michael.gr/post/2019-12-on-software-testing/media/test-pattern.webp"
width="960"
height="600"
srcset="https://blog2.michael.gr/post/2019-12-on-software-testing/media/test-pattern_hu_381d45a688135199.webp 480w, https://blog2.michael.gr/post/2019-12-on-software-testing/media/test-pattern_hu_d0ce67d078ac478f.webp 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="160"
data-flex-basis="384px"
&gt;
&lt;/p&gt;
&lt;h3 id="what-to-reply-to-a-non-programmer-who-thinks-that-testing-is-unnecessary-or-secondary"&gt;What to reply to a non-programmer who thinks that testing is unnecessary or secondary
&lt;/h3&gt;&lt;p&gt;At some point during his or her career, a programmer might come across the following argument, presented by some colleague, partner, or decision maker:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Since we can always test our software by hand, we do not need to implement Automated Software Testing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Apparently, I reached that point in my career, so now I need to debate this argument. I decided to be a good internet citizen and publish my thoughts. So, in this post I am going to be deconstructing that argument, and demolishing it from every angle that it can be examined. I will be doing so using language that is easy to process by people from outside of our discipline.&lt;/p&gt;
&lt;p&gt;(Useful pre-reading: &lt;a
href="https://blog2.michael.gr/post/2022-11-about-these-papers/"
&gt;About these papers&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;In the particular company where that argument was brought forth, there exist mitigating factors which are specific to the product, the customers, and the type of relationship we have with them, all of which make the argument not as unreasonable as it may sound when taken out of context. Even in light of these factors, the argument still deserves to be blown out of the water, but I will not be bothering the reader with the specific situation of this company, so as to ensure that the discussion is applicable to software development in general.&lt;/p&gt;
&lt;p&gt;In its more complete form, the argument may go like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Automated Software Testing represents a big investment for the company, where all the programmers in the house are spending copious amounts of time doing nothing but writing software tests, but these tests do not yield any visible benefit to the customers. Instead, the programmers should ensure that the software works by spending only a fraction of that time doing manual testing, and then we can take all the time that we save this way and invest it in developing new functionality and fixing existing issues.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To put it more concisely, someone might say something along these lines:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I do not see the business value in Automated Software Testing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This statement is a bunch of myths rolled up into an admirably terse statement. It is so disarmingly simple, that for a moment you might be at loss of how to respond. Where to begin, really. We need to look at the myths one by one. Here it goes:&lt;/p&gt;
&lt;h3 id="myth-1-software-testing-represents-a-big-investment"&gt;Myth #1: Software testing represents a &lt;em&gt;big&lt;/em&gt; investment.
&lt;/h3&gt;&lt;p&gt;No it doesn't. Or maybe it does, but its ROI is so high that you absolutely don't want to miss it.&lt;/p&gt;
&lt;p&gt;If you do not have software testing in place, then it is an established fact in our industry that you will end up spending an inordinate amount of time researching unexpected application behavior, troubleshooting code to explain the observed behavior, discovering bugs, fixing them, and often repeating this process a few times on each incident because the fix for one bug often creates another bug, or causes pre-existing bugs to manifest, often with the embarrassment of an intervening round-trip to the customer, because the &amp;quot;fixed&amp;quot; software was released before the newly introduced bugs were discovered.&lt;/p&gt;
&lt;p&gt;Really, it works the same way as education. To quote a famous bumper sticker:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You think education is expensive? Try ignorance!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Furthermore, your choice of Manual Software Testing vs. Automated Software Testing has a significant impact on the development effort required after the testing, to fix the issues that the testing discovers. It is a well established fact in the industry that the sooner a bug is discovered, the less it costs to fix it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The earliest time possible for fixing a mistake is when making it. That's why we use strongly typed programming languages, together with Integrated Development Environments that continuously compile our code as we are typing it: this way, any syntax error or type violation is immediately flagged by the IDE with a red underline, so we can see it and fix it before proceeding to type the next line of code. The cost of fixing that bug is near zero. (And one of the main reasons why virtually all scripting languages are absolutely horrible is that in those languages, even a typo can go undetected and become a bug.)&lt;/li&gt;
&lt;li&gt;If you can't catch a bug at the moment you are introducing it, the next best time to catch it is when running automated tests, which is what you are supposed to do before committing your changes to the source code repository. If that doesn't happen, then the bug will be committed, and this already represents a considerable cost that you will have to pay later for fixing it.&lt;/li&gt;
&lt;li&gt;The next best time to catch the bug is by running automated tests as part of the Continuous Build System. This will at least tell you that the most recent commit contained a bug. If there is no Continuous Build with Automated Software Testing in place, then you suffer another steep increase in the price that you will have to pay for eventually fixing the bug.&lt;/li&gt;
&lt;li&gt;By the time a human being gets around to manually testing the software and discovering the bug, many more commits may have been made to the source code repository. This means that by the time the bug is discovered, we will not necessarily know which commits contributed to it, nor which programmers made the relevant commits, and even if we do, they will at that moment be working on something else, which they will have to temporarily drop, and make an often painful mental context switch back to the task that they were working on earlier. Naturally, the more days pass between committing a bug and starting to fix it, the worse it gets.&lt;/li&gt;
&lt;li&gt;At the extreme, consider trying to fix a bug months after it was introduced, when nobody knows anything about the changes that caused it, and the programmer who made those changes is not even with the company anymore. Someone has to become intimately familiar with that module in order to troubleshoot the problem, consider dozens of different commits that may have contributed to the bug, find it, and fix it. The cost of fixing that bug may amount to more than a programmer's monthly salary.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is why the entire software industry today literally swears in the name of testing: it helps to catch bugs as early as possible, and to keep the development workflow uninterrupted, so it ends up saving huge amounts of money.&lt;/p&gt;
&lt;h3 id="myth-2-software-testing-represents-an-investment"&gt;Myth #2: Software testing represents an investment.
&lt;/h3&gt;&lt;p&gt;No, it does not even. Software testing is regarded by our industry as an integral part of software development, so it is meaningless to examine it as an investment separate from the already-recognized-as-necessary investment of developing the software in the first place.&lt;/p&gt;
&lt;p&gt;Beware of the invalid line of reasoning which says that in order to implement a certain piece of functionality all we need is 10 lines of production code which cost 100 bucks, whereas an additional 10 lines, that would only be testing the first 10 lines, and would cost an extra 100 bucks, are optional.&lt;/p&gt;
&lt;p&gt;Instead, the valid reasoning is that in order to implement said functionality we will need 20 lines of code, which will cost 200 bucks. It just so happens that 10 of these lines will reside in a subfolder of the source code tree called &amp;quot;production&amp;quot;, while the other 10 lines will reside in a subfolder of the same tree called &amp;quot;testing&amp;quot;; however, the precise location of each group of lines is a trivial technicality, bearing no relation whatsoever to any notion of &amp;quot;usefulness&amp;quot; of one group of lines versus the other. The fact is that all 20 of those lines of code are essential in order to accomplish the desired result.&lt;/p&gt;
&lt;p&gt;That's because production code without corresponding testing code cannot be said with any certainty to be implementing any functionality at all. The only thing that can be said about testless code is that it has so far been successful at creating the impression to human observers that its behavior sufficiently resembles some desired functionality. Furthermore, it can only be said to be successful to the extent that it has been observed thus far, meaning that a new observation tomorrow might very well find that it is doing something different.&lt;/p&gt;
&lt;p&gt;That's a far cry from saying that &amp;quot;this software does in fact implement that functionality&amp;quot;.&lt;/p&gt;
&lt;h3 id="myth-3-software-testing-is-just-sloppiness-management"&gt;Myth #3: Software testing is just sloppiness management.
&lt;/h3&gt;&lt;p&gt;This is usually not voiced, but implied. So, why can't programmers write correct software the first time around? And why on earth can't software just stay correct once written?&lt;/p&gt;
&lt;p&gt;There is a number of reasons for this, the most important ones have to do with the level of maturity of the software engineering discipline, and the complexity of the software that we are being asked to develop.&lt;/p&gt;
&lt;h4 id="maturity"&gt;Maturity
&lt;/h4&gt;&lt;p&gt;Software development is not a hard science like physics and math. There exist some purely scientific concepts that you learn in the university, but they are rarely applicable to the every day reality of our work. When it comes to developing software, there is not as much help available to us as there is to other disciplines by means of universal laws, fundamental axioms, established common practices and rules, ubiquitous notations, books of formulas and procedures, ready made commercially available standardized components to build with, etc. It is difficult to even find parallels to draw for basic concepts of science and technology such as experimentation, measurement, and reproducibility. That's why software engineering is sometimes characterized as being more of an art than a science, and the fact that anyone can potentially become a programmer without necessarily having studied software engineering does not help to dispel this characterization.&lt;/p&gt;
&lt;p&gt;Automated Software Testing is one of those developments in software engineering that make it more like a science than like an art. With testing we have finally managed to introduce the concepts of experimentation, measurement, and reproducibility in software engineering. Whether testability alone is enough to turn our discipline into a science is debatable, but without testing we can be certain that we are doing nothing but art.&lt;/p&gt;
&lt;h4 id="complexity"&gt;Complexity
&lt;/h4&gt;&lt;p&gt;The software systems that we develop today are immensely complex. A simple application which presents a user with just 4 successive yes/no choices has 16 different execution paths that must be tested. Increase the number of choices to 7, and the number of paths skyrockets to 128. Take a slightly longer but entirely realistic use case sequence of a real world application consisting of 20 steps, and the total number of paths exceeds one million. That's an awful lot of complexity, and so far we have only been considering yes/no choices. Now imagine each step consisting of not just a yes/no choice, but an entire screen full of clickable buttons and editable fields which are interacting with each other. This is not an extreme scenario, it is a rather commonplace situation, and its complexity is of truly astronomical proportions.&lt;/p&gt;
&lt;p&gt;Interestingly enough, hardware engineers like to off-load complexity management to the software. Long gone are the times when machines consisted entirely of hardware, with levers and gears and belts and cams all carefully aligned to work in unison, so that turning a crank at one end would cause printed and folded newspapers to come out the other end. Nowadays, the components of the hardware tend to not interact with each other, because that would be too complex and too difficult to change; instead, every single sensor and every single actuator is connected to a central panel, from which software takes charge and orchestrates the whole thing.&lt;/p&gt;
&lt;p&gt;However, software is not a magical place where complexity just vanishes; you cannot expect to provide software with complex inputs, expect complex outputs, and at the same time expect the insides of it to be nothing but purity and simplicity: a system cannot have less complexity than the complexity inherent in the function that it performs.&lt;/p&gt;
&lt;p&gt;The value of moving the complexity from the hardware to the software is that the system is then easier to change, but when we say &amp;quot;easier&amp;quot; we do not mean &amp;quot;simpler&amp;quot;; all of the complexity is still there and must be dealt with. What we mean when we say &amp;quot;easier to change&amp;quot; is that in order to make a change &lt;em&gt;&lt;strong&gt;we do not have to begin by sending new blueprints to the steel foundry&lt;/strong&gt;&lt;/em&gt;. That's what that you gain by moving complexity from the hardware to the software: being able to change the system without messy, time-consuming, and costly interactions with the physical world.&lt;/p&gt;
&lt;p&gt;So, even though we have eliminated those precisely crafted and carefully arranged levers and gears and belts and cams, their counterparts now exist in the software, you just do not see them, you have no way of seeing them unless you are a programmer, and just as the slightest modification to a physical machine of such complexity would be a strenuous ordeal, so is the slightest modification to a software system of similar complexity a strenuous ordeal.&lt;/p&gt;
&lt;p&gt;Software can only handle complexity if done right. You cannot develop complex software without sophisticated automated software testing in place, and even if you develop it, you cannot make any assumptions whatsoever about its correctness. Furthermore, even if it appears to be working correctly, you cannot make the slightest change to it unless automated software testing is in place to determine that it is still working correctly after the change. That is because you simply cannot test thousands or millions of possible execution paths in any way other than in an automated way.&lt;/p&gt;
&lt;h3 id="myth-4-testing-has-no-visible-benefit-to-the-customers"&gt;Myth #4: Testing has no visible benefit to the customers
&lt;/h3&gt;&lt;p&gt;Yes it does. It is called reliable, consistent, correctly working software. It is also called software which is continuously improving instead of remaining stagnant due to fear of it breaking if sneezed at. It is also called receiving newly introduced features without losing old features that used to work but are now broken. And it is even called receiving an update as soon as it has been introduced instead of having to wait until some poor souls have clicked through the entire application over the course of several days to make sure everything still works as it used to.&lt;/p&gt;
&lt;h3 id="myth-5-manual-testing-can-ensure-that-the-software-works"&gt;Myth #5: Manual testing can ensure that the software works.
&lt;/h3&gt;&lt;p&gt;No it cannot. That's because the complexity of the software is usually far greater than what you could ever possibly hope to test by hand. An interactive application is not like a piece of fabric, which you can visually inspect and have a fair amount of certainty that it has no defects. You are going to need to interact with the software, in a mind-boggling number of different ways, to test for a mind-boggling number of possible failure modes.&lt;/p&gt;
&lt;p&gt;When we do manual testing, in order to save time (and our sanity) we focus only on the subset of the functionality of the software which may have been affected by recent changes that have been made to the source code. However, the choice of which subsets to test is necessarily based on our estimations and assumptions about what parts of the program may have been affected by our modifications, and also on guesses about the ways in which these parts could behave if adversely affected. Alas, these estimations, assumptions, and guesses are notoriously unreliable: it is usually the parts of the software that nobody expected to break that in fact break, and even the suspected parts sometimes break in ways quite different from what anyone had expected and planned to test for.&lt;/p&gt;
&lt;p&gt;And this is by definition so, because all the failure modes that we can easily foresee, based on the modifications that we make, we usually examine ourselves before even calling the modifications complete and committing our code.&lt;/p&gt;
&lt;p&gt;Furthermore, it is widely understood in our industry that persons involved in the development of software are generally unsuitable for testing it. No developer ever uses the software with as much recklessness and capriciousness as a user will. It is as if the programmer's hand has a mind of its own, and avoids sending the mouse pointer in bad areas of the screen, whereas that is precisely where the user's hand is guaranteed to send it. It is as if the programmer's finger will never press that mouse button down as heavily as the user's finger will. Even dedicated testers start behaving like the programmers after a while on the job, because it is only human to employ acquired knowledge about the environment in navigating about the environment, and to re-use established known good paths. It is in our nature. You can ask people to do something which is against their nature, and they may earnestly agree, and they may even try their best, but the results are still guaranteed to suffer.&lt;/p&gt;
&lt;p&gt;Then there is repetitive motion fatigue, both of the physical and the mental kind, that severely limit the scope that any kind of manual testing will ever have.&lt;/p&gt;
&lt;p&gt;Finally, there is the issue of efficiency. When we do manual software testing, we are necessarily doing it in human time, which is excruciatingly slow compared to the speed at which a computer would carry out the same task. A human being testing permutations at the rate of one click per second could theoretically test one million permutations in no less than 2 working months, the computer may do it in a matter of minutes. And the computer will do this perfectly, while the most capable human being will do this quite sloppily in comparison. That's how inefficient manual software testing is.&lt;/p&gt;
&lt;h3 id="myth-6-manual-testing-takes-less-time-than-writing-tests"&gt;Myth #6: Manual testing takes less time than writing tests.
&lt;/h3&gt;&lt;p&gt;No it doesn't. If you want to say that you are actually doing some manual testing worth speaking of, and not a joke of it, then you will have to spend copious amounts of time doing nothing but that, and you will have to keep repeating it all over again every single time the software is modified.&lt;/p&gt;
&lt;p&gt;In contrast, with software testing you are spending some time up-front building some test suites, which you will then be able to re-execute every time you need them, with comparatively small additional effort. So, manual testing for a certain piece of software is an effort that you have to keep repeating, while writing automated test suites for that same piece of software is something that you do once and from that moment on it keeps paying dividends.&lt;/p&gt;
&lt;p&gt;This is why it is a fallacy to say that we will just test the software manually and with the time that we will save we will implement more functionality: as soon as you add a tiny bit of new functionality, you have to repeat the testing all over again. Testing the software manually is a never ending story.&lt;/p&gt;
&lt;p&gt;The situation is a lot like renting vs. buying: with renting, at the end of each month you are at exactly the same situation as you were in the beginning of the month: the home still belongs in its entirety &lt;strong&gt;not&lt;/strong&gt; to you, but to the landlord, and you must now pay a new rent in full, in order to stay for one more month. With buying, you pay a lot of money up front, and some maintenance costs and taxes will always be applicable, but the money that you pay goes into something tangible, it is turned into value in your hands in the form of a home that you now own.&lt;/p&gt;
&lt;p&gt;Furthermore, the relative efficiency of manual testing is usually severely underestimated. In order to do proper manual testing, you have to come up with a meticulous test plan, explaining what the tester is supposed to do, and what the result of each action should be, so that the tester can tell whether the software is behaving according to the requirements or not. However, no test plan will ever be as unambiguous as a piece of code that is actually performing the same test, and the more meticulous you try to be with the test plan, the less you gain, because there comes a point where the effort of writing the test plan starts being comparable to the effort of writing the corresponding automated test instead. So, you might as well write the test plan down in code to begin with.&lt;/p&gt;
&lt;p&gt;Of course one round of writing automated software testing suites will always represent more effort than a few rounds of manually performing the same tests, so the desirability of one approach vs. the other may depend on where you imagine the break-even point to be. If you reckon that the break-even point is fairly soon, then you already see the benefit of implementing automated software testing as soon as possible. If you imagine it will be after the IPO, then you might think it is better to defer it, but actually, even in this case you might not want to go this way, more about that later.&lt;/p&gt;
&lt;p&gt;Well, let me tell you: in the software industry the established understanding is that the break-even point is &lt;strong&gt;extremely&lt;/strong&gt; soon. Like &lt;strong&gt;write-the-tests-before-the-app&lt;/strong&gt; soon. (A practice known as Test-Driven Development.)&lt;/p&gt;
&lt;h3 id="myth-7-you-can-keep-developing-new-functionality-and-fixing-existing-issues-without-software-testing-in-place"&gt;Myth #7: You can keep developing new functionality and fixing existing issues without software testing in place.
&lt;/h3&gt;&lt;p&gt;In theory you could, but in practice you can't. That's because every time you touch the slightest part of the software, everything about the software is now potentially broken. Without automated software testing in place, you just don't know. This is especially true of software which has been written messily, which is in turn especially common in software which has been written without any Automated Software Testing in place from the beginning. Paradoxically enough, automated software testing forces software designs to have some structure, this structure reduces failures, so then the software has lesser testing needs.&lt;/p&gt;
&lt;p&gt;To help lessen change-induced software fragility, we even have a special procedure governing how we fix bugs: when a bug is discovered, we do not always just go ahead and fix it. Instead, what we often do is that we first write a test which checks for the bug according to the requirements, without making any assumptions as to what might be causing it. Of course, since the bug is in the software, the test will initially be observed to fail. Then, we fix the bug according to your theory as to what is causing it, and we should see that test succeeding. If it doesn?t, then we fixed the wrong bug, or more likely, we just broke something which used to be fine. Furthermore, all other tests better also keep succeeding, otherwise in fixing this bug we broke something else. As a bonus, the new test now becomes a permanent part of the suite of tests, so if this particular behavior is broken again in the future, this test will catch it.&lt;/p&gt;
&lt;p&gt;If you go around &amp;quot;fixing bugs&amp;quot; without testing mechanisms such as this in place, you are not really fixing bugs, you are just shuffling bugs around. The same applies to features: if you go around &amp;quot;adding features&amp;quot; without the necessary testing mechanisms in place, then by definition you are not adding features, you are adding bugs.&lt;/p&gt;
&lt;h3 id="myth-8-software-testing-has-no-business-value"&gt;Myth #8: Software testing has no business value
&lt;/h3&gt;&lt;p&gt;Yes it does. The arguments that I have already listed should be making it clear that it does, but let me provide one more argument, which shows how Automated Software Testing directly equates to business value.&lt;/p&gt;
&lt;p&gt;A potentially important factor for virtually any kind of business is investment. When an investor is interested in a software business, and if they have the slightest clue as to what it is that they are doing, they are likely to want to evaluate the source code before committing to the investment. Evaluation is done by sending a copy of the software project to an independent professional software evaluator. The evaluator examines the software and responds with investment advice.&lt;/p&gt;
&lt;p&gt;The evaluator may begin by using the software as a regular user to ensure that it appears to do what it is purported to do, then they may examine the design to make sure it makes sense, then they may examine the source code to make sure things look normal, etc. After spending not too much time on these tasks, the evaluator is likely to proceed to the tests. Software testing is so prevalent in the software industry, that it is unanimously considered to be the single most important factor determining the quality of the software.&lt;/p&gt;
&lt;p&gt;If there are no tests, this is very bad news for the investment advice.&lt;/p&gt;
&lt;p&gt;If the tests do not pass, this is also very bad news.&lt;/p&gt;
&lt;p&gt;If the tests succeed, then the next question is how thorough they are.&lt;/p&gt;
&lt;p&gt;For that, the evaluator is likely to use a tool called &amp;quot;Code Coverage Analyzer&amp;quot;. This tool keeps track of the lines of code that are being executed as the program is running, or, more likely, as the program is being exercised by the tests. By running the tests while the code coverage analysis tool is active, the evaluator will thus obtain the code coverage metric of the software. This is just a single number, from 0 to 100, and it is the percentage of the total number of source code lines that have been exercised by the tests. The more thorough the tests are, the higher this number will be.&lt;/p&gt;
&lt;p&gt;This is a very useful metric, because in a single number it captures an objective, highly important quality metric for the entirety of the software system. It also tends to highly correlate to the actual investment advice that the evaluator will end up giving. The exact numbers may vary depending on the product, the evaluator, the investor, the investment, and other circumstances, but a rough breakdown is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;below 50% means &amp;quot;run in the opposite direction, this is as good as Ebola.&amp;quot;&lt;/li&gt;
&lt;li&gt;50-60% means &amp;quot;poor&amp;quot;,&lt;/li&gt;
&lt;li&gt;60-70% means &amp;quot;decent&amp;quot;,&lt;/li&gt;
&lt;li&gt;70-80% means &amp;quot;good&amp;quot;,&lt;/li&gt;
&lt;li&gt;80-90% means &amp;quot;excellent&amp;quot;,&lt;/li&gt;
&lt;li&gt;90-100% means &amp;quot;exceptional&amp;quot;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, the graph of programming effort required vs. code coverage achieved is highly non-linear. It is relatively easy to pass the 45% mark; it becomes more and more difficult as you go past the 65% mark; it becomes exceedingly difficult once you cross the 85% mark.&lt;/p&gt;
&lt;p&gt;In my experience and understanding, conscientious software houses in the general commercial software business are striving for the 75% mark. In places where they only achieve about 65% code coverage they consider it acceptable but at the same time they either know that they could be doing better, or they have low self-respect. High criticality software (that human life depends on, or a nation's reputation,) may have 100% coverage, but a tremendous effort is required to achieve this. In any case, what matters is not so much what the developers think, but what the evaluator thinks; and evaluators tend to use the established practices of the industry as the standard by which they judge. The established practices call for extensive software testing, so if you do not do that, then your evaluation is not going to look good.&lt;/p&gt;
&lt;p&gt;So, is there business value in software testing? investment prospects alone say yes, regardless of the technical merits of it. Furthermore, software evaluation may likely be part of the necessary preparations for an IPO to take place, so even if you imagined the break-even point of automated testing vs. manual testing to be after the IPO, there is still ample reason to have them all in perfect working order well before the IPO.&lt;/p&gt;
&lt;p&gt;The above is applicable for businesses that are exclusively into software development. I do not know to what degree parallelisms can be drawn with companies for which software is somewhat secondary, but I suspect it is to no small extent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Old comments&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;metamaker 2020-11-14 23:03:33 UTC&lt;/p&gt;
&lt;p&gt;| ... in a single number it captures an objective, highly important quality metric for the entirety of the software system.&lt;/p&gt;
&lt;p&gt;I wished to find in an article more about spec tests (BDD, Gherkin). Code lines coverage is not always applicable, and even in the case of unit tests where it is applicable, branch+predicate coverage is as relevant as ever.&lt;/p&gt;
&lt;p&gt;So, devs end up with a need to convert use cases to autotests. I had a great Product Owner (C++ dev in past), who was writing Gherkin scripts inside Jira tickets xD, and team needed to just connect actions to words - then voila! we have autotests for use cases that a user encounters.&lt;/p&gt;
&lt;p&gt;The excuse - it is difficult to setup runner for specs. The solution - fire knaves, hire pros! :D&lt;/p&gt;
&lt;p&gt;| Software testing has no business value&lt;/p&gt;
&lt;p&gt;This is THE PLAGUE of modern software engineering - business decides how programmers should do their work. Moreover there is the BELIEF that writing bug-free code is easy. In the end of the day, software rot trumps all business decisions and team ends up with polluted unsupportable code. This is the one single reason why now I don't even consider job offers to random teams that have already 2-3 years old software - just too high risk to end up with already non-fixable $hitcode (was there, seen undocumented SQL scripts with 5000 LOC and zero documentation - never again).&lt;/p&gt;
&lt;p&gt;I wish everyone to end up sooner or later in a team with good practices and low stress! Stay good!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;metamaker 2020-11-14 23:03:27 UTC&lt;/p&gt;
&lt;p&gt;Yo Mike! Big kudos for the great article!&lt;/p&gt;
&lt;p&gt;(I am sure you know all that I want to mention in the next paragraphs, I just need to vent my thoughts and feelings after reading; hope it is thought-provoking, because thinking === GREATER GOOD)&lt;/p&gt;
&lt;p&gt;| ... so it ends up saving huge amounts of money.&lt;/p&gt;
&lt;p&gt;This is the good reason, but not the BEST (which I will mention below). Be like my old team when one day PM told that single calling code equals single country (+1 CC?), and someone wrote a component relying on this &amp;quot;well-known fact&amp;quot;. After half year we randomly found why some phone numbers were messed up. Ironically, the harm that was done to our company - zero bucks, we haven't lost anything due to this bug. We were b2b company that signed up other companies on board and thus really cared about having more sales and signed contracts rather than a good product.&lt;/p&gt;
&lt;p&gt;Tests are useless waste of time for company that is sales driven. Am I right? Or not so?&lt;/p&gt;
&lt;p&gt;The BEST thing for writing tests is that it documents expectations on code level (not biz, but for us, devs). If you ever need to fix something done by some random dude who now moved to Arctica, test is a good guidance (ofc, if that person wrote a good test and not some mocked up from top to bottom monster).&lt;/p&gt;
&lt;p&gt;A quick thought about code reviews. There is the BELIEF that reviews prevent bad code (poorly written tests including). In fact, I have never seen in my career teams where code reviews were helpful (but I was in a great team without no code reviews and permit to fix random places during the development - we trusted each other and cared about well being). If you have a good tech lead, but unsure about the rest of the team, for the God's sake, let tech lead be the only person who reviews code. By not doing so, if there is less than 51% of team are competent developers, you end up with political circus (been there, seen it, friends get LGTM for $hitcode, foes get comments like &amp;quot;change space, change quote, move comment to next line&amp;quot;; so... you end up making situational friends ;) - needless to say, what happens to code base).&lt;/p&gt;
&lt;p&gt;| ... every time you touch the slightest part of the software, everything about the software is now potentially broken.&lt;/p&gt;
&lt;p&gt;Not mentioning that since you are not a solo developer, you end up with merge conflicts due to other people work (even logical, e.g. Country class starts using 2-letter codes instead of 3-letter and uses same old good String for input of constructor - well, hope that your buddy added invariant to the class constructor). Tests synchronize decision making process, they autofix logical bugs between you and buddy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>On JUnit's random order of test method execution</title><link>https://blog2.michael.gr/post/2018-04-random-order-of-tests/</link><pubDate>Sun, 08 Apr 2018 20:15:05 +0000</pubDate><guid>https://blog2.michael.gr/post/2018-04-random-order-of-tests/</guid><description>&lt;p&gt;This is a rant about JUnit, or more precisely, a rant about JUnit's inability to execute test methods in natural method order.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition: Natural method order is the order in which methods appear in the source file.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="what-is-the-problem"&gt;What is the problem?
&lt;/h3&gt;&lt;p&gt;(Useful pre-reading: &lt;a
href="https://blog2.michael.gr/post/2022-11-about-these-papers/"
&gt;About these papers&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Up until and including Java 6, when enumerating the methods of a java class, the JVM would yield them in natural order. However, when Java 7 came out, Oracle changed something in the internals of the JVM, and this operation started yielding methods in random order.&lt;/p&gt;
&lt;p&gt;Apparently, JUnit was executing methods in the order in which the JVM was yielding them, so as a result of upgrading to Java 7, everybody's tests started running in random order. This caused considerable ruffling of feathers all over the world.&lt;/p&gt;
&lt;p&gt;Now, the creators of the Java language are presumably running unit tests just like everyone else, so they probably noticed that their own tests started running in random order before releasing Java 7 to the world, but apparently they did not care.&lt;/p&gt;
&lt;p&gt;Luckily, the methods are still being stored in natural order in the class file, they only get garbled as they are being loaded by the class loader, so you can still discover the natural method order if you are willing to get just a little bit messy with bytecode.&lt;/p&gt;
&lt;p&gt;However, that's too much work, and it is especially frustrating since the class loader is in a much better position to correct this problem, but it doesn't. (The class loader messes up the method order probably because it stores them in a HashMap, which yields its contents in Hash order, which is essentially random. So, fixing the problem would probably have been as simple as using a LinkedHashMap instead of a HashMap.)&lt;/p&gt;
&lt;p&gt;People asked the creators of JUnit to provide a solution, but nothing was being done for a long time, allegedly because &lt;em&gt;if You Do Unit Testing Properly?, you should not need to run your tests in any particular order, since there should be no dependencies among them&lt;/em&gt;. So, the creators of JUnit are under the incredibly short-sighted impression that if you want your tests to run in a particular order, it must be because you have tests that depend on other tests.&lt;/p&gt;
&lt;p&gt;When the creators of JUnit finally &lt;em&gt;did&lt;/em&gt; something to address the issue, (it did not take them long, only, oh, until Java 8 came out,) their solution was completely half-baked: the default mode of operation was &lt;em&gt;still&lt;/em&gt; random method order, but with the introduction of a special annotation one could coerce JUnit to run test methods either in alphabetic order, (which is nearly useless,) or in some other weird, ill-defined, so-called &amp;quot;fixed&amp;quot; order, which is not alphabetic, nor is it the natural order, but according to them it guarantees that the methods will be executed in the same order from test run to test run. (And is &lt;em&gt;completely&lt;/em&gt; useless.)&lt;/p&gt;
&lt;p&gt;So, apparently, the creators of JUnit were willing to do anything except the right thing, and even though JUnit 5 is said to have been re-written from scratch, the exact same problem persists.&lt;/p&gt;
&lt;h3 id="why-is-this-a-problem"&gt;Why is this a problem?
&lt;/h3&gt;&lt;p&gt;Well, let me tell you why running tests in natural method order is important:&lt;/p&gt;
&lt;p&gt;We tend to test fundamental features of our software before we test features that depend upon them, so if a fundamental feature fails, we want that to be the very first error that will be reported. (Note: it is the features under test that depend upon each other, not the tests themselves!)&lt;/p&gt;
&lt;p&gt;The test of a feature that relies upon a more fundamental feature whose test has already failed might as well be skipped, because it can be expected to fail, but if it does run, reporting that failure before the failure of the more fundamental feature is an act of sabotage against the developer: it is sending us looking for problems in places where there are no problems to be found, and it is making it more difficult to locate the real problem, which usually lies in the test that failed first &lt;strong&gt;in the source file&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To give an example, if I am writing a test for my awesome collection class, I will presumably first write a test for the insertion function, and further down I will write a test for the removal function. If the insertion test fails, the removal test does not even need to run, but if it does run, it is completely counter-productive to be shown the results of the removal test before I am shown the results of the insertion test. If the insertion test fails, it is game over. As they say in the far west, there is no point beating a dead horse. How hard is this to understand?&lt;/p&gt;
&lt;p&gt;Another very simple, very straightforward, and very important reason for wanting the test methods to be executed in natural order is because seeing the test method names listed in any other order is &lt;strong&gt;brainfuck&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blog2.michael.gr/post/2018-04-random-order-of-tests/media/grumpy-cat-random-order.jpg"
width="512"
height="512"
srcset="https://blog2.michael.gr/post/2018-04-random-order-of-tests/media/grumpy-cat-random-order_hu_1cbcf661f6886545.jpg 480w, https://blog2.michael.gr/post/2018-04-random-order-of-tests/media/grumpy-cat-random-order_hu_37da57f9abdefd51.jpg 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="100"
data-flex-basis="240px"
&gt;
&lt;/p&gt;</description></item></channel></rss>