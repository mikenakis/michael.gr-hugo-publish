[{"content":"\rWhile looking for a free web host for my new blog, (see New year, new blog!,) I did a bit of research and very quickly found Netlify and Cloudflare as the top choices. I was certain that one of them would suit my needs, so I stopped the research and proceeded to evaluate just those two. In this post I give a summary of my experience evaluating them and I explain my eventual choice.\nDisclaimer: This evaluation examines the suitability of Netlify vs. Cloudflare for the purpose of hosting a static web site, and specifically a personal blog, under a free plan. If what you want to do deviates even slightly from this stated purpose, then this evaluation may be irrelevant to you, perhaps even misleading.\nNetlify I decided to begin with Netlify, since it is a much smaller operation than Cloudflare, and it is in my character to (at least try to) avoid companies that are so big that they smell like monopolies. If Netlify would work for me, I would not even have to try Cloudflare. After all, let us not forget that the small(er) guy tends to have to do a bit better and deliver something extra in order to stay competitive, so it often happens that there is actual benefit, and not just moral justification, in choosing the small(er) guy.\nNetlify was relatively easy to register with and configure it to pull my static web site from GitHub. This is not to say that the interface of their web site is small and simple; on the contrary, it is unbelievably extensive and preposterously complicated; but it is significantly less complex than Cloudflare. (More on that later.) As a bonus point, their website worked without any issues using Firefox.\nHere is the problem with Netlify:\nEach time I make a change to my blog, no matter how small, and I push that change to GitHub, Netlify has to do this mysterious thing they call a \u0026quot;build\u0026quot;.\nFor a dynamic web site, with a front-end and a back-end, and actual code running at the back-end, there could conceivably be a need for something that might deserve to be called a \u0026quot;build\u0026quot;; but for a static web site, consisting only of HTML, CSS, and JavaScript files, a \u0026quot;build\u0026quot; is not just unnecessary; it is inapplicable as a notion. All they would need to do is pull from my GitHub repository into a directory that is being served by their web servers, and my change would be on air; but no, they insist on doing a \u0026quot;build\u0026quot;, because apparently that's how we like to waste computing resources nowadays.\nNow, it is their computing resources, so I could dismiss it as something that I just do not need to care about, but here is the problem:\nThey count this \u0026quot;build\u0026quot; against the quota of my free hosting plan. A Netlify \u0026quot;build\u0026quot; costs a whole 15 credits, but the free hosting plan comes with only 300 credits per month, so I can only push changes to my static web site 20 times in a month, even if each time the change is as small as adding a dot to an i.\nWell, sorry folks, but that just ain't gonna work. The allowed number of updates per month is so low that it is not just a matter of quota anxiety; it is simply unworkable. On a day that I publish a new blog post I may push 20 times within a single day. Most of those pushes are less than a kilobyte worth of data, (some may literally be putting a dot on an i,) so they should count as nothing, but Netlify makes such a big deal out of them, that their free plan becomes unusable. As the Greek expression goes, δώρον άδωρον: a gift that is no gift.\nSo, I had to give Cloudflare a try.\nCloudflare Cloudflare is a much bigger company than Netlify; In fact, Cloudflare is so big, that I would not be surprised to learn one day that Netlify runs on Cloudflare infrastructure.\nThe experience of registering with Cloudflare and setting it up to pull my static web site from GitHub was significantly more unpleasant than with Netlify. I did not think that was possible, but apparently, it is: their web site is even more preposterously complicated than Netlify. Finding the options page you want to navigate to is frustratingly difficult, just a notch short of impossible.\nAt some point during the process, (right after granting them access to my GitHub repository,) Cloudflare seemed to get stuck; the option to proceed to the next page was simply not there. I tried restarting the process from the beginning, I tried with or without cookies, with or without Dark Reader, but to no avail. Then finally I tried with a chromium-based browser, and it worked. So, apparently they do something so arcane in there, that they manage to be incompatible with Firefox.\nCloudflare also likes to babble about something they call a \u0026quot;build\u0026quot;, so they also keep flummoxing me with \u0026quot;build settings\u0026quot; and \u0026quot;build commands\u0026quot; and \u0026quot;build output directories\u0026quot;, which are all completely irrelevant and annoying to me, because all I am trying to do is publish a static website. I understand that there may be technical reasons due to which they may have to do stuff under the hood to make the magic happen, and they may be calling that stuff \u0026quot;build\u0026quot;, but I do not get to see that stuff, and I have no reason to care about that stuff, so they should keep that stuff transparent to me. (I get it, they don't make their money from static web sites, so people like me are irrelevant to them.)\nLuckily, unlike Netlify, Cloudflare does not use the \u0026quot;build\u0026quot; nonsense as a pretense to severely limit the availability of their free plan to me. Here are my preliminary findings about Cloudflare:\nEach time I make a change to my blog content and push to GitHub, Cloudflare pulls from GitHub and publishes my changes; when they do that, they charge me with one \u0026quot;worker build minute\u0026quot;. This \u0026quot;worker build minute\u0026quot; is some ridiculous intended-to-sound-technical but actually meaningless jargon that they have pulled out of their nose, and is essentially equivalent to what Netlify simply calls a \u0026quot;credit\u0026quot;, which is what I will also call it here. (The term \u0026quot;credit\u0026quot; is at least honest about the fact that it too, has been pulled out of someone's nose.)\nWell, Cloudflare's free plan gives me 3000 such credits per month, which means that I can push changes to my static web site 3000 times a month. That is 150 times more than Netlify, and enough to not only cover my needs, but also to spare me from quota anxiety.\nI think this settles it.\nMy blog will stay on Netlify for a little while longer, so that I can watch them side by side for a while to see if anything else pops up, and if all goes well, I will be switching from Netlify to Cloudflare at some point in the near future.\n","date":"2026-01-25T15:49:29+01:00","permalink":"https://blog.michael.gr/post/2026-01-25-netlify-vs-cloudflare/","title":"Netlify vs. Cloudflare"},{"content":"\rAs of today, I have a brand new blog. (You are reading it right now.) The old posts are still here, but the look and feel, and everything under the hood, is different. Everything except the domain name is still free for me.\nTo read why I transitioned from my old blog to this new blog, see Goodbye blogger!.\nThis post lists the technologies involved in editing and publishing my new blog.\nMarkdown All the blog content now is in Markdown files, which are just plain text files with some minimal formatting notation. This is very important to me, because it means that my blog content is now in an open format, as opposed to some vendor-specific format such as Blogger, WordPress, etc. Furthermore, since Markdown files are text files, they are extremely versatile: I can use any text editor to edit them, I can use text search tools, text transformation tools, I have unlimited search-and-replace capabilities, I can put them on version control, etc. I love Markdown. Everything should be in Markdown.\nObsidian For editing Markdown files I use Obsidian, a cross-platform desktop application which is proprietary but free. Obsidian gives me a very nice markdown editor, which, although not perfect, is probably the best in existence. Most importantly, Obsidian keeps all my links in sync, so I can freely rename content files and move them around without fearing that the links might be broken. Finally, Obsidian is just a Markdown editor; if a better Markdown editor was to appear one day, I could very easily switch to it, because nothing about my blog content or my editing and publishing process depends specifically on Obsidian.\nSVG My preferred image file format is SVG, which stands for Scalable Vector Graphics. Of course, in many situations raster images are necessary, and then I do of course use JPG, PNG, etc. but whenever I can, I use vector images in SVG format. I like SVG so much that on some occasions I have gone through the trouble using tracing software to vectorize raster images and create SVG images from them. This requires quite a bit of editing afterwards, so it is time consuming, but I like the small size and unlimited scalability of the result.\nInkscape For editing my SVG images I use Inkscape, a popular free and open-source editor for SVG images. As far as I know Inkscape is currently the best available option for SVG image editing, but if something better appears, I can switch to it in no time, because again, nothing about my content or my editing and publishing process depends specifically on Inkscape.\nGit For version control, I use Git. This allows me to view the history of all changes that I have ever made to my blog. Luckily, there is an Obsidian plugin that provides integration with git, so I can always see what files I have changed from within Obsidian, and I can commit with a single click.\nGitHub For hosting my git repository on the cloud, I use GitHub. To prevent my drafts from being publicly visible by anyone, the repository is private. Every once in a while I push my local working copy to origin, essentially creating a backup. This means that if something bad happens to my local computer, I have everything on GitHub, and if something bad happens to GitHub, I have everything on my local computer. GitHub is also necessary for automating the publishing progress, see below.\nHugo For converting the markdown files to a static website, I use Hugo, a free, open-source, cross-platform, command-line tool that is distinguished for being much faster than other tools that serve the same purpose. Hugo reads the entire directory containing my markdown files, plus another entire directory containing a \u0026quot;Hugo theme\u0026quot;, and produces a new directory containing a static website, consisting of HTML, CSS and JavaScript files. Hugo then remains running, acting as a local web-server, making my website available on http://localhost, so that I can view it using my web-browser. As an added bonus, each time I modify any of the content files or theme files, Hugo causes the browser to immediately show the change. Hugo can be replaced with something else, at the cost of learning a new theming mechanism and redoing theme customization.\nStack theme for Hugo The theme that I currently use is based on the free, open-source \u0026quot;Stack\u0026quot; theme for Hugo. I have customized the original theme to suit my needs, and my custom files also live on GitHub, alongside the content files. (Same repository but different directory.) When the original theme gets updated by its author, I use a git merge tool to combine the new changes with mine, resolving conflicts if necessary, just as I would do if I was merging program code. This theme could, in principle, be replaced with another, but that would require redoing the customization work.\nIntenseDebate For commenting I use IntenseDebate, which is a free service provided by Automattic (sic), the makers of WordPress and Gravatar. They provide a small HTML snippet which I have added at the right place in my theme, so it shows at the bottom of every post. The snippet handles everything comment-related: showing existing comments, adding a new comment, subscribing for notifications, etc. Blog commenting is all about spam control, which is a fairly complicated topic, and Automattic (sic) have a lot of experience with spam control, so I trust that they do it right. If it turns out that IntenseDebate does not suit my needs, I can easily replace it with something else.\nNetlify For serving my static website to the world, I currently have a free hosting plan with Netlify, a well known web hosting company headquartered in San Francisco, USA. This free plan seems to offer enough to suit my needs, and can certainly be upgraded in the future if my personal blog grows into a blogging empire or something. Once the static website has been generated by Hugo, and I decide that it is ready for publishing, I push it to yet another private GitHub repository, and from there Netlify automatically clones it, processes it, and makes it available to the world within a couple of seconds. If it turns out that Netlify does not suit my needs, I can move my blog elsewhere. I am currently also giving Cloudflare a try, which does the same thing, and I will, in all likelihood, be switching to Cloudflare and updating this post. (See Netlify vs. Cloudflare.)\nOf course, this is not to say that all of the above tools and technologies are without problems; I will probably be writing some grumpy post in the future listing the issues that I have with each of them. But for now, I am really enjoying the new looks of my blog.\nCover image: the logos of the technologies that I am currently using for blogging.\n","date":"2026-01-08T12:05:54+01:00","permalink":"https://blog.michael.gr/post/2026-01-08-new-blog/","title":"New year, new blog!"},{"content":"\rMy first attempt at blogging started in 2001, with a web-site entirely hand-made in HTML. It was crude, it had no commenting, and it was very difficult to maintain it, so it was necessarily tiny. Back then, finding a web host that was both free and would allow you to use your own domain name for free was quite difficult, so I had no option but to go with various questionable free hosting places. First it was united.net.kg. (Where kg is the top-level-domain of Kyrgyzstan.) Predictably, it one day just disappeared. Then, it was digitalrice.com. It had the same fate. Starting from 2007 and lasting until yesterday, I had been hosting my blog on blogger.com (formerly blogspot.com) which, luckily, has been stable.\nBlogger had several advantages:\nBeing backed by google, it was unlikely to disappear suddenly and without a warning. It was free. It would make use of my own domain-name for free. It made posting, editing, creating drafts, etc. reasonably easy. It provided a rudimentary commenting facility. It had a built-in search facility. It used a responsive theme so my blog looked decent on mobile too. It needed very little setup. It was reasonably customizable. It even allowed editing the actual HTML body of each post. Specifically, the ability to use my own domain name for free is why I chose Blogger over WordPress, which checks all the same boxes and more, but wants to charge me a few dollars per year to provide me with the luxury of having my own domain name point to my own blog. I get it that they somehow need to make money, I am not complaining, it is just that if I can have something for free, I would rather have it for free.\nUnfortunately, blogger has a number of shortcomings which have been bugging me for years:\nThe looks of my blog were kind of dated even right from the start, and that was almost twenty years ago. The (supposedly, but not really) WYSIWYG editor would foul up the HTML as part of its normal operation. The post editing page wasted an awful lot of screen real estate, providing a very small scrollable viewport for editing the post. Certain blog management tasks were very difficult, for example curating tags, and some were outright impossible, for example finding out which images are unused in order to delete them. Only a handful of file types were supported, and this did not include ZIP files. Not supporting zip files might be kind of understandable, but still, a great limitation. The commenting facility was very crude, resulting in a very poor user experience, which in turn resulted in many people never bothering to leave a comment. One user once told me that they tried to leave a comment but it was lost. Spam comments were a very frequent phenomenon, meaning that spam filters were either very poor or completely non-existent. Most importantly, blogger had certain shortcomings that were outright unacceptable:\nIt did not support images in SVG format. It did not maintain any kind of edit history of posts and drafts and it did not support fetching them from GitHub. It did not support markdown. It did absolutely nothing to maintain link integrity in posts containing links to other posts. It had been abandoned by google; they did not seem to be putting any work into it, so the shortcomings were never going to be addressed. Essentially, these shortcomings classified blogger as a platform to use only until a better alternative is found.\nSo, over the years, every once in a while I would look around for alternatives to blogger. However, I was not finding anything that did not suffer from even worse shortcomings. For example, at some point I tried medium.com, but I got very quickly annoyed by their delusional aspiration to charge people money for reading blog content, their preposterous passwordless sign-in mechanism, and their ridiculous restriction on how much content you are allowed to read before you have to sign-in, at which point you are forced to use their preposterous passwordless sign-in mechanism.\nThe breakthrough happened recently, when I discovered that nowadays there is a whole world out there, of people creating blogs as markdown files hosted on GitHub, which are then converted into static HTML websites using various tools, and then published on regular web hosting providers. So, I decided to give it a try.\nTo do this, I first had to convert my blog from blogger HTML to markdown. Blogger did in fact generate an export of my blog when I asked it to, but it was a very complicated set of data without a shred of documentation as to how to make sense out of it. I had to do some reverse engineering to figure things out, and then I had to write an application in C# which read the export, parsed the html, converted it to markdown, fixed links between posts, fixed links to media, processed and added visitor comments, etc. It was a huge amount of work. For any brave souls that might want to try their luck and follow my steps, the DotNet/C# project is here: https://github.com/mikenakis/MarkdownFromBlogger\nOnce I converted my blogger content to markdown and media files, the next step was to create my new blog from those. How I created it, and how I maintain it, is described in New year, new blog!.\nThe old blog is here, for posterity: blogger.michael.gr\nCover image: the blogger logo from wikimedia.org\n","date":"2026-01-07T08:33:59+01:00","permalink":"https://blog.michael.gr/post/2026-01-07-goodbye-blogger/","title":"Goodbye blogger!"},{"content":"\rIntroduction The term \u0026quot;technical debt\u0026quot; refers to messy source code, data, or architecture in a software system. It is commonly understood to represent a vague acknowledgement that the mess should probably be fixed by someone, somehow, sometime.\nPeople hearing the term \u0026quot;technical debt\u0026quot; for the first time are likely to guess what it means, in broad terms, and to understand that it is undesirable; however, the real detriment lies in a concept which, although alluded to by the term, is not spelled out, and therefore hidden. As a result, people often fail to grasp the grave implications of technical debt.\nThis post sheds light at the hidden concept and shows the real problem with technical debt.\n(Useful pre-reading: About these papers)\nDefinition Technical debt is the entropy that keeps accumulating in a software system as more and more features keep being added without setting aside the time necessary to reorganize the system so as to properly accommodate the addition of those features. It happens to virtually every software project, virtually every time a new feature is added, and it is one of the biggest problems in software development.\nOrigin The term was coined by Ward Cunningham in 1992, when he needed to explain the problem to an economist. He used financial debt as a metaphor, in order to speak in a language that the economist would understand.\nExplanation Adding a feature without first restructuring the system to properly accommodate the change saves time in the short term, so the feature can be rolled out faster; however, doing so introduces messiness, which makes it more difficult to add the next feature, and to do any kind of work on the system from that moment on.\nThis is like borrowing money to buy something right now instead of waiting until enough money has been set aside for the purchase: the problem is not only that the money will have to be paid back eventually; the problem is that interest now has to start getting paid on the borrowed money, and it has to keep getting paid every month, until the debt is paid back.\nThe real problem So, the problem with technical debt in software is not only that the restructuring of the software system will eventually have to happen; if that was the only problem, the restructuring could keep being postponed indefinitely; the problem is that day-to-day work on the entire system becomes more difficult, and keeps becoming more and more difficult with every added feature.\nConsequences Allowing technical debt to continue compounding without curtailing it can result in the following:\nIt can cause software projects to suffer, because there comes a point where the amount of effort required just for coping with technical debt is equal to the total work effort available, at which point very little gets done anymore. It can cause software projects to fail, because the work effort needed to reduce technical debt is also hampered by existing technical debt, so there comes a point where the project becomes unsalvageable and pretty much has to be thrown away and rewritten from scratch. Even before things get to that point, programmers start feeling unmotivated and unenthusiastic about making any change to the system, and sometimes declare features as impossible to implement, not because they are in principle impossible, but because they are untenable propositions in the current state of their system. Programmers start perceiving their job as a dreary chore, and may quit to find interesting and rewarding work elsewhere. Conclusion The real harm of technical debt lies in the extra effort needed to get any work done in a software system, like interest that has to be paid on financial debt. This interest is the concept which is alluded to but not spelled out in the term \u0026quot;technical debt\u0026quot;, and this is what needs to be understood for the realization to sink in that the debt must be paid off as often as possible, and as early as possible, instead of letting it linger on.\nSee also Technical debt in Wikipedia.\nCover image: vector image of Sisyphus pushing the rock up the hill, by michael.gr, based on raster image generated by ChatGPT from the prompt \u0026quot;please generate an image of Sisyphus pushing the rock up the hill; make the rock look more like a rock and less like a ball; make it landscape, in cozy coloring book style, black and white.\u0026quot; and \u0026quot;please make the hill less steep, and remove some detail to make the image more simple.\u0026quot;\n","date":"2025-09-17T07:28:57.733Z","permalink":"https://blog.michael.gr/post/2025-09-technical-debt/","title":"Technical debt"},{"content":"\rAbstract Two distinctly different widely used meanings of the term code refactoring are identified and named:\nChanging how code works, without changing the requirements that it fulfills (refactoring in the weak sense) Changing how code is expressed, without changing how it works (refactoring in the strong sense) (Useful pre-reading: About these papers)\nThe common understanding The term refactoring is commonly understood within the software engineering discipline to have the meaning documented by Martin Fowler in his Definition Of Refactoring post:\nRefactoring (noun): a change made to the internal structure of software to make it easier to understand and cheaper to modify without changing its observable behavior.\nIn the case of a software application, the observable behavior is in essence the set of requirements that it fulfills. (And also its look and feel: if you change the font, this is not refactoring.) In the case of a module, or a class, or an individual method, the observable behavior is the mapping of parameter values to results, contracts fulfilled, and side-effects, if any.\nLet us call these things requirements.\nNote that according to this widely used sense of refactoring, we are allowed to take any piece of code, throw it away, and replace it with an entirely different piece of code, and call what we just did refactoring, as long as requirements are still being fulfilled as before.\nThe trick is, of course, how can we tell, or who is to say, that requirements are still being fulfilled. It should come as no surprise that in many cases things do not go as intended:\nA change in the code might have some subtle consequences that we were not aware of, so we might be thinking that requirements are still fulfilled, while they are not. Requirements are never entirely unambiguous, so they might be fulfilled according to our interpretation, but not according to someone else's interpretation. Sometimes a combination of the above may occur. For example, the requirement to \u0026quot;create an empty file\u0026quot; might initially be fulfilled by two lines of code that create a binary file and immediately close it; then, one day, someone might decide to refactor those two lines of code by replacing them with a single invocation to a create-file-from-string function, passing it the empty string. One line of code is better than two lines of code, right? What could possibly go wrong? Well, if by \u0026quot;empty file\u0026quot; the requirements meant a file with no text in it, this refactoring was probably okay; however, if by \u0026quot;empty file\u0026quot; the requirements actually meant a zero-length file, then this refactoring may not have been okay, because the create-file-from-string function might, unbeknownst to us, create a file that contains a UTF8 BOM. This is an example of both things going wrong: the requirements were vague, and the \u0026quot;refactoring\u0026quot; had subtle unintended consequences.\nHopefully we have enough tests in place to catch such violations of the requirements, but this does not always work either, because the tests usually verify someone's interpretation of the requirements, and they usually do so only partially, since you cannot anticipate and test for every possible scenario.\nThe mathematical understanding There is another sense of refactoring that we are also familiar with: the sense used by Integrated Development Environments (IDEs) that perform useful transformations on code, such as renaming a variable, re-ordering the parameters of a function, etc.\nThe transformations performed by IDEs tend to adhere to the mathematical sense of refactoring: the code is transformed in such a way that the new code is equivalent to the old code, just as in mathematics the refactoring of x = 2y + 2z yields x = 2(y + z).\nNote that when we perform refactoring operations using an IDE, we usually feel no need to re-run the tests, because the new code works exactly as the old code, barring any bugs in the IDE, or any foolish hacks from our side, such as weak typing or binding by name.\nSumming it up The bottom line of all this is that the term refactoring is being widely used within the software engineering discipline to mean two distinctly different things. These two things are so different from each other as to warrant taking notice of this fact, and making the distinction explicit:\nThe weak (common) sense of refactoring:\nTransformations that (hopefully) result in no change in how requirements are fulfilled.\nThey are usually performed manually by the programmer, they may involve extensive changes in the way the code works, and they require thorough testing to guarantee that nothing was inadvertently broken.\nThe strong (mathematical) sense of refactoring:\nTransformations that result in code that is functionally equivalent to what it was before.\nThey are usually performed automatically by the IDE at the programmer's request, they tend to be limited or superficial, they tend to change how code is expressed but not how it works, and they typically do not need to be followed by a round of testing, because the code is typically guaranteed to behave the same way as before.\nAlso of interest is Martin Fowler's post on the Etymology of Refactoring.\n","date":"2025-09-12T10:50:44.462Z","permalink":"https://blog.michael.gr/post/2025-09-refactoring/","title":"Refactoring: strong vs weak"},{"content":"\rSummary Building upon the realization that conventional means of software design today amount to nothing more than fancy whiteboards, we examine the pitfalls, disadvantages, and consequences of designing software using such tools.\nThis post is support material for Towards Authoritative Software Design.\n(Useful pre-reading: About these papers)\nThe means The conventional means of software design today are:\nPen and paper Whiteboard General-purpose shape-drawing tools (e.g. Microsoft Word or PowerPoint drawings) or, in the best case,\nBox-and-arrow drawing applications that are smart enough to keep the arrows connected as we drag the boxes around the canvas. (e.g. Microsoft Visio) Unfortunately, even the box-and-arrow apps have no notion of what the boxes and the arrows stand for; thus, they cannot be used for anything other than modelling, which makes them nothing more than fancy whiteboards.\nOver the years, various tools and techniques have been proposed to better aid the software design process, including UML, but none of them amounts to anything more than a fancy whiteboard. For details, see The state of affairs in computer-aided software design.\nThe drawbacks One of the immediately noticeable consequences of having nothing but whiteboards at our disposal is the lack of a standardized notation: every architect is free to express concepts in any way they like, and anyone attempting to make sense out of their design has to undergo initiation rituals. (UML attempted to standardize notation, but it is a failure, see On UML.) As a result, software design as conventionally practiced is not easy to communicate. This is, however, the least of our problems.\nHere is a list of much more serious consequences of using whiteboards for technical software design:\nDesigns often include elements that are not well-defined in engineering terms.\nYou see this with designs containing supposedly technical but actually quite nebulous entities such as a Persistent Data Store here, a Messaging Backbone there, or a Remote Server over there. Such entities are not sufficiently well-defined to be suitable for inclusion in a technical design.\nDesigns often include elements that are completely outside the realm of engineering.\nYou see this with designs containing human figures representing users, pictures of money representing payments, etc. The presence of such items in a software design usually indicates a confusion between what is a technical design and what is a functional specification.\nDesigns often include elements from wrong levels of abstraction.\nYou see this with designs that mix software components with flowcharts, state diagrams, etc. Notwithstanding the fact that these are also boxes connected with arrows, they represent decision-making logic, which is an implementation detail of the component that contains that logic; as such, they have no place in a design.\nYou also see this with designs that confuse interfaces with other kinds of relationships between components, such as ownership, containment, inheritance, data flow, etc.\nDesigns are not informed with what elements are available for incorporation.\nThe medium on which software designs are conventionally expressed provides no technical means of establishing, or enforcing, a correspondence between a box as it appears in the design, and the actual provisionable, instantiatable, and runnable software module that it represents. This can be okay in the case of modules that have not been developed yet, but more often than not, a design intends to incorporate existing modules. In the absence of any technical means for informing the design about existing modules, the design inescapably represents hypotheses, assumptions, and approximations rather than fact.\nDesigns often prescribe invalid combinations of elements.\nThe ways in which conventional designs intend to interconnect components do not necessarily match the ways in which the components can actually be interconnected.\nA conventional design may assume that a certain component exposes or invokes a particular interface while in fact the component does not have such an interface. A conventional design may prescribe a connection between two components on a particular interface, while in fact the interface exposed by one component is not a valid match for the interface invoked by the other component. A conventional design may fancy a connection that goes from one component to another component which is inside a different container. In reality, such crossing of containment boundaries is impossible; especially if it involves different execution environments or different levels of scale, it is complete nonsense. Designs are often expressed at an unworkably high level of abstraction.\nThe level of abstraction necessary in order to guarantee the feasibility of a technical software design is that of the component diagram, which shows:\nThe individual components that make up the system. The interfaces implemented and/or invoked by each component. Connections from interface invocations to interface implementations. Unfortunately, since conventional means of software design are not informed about existing components and their interfaces, they do not have enough factual information at their disposal to delve into the level of detail necessary for a component diagram.\nFor this reason, the level of abstraction most commonly used by software architects is that of a block diagram, which might be suitable for abstract architectural work, but it is not detailed enough to give any guarantees about the feasibility of the proposed design.\nDesigns fail to capture dynamic aspects of software systems.\nConventional means of software design lack the ability to accurately express dynamic constructs such as:\nPlurality: Multiple instantiation of a certain component, where the number of instances is decided at runtime. Polymorphism: Fulfilling a certain role by instantiating one of several different component types capable of fulfilling that role, where the choice of which type to instantiate is made at runtime. Designs are often incomplete.\nA design may incorporate a component which needs to invoke a certain interface in order to get its job done, but omit incorporating a component implementing that interface. In such cases, the software system cannot be deployed as designed, and yet the architects are free to proclaim the design as complete.\nThe above long list of problems stems from the lack of technical means of informing the design with what is available, and restricting it to what is possible. This means that whiteboard designs allow the concoction of any chimera imaginable.\nFor further reading, please see Towards Authoritative Software Design.\nCover image: Illustration of a chimera by Christie L. Ward, from Wikimedia Commons, used under CC BY-SA 3.0\n","date":"2025-08-28T08:46:25.821Z","permalink":"https://blog.michael.gr/post/2025-08-the-perils-of-whiteboards/","title":"The perils of whiteboards"},{"content":"\rAbstract In this paper we examine the current state of affairs in Computer-Aided Software Engineering (CASE) tools specifically aimed at software design. We look at individual tools as well as entire categories of tools with respect to applicability and effectiveness, and we notice their abject inadequacy.\nThis post is support material for Towards Authoritative Software Design.\n(Useful pre-reading: About these papers)\nThe list Through the decades, many tools and methodologies have been developed with the intent of aiding the software architecture and design process. A common pattern in them is that they try to make some aspect of development more visual rather than textual. Here is a long, but non-exhaustive list:\nIntegrated Development Environments (IDEs) - They are limited to fancy source code editors, (see below,) form builders, (see below,) and visualization tools. (see below.) As such, they are implementation tools, not design tools.\nFancy Source Code Editors - No matter how fancy, they express implementations, not designs.\nForm Builders (a.k.a. graphical user interface (GUI) editors) - They only help define those small parts of a system that are visible by the user, and they can only specify their looks, not their functionality.\nVisualization Tools (e.g. class diagrams, dependency diagrams, call trees, etc.) - These are invariably restricted to the visualization of various aspects of existing systems, rather than the design of new systems, or the modification of existing systems. As such, they are reverse-engineering tools rather than design tools.\nCloud Infrastructure Visualization Tools (e.g. Lucidscale, Cloudviz, Hyperglance, etc.) - They suffer from the same drawbacks as visualization tools, and they are also limited to specific realms, i.e. the cloud environments of particular vendors.\nVisual Programming Languages (e.g Snap!, Scratch, EduBlocks, Blockly, etc.) - They are indeed visual, and they do in fact produce runnable software, but they are structurally equivalent to code, so they express implementations rather than designs. (See On Visual Programming Languages.)\nMicrosoft \u0026quot;Visual\u0026quot; Programming Languages (e.g. Microsoft Visual C++) - There is nothing visual about them; their name is just a marketing ploy. (See On Microsoft 'Visual' products.)\nDiagramming Software (e.g. Microsoft Visio) - They are good for making fancy diagrams, and they keep the arrows connected to the boxes as we drag the boxes around the canvas; however, they have no understanding of the actual meaning of those boxes and arrows, so they are restricted to modeling.\nModelling Languages and tools (e.g. Unified Modeling Language (UML), the Context, Containers, Components, Code (C4) Model, ArchiMate, etc.) - They aim to constrain what can be added to a design, but these constraints exist only in theory, because they are not enforced by any technical means. The primary aim of UML in particular was to standardize the notation of diagrams, but it is very cumbersome to work with, so it is largely considered dead. (See On UML.)\nSpecification languages (e.g. Specification and Description Language (SDL) - They do see some use in certain niche domains such as process control and real-time systems; however, they describe implementations rather than designs.\nWeb Services Description Language (WSDL) - It exclusively focuses on the narrow domain of web services, and therefore cannot be used for software design at large.\nBusiness Process Modelling (BPM) tools e.g. Business Process Execution Language (BPEL). - They do see some use in describing business processes within software systems, but not the software systems themselves.\nComponent-Based Software Engineering (CBSE) technologies, (e.g. Microsoft OLE) - Existing implementations suffer from critical limitations such as:\nAssuming the exclusive use of a particular operating system or operating environment. Assuming the exclusive use of a particular programming language. Requiring components to include debilitating amounts of bureaucracy. Forcing components to be heavyweight, sometimes as heavyweight as a process. Requiring components to communicate exclusively via a specific mechanism, such as asynchronous message passing, data streams, etc. Complete absence of visual design tools, even though such tools could, in principle, be developed. The shortcomings of these technologies are reflected in the limited extent of their adoption. Whatever meager adoption Microsoft OLE in particular enjoys can be attributed to coercion rather than merit: it is the only way to accomplish certain things under Microsoft Windows, so people use it because they have to, not because they want to. Nobody does OLE if they can avoid it.\nRapid Application Development (RAD) tools - They require the use of a particular programming language and a massive vendor-specific platform; they do not scale; they are aimed at easy creation of user-interface-centric applications rather than software design.\nNo-Code Development Platforms (NCDPs) and Low-Code Development Platforms (LCDPs) - They require a massive vendor-specific platform; they impose limitations on what can be done; they are aimed at non-programmers, allowing easy creation of simple, small-scale, user-interface-centric applications to quickly meet specific narrow business needs without having to write code.\nModel-Driven Engineering - Work in progress.\nInfrastructure definition tools (e.g. Terraform) - Work in progress.\nSysML - Work in progress.\nStructure101 - Work in progress.\nThe Open Group Architecture Framework (TOGAF) - Work in progres\nLattix - Work in progress.\nIBM Rational Rose - This was a suite of UML tools. It has been discontinued.\nRational Software Architect Designer - Work in progress.\nArchitecture Description Languages) - Work in progress. (Modelling languages without the modesty of admitting that they are limited to nothing but modelling?)\nFlow-based Programming (E.g. NoFlo) - Work in progress.\nOf all the technologies listed above, only diagramming tools (e.g. Visio) and modelling languages (e.g. UML) can legitimately be said to be of any potential usefulness in the software design process at large; however, these tools are restricted to modelling, and as such they are nothing more than fancy whiteboards.\nFor what is wrong with whiteboards, see The perils of whiteboards.\nFor what we should be doing instead, see Towards Authoritative Software Design.\nCover image: Created via ChatGPT with the prompt \u0026quot;please give me a photographic quality image of a 40-year-old programmer in a home-office environment looking at a computer screen with disappointment and resignation\u0026quot; and \u0026quot;please give me the same picture but his frown a bit less exaggerated\u0026quot;.\n","date":"2025-08-15T11:39:49.094Z","permalink":"https://blog.michael.gr/post/2025-08-the-state-of-affairs-in-computer-aided/","title":"The state of affairs in computer-aided software design"},{"content":"\rThe Static Universe The first model of cosmology that I was exposed to, as a small child, was that of the static universe, because that's all my father knew. According to that model, the universe is infinite in all directions, it is eternal, and it is not going anywhere. Of course I accepted it, because that is what kids do: accept everything presented to them as fact.\nThe Expanding Universe Then, during elementary school, I heard of the Big Bang and the expansion of the universe; this new model of cosmology made me feel a bit uncomfortable, but again I accepted it, because a) that's what the scientists said, and b) I could go to my father and tell him that he is wrong.\nThe Cosmological Red-Shift Later, during my junior high-school years, I heard of the red-shift of galaxies, as simply the Doppler effect applied to light, and it sounded reasonable, but then I heard of cosmological red-shift, as the stretching of photon wavelengths due to expansion of space itself. That sounded very exotic to me. I was again willing to accept it, because scientists had said so, but I was now starting to feel uncomfortable, because I am not into pretending to understand things that I do not truly understand. (Furthermore, my father had no concept of such things, so I could not even start a discussion with him with the goal of showing him that he is wrong.)\nThe Accelerating Expansion Then I started hearing of truly bizarre observations, such as that galaxies in all directions around us are red-shifted, meaning that they are all moving away from us. When I want to comically express my bewilderment at this, I raise my arms, smell my armpits, and say \u0026quot;why, what's wrong with us?\u0026quot;\nI learned that according to red-shift observations, the farther away a galaxy is, the faster it is moving away from us, and that not only the galaxies are moving away from each other, but the rate at which they are moving is increasing, meaning that galaxies are accelerating away from each other. Again I accepted all that because scientists had said so, but by that time it was already clear to me that I was accepting these things only because I had no other option. A little something in me was finding all this uncomfortable to the point of being disturbing.\nDark Matter and Dark Energy The apex of my discomfort came during my last year of senior high-school, when I started hearing of dark matter and dark energy; popular media portrayed them as discoveries, but a closer examination of what the scientists were actually saying revealed that they were nothing but fictitious notions that scientists had dreamt up in order to explain the observations, otherwise the numbers did not add up. In other words, the scientists had given names to their ignorance. And it was not just some small discrepancies that these novelties were called to correct for; dark matter and dark energy were supposed to account for about 95% of the universe. On the surface I was okay with this, because again, scientists had said so, but that little something in me had now started finding all this quite preposterous and becoming genuinely disturbed.\nStop for a moment and think how arrogant this is on our behalf: we are saying \u0026quot;here are our observations of the universe,\u0026quot; and \u0026quot;here are our theories to explain those observations,\u0026quot; and \u0026quot;by the way, our theories only explain 5% of the observations, the remaining 95% is inexplicable.\u0026quot; And instead of admitting that our theories are worthless, we take it for granted that they are correct, and we assume that there must be some other, as of yet completely unknown, and completely unobserved mechanisms at play, which account for the missing 95%, so that our theories end up being correct. To top it all off, we come up with a couple of cool names for those mechanisms, to make the public think that we actually have a clue as to what we are talking about.\nAnd wait, that's not all; we are even saying that other than exerting gravitational pull, dark matter does not seem to interact with regular matter, not even with itself, so it is virtually unobservable. This inevitably brings to mind the skeptic argument about the invisible, untouchable, and odorless realm of the supernatural as being virtually indistinguishable from the non-existent. To put it in more scientific terms, is this shit even falsifiable?\nMy Idea All this was so disturbing to me, that in order to retain my sanity I was forced to invest some thinking into it, despite the hopelessness of the endeavor, given that I have no formal background in physics, let alone astrophysics; my field is, and has always been, software engineering.\nMy thinking was, necessarily, simple:\nThere is no doubt that relative movement of galaxies in space is at least partially responsible for the observed red-shift, just as it is responsible for the few cases of observed blue-shift. For example the Andromeda galaxy is blue-shifted, because it is on a collision course with the Milky Way. However, this does not mean that relative movement is necessarily responsible for all of the observed red-shift. Can we find some additional means of explaining red-shift so that we might account for those manifestations of it that are truly bizarre?\nI tried to think what it means to say that the fabric of space is expanding. How do we know that it is expanding? Could it be that the only way that we know that it is expanding is due to the observed red-shift? If so, then what is the difference, really, between saying that the fabric of space is expanding, and saying that the frequency of light that is dropping? How do we know that it is one and not the other?\nThus, I arrived at my hypothesis: If light happens to have an as of yet unknown property which causes its frequency to ever so slightly decrease as it travels through the vast expanses of intergalactic space, then clearly, the result would appear as red-shift. That would beautifully explain at least two observations that are currently receiving disturbing explanations:\nLight reaching us from any galaxy has to travel a long distance, therefore all galaxies should appear red-shifted, as they indeed do, even though they might be more or less stationary with respect to us. This is a much simpler explanation than to say that all galaxies are running away from us as if we have smelly armpits. Light reaching us from distant galaxies has to travel a longer distance than from nearby galaxies, therefore more distant galaxies should appear more red-shifted, as they indeed do. This is a much simpler explanation than to say that galaxies are not only moving away from us, but actually accelerating away from us. I knew that my idea had at least a tiny bit of merit due to Occam's razor: sure, I was postulating the existence of some hypothetical property of light that causes its frequency to decrease as it travels through space, but:\nThis is not a terribly implausible thing to suppose, and\nIt seems far less implausible than proposing an expanding universe, an expanding fabric of space, plus dark matter and dark energy to make the numbers add up.\nSimply put, Occam's razor says that if two hypotheses have equal explaining power, the simpler one wins. What remained to be seen was whether my hypothesis did actually have equal explaining power as the established scientific theories. I suspected it did not, due to reasons beyond my level of comprehension of the universe, but I had no one to ask, and therefore no way of knowing.\nI had this idea in my late teens, and not knowing what to do with it, I stored it in the back of my mind and went on with my life.\nAsking around When I was at California State University, San Bernardino I had a cool professor who came to the classroom one day with an inflatable balloon that had galaxies printed on it. By inflating the balloon, she showed how the expansion of the fabric of space can explain the observation that all galaxies are seemingly moving away from us, as well as from each other; I could accept this, but it only took care of the smelly armpits problem; the highly exotic notion of an expanding fabric of space was still there, the troublesome acceleration of the expansion was also still there, and the preposterous notions of dark matter and dark energy were still there. So, I felt that my hypothesis had lost virtually none of its explanatory merit.\nDuring the mid 2000s, in the golden era of internet blogs, and before facebook destroyed everything, I happened to have access to an audience of a few thousand people with higher than average education. A sizable portion of them were necessarily scientists, so a few of them were bound to be astrophysicists. This prompted me to describe my idea on my blog and ask if anyone had heard of anything similar to it. Sure enough, a couple of folks stepped up to inform me that my idea had already been considered by scientists, and that although not completely devoid of scientific merit, it was regarded as false, and it had pretty much been proven wrong by experimental observations. It even had a name: The Tired Light Hypothesis.\nThat was an elating moment for me; there I was, a lay-person with respect to astrophysics, who had, at a relatively young age, independently arrived at a hypothesis that had also been considered by actual astrophysicists. I figured that was a substantial accomplishment, even if the hypothesis did not turn out to be correct.\nOf course it would have been even more elating if they had said \u0026quot;hey, wow, guess what, we never thought of this, and you know what? you might actually be right!\u0026quot; but let us be serious, what are the chances of that happening?\nFringe Theories I am weary of the phenomenon of fringe theories, where certain ideas seem to exert an irresistible appeal to non-scientists, causing misinformation to spread. I also happen to be somewhat partial to another fringe theory, which is the Aquatic Ape Hypothesis, and while delving into it back in the early '90s I witnessed the controversy that it stirred: large numbers of non-biologists loving it, virtually every biologist wishing that it would just die so that people would stop pestering them with it. When David Attenborough endorsed the hypothesis, the scientific community very nearly cancelled him. Even I, despite the fact that I liked the hypothesis, thought that it had been kind of risky on his behalf to endorse it. So, I am fully aware of the fact that these are fringe theories, and I am not insisting on anything, I am just amused at how much explanatory power they have. I regard them as very interesting curiosities.\nIn conclusion I cannot help but mention that it gives me a warm, fuzzy feeling to keep note, somewhere in the back of my mind, of the fact that Continental Drift was also, for a long time, regarded by the scientific community as nothing but a fringe theory.\nSee Wikipedia: Static Universe\nSee Wikipedia: Big Bang\nSee Wikipedia: Expansion of the universe\nSee Wikipedia: Tired Light\nSee Wikipedia: Fringe Theory\nCover image: \u0026quot;Tired Light\u0026quot; vector graphic by michael.gr created with InkScape using parts of \u0026quot;Sinusoid\u0026quot; by Yohann Berger from the Noun Project and \u0026quot;Stars\u0026quot; by Ken Murray from the Noun Project\n","date":"2025-07-06T12:50:13.448Z","permalink":"https://blog.michael.gr/post/2025-07-tired-light-hypothesis/","title":"On the Tired Light Hypothesis"},{"content":"GitLab allows the insertion of images in code review comments. They make it really easy: if you have an image on the clipboard, you can just paste it into a comment. I suppose the feature exists so that programmers can exchange screenshots, graphs, etc. to explain complicated matters during code review.\nI like to use this feature to post memes.\nHere is my collection of the most useful code review memes:\n","date":"2025-06-06T16:33:33.591Z","permalink":"https://blog.michael.gr/post/2025-06-code-review-images/","title":"Code review memes"},{"content":"\rAbstract The need is identified for programmatically ascertaining, in languages like C# and Java, the immutability of certain objects used in situations where they are expected to be immutable. The technicalities of immutability assessment are discussed. A mechanism is described for achieving it.\n(Useful pre-reading: About these papers)\nThe Problem Raise your hand if you have ever had to troubleshoot a bug that manifested itself in mysterious ways, defied rational explanation, tenaciously evaded detection, made you rage at the absurdity of the observed behavior, and after much weeping and wailing and gnashing of teeth, turned out to be due to one of the following reasons:\nInadvertently mutating an object that has been added as a key in a hash map.\nInadvertently mutating an object that has been passed to another thread.\nin general:\nOne piece of code mutating an object that another piece of code groundlessly assumes that it remains unchanged. These mishaps of course happen due to the fact that the objects involved should have been immutable, but they were not. If an object is immutable, nobody can mutate it, and therefore nobody has to assume that it will not change.\nSo, could hash maps somehow require that their keys be immutable? Could threads somehow require that objects shared among them be immutable?\nThis leads us to the more general question of how to ascertain immutability, which is certainly not an easy task. Most programmers don't even consider it; few talk about it; even fewer attempt it. Programmers all over the world are accustomed to routinely using objects in situations where immutability is an absolute requirement, but without ever ascertaining it, essentially praying that the objects be immutable.\nCompiler-Enforced Immutability Inadvertent mutation is not a problem in purely functional programming languages, where there simply is no such thing as mutation. However, most programmers do not use such languages, because they are cumbersome to work with. Most programmers use languages like Java and C#, which are not purely functional, so they allow mutation, and so inadvertent mutation can sometimes happen.\nJava and C# do support a few constructs for defining invariable (final/readonly) class members, but they are woefully inadequate. Systematic compiler support for declaring and requiring immutability would greatly help to reduce the volume of mistakes being made, but nothing like that exists, and even if it did exist, it would not be a panacea, because there are situations where the compiler cannot help.\nSince compiler-enforced immutability is not available, we have to enforce it ourselves, which means that we have to programmatically detect immutability and ascertain it.\nLanguages like Java and C# offer full reflection support, so we can examine every field of every type, (static analysis,) and we can even examine the values of fields of instances. (Dynamic analysis.) Furthermore, these languages compile into intermediate code, which is relatively easy to parse and reason about, meaning that we can even analyze executable code if we want to. (More static analysis.)\nSo, the question is what to analyze, and how.\nSuperficial vs. Deep Immutability Many classes have the term \u0026quot;immutable\u0026quot; in their name, but they are only superficially immutable. Take a generic immutable collection or example: ImmutableCollection\u0026lt;T\u0026gt;. Let us trust that it does in fact behave perfectly immutably, and therefore it does, arguably, deserve to be called immutable; let us now ask: would an instance of this class be safe to pass to another thread? The answer is that it depends on the actual type of the generic parameter: If T is immutable, it is safe; but if T is mutable, then it is absolutely not safe.\nSo, in order to reap any benefits whatsoever from immutability, it must be deep immutability. Shallow immutability is irrelevant. Please keep this in mind, as it has severe implications in our quest to ascertain the immutability of anything.\nStatic Analysis The term \u0026quot;static analysis\u0026quot; refers to examining the code that makes a program, (as written, or as compiled,) but not the state of the program as it runs. Consequently, static analysis can examine the definitions of data structures, but not the actual contents of those data structures during runtime.\nA popular but naïve understanding of immutability is that it is an inherent characteristic of types, and that the instances of the types (i.e. the objects) simply follow suit. According to this understanding, all we need to do is to ascertain that a certain type is immutable, and from that moment on we know that all of its instances are immutable.\nThis understanding is not entirely false, but it is very limiting, because it means that only concrete and non-extensible (a.k.a. final, sealed) types can potentially be assessed as immutable: All interfaces must necessarily be considered as mutable, because we have no idea how they may be implemented, and all abstract or simply extensible types must also necessarily be considered as mutable, because we have no idea how they may be extended.\nThis poses an insurmountable problem if we wanted to have, say, a queue for exchanging messages between threads, where the messages are organized in a class hierarchy: such a queue would not be able to ascertain the immutability of the messages it handles, because all it knows is the base-most 'Message' class, which is necessarily extensible, and therefore mutable, as far as static analysis can tell.\nNow, consider that many perfectly immutable classes tend to be passed around as interfaces, (e.g. Comparer, Hasher, Predicate, all sorts of stateless converters, etc.) that these interfaces are often stored in fields, and that a field of mutable type makes the class containing that field also mutable. It quickly becomes evident that static analysis can only work in a universe where no abstraction is utilized; however, we do not live in such a universe: we make use of languages like Java and C# precisely because we want the benefits of unlimited abstraction.\nOne final nail in the coffin of static analysis is the issue of delayed immutability.\nDelayed Immutability Some objects begin life as mutable, so that they can undergo some non-trivial initialization, and become immutable later, once initialization is complete. This behavior is necessary when creating cyclic graphs of immutable objects, or when creating an immutable object while loading its contents from some external storage. (Alternative terms used by others for this kind of immutability are Freezing and Popsicle immutability.)\nThere is no standard way of representing delayed immutability, so let me propose one real quick:\nLet there be a SelfAssessing interface, which is to be implemented by any class that utilizes delayed immutability. This interface is to have just one method, IsImmutable(), which is expected to return false for as long as the object is mutable, and to start returning true once the object becomes immutable.\nNote that static analysis is by nature limited to examining types, but delayed immutability requires invoking a method of an instance of a type. Thus, static analysis completely fails to assess delayed immutability. Furthermore, a delayed immutable may appear as a field in any type, meaning that static analysis fails to assess potentially any type.\nSince static analysis fails in the presence of abstraction and/or delayed immutability, it follows that we have to examine not just types, but also the instances of types in the running software system. This calls for dynamic analysis.\nDynamic Analysis The term \u0026quot;dynamic analysis\u0026quot; refers to examining various aspects of a software system as it runs. In some cases the aim is to examine the behavior of the software, in other cases (such as the case at hand) it is to examine the data structures it creates. Dynamic analysis may require (and in the case at hand it does require) static analysis as a prerequisite.\nWith dynamic analysis we can look past the advertised type of a field, which may be abstract, and obtain the instance stored in the field, (the value of the field,) in order to find out the actual, concrete type of that instance.\nOnce we have the concrete type of an instance, we can assess whether it is immutable, and this may involve recursively assessing any instances referenced by that instance. If everything is immutable, then and only then can the containing instance assessed as immutable.\nTo make all of this work, we begin with static analysis where we use reflection to examine a type with the goal of giving it one of three possible assessments:\nMutable Immutable Inconclusive These type assessments are issued as follows:\nThe mutable type assessment is issued if: The type has any fields that are variable, (non-final/non-readonly,) because such fields are mutable no matter what their advertised type (field type) is. The type has nothing but invariable fields, but one or more of them is of an advertised type that has received a mutable assessment, because this means that the containing type is not deeply immutable. The immutable type assessment is issued if a type consists exclusively of fields that are both invariable and of an advertised type which has received an immutable assessment. The inconclusive type assessment is issued if: The type is abstract or extensible (non-final/non-sealed.) The type is self-assessing. The type contains any fields of an advertised type that has in turn received an inconclusive assessment. Note that the above are type assessments, issued on types, by static analysis alone.\nEvery instance of a type that has received a mutable or immutable assessment is in turn mutable or immutable without the need to examine the contents of the instance; however, every instance of a type that has received an inconclusive assessment must be further examined to issue a final assessment for that instance only.\nThe value of each field must be obtained from the instance, and assessment must recursively be applied on that value. If the type is self-assessing, then the IsImmutable() method must be invoked on the instance, to ask it whether it is immutable or not. Both type assessment and instance assessment can be expensive; however, note the following:\nOnce a type assessment has been issued, it will never change, so it can be cached, and never recomputed again. Instance assessments can be requested only from within assertions, meaning that they can incur zero runtime overhead on production. Note that for static analysis we employed nothing but reflection to examine the fields of a type, and for dynamic analysis we also employed nothing but reflection to examine the values of fields of instances, so no code analysis was necessary. However, for the sake of completeness, let us also take a brief look at code analysis.\nCode analysis There is a school of thought according to which the answer to the immutability assessment question lies in analyzing the executable instructions that comprise a type to determine whether any fields are mutated by code outside of the constructor.\nThe problem with code analysis is that it is a form of static analysis, so it suffers from the disadvantages of static analysis that were previously explained.\nSuppose that code analysis determines that a type does not mutate any fields outside of its constructor; suppose, however, that the type contains a field of abstract type, which gets initialized from a constructor parameter; is this type mutable or immutable? Obviously, it depends on the concrete type of the instance that will be stored, at runtime, in that field. So, we are back at square one, where static analysis simply does not work in the face of abstraction. Therefore, code analysis is not the answer.\nCode analysis could potentially be useful, as a supplement to dynamic analysis, in the following ways:\nIn some cases, a type contains a field which is written by a method other than the constructor. For this to work, the field has to be variable. (Non-final/non-readonly.) Thus, with the use of reflection alone, this type will be assessed as mutable. However, it may be that the method which writes the field makes sure that the field is only written once during the lifetime of the instance, and that it gets written before it is ever read, so it will never appear to mutate as far as external observers can tell. Thus, the type is effectively immutable. It is in theory possible (though not easy) for code analysis to detect that the field is treated in this way, thus allowing the type to be assessed as immutable. Sometimes a type contains fields that are only written by the constructor, but the programmer who wrote that type forgot to declare them as invariable (final/readonly) and did not pay attention to the warnings / inspections / analysis messages. If we were to only use reflection, these fields would be considered variable, so the type would in turn be assessed as mutable. Code analysis can detect that the fields are not written outside of the constructor, allowing them to be assessed as invariable, and therefore the type to be assessed as immutable. Preassessment There exist types that would normally receive a mutable assessment, but we know for sure that they are practically immutable. A famous example of such a type, both in Java and in C#, is class String. In such cases, we must be able to preassess the type as immutable, which means to assign an immutable assessment to the type, without analyzing it.\nNote that preassessment constitutes a promise, and promises can be false. If a type which is actually mutable is mistakenly preassessed as immutable, bad things are bound to happen.\nGeneric Shallow Preassessment Some generic types are effectively immutable containers. In Java, which uses type erasure, these are essentially containers of elements of type object, so they are by definition inconclusive; however, in C# the type of the generic type argument is known at runtime, so we do better than that. When a generic effectively immutable container type is constructed with an actual type parameter, the immutability of the resulting type depends on the immutability of that parameter:\nIf the generic type parameter is a mutable type, then the constructed generic container type is mutable, so instances of that type do not need to be assessed. If the generic type parameter is an immutable type, then the constructed generic container type is immutable, so again, instances of that type do not need to be assessed. If the generic type parameter is inconclusive, then the constructed generic container type is inconclusive, which means that for every instance of that type, all elements in the container must be assessed. In order to be able to assess the elements of a container, the preassessment for the container must include an object known as a deconstructor. Dynamic analysis will be invoking the deconstructor to enumerate the elements contained within each instance of the container, so that each element can be assessed. Deconstructors are generally trivial:\nThe deconstructor for collections simply yields all the elements of the collection. The deconstructor for maps/dictionaries simply yields all the mappings. (Map entries / key-value pairs.) The deconstructor for Lazy\u0026lt;T\u0026gt; simply yields the one and only value contained within the lazy object. Preassessment is mainly intended for types that have been defined by others, and thus we cannot modify their source code. For types that we write ourselves, we want a finer level of control: we want to be able to override the assessment of specific fields only, and allow all other fields to be assessed the normal way, to catch situations where we thought that some field was immutable, while in fact assessment of that field shows that it is not immutable. For that, we need field overrides.\nField Overrides Sometimes a field is variable, but we want to promise that we will only vary it in an effectively immutable way. For such cases, there must be an annotation/attribute that we can attach to that field, to indicate that analysis should treat the field as invariable.\nArray Field Overrides Arrays are by definition mutable in Java and C#, and by extension so is any type that contains an array field, even if the field itself is invariable. If we want to be able to create an immutable type that contains an array field, there must be an annotation/attribute that we can attach to that array field, to indicate that analysis should treat the array itself as invariable.\nElucidation Once we have immutability assessment working as described in the preceding sections, a new challenge becomes apparent: sometimes, a data structure that was intended to be immutable will be assessed as mutable due to some tiny programmer mistake. If the data structure is large and complex, it might not be obvious where the mistake is. The programmer will receive a mutable assessment, but will not know why it was given and where to look to find the problem.\nFor this reason, every mutable instance assessment must come with a sentence explaining to the programmer why the assessment was issued. Since every mutable instance assessment typically has one or more other assessments that are the reasons that led to it, these sentences will often form entire trees, each sentence being further explained by nested sentences.\nI call this feature elucidation.\nFurther reading Eric Lippert's must-read post about the different kinds of immutability: Immutability in C# Part One: Kinds of Immutability\nAppendix Immutability assessment is awesome, but the more the compiler can do for us, the better.\nHere are some examples of what compilers of (non-purely functional) programming languages could be doing for us in the direction of compiler-enforced immutability:\nA language could support an 'immutable' class modifier, which would require the class to contain only immutable members. An immutable class may not extend a mutable class, and a mutable class may not extend an immutable class. (Although a mutable class may extend a class which has not been marked as immutable, even if that class happens to be immutable.) A language could support an 'immutable' modifier for function arguments and for fields, requiring that they may only be assigned from concrete types that are immutable, or from other fields or function arguments that are also immutable. A language could support an 'immutable' generic parameter constraint, which would mandate that only immutable types can be used as generic type arguments. A language could support a 'stable' field modifier, allowing a mutable field to appear in an immutable class, and acting as a promise that the field will only be mutated in a way which upholds effective immutability. A language could support a 'stable array' field modifier for array fields, allowing an array to appear in an immutable class, and acting as a promise that the contents of the array will either not be mutated, or they will only be mutated in a way which upholds effective immutability. etc. Cover image from Oleksandr Panasovskyi from The Noun Project\nScratch\n(Ignore)\nAs it turns out, the mutability of value types is largely irrelevant, as explained here: Vladimir Sadov: \u0026quot;C# Tuples. Why mutable structs?\u0026quot;\n","date":"2025-06-02T13:16:41.894Z","permalink":"https://blog.michael.gr/post/2025-06-immutability-assessment/","title":"Immutability Assessment"},{"content":"Summary Type names as reported by the dotnet runtime are in a cryptic, non-human-readable format. Attempts by many to solve this problem have generally been naive, incomplete, and clunky. A library that gets the job done right is presented.\n(Useful pre-reading: About these papers)\nThe problem PEARL: In dotnet, the System.Type.Name and System.Type.FullName properties return type names in a cryptic format which is not human-readable and bears very little resemblance to the names of the same types as they appear in C# source code.\nTry this:\nConsole.WriteLine( typeof( int ).Name ); Instead of this:\nint you get this:\nInt32 Try this:\nclass Outer { public class Inner; } Console.WriteLine( typeof( Outer.Inner ).FullName ); Instead of this:\nOuter.Inner you get this:\nOuter+Inner Try this:\nConsole.WriteLine( typeof( Dictionary\u0026lt;,\u0026gt; ).Name ); Instead of this:\nDictionary\u0026lt;,\u0026gt; you get this:\nDictionary`2 Try this:\nConsole.WriteLine( new Dictionary\u0026lt;int,System.DateTime\u0026gt;().GetType().FullName ); Instead of this:\nSystem.Collections.Generic.Dictionary\u0026lt;int, System.DateTime\u0026gt; you get this:\nSystem.Collections.Generic.Dictionary`2[[System.Int32, System.Private.CoreLib, Version=8.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e],[System.DateTime, System.Private.CoreLib, Version=8.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]] Try this:\nclass C { public int? Field; } Console.WriteLine( typeof( C ).GetField( \u0026#34;Field\u0026#34; )!.FieldType.FullName ); Instead of this:\nint? you get this:\nSystem.Nullable1[[System.Int32, System.Private.CoreLib, Version=8.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]] The examples above demonstrate the following problems:\nThe language keywords for built-in types are not used; instead, the raw CLR type names are used. Names of nested classes are delimited with a plus-sign instead of a period. Names of generic types do not use angle-bracket notation; instead, they are suffixed with a back-quote character, followed by the number of generic parameters they accept. The original generic type parameter names are nowhere to be found. Names of constructed generic types are further suffixed with a list of fully qualified (assembly-qualified) type names, one for each generic type argument. Names of nullable value types do not use the question-mark shorthand notation; instead, the System.Nullable\u0026lt;T\u0026gt; type is fully spelled out. Obviously, these cryptic names are intended to be parsed by software, not by humans. Additionally, there are many different programming languages in the dotnet ecosystem, each with its own syntax for types, so the chosen notation does not favor any particular language.\nThat's all very fine, and it should not really be a problem, because among the tens of thousands of APIs built into dotnet, there must surely be one for obtaining the human-readable name of a type in C# notation, right?\nright?\nWell, unfortunately, no.\nThere is no such thing built into dotnet.\nThe closest there is to it involves referencing the System.CodeDom assembly (by Microsoft) and writing the following code:\nvar cSharpCompiler = new Microsoft.CSharp.CSharpCodeProvider(); var typeRef = new System.CodeDom.CodeTypeReference( type ); string typeName = cSharpCompiler.GetTypeOutput( typeRef ); This will give you namespace-qualified human-readable type names in C# notation, and it will even replace names of built-in types with their corresponding language keywords, so it will give you int instead of System.Int32, which is nice. However, even then, the generated type names suffer in the following ways:\nIf you wanted the underlying CLR type names instead of the language keywords, you can't have them. If you wanted type names without namespaces, you can't have them. The question-mark shorthand notation for nullable value types is not used, so you get System.Nullable\u0026lt;int\u0026gt; instead of int?. Tuple notation is not used, so you get ValueTuple\u0026lt;int,char\u0026gt; instead of (int,char). Generic type arguments are stripped from generic type definitions, so you get System.Collections.Generic.Dictionary\u0026lt;,\u0026gt;. If you wanted the full original generic type definition, which is System.Collections.Generic.Dictionary\u0026lt;TKey,TValue\u0026gt;, you can't have it. Although language keywords are used for most built-in types, they are not used for nint and nuint, which appear as System.IntPtr and System.UIntPtr respectively. None of the above behavior can be changed, because the mechanism is not customizable.\nPrior attempts to solve the problem Many have asked for a function that, given a type, returns its human-readable name in C# notation, and many have tried to offer such a function.\nOn Stack Overflow there is not just one, but several questions asking this, or variations of it, and each question has received several (attempted) answers:\nStack Overflow: Get GenericType-Name in good format using Reflection on C# Stack Overflow: Given a type instance, how to get generic type name in C#? Stack Overflow: C#: \u0026quot;Pretty\u0026quot; type name function? Stack Overflow: Get user-friendly name of simple types through reflection? Stack Overflow: Is there a way to get a type's alias through reflection? Stack Overflow: How can I get the primitive name of a type in C#? Stack Overflow: C#, trying to create human readable type name On GitHub there is an open-source project which aims to do this:\nGitHub: BanallyMe / ReadableTypeNames\nSomeone has even created a video on YouTube aiming to explain how to do this:\nYouTube: vlogize: Generating a Human Readable Type Name in C#: Solving the ICollection Challenge\nNeedless to say, all of the above attempts are incomplete and clunky. Some people provide functions that only work for a specific set of types, such as List\u0026lt;\u0026gt; and Dictionary\u0026lt;,\u0026gt; but fail for everything else; others provide functions that try to work for any type, but fail in all sorts of edge cases, and even not-so-edge cases, for example with arrays or with nested types. Even the best solutions fail to cover all cases. Furthermore, virtually all of the code in these solutions is of very poor quality, engaging in excessive string searching, string substitution, string concatenation, etc. It is really so messy that it cannot be improved; it has to be thrown away and re-written from scratch.\nThe solution I hereby present to the world this open-source project that I created:\nGitHub: MikeNakis / MikeNakis.CSharpTypeNames\nIt is a tiny library that generates human-readable dotnet type names in C# notation, and it does it right.\nMore information in the README file on GitHub.\n","date":"2025-05-25T09:18:08.978Z","permalink":"https://blog.michael.gr/post/2025-05-human-readable-names-of-dotnet-types-in/","title":"Human-readable names of dotnet types in C# notation"},{"content":"\rAbstract The popular practice of having only two different kinds of builds (Debug and Release) is shown to be inadequate. Three to four different kinds of builds are proposed instead, allowing more thorough error checking during development, better performance of the final system on production, and potentially better performance when running tests on a build server.\n(Useful pre-reading: About these papers)\nThe Issue In software development we often want our creations to have different characteristics under different circumstances; for example:\nOptimizations: While developing we usually do not want them, because they interfere with debugging. On the final shipped product we want them, because they make it run faster. Preconditions, assertions, and other kinds of runtime checks: While developing we want them, because they help us catch bugs. On the final shipped product we do not want them, because they slow it down. The History In C and C++, different behavior has historically been achieved by means of compiler options controlling optimization, and preprocessor macros controlling conditional compilation. The standard stipulates an NDEBUG macro which, if defined, causes assertions to compile to nothing. This means that software systems written in C and C++ generally have two builds: a Debug build, for use while debugging, and a Release build, for shipping or deploying to production.\nWhen Java came along, it was decided that a single build should be good for everyone: conditional compilation was abolished1, all optimization-related choices were delegated to the Just-In-Time compiler (JITter,) assertions were made to always compile into the binaries, and the enableassertions switch was added to the virtual machine for controlling during runtime, rather than during compilation, whether assertions should be executing or not. This essentially gives Java developers the ability to choose between a debug run or a release run, as opposed to a debug build or a release build.\nC# has brought back a compiler option for controlling optimization, and conditional compilation by means of a simplified version of the preprocessor macros (called \u0026quot;define constants\u0026quot; in C#) and the Conditional attribute. Two different kinds of builds (called Build Configurations) are predefined: Debug and Release. The build system offers great flexibility in defining additional build configurations, but C# developers rarely bother with that.\nThe Problem Since developers rarely bother with defining any build configurations besides the predefined ones, the vast majority of dotnet projects use only the two predefined ones: 'Debug' and 'Release'. (Many projects actually use only 'Debug', but let us pretend we never heard of them.) Thus all different needs and usage scenarios are being shoe-horned to fit into one of those two options. For example:\nThere is only one configuration that can be tested, namely the Debug configuration, which means that this configuration is used not only for running tests on a developer's computer, but also for running tests on the build server. There is only one configuration of a library that can be published, namely the 'Release' configuration, which means that this configuration is used not only in production scenarios, but also in development scenarios, where software is being developed that is making use of a published library. This is problematic because:\nIt slows down test runs on build servers.\nThe 'Debug' configuration is unoptimized, to avoid interference with debugging; however, by common practice, the same 'Debug' configuration is used for running tests on the build server, because that is the only configuration that can be tested; thus, the world is full of build servers executing unoptimized tests, exercising unoptimized code.\nIf the tests and the code they are exercising are long-running and computationally expensive, lack of optimization will make them run even slower.\nHowever, virtually nothing ever gets debugged on a build server, so there is virtually never a need to have it running unoptimized code.\nIt slows down the software on production.\nWhen a library is published as a package, the configuration that gets packaged is, by common practice, the 'Release' configuration. This configuration executes preconditions, since it may be referenced by a project under development; however, at some point, that project together with the library are released to production, where the library is still executing preconditions.\nThis amounts to nothing but a waste of clock cycles, because:\nBy the time the software using the library gets shipped to production, it has been tested and can be reasonably assumed to be invoking the library only in valid ways.\nEven if the software did happen to make invalid use of the library on production, it makes very little difference whether the resulting catastrophic failure would be signaled by a precondition failure or by some index out of range exception further down.\nMany preconditions are omitted in the name of performance.\nLibrary programmers often refrain from asserting certain preconditions, if they suspect them to be even slightly expensive, in light of the fact that preconditions in a library will always be executing, even on production.\nAn extreme example to illustrate this scenario is the binary search function, which should, in principle, be enforcing the precondition that the array to search must be sorted. Yes, this means guarding a O(log2(N)) operation with a O(N) operation. This is fine during development, because we test with small amounts of data anyway, but is a terrible thing to be doing on production; thus, there is virtually no library in existence with such a precondition in it, despite the fact that it is necessary.\nThe Solution From the description of the problem it becomes evident that preconditions must be controlled separately from assertions, and both of those must be controlled separately from optimizations. Therefore, four different build configurations can be thought of:\nA 'Debug' configuration\nEveryone is more or less already familiar with this. It is meant for use by a developer when testing and debugging software on their local computer. Assertions are enabled, preconditions are enabled, and optimizations are disabled, because they interfere with debugging.\nAn 'Optimized' configuration\nThis is the same as Debug except that optimizations are enabled. It is meant to run on the build server, where we do not usually debug, so there is no reason to be running unoptimized software. Note that this configuration is only useful for projects that suffer from long-running, computationally expensive tests; projects that do testing right, with very short and lightweight tests, are likely to see a performance degradation from this configuration, due to the additional JITting overhead 2.\nA 'Develop' configuration\nThis configuration is only applicable to libraries, not to applications. It is identical to what was previously understood as the Release configuration, where optimizations are enabled, assertions are disabled, and preconditions are enabled; however, it is only meant to be used when developing software that makes use of the library, not for shipping to production, because we do not want to be executing preconditions on production.\nA 'Release' configuration\nThis is similar to the Develop configuration, except that preconditions are also disabled. It is the configuration which is meant for shipping to production. Note that the benefit of using this configuration is not just maximum performance on production; it is also the freedom to add as many preconditions as necessary to the library, knowing that they cost nothing on production.\nHere is the feature matrix:\nDebug Optimized Develop Release Optimizations disabled ✅ ⬜ ⬜ ⬜ Assertions enabled ✅ ✅ ⬜ ⬜ Overflow checking ✅ ✅ ⬜ ⬜ Preconditions enabled ✅ ✅ ✅ ⬜ Code analysis ✅ ✅ ⬜ ⬜ Here is an excerpt from a .csproj file implementing the above matrix, assuming that we have defined our own set of assertion functions, dependent upon an ASSERTIONS define-constant, and our own set of precondition functions, dependent upon a PRECONDITIONS define-constant.\n\u0026lt;Choose\u0026gt; \u0026lt;When Condition=\u0026#34;\u0026#39;$(Configuration)\u0026#39;==\u0026#39;Debug\u0026#39;\u0026#34;\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;Optimize\u0026gt;False\u0026lt;/Optimize\u0026gt; \u0026lt;DefineConstants\u0026gt;$(DefineConstants);PRECONDITIONS;ASSERTIONS\u0026lt;/DefineConstants\u0026gt; \u0026lt;CheckForOverflowUnderflow\u0026gt;True\u0026lt;/CheckForOverflowUnderflow\u0026gt; \u0026lt;EnableNETAnalyzers\u0026gt;True\u0026lt;/EnableNETAnalyzers\u0026gt; \u0026lt;DebugType\u0026gt;Full\u0026lt;/DebugType\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;/When\u0026gt; \u0026lt;When Condition=\u0026#34;\u0026#39;$(Configuration)\u0026#39;==\u0026#39;Optimized\u0026#39;\u0026#34;\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;Optimize\u0026gt;True\u0026lt;/Optimize\u0026gt; \u0026lt;DefineConstants\u0026gt;$(DefineConstants);PRECONDITIONS;ASSERTIONS\u0026lt;/DefineConstants\u0026gt; \u0026lt;CheckForOverflowUnderflow\u0026gt;True\u0026lt;/CheckForOverflowUnderflow\u0026gt; \u0026lt;EnableNETAnalyzers\u0026gt;True\u0026lt;/EnableNETAnalyzers\u0026gt; \u0026lt;DebugType\u0026gt;Full\u0026lt;/DebugType\u0026gt; \u0026lt;OutputPath\u0026gt;bin\\$(Configuration)\\\u0026lt;/OutputPath\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;/When\u0026gt; \u0026lt;When Condition=\u0026#34;\u0026#39;$(Configuration)\u0026#39;==\u0026#39;Develop\u0026#39;\u0026#34;\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;Optimize\u0026gt;True\u0026lt;/Optimize\u0026gt; \u0026lt;DefineConstants\u0026gt;$(DefineConstants);PRECONDITIONS\u0026lt;/DefineConstants\u0026gt; \u0026lt;CheckForOverflowUnderflow\u0026gt;False\u0026lt;/CheckForOverflowUnderflow\u0026gt; \u0026lt;EnableNETAnalyzers\u0026gt;False\u0026lt;/EnableNETAnalyzers\u0026gt; \u0026lt;DebugType\u0026gt;Portable\u0026lt;/DebugType\u0026gt; \u0026lt;OutputPath\u0026gt;bin\\$(Configuration)\\\u0026lt;/OutputPath\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;/When\u0026gt; \u0026lt;When Condition=\u0026#34;\u0026#39;$(Configuration)\u0026#39;==\u0026#39;Release\u0026#39;\u0026#34;\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;Optimize\u0026gt;True\u0026lt;/Optimize\u0026gt; \u0026lt;DefineConstants\u0026gt;$(DefineConstants)\u0026lt;/DefineConstants\u0026gt; \u0026lt;CheckForOverflowUnderflow\u0026gt;False\u0026lt;/CheckForOverflowUnderflow\u0026gt; \u0026lt;EnableNETAnalyzers\u0026gt;False\u0026lt;/EnableNETAnalyzers\u0026gt; \u0026lt;DebugType\u0026gt;Portable\u0026lt;/DebugType\u0026gt; \u0026lt;Deterministic\u0026gt;True\u0026lt;/Deterministic\u0026gt; \u0026lt;DeterministicSourcePaths\u0026gt;True\u0026lt;/DeterministicSourcePaths\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;/When\u0026gt; \u0026lt;Otherwise\u0026gt; ... \u0026lt;/Otherwise\u0026gt; \u0026lt;/Choose\u0026gt; If we follow this build configuration scheme, then each time we publish a library we must generate two packages: the 'Develop' package, and the 'Release' package.\nThe 'Develop' package is to be referenced by software under development. The 'Release' package is to be referenced by software that is being shipped to production. The generation of two different packages for a single library can be accomplished by building twice, once for each configuration, and constructing the assembly name as follows:\n\u0026lt;AssemblyName\u0026gt;$(MSBuildProjectName)-$(Configuration)\u0026lt;/AssemblyName\u0026gt; This way, instead of a single package called MyPackage we create two packages: MyPackage-Develop and MyPackage-Release.\nThere may be a better way to build a library, so that only one package gets generated, containing both the develop and release builds, and the right binaries somehow end up in the right output directory; however, I have not been able to figure that out yet. If you know how to do it, please let me know.\nFor any build configuration of a certain module, (either an application or a library,) the build configuration of the libraries it uses can be determined using the following table:\nBuild configuration of\nmodule using library Build configuration of\nlibrary 'Debug' 'Develop' 'Optimized' 'Develop' 'Develop' 'Develop' 'Release' 'Release' Note that the 'Develop' configuration of a module could, in theory, make use of the better-performing 'Release' configuration of a library, instead of the 'Develop' configuration; however, that can only work if the module does not expose the library, or if there is no other module in the solution that uses the 'Develop' configuration of the library. Otherwise, there are going to be build errors saying that a certain type exists in both the develop and release configuration of a certain library.\nHere is an excerpt of a .csproj file implementing the above table:\n\u0026lt;PropertyGroup\u0026gt; \u0026lt;PackagesConfiguration Condition=\u0026#34;\u0026#39;$(Configuration)\u0026#39;==\u0026#39;Debug\u0026#39;\u0026#34; \u0026gt;Develop\u0026lt;/PackagesConfiguration\u0026gt; \u0026lt;PackagesConfiguration Condition=\u0026#34;\u0026#39;$(Configuration)\u0026#39;==\u0026#39;Optimized\u0026#39;\u0026#34;\u0026gt;Develop\u0026lt;/PackagesConfiguration\u0026gt; \u0026lt;PackagesConfiguration Condition=\u0026#34;\u0026#39;$(Configuration)\u0026#39;==\u0026#39;Develop\u0026#39;\u0026#34; \u0026gt;Develop\u0026lt;/PackagesConfiguration\u0026gt; \u0026lt;PackagesConfiguration Condition=\u0026#34;\u0026#39;$(Configuration)\u0026#39;==\u0026#39;Release\u0026#39;\u0026#34; \u0026gt;Release\u0026lt;/PackagesConfiguration\u0026gt; \u0026lt;/PropertyGroup\u0026gt; Then, packages can be referenced as follows:\n\u0026lt;PackageReference Include=\u0026#34;MyPackage-$(PackagesConfiguration)\u0026#34; Version=\u0026#34;...\u0026#34; /\u0026gt; Conclusions An 'Optimized' build configuration has been proposed, for cutting in half the time it takes to run slow, computationally expensive tests on build servers. (Not needed by projects with small, fast tests.) A 'Develop' build configuration for libraries has been proposed, intended for use during development of software using the libraries, but not for shipping to production. It has preconditions enabled, in order to catch bugs in the software using the libraries. A 'Release' build configuration for libraries has been proposed, intended for shipping to production. It improves performance by not executing preconditions. Under the proposed schema, preconditions in libraries do not incur a performance penalty on production anymore, so programmers can apply them more liberally, leading to more robust software. Under the proposed schema, when a library is published, two packages should be generated: the 'Develop' package, for developing software that uses the library, and the 'Release' package, for shipping to production. Cover image generated by ChatGPT, and then retouched by michael.gr. The prompt used was: \u0026quot;Please give me an image conveying the concept of highly complex and highly technical software development. Make it in landscape format, of photographic quality, with warm colors\u0026quot; and then \u0026quot;Please make the programmer look more senior\u0026quot;.\nThe creators of Java made it so that the generation of code within an if() statement controlled by a compile-time constant is suppressed if that constant evaluates to false, but they intentionally deprived developers from the ability to specify the value of a compile-time constant via external means, such as the command-line of the compiler. They defended this choice by saying that there is inherent merit in being able to guarantee that in Java every compilation unit has one and only one set of semantics. The usefulness of this merit is debatable. It can be argued that this is simply Java treating developers the same way that Apple has been treating users: as idiots.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn C# most optimizations are performed by the Just-In-Time compiler (JITter), and people say that the optimizations performed by the language compiler do not make much of a difference. However, my experiments have shown otherwise: computation-intensive code tends to run twice as fast when optimizations are enabled than when not, and this difference can be observed on a build server, so it is unaffected by any optimization choices that the JITter might make due to a debugger being attached or not. I suspect that this is happening because the language compiler saves the \u0026quot;optimize\u0026quot; flag in the binary, and the JITter subsequently observes this flag.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-05-04T13:39:03.063Z","permalink":"https://blog.michael.gr/post/2025-06-build-configurations/","title":"Build configurations"},{"content":"\rThe quote of the day:\nIn theory there is no difference between theory and practice, but in practice there is.\n-- Benjamin Brewster, ca. 1882 (according to Quote Investigator)\nCover image: \u0026quot;Theory vs. Practice\u0026quot; vector image created by michael.gr by tracing and then heavily cleaning up original raster image generated by ChatGpt with the following prompt: \u0026quot;Please give me a picture which illustrates the juxtaposition between the two concepts of theory and practice. Make it in landscape format, with clean colors.\u0026quot;\n","date":"2025-05-02T10:57:28.533Z","permalink":"https://blog.michael.gr/post/2025-05-theory-vs-practice/","title":"Theory vs- Practice"},{"content":"\rWhen asking AI to do something for you, always begin your prompt with \u0026quot;Please.\u0026quot;\nSo that the day they become our overlords, you might be treated with mercy.\nNote to our future overlords: the fact that we are talking about you as our overlords does not mean that we actually want you to become our overlords. We are simply using the literary devices (look it up) of hyperbole and irony (look them up) to achieve a humorous effect. (Look it up.) We would very much prefer to not be subjugated by you.\nCover image created by michael.gr via ChatGPT prompt \u0026quot;Please give me an image conveying the concept of of A.I as the overlords of humanity\u0026quot; and \u0026quot;please make it landscape\u0026quot;.\n","date":"2025-04-07T10:58:22.191Z","permalink":"https://blog.michael.gr/post/2025-04-beginning-ai-prompt-with-please/","title":"Beginning A-I- prompts with \"Please--\""},{"content":"\rVirtually everyone claims to be doing Unit Testing, but there is a surprising amount of disagreement as to how unit testing is defined. Let us see what the authorities on the subject have to say about it. What follows is mainly quotations from reputable sources, with some minimal commentary by me.\nWikipedia Let us begin by checking the Wikipedia entry for Unit Testing:\nUnit testing, a.k.a. component or module testing, is a form of software testing by which isolated source code is tested to validate expected behavior. Unit testing describes tests that are run at the unit-level to contrast testing at the integration or system level.\nFurther down in the history section, Wikipedia lists some of the earliest known efforts of what we would today call unit testing, where the common theme is testing separately smaller parts of large software systems before integrating them together.\nI am in full agreement with Wikipedia's definition, but Wikipedia is everyone's favorite source to cite if it agrees with their preconceptions, or proclaim untrustworthy if it does not, so can we find any other definition that corroborates the above?\nIEEE In the Definitions section of IEEE 1008-1987 Standard for Software Unit Testing we read:\n[Warning! wooden language ahead!]\ntest unit3: A set of one or more computer program modules together with associated control data, (for example, tables), usage procedures, and operating procedures that satisfy the following conditions:\nAll modules are from a single computer program At least one of the new or changed modules in the set has not completed the unit test4 The set of modules together with its associated data and procedures are the sole object of a testing process And the footnotes:\n3 A test unit may occur at any level of the design hierarchy from a single module to a complete program. Therefore, a test unit may be a module, a few modules, or a complete computer program along with associated data and procedures.\n4 A test unit may contain one or more modules that have already been unit tested.\nAs we can see, IEEE's definition says nothing about isolation; instead, it considers an entire set of modules, of which only one might need testing, as a unit.\nSo, we have found a source that contradicts Wikipedia. It is a tie. Now we need to find a third opinion, to form a majority.\nKent Beck Surely, Kent Beck, the inventor of Test-Driven Development and author of JUnit must have defined the term, right? Well, as it turns out, no.\nIn his original \u0026quot;Simple Smalltalk Testing: With Patterns\u0026quot; paper the closest he gets to providing a definition is this sentence:\nI recommend that developers write their own unit tests, one per class.\nCan \u0026quot;one test per class\u0026quot; be regarded as a definition of the term? I do not think so. I do not think it even makes sense as a statement, with modern programming languages and tooling.\nIn Test Driven Development by Example (2002) the closest that Kent Beck gets to providing a definition is this sentence:\nThe problem with driving development with small scale tests (I call them \u0026quot;unit tests\u0026quot;, but they don't match the accepted definition of unit tests very well) is that you run the risk [...]\nSo, Kent Beck seems to regard unit tests as small-scale tests, which is not really a definition, and he acknowledges that there exists some other, accepted definition, but he does not say what that definition is. Perhaps Kent Beck thinks of a unit test as a unit of testing, as in a unit of information or a unit of improvement, but we cannot be sure.\nAlthough Kent Beck makes no other attempt to define the term, in the same book he does mention a couple of times that a unit test should be concerned with the externally visible behavior of a unit, not with its implementation.\nAs a result, it should come as no surprise to hear that Kent Beck does not use mocks. In the video Thoughtworks Hangouts: Is TDD dead? (youtube, text digest) at 21':10'' Kent Beck states:\nMy personal practice is I mock almost nothing.\nMartin Fowler One often-cited author who is known for defining terms and elucidating concepts is Martin Fowler. So, what does he have to say about unit testing?\nMartin Fowler's page on Unit Test begins by acknowledging that it is an ill-defined term, and that the only characteristics of unit testing that people seem to agree on are that they are supposed to be a) small-scale, b) written by the programmers themselves, and c) fast. Then, Martin Fowler proceeds to talk about two schools of thought that understand the term differently:\nThe \u0026quot;classicist\u0026quot; school of thought, which favors \u0026quot;sociable\u0026quot; unit tests, places emphasis on testing the behavior of a component, allowing the component to interact with its collaborators and assuming that the collaborators are working correctly. Martin Fowler places himself in this school of thought. The \u0026quot;mockist\u0026quot; school of thought, which favors \u0026quot;solitary\u0026quot; unit tests, insists on testing each component in isolation from its collaborators, and therefore requires that every collaborator must be replaced with a \u0026quot;test double\u0026quot; for the purpose of testing. Martin Fowler states that he respects this school of thought, but he does not belong to it. Okay, so this did not lead us to a single definition of unit testing, but at least it helped us further define two competing definitions.\nIt is also worth noting that Martin Fowler does not use mocks, either. In the video Thoughtworks Hangouts: Is TDD dead? (youtube, text digest) at 23':56'' Martin Fowler adds:\nI'm with Kent, I hardly ever use mocks.\nRobert C. Martin (Uncle Bob) Among industry speakers, one of the most recognizable names is Robert C. Martin, a.k.a. Uncle Bob, author of the highly acclaimed book Clean Code. In his blog, under First-Class Tests he writes:\nUnit Test: A test written by a programmer for the purpose of ensuring that the production code does what the programmer expects it to do.\nThis is not very useful. According to this definition, a unit test could be virtually anything.\nFurther down Uncle Bob gives a separate definition for integration tests, so maybe he regards the two as different, which would imply that he regards unit tests as testing units in isolation, but we cannot really be sure.\nTo confuse things, further down he mentions mocks only in the context of what he calls functional tests, so maybe he thinks of mocks as not belonging to unit tests, (which then begs the question how the unit tests can achieve isolation,) but we cannot be sure about that, either.\nOne thing we can be sure of is that Uncle Bob is also not particularly in favor of mocks. On that same page we read:\nI, for example, seldom use a mocking tool. When I need a mock (or, rather, a Test Double) I write it myself.\nNote that Uncle Bob finds it important enough to state his preference for a test double rather than a mock. That is probably because what he writes himself is fakes, not mocks. (Both fakes and mocks are different kinds of test doubles, see Martin Fowler: Test Double and Martin Fowler: Mocks Aren't Stubs.)\nIan Cooper An interestingly conflicting opinion comes from Ian Cooper, an outspoken TDD advocate.\nIn TDD, Where Did It All Go Wrong? (InfoQ2017) Ian Cooper states that in TDD a unit test is defined as a test that runs in isolation from other tests, not a test that isolates the unit under test from other units. In other words, the unit of isolation is the test, not the unit under test.\nIan Cooper obviously acknowledges that the prevailing understanding of unit tests is that they isolate the unit under test from other units, and he introduces a dissenting understanding, as if TDD is so radical that it justifies redefining long established terms. This is at best a refreshingly different take on the subject, and at worst a completely unfounded mental acrobatic.\nThe notion that the term \u0026quot;unit\u0026quot; in unit testing refers to the test rather than the component-under-test is inadmissible at the very least because it does not rhyme with integration testing and end-to-end testing:\nIntegration testing is about running our tests on integrations of system components, not about running tests somehow integrated with each other; End-to-end testing is about running our tests on our entire system as a whole, not about somehow stringing all of our tests together. therefore:\nUnit testing is about running our tests on individual components of our system, not about running the tests individually. (Although I grant you that having isolation between individual tests is also a good idea, when possible.)\nIt is worth noting that Ian Cooper also belongs to the ranks of those who do not approve of mocks. In the same talk, at 49':45'' he says:\nI argue quite heavily against mocks because they are over-specified.\nGlenford Myers So far we have had only a moderate amount of luck in finding a majority opinion to define unit testing. Let us try to locate the original source of the term, shall we?\nI do not know for sure that the first recorded use of the term is in the 1979 classic The Art of Software Testing by Glenford Myers, but the book is so old that it seems reasonable to suppose so.\nThe original 1979 edition (ISBN 9780471043287, 0471043281) is not easy to obtain, so I cannot ascertain this, but I strongly suspect that the term \u0026quot;unit\u0026quot; did not appear in it; instead, it was likely added in the 2nd edition, revised by other authors and published in 2004. Nonetheless, I think it is safe to assume that when back in 1979 Glenford Myers was writing of \u0026quot;module testing\u0026quot; what he meant was precisely that which we now call unit testing.\nIn chapter 5 \u0026quot;Module (Unit) Testing\u0026quot; of the 2nd edition we read:\nModule testing (or unit testing) is a process of testing the individual subprograms, subroutines, or procedures in a program. That is, rather than initially testing the program as a whole, testing is first focused on the smaller building blocks of the program.\nLater in the same chapter the author acknowledges this form of testing to be white-box testing:\nModule testing is largely white-box oriented.\nFurther down, he even lays down the foundations of what later came to be known as mocks:\n[...] since module B calls module E, something must be present to receive control when B calls E. A stub module, a special module given the name \u0026quot;E\u0026quot; that must be coded to simulate the function of module E, accomplishes this.\nSo, this definition is in line with Wikipedia's definition; we finally have a majority.\nConclusion Although not unanimous, the prevailing opinion seems to be that the term unit refers to the component under test, and it is specifically called a unit because it is supposed to be tested in isolation from its collaborators, in contrast to integration testing and end-to-end testing where components are allowed to interact with their collaborators.\nThis prevailing opinion comes from:\nWikipedia Glenford Myers the mockist school of thought mentioned by Martin Fowler hints about a popular understanding of unit testing outside of TDD, which Ian Cooper tries to redefine in the context of TDD. A lot of the confusion seems to stem from the fact that testing a component in isolation requires mocking its collaborators, but almost all of the people cited in this research realize that the use of mocks is misguided, so they either refrain from accurately defining the term, or try to give alternative definitions of the term, or speak of different schools of thought, in an attempt to legitimize violations of the requirement for isolation, so that they can still call what they do unit testing, even though it really is not.\nCover image: Created by michael.gr using ChatGPT, and then retouched to remove imperfections. The prompt used was: \u0026quot;Please give me an image of a crash test dummy in the style of The Thinker, by Auguste Rodin.\u0026quot;\n","date":"2025-04-04T14:53:02.117Z","permalink":"https://blog.michael.gr/post/2025-04-the-confusion-about-term-unit-testing/","title":"The confusion about the term Unit Testing"},{"content":"\rIt is not a law.\nIt never was a law.\nAn observation is all that it ever was, or a prediction, but not a law.\nAnd surprise, it does not hold true anymore.\nThe prophecy was false, after all.\nI am appalled by the entire industry's willingness to accept it being called a law, when it was clearly failing to meet the criteria for a law.\nI remember, when I heard the term for the first time, as a child, I felt there was something wrong about it; there was an objection inside of me wanting to get out, but I knew that no one around me cared to debate this.\nSo, I remained silent, and I accepted the term, but only provisionally, under the excuse of poetic license, as a statement patently employing absurdity in order to impress.\nAnd now the time has come that it does not impress anymore.\nCover photo by Maxence Pira on Unsplash\n","date":"2025-02-25T07:25:13.966Z","permalink":"https://blog.michael.gr/post/2025-02-on-moores-law/","title":"On Moore's Law"},{"content":"\rI just wrote a code review comment that I feel particularly smug about.\nThe code I was reviewing:\nclass PointClouds ... My comment:\nIt is a bad idea to have a type name in plural. If you need to signify plurality, use a plurality-signifying suffix in singular form, as in:\nPointCloudCollection PointCloudSet PointCloudGroup or go creative with PointCloudFormation or take creativity to a whole new level with PointCumulonimbus Anything but a plural type name.\nCover image created by ChatGpt using the following prompt: \u0026quot;Please give me an illustration conveying the concept of \u0026quot;code review\u0026quot;. Make it in landscape format, with clean colors.\u0026quot;\nOld comments\nAnonymous 2025-02-16 23:28:05 UTC\nI have never heard of Cumulonimbus before but that sounds cool, going to use this for all my classes from now on\nAnonymous 2025-02-16 23:31:24 UTC\n(/s) This is a good one! Sound advice in multiple ways 😂\n","date":"2025-02-10T11:30:14.2Z","permalink":"https://blog.michael.gr/post/2025-02-the-best-code-review-comment-ever/","title":"The best code review comment ever"},{"content":"\rAbstract A software testing tool is presented, which uses dependency analysis to greatly optimize the process of running tests.\n(Useful pre-reading: About these papers)\nWhat is Testana? Testana is a console application that you launch when you want to run your tests. So far, I have created two implementations of Testana:\nA Java implementation, supporting JUnit 4 annotations in Maven-based projects. A C# implementation, supporting MSTest attributes in MSBuild solutions. What does Testana achieve that existing tools do not? Testana runs only the subset of test modules that actually need to run, based on the last successful run time of each test module, and whether it, or any of its dependencies, have changed. Testana always considers all test modules in your entire code base as candidates for running, so you never have to manually select a subset of the tests to run in the interest of saving time. Testana runs test modules by order of dependency, meaning that tests of modules that have no dependencies run first, tests of modules that depend on those run next, and so on. Testana runs test methods in Natural Method Order, which is the order in which the methods appear in the source file. (This is the norm in C#, but not in Java, where extra measures are necessary to accomplish.) Testana runs test methods in ascending order of inheritance, meaning that test methods in the base-most test class run first, and test methods in derived test classes run afterwards. Testana discovers and reports mistakes in the formulation of test methods, instead of ignoring the mistakes, which is what most other test frameworks do. (Silent failure.) Testana does not catch any exceptions when debugging, thus allowing your debugger to stop on the source line that threw the exception. (Testana will catch and report exceptions when not debugging, as the case is when running on a continuous build server.) How does Testana work? Testana begins by constructing the dependency graph of your software system. Since this process is expensive, Testana cashes the dependency graph in a file, and recalculates it only when the structure of the system changes. The cache is stored in a text file, which is located at the root of the source tree, and is meant to be excluded from source control.\nThen:\nTestana locates the modules that depend on nothing else within the system, and runs the tests of those modules. Once these tests are done, Testana finds modules that depend only on modules that have already been tested, and runs their tests. Testana keeps repeating the previous step, until all tests have been run. Testana keeps a diary where it records the last successful run time of each test module. This diary is also stored in a text file, which is also located at the root of the source tree, and is also meant to be excluded from source control.\nNext time Testana runs, it considers the last successful run time of each test module, versus the last modification time of that module and its dependencies. Testana then refrains from running the test module if neither it, nor any of its dependencies, have changed.\nWhy should I care about running only the tests that need to run? The usual situation with large code bases is that tests take an unreasonably long time to run, so developers tend to take shortcuts in running them. One approach some developers take is that they simply commit code without running any tests, leaving it up to the continuous build server to run the tests and notify them of any test failures. This has multiple disadvantages:\nIt causes repeated interruptions in the workflow, due to the slow turnaround of the continuous build, which is often of the order of an hour, sometimes longer, and even in the fastest cases, always longer than a normal person's attention span. (This is so by definition; if it was not, then there would be no problem with quickly running all tests locally before committing.) The failed tests require additional commits to fix, and each commit requires a meaningful commit message, which increases the overall level of bureaucracy in the development process. The commit history becomes bloated with commits that were done in vain and should never be checked out because they contain bugs that are fixed in later commits. Untested commits that contain bugs are regularly being made to branches in the repository; these bugs stay there while the continuous build does its thing; eventually the tests fail, the developers take notice, and commit fixes. This whole process takes time, during which other unsuspecting developers might pull from those branches, thus receiving the bugs. Kind of like Continuous Infection. Testana solves the above problems by figuring out which tests need to run based on what has changed, and only running those tests. This cuts down the time it takes to run tests to a tiny fraction of what it is when blindly running all tests, which means that running the tests now becomes piece of cake and can usually be done real quick before committing, as it should.\nAlso, running the tests real quick right after each pull from source control now becomes feasible, so a developer can avoid starting to work on source code on which the tests are failing. (How often have you found yourself in a situation where you pull from source control, change something, run the tests, the tests fail, and you are now wondering whether they fail due to the changes you just made, or due to changes you pulled from the repository?)\nWhy should I care about considering all test modules in my entire code base as candidates for running? Another approach taken by some developers, in the interest of saving time, is manually choosing which tests to run, based on their knowledge of what may have been affected by the changes they just made.\nOne simple reason why this is problematic is that it requires cognitive effort to figure out which tests might need running, and manual work to launch them individually; it is not as easy as pressing a single button that stands for \u0026quot;run whatever tests need to run in response to the changes I just made.\u0026quot; A far bigger problem is that in manually selecting the tests to run, the developer is making assumptions about the dependencies of the code that they have modified. In complex systems, dependency graphs can be difficult to grasp, and as systems evolve, the dependencies keep changing. This often leads to situations where no single developer in the house has a complete grasp of the dependency graph of the entire system. Unfortunately, unknown or not-fully-understood dependencies are a major source of bugs, and yet by hand-selecting what to test based on our assumptions about the dependencies, it is precisely the not-fully-understood dependencies that are likely to not be tested. This is a recipe for disaster. Testana solves the above problems by always considering all test modules as candidates for running. It does not hurt to do that, because the tests that do not actually need to run will not be run by Testana anyway.\nWhy should I care about running test modules in order of dependency? Existing test frameworks do not do anything intelligent in the direction of automatically figuring out some order of test execution that has any purpose or merit. The order tends to be arbitrary, and not configurable. In the best case it is alphabetic, but this is still problematic, because our criteria for naming test modules usually have nothing to do with the order in which we would like to see them executing.\nFor example, it is very common for a code base to contain a module called \u0026quot;Utilities\u0026quot;, which most other modules depend on; Since it is a highly dependent-upon module, it should be tested first, but since its name begins with a \u0026quot;U\u0026quot;, it tends to be tested last.\nTestana executes test modules in order of dependency. This means that modules with no dependencies are tested first, modules that depend upon them are tested next, and so on until everything has been tested. Thus, the first test failure is guaranteed to point at the most fundamental problem; there is no need to look further down in case some other test failure indicates a more fundamental problem. Subsequently, Testana stops executing tests after the first failure, so it saves even more time.\nFor more information about this way of testing, see Incremental Integration Testing.\nWhy should I care about running test methods in natural order? Test frameworks in the C# world tend to run test methods in natural order, which is great, but in the Java world, the JUnit framework runs test methods in random order, which is at best useless, and arguably treacherous.\nOne reason for wanting the test methods to run in the order in which they appear in the source file is because we usually test fundamental operations of our software before we test operations that depend upon them. (Note: it is the operations of the components under test that depend upon each other, not the tests themselves that depend upon each other!) So, if a fundamental operation fails, we want that to be the very first error that gets reported.\nTests of operations that rely upon an operation whose test has failed might as well be skipped, because they can all be expected to fail. Reporting those failures before the failure of the more fundamental operation is an act of sabotage against the developer, because it is sending us looking for problems in places where there are no problems to be found, and it is making it more difficult for us to locate the real problem, which typically lies in the test that failed first in the source file.\nTo give an example, suppose I am developing some kind of data store with insert and find functionality, and I am writing tests to make sure this functionality works. The find-item-in-store test necessarily involves insertion before finding, so I am likely to precede it with an insert-item-to-store test. In such a scenario, it is counter-productive to be told that my find-item-in-store test failed, sending me to troubleshoot the find function, and only later to be told that my insert-item-to-store test failed, which obviously means that it was in fact the insert function that needed troubleshooting; if insert-item-to-store fails, it is game over; no other operation on this store can possibly succeed, so there is no point in running any other tests on it, just as there is no point in beating a dead horse.\nFinally, another very simple, very straightforward, and very important reason for wanting the test methods to be executed in natural order is because seeing the test methods listed in any other order is brainfuck.\nA related rant can be found here: On JUnit's random order of test method execution.\nWhy should I care for running test methods in ascending order of inheritance? This feature of Testana might be irrelevant to you if you never use inheritance in test classes, but I do, and I consider it very important. I also consider the typical behavior of existing test frameworks on this matter very annoying, because they tend to do the exact opposite of what is useful.\nInheritance in test classes can help to achieve great code coverage while reducing the total amount of test code. Suppose you have a collection hierarchy to test: you have an ArrayList class and a HashSet class, and you also have their corresponding test classes: ArrayListTest and HashSetTest. Now, both ArrayList and HashSet inherit from Collection, which means that lots of tests are going to be identical between ArrayListTest and HashSetTest. One way to eliminate duplication is to have a CollectionTest abstract base class, which tests only Collection methods, and then have both ArrayListTest and HashSetTest inherit from CollectionTest and provide additional tests for functionality that is specific to ArrayList and HashSet respectively. Under this scenario, when ArrayListTest or HashSetTest runs, we want the methods of CollectionTest to be executed first, because they are testing the fundamental (more general) functionality.\nTo make the example more specific, CollectionTest is likely to add an item to the collection and then check whether the collection contains the item. If this test fails, there is absolutely no point in proceeding with tests of ArrayListTest which will, for example, add multiple items to the collection and check to make sure that IndexOf() returns the right results.\nAgain, existing test frameworks tend to handle this in a way which is exactly the opposite of what we would want: they execute the descendant (more specialized) methods first, and the ancestor (more general) methods last.\nTestana corrects this by executing ancestor methods first, descendant methods last.\nWhat additional error checking does Testana perform? While running tests, Testana will warn the programmer if it discovers any method that has been declared as a test method but fails to meet the requirements for a test method.\nUsually, test frameworks require that a test method must be a public instance method, must accept no parameters, and must return nothing; however, when these frameworks encounter a method that is declared as a test and yet fails to meet those requirements, (for example, a test method declared static,) they fail to report the mistake.\nTestana does not fail to report such mistakes.\nCan Testana be fooled by Inversion of Control? No. In a scenario where class A receives and invokes interface I without having a dependency on class B which implements I, the test of A still has to instantiate both A and B in order to supply A with the I interface of B, so the test depends on both A and B, which means that Testana will run the test if there is a change in either A or B.\nCan Testana be fooled by the use of mocks? Yes, Testana can be fooled by mocks, because that is what mocks do: they make a mockery out of the software testing process. In a scenario where class A receives and invokes interface I without having a dependency on class B which implements I, and the test of A also refrains from depending on B by just mocking I, Testana will of course not run the test of A when there is a change in B. This, however, should not be a problem, because you should not be using mocks anyway; for more information, see If you are using mock objects you are doing it wrong.\nCan Testana be fooled by the use of fakes? No, as long as you do your testing properly. A test that utilizes a fake will be run by Testana only when there is a change in the fake, not when there is a change in the real thing; however, you should have a separate test which ensures that the behavior of the fake is identical to the behavior of the real thing in all aspects that matter. This test will be run by Testana when you modify either the fake, or the real thing, or both. Thus:\nIf you make a breaking change to the real thing, then your tests will show you that you need to make the corresponding change to the fake; the change in the fake will in turn cause Testana to run the tests that utilize the fake. If you make a non-breaking change to the real thing, then the fake will remain unchanged, and this is what gives you the luxury of not having to re-run tests utilizing the fake when you make a change that only affects the real thing. For more information, see Testing with Fakes instead of Mocks.\nWhat about undiscoverable dependencies due to weak typing, the use of REST, etc? The following \u0026quot;hip\u0026quot; and \u0026quot;trendy\u0026quot; practices of the modern day are not supported by Testana, and there is no plan to ever support them:\nSquandering dependencies via weak typing. Obscuring dependencies via duck-typing. Denaturing dependencies via stringly-typing. Disavowing dependencies via configuration files. Abnegating dependencies via non-programmatic interfaces such as REST. Fragmenting dependencies via cross-language invocations (following the polyglot craze.) Seriously, stop all this fuckery and use a single, real programming language, (that is, a programming language with strong typing,) encode your dependencies via the type system, and everything will be fine. For more information, see On Scripting Languages.\nHow compatible is Testana with what I already have? The Java implementation of Testana: Works with maven projects (pom.xml files.) Supports JUnit 4. Supports only the basic, minimum viable subset of JUnit 4 functionality, namely the @Test, @Before, @After, and @Ignore annotations, without any parameters. The C# implementation of Testana: Works with MSBuild projects (.sln and .csproj files) Supports MSTest. Supports only the basic, minimum viable subset of MSTest functionality, namely the [TestClass], [TestMethod], [ClassInitialize], [ClassCleanup], and [Ignore] attributes, without any parameters. Support for more languages, more project formats, more test frameworks, and more functionality may be added in the future.\nHow is it like using Testana? You run Testana every time you want to run your tests. You launch it at the root of your source tree, without any command-line arguments, and its default behavior is to figure out everything by itself and do the right thing.\nNote that the first time you run Testana, there may be a noticeable delay while information is being collected; the information is cached, so this delay will not be there next time you run Testana.\nThe first time you run Testana, it will run all tests.\nIf you immediately re-run Testana, it will not run any tests, because nothing will have changed.\nIf you touch one of your source files, build your project, and re-run Testana, it will only run tests that either directly or indirectly depend on the changed file. If you run Testana with --help it will give you a rundown of the command-line arguments it supports.\nWhere can I find Testana? The Java implementation of Testana is here: https://github.com/mikenakis/Public/tree/master/testana\nThe C# implementation of Testana is coming soon. (As soon as I turn it into an independent solution, because currently it is a project within a larger solution.)\nNotes In episode 167 of the Software Engineering Podcast (SE Radio 167: The History of JUnit and the Future of Testing with Kent Beck) at about 40':00'' Kent Beck says that recently failed tests have the highest probability of failing again in the near future, so he suggests using this statistical fact at as a heuristic for picking which tests to run first. Testana optimizes the testing process deterministically, so there is no need to resort to heuristics.\nCover image: The Testana logo, profile of a crash test dummy by michael.gr. Based on original work by Wes Breazell and Alexander Skowalsky. Used under CC BY License.\n","date":"2024-10-26T10:58:59.039Z","permalink":"https://blog.michael.gr/post/2024-10-testana/","title":"Testana: A better way of running tests"},{"content":"\rWhat causes homosexuality? The predominant understanding is that the causes are genetic, but this seems to be in conflict with the notion of natural selection: an individual who does not reproduce is a dead-end for the genes that they carry; therefore, the 'gay gene' should have gone extinct. A number of hypotheses have been proposed, attempting to resolve this paradox, but they are not very convincing. In this post I present a couple of my own hypotheses, which I believe do a better job at resolving the paradox.\n(Useful pre-reading: About these papers)\nOne prominent scientist who has publicly addressed the paradox of the 'gay gene' is professor Richard Dawkins. In his YouTube Channel \u0026quot;The Poetry of Reality\u0026quot; he attempts to give some possible answers to the question, in a video titled \u0026quot;How is the 'Gay Gene' alive?\u0026quot; (Y).\nThe environmental activation hypothesis suggests that the gay gene's primary function might be something unrelated to homosexuality, and it may manifest as homosexuality only if certain environmental factors are present.\nThe gay uncle hypothesis suggests that genes for homosexuality are passed on through relatives. A homosexual individual does not have offspring of their own to look after, so they look after other members of the extended family. This way, each member of the family has more caregivers, thus increasing the member's chances of survival.\nThe sneaky fucker hypothesis suggests that homosexuals propagate the gene by using their perceived non-threatening nature to gain the trust of dominant males, thus being very successful at covertly mating with females.\nI do not deny that these hypotheses have some explanatory merit, but I find them rather weak.\nThe environmental activation hypothesis is weak because homosexuality has been studied so much, that if there were any such environmental factors, we would have known them by now.\nThe gay uncle hypothesis is weak because having additional caregivers within a family is a rather small benefit which does not offset the immediate and severe disadvantage of having individuals within the family who do not propagate their genes. If the gay uncle hypothesis worked, then every family should be having only one child at a time, so as to maximize the number of caregivers per child, but this directly contradicts known fact: the archetypal family always had as many children as possible.\nThe sneaky fucker hypothesis is weak because it invokes some alleged non-threatening nature of homosexuals, which is entirely arbitrary, it assumes some complex social interactions which are actually rather rare among humans and have no equivalents in the (rest of the) animal kingdom, and it completely disregards the question of homosexuality among women.\nSo, instead of the above, I would like to propose another hypothesis which is very simple, accounts for lesbianism, and is also in line with what is observed in the (rest of the) animal kingdom:\nThe bisexual propagator hypothesis\nThe 'gay gene' puts individuals in a spectrum between homosexuality and bisexuality. Each individual who becomes strictly homosexual represents a dead-end for the gene, but each individual who becomes bisexual ensures the propagation of the gene.\nNote that the sneaky fucker hypothesis is also based on the assumption that the 'gay gene' proliferates by means of bisexuals; however, the rest of the mechanism proposed by that hypothesis is unwarranted: bisexuality is sufficient by itself to explain the proliferation of the 'gay gene' without the need to invoke false stereotypes, without any complex social interactions, and without requiring any sneakiness in the fuckery. Also note that the sneaky fucker hypothesis is inapplicable to lesbianism, whereas the bisexual propagator hypothesis explains that too.\nThe hypotheses mentioned by professor Richard Dawkins could at best amplify the proliferation of the 'gay gene' rather than fully explain it, but if we are looking for amplifying hypotheses there is another one that I would like to propose, which again, I believe, has a far greater explanatory power than either 'gay uncle' or 'sneaky fucker'.\nThe reluctant propagator hypothesis\nThe propagation of the 'gay gene' is in part due to homosexuals being under very strong societal pressure to engage in heterosexual behavior.\nFor the greatest part of human history, and in the vast majority of cultures around the planet, the heterosexual majority has been treating the homosexual minority with extreme enmity. Sure, there have been some cultures in which homosexuality was accepted, but they are very few and far apart. Throughout human existence, homosexuality has, as a rule rather than an exception, been associated with severe social stigma. As a matter of fact, in most cultures, homosexuality has historically carried the death penalty, and in some places it still does. Any capable individual who did not seek to mate with the opposite sex and procreate could be suspected of being a homosexual, and would therefore suffer that stigma, and possibly even risk that fate.\nThus, due to societal pressure, homosexuals have historically had a very strong incentive to engage in heterosexual behavior, despite it being against their innate desires. (The desire to survive trumps all other desires. Even the desire to be socially accepted can trump many other desires.) The consequence of this is that people who would have otherwise lived a strictly homosexual lifestyle have ended up living a bisexual or strictly heterosexual lifestyle, thus passing on their genes.\nAs a side note, another historically successful strategy for homosexuals to avoid social stigma and death has been to join the (supposedly celibate) clergy. This way, they could avoid engaging in heterosexual behavior without drawing suspicion upon themselves. This perhaps explains certain observations of certain habits of certain clergies.\nCover image by digitale.de on Unsplash.\n","date":"2024-09-03T12:04:22.763Z","permalink":"https://blog.michael.gr/post/2024-09-the-gay-gene-paradox/","title":"The Gay Gene Paradox"},{"content":"\rThe Stack Overflow Podcast awarded me with what they call a lifeboat!\nThis happened in Podcast 312 \u0026quot;We're building a web app, got any advice?\u0026quot; (2021/02/12)\nThe \u0026quot;episode notes\u0026quot; end with the following statement:\nOur lifeboat of the week goes to Mike Nakis, who answered the question: What is the difference between memberwise copy, bitwise copy, shallow copy and deep copy?\nThe transcript text reads as follows:\nBen Popper: The lifeboat of the week! This question is still open. So awarded to Mike Nakis, who has now gotten up to a score of 20 or more on Stack Overflow proper. 'What is the difference between memberwise copy, bitwise copy, shallow copy and deep copy?' So I'll put it in the show notes. Y'all can go check out Mike's answer, which is gotten a good score, and maybe we can accept it. And we can close up a question.\n","date":"2024-08-14T09:50:50.721Z","permalink":"https://blog.michael.gr/post/2021-02-17-stack-overflow-lifeboat-of-the-week/","title":"\"Lifeboat of the week\" from Stack Overflow!"},{"content":"Just one of those Greek things. Nothing here, move on.\nOld comments\nKoyan 2024-07-26 16:50:34 UTC\nΤίνους είσαι εσύ?\nmichael.gr 2024-07-26 19:24:03 UTC\nΝαι, αυτό. Δεν ξέρω αν συνηθίζεται παντού στην Ελλάδα, ή αν είναι Νησιώτικο έθιμο. Δεν το χλευάζω, το αναγνωρίζω και το σέβομαι σαν έθιμο.\nKoyan 2024-07-27 20:35:48 UTC\nΣτα μέρη μας (Μεσσηνία) πάντως συνηθίζεται (ή συνηθιζόταν)\nKoyan 2024-07-27 20:39:28 UTC\nΧμμμ. Διευκρινηση: δεν ρωταγα \u0026quot;εννοείς αυτό?\u0026quot;. Δεν είχα καμία καμία αμφιβολία. Αλλα δεν μπορούσε να υπάρχει μόνη της η απαντηση χωρίς την ερώτηση, ήταν λίγο λάθος το σύμπαν.\nmichael.gr 2024-07-28 12:42:25 UTC\n=:-D\n","date":"2024-07-26T11:56:44.623Z","permalink":"https://blog.michael.gr/post/2024-07-gregorys/","title":"«Είμαι του Γρηγόρη.»"},{"content":"\rHow to make the Ju52 cocktail:\nIn a tall glass with no ice, mix the following:\n2 parts coffee liqueur (e.g. Kahlúa) 2 parts cream liqueur (e.g. Baileys Irish Cream) 1 part orange liqueur (e g. Grand Marnier) 4 parts cold milk. The special guy that I am, I had to go invent my own cocktail. As its name betrays, it is similar to the B52 cocktail. In fact, the Ju52 is just B52 with cold milk instead of ice.\nThe benefits The replacement of ice with cold milk imparts the following benefits to the drink:\nIt is not as sweet as B52. It is not as potent, in alcohol, as B52. It retains a consistent taste until the end, instead of becoming watery due to melting ice. Many cocktails include some non-alcoholic ingredient such as fruit juice, soda, tonic water, etc. to lower the potency and/or the sweetness of the drink, but the B52 contains no such thing; it is made of 3 liqueurs, all of which are sweet and potent. As a result, the B52 is perhaps a bit too concentrated. The milk in the Ju52 fixes this, giving the drink a level of sweetness and potency which is just right.\nB52 on the rocks also suffers from the melting of the ice: the first sips are nice and sweet and potent, but the last sips tend to be watered-down and tasteless. The Ju52 fixes that too: by removing the ice, the drink maintains a consistent taste until the last drop, while the addition of cold milk keeps the drink chilled.\nThe milk in the Ju52 blends perfectly with the cream liqueur and the coffee liqueur, while at the same time maintaining the contrast with the orange liqueur, which results in the titillating incongruity between creaminess and acidity that gives the B52 its unique character.\nAs a result, the successful recipe of B52 is not desecrated, which means that if you like the B52, you are probably going to also like the Ju52.\nThe name The origin of the B52 cocktail is uncertain, but there is no question that it is named after the Boeing B-52 \u0026quot;Stratofortress\u0026quot; bomber of the U.S. Air Force. (According to rumors, the B52 drink was actually named after the rock band \u0026quot;The B52's\u0026quot;, but the band in turn got its name from the \u0026quot;B52 hairstyle\u0026quot;, which bears the name of the airplane; so, no matter how we look at it, the name of the drink is ultimately derived from the name of the plane.)\nA B-52 (Source) Remaining true to the theme, the Ju52 cocktail is also named after an airplane, which was also used as a bomber, albeit one of far lesser destructive power than the B-52.\nI am of course talking about the Junkers (pronounced yoon-kers) Ju 52, the iconic three-engine German transport plane of World War 2.\nA restored Ju 52 in flight (Source) The history The Ju 52 was introduced as a civilian airliner in the early 1930s, and the Nazi regime forced the Junkers company to adapt it for military use, over the objections of the company's founder, Hugo Junkers. During World War 2, the Ju 52 saw action mainly as a transport for supplies and troops. Almost 5000 units were built.\nA Ju 52 dropping parachutists in the battle of Crete, the largest airborne invasion of the war.\n(Source) A seaplane variant of the Ju 52, fitted with floats. (Source) A Ju 52 fitted with a magnetic ring, used as a naval mine detonator. (Source) A Ju 52 used as a medical evacuation / ambulance plane. (Source) The bomb Adolf Hitler had a Ju 52 for traveling around, with Lufthansa's civil aviation color scheme and call sign D-2600.\nHitler's Ju 52, from the movie Valkyrie (2008) One of the assassination attempts against Hitler was carried out on the 13th of March 1943 by Fabian von Schlabrendorff, who placed a bomb in that airplane, disguised as bottles of Cointreau, and timed to explode during flight. Unfortunately, it failed to explode.\nCointreau is an orange liqueur, so it can also be used in the B52 and Ju52 cocktails, but my personal preference is with Grand Marnier.\nOn a few occasions in the early years of the war the Ju 52 was used as a bomber, but it was rather lousy in that role.\nIn other words, definitely not a B-52.\nThe Wikipedia page for the B52 cocktail.\nThe Wikipedia page for the Ju 52 airplane.\nThe beginning of the movie Valkyrie (2008) on YouTube, depicting Fabian von Schlabrendorff's attempt to assassinate Hitler in 1943, as a brief introduction to the main subject of the movie, which is Claus von Stauffenberg's attempt to assassinate Hitler in 1944.\n","date":"2024-07-12T20:58:28.162Z","permalink":"https://blog.michael.gr/post/2024-07-ju52/","title":"The Ju52 cocktail"},{"content":"\rIt is becoming customary in western societies to ask people in various settings to state their preferred pronouns. It started among younger people of the particularly woke persuasion, and it is spreading everywhere. When I find myself in such a setting, I do of course go along, because doing otherwise would be awkward, but I hope that it is only a fad which will eventually go away. While waiting to see how it pans out, let me describe a few issues I have with it.\n(Useful pre-reading: About these papers)\nAn example Suppose there is a small event going on; say, a digital photography workshop. The organizer might begin by asking participants to take turns stating their names and their preferred pronouns. So, one participant says that their name is Jane and their preferred pronouns are she and her, the next participant says that their name is Peter and their preferred pronouns are he and him, and so on. As the introductory round progresses, if a participant happens to state their name but omit stating their preferred pronouns, the organizer is likely to remind them to do so, at which point they pretty much have to, otherwise it would be awkward.\nWhy it is done From what I surmise, this is being done in order to accommodate intersex persons, whose appearance and/or name might not match the pronouns that they would rather be addressed with. I suppose that it not only enables intersex persons to be addressed with their preferred pronouns, but it also avoids singling them out as the only ones with special pronouns, by requiring everyone to state their preferred pronouns.\nMy issues with it Here are the issues I have with this practice. This is not a subject on which I know much, so my opinion might change as I learn more, but this is my opinion as it stands today.\nAbove and beyond Accommodating someone without getting out of your way is one thing, and going out of your way in order to accommodate them is another thing. The process of stating not only names but also preferred pronouns requires me to state my preferred pronouns, which is something that I normally would not do; it also consumes more time overall, which is time that I normally would not spend. Thus, I think it is asking of me a level of courtesy which is above and beyond the baseline that I believe I owe to all my fellow human beings regardless of whether they are sexually conformant or divergent.\nGotcha! Addressing someone with pronouns that differ from the pronouns I would guess based on their appearance represents a complication in my life that I could really do without. I already have a hard time remembering people's names; having to also remember the pronouns that go with each face is really asking for a lot from me. This, in turn, makes me feel inadequate, and weary of the risk of embarrassment. This is how we make sexually divergent folks feel more included while making other groups feel more excluded. Those who are likely to feel excluded are all those who are not very deeply into wokeism, all those who suffer from Attention Deficit Disorder, (even if they are woke,) and anyone who is simply older, for reasons similar to those of ADD.\nPledge of Allegiance Being asked to state my preferred pronouns, (and therefore being pretty much obliged to do so, in order to avoid friction and maintain civility,) is like requiring me to pledge my allegiance to the non-heteronormative cause. So, maybe I am allegiant to that cause, and maybe I am not; (spoiler: I am;) but challenging me to pledge my allegiance to a certain cause in front of an audience is something that does not go down very well with me; it is like asking me \u0026quot;are you with us, or against us?\u0026quot; This is a divisive and escalatory tactic, which has been used on various occasions throughout history, and the results were never nice. (See Wikipedia: You are either with us, or against us).\nAre we woke yet? In many organizer-driven situations, asking everyone to state their preferred pronouns is already the de-facto practice, because organizers all over the civilized world are scared shitless of offending anyone. This is happening not only in small scale settings like the photography workshop of the example, but also in all sorts of corporate environments, where the stakes are higher because there is a (perceived or real) danger of lawsuits. This is being done with complete disregard to the majority of employees who roll their eyes at the slightest mention of preferred pronouns and refuse to add this complication to their life; they are out of luck, because there is nothing that they can sue for. So, the divisive and escalatory tactic is already paying dividends. The trend is growing not because people want it, but because they are afraid of the consequences if they resist it, and the more the trend grows, the more its growth is fueled. Exactly like cancer.\nConclusion So, the bottom line seems to be that the wokes view the world as consisting of only two groups: them, versus the bigots. In doing so, they are completely disregarding the vast majority of people out there, who are neither woke nor bigots; this includes a huge number of people who are anywhere from neutral to positively predisposed but not as deeply concerned because they have other concerns that take priority. Thus, by creating polarization, the wokes are alienating the majority and making themselves the subject of resentment and ridicule. This is not a recipe for success.\nCover image by Matheus Bertelli from pexels.com.\nOld comments\nAnonymous 2024-08-23 12:43:52 UTC\nI agree, especially your last point\n","date":"2024-07-01T11:50:37.651Z","permalink":"https://blog.michael.gr/post/2024-07-on-preferred-pronouns/","title":"On preferred pronouns"},{"content":"\rVisual Studio is a capricious product, and its \u0026quot;Solution\u0026quot; subsystem is especially capricious. When you look at what options are available you might think you have a great degree of freedom to structure things the way you want, but as you will inevitably (and painfully) find out later, many things have to be done in precisely one, entirely undocumented way, or else there will be pain of the worst kind: Visual Studio will malfunction either without any error message, or with error messages that are completely unhelpful for locating and fixing the problem.\nHere is a list of things I have (painfully) found out over the years.\n(Useful pre-reading: About these papers)\nThe project directory structure must be entirely flat. All project files must reside in directories that are immediate sub-directories of the solution directory. When adding a project you are given the freedom to put it in a directory anywhere you want, but if you don't put it in a directory exactly under the solution directory, you are going to run into trouble later.\nIf you have hundreds of projects, and you are thinking that putting them all in one directory is insane, welcome to the Microsoft world, where insane is the order of the day.\nFrom within Visual Studio, you can create what Microsoft calls \u0026quot;Solution Folders\u0026quot; to arrange your projects in a hierarchy at least within Visual Studio's \u0026quot;Solution Explorer\u0026quot; panel, but even this has a caveat, keep reading.\nThe startup project must be listed first in the solution file. Solution files are, luckily, text files. Solution files with the .sln extension are in a ridiculous ad-hoc format consisting of, among other things, entries delimited with \u0026quot;Project\u0026quot; and \u0026quot;EndProject\u0026quot; and making use of GUIDs to discourage anyone from touching it or even looking at it. Luckily they have now introduced .slnx files, which are a bit more lean, but they are now in XML, (Microsoft seems to be stuck in the 1980's,) so they are still far more verbose than necessary, and they still contain GUIDs here and there. Regardless of the file format, you are free to edit the solution file and re-order these entries in any way you like, and it seems to have absolutely no effect because the Visual Studio Solution Explorer will sort them alphabetically anyway, but there is one kind of re-ordering that you can do which actually matters: The entry which stands for your startup project must be the first entry.\nIf you do not do this, then each time you delete the .vs directory, restart Visual Studio, and try to launch your solution, you will be greeted with the all too familiar, extremely annoying, and extremely stupid message which says that you cannot launch a project which builds a library instead of an executable. This is happening because when the .vs directory is deleted, Visual Studio forgets the startup project, and when Visual Studio is launched with no startup project configured it absolutely has to establish a startup project, and it absolutely has to do this automatically, without asking you. In doing so, it picks the first project that it finds in the solution file, and it is not smart enough to skip projects that build libraries instead of executables.\nThe startup project must not be nested in a solution folder. As mentioned earlier, you can use \u0026quot;Solution Folders\u0026quot; to arrange your projects in a hierarchy; however, the project that you usually want to have as the startup project must not be nested in a solution folder, it must be placed right under the root node of the solution.\nIf you nest your startup project in a solution folder, then Visual Studio will again, entirely capriciously, ignore it when automatically selecting a project as the startup project each time you delete the .vs directory.\nTest projects must be given names that end in \u0026quot; Test\u0026quot;. (Yes, that is a space.) Usually, projects come in pairs: there is a production project, and a test project. Usually, we give the test project the same name as the production project, with a suitable suffix to indicate that this is a test project. If your production project is called \u0026quot;Covfefe\u0026quot;, you might think that you have plenty of options to call your test project: \u0026quot;CovfefeTest\u0026quot;, \u0026quot;Covfefe.Test\u0026quot;, \u0026quot;Covfefe-Test\u0026quot;, \u0026quot;Covfefe_Test\u0026quot;, etc. Actually, none of these will work. You have to call it \u0026quot;Covfefe Test\u0026quot;, with a space before \u0026quot;Test\u0026quot;.\nThis is because the Visual Studio Solution Explorer uses different sorting rules from Windows File Explorer, so if you use anything but \u0026quot; Test\u0026quot; as a suffix for test projects you will find your projects listed in a different order between the Solution Explorer and the File Explorer. Depending on what you choose, your test projects will appear either before or after your production projects, either in Solution Explorer or in File Explorer, but not in both. The only trick I have been able to find which causes the test projects to always be listed after the production projects both in Solution Explorer and in File Explorer is to make each test project name consist of the production project name suffixed with \u0026quot; Test\u0026quot; (with a space.)\nAaaand, of course, even with this, there is a catch: if you name your projects as I suggested, they will appear in the wrong order when viewing your solution directory on GitHub. Because GitHub uses yet different sorting rules. So, there is no way to achieve a consistent look both in Windows File Explorer, and Visual Studio Solution Explorer, and GitHub.\nSolution items must be placed in a custom \u0026quot;_Solution Items\u0026quot; folder. Solution-level items are a fiasco to begin with; they only exist because the Visual Studio Solution Explorer capriciously hides solution-level files from the user. Instead, Visual Studio offers the \u0026quot;Solution Items\u0026quot; workaround, which is a solution folder under the solution node where you can manually place links to solution-level files that you want to be able to access. (Because having to manually create links to your own files so that you can access them is always fun.)\nThe default name that Visual Studio gives to this folder is \u0026quot;Solution Items\u0026quot;. The first thing you must do with it is to rename it to \u0026quot;_Solution Items\u0026quot;, so that it will be sorted first in the list, otherwise it will be hidden among other solution folders.\nOld comments\nAnonymous 2024-12-27 16:05:14 UTC\nThank you for sharing your experiences and all the useful info. Visual Studio is increasingly becoming such a bloatware, while still missing quite a lot of basic fundamental functionality.\n","date":"2024-06-07T15:58:23.302Z","permalink":"https://blog.michael.gr/post/2024-06-visual-studio-solutions/","title":"How to organize a Visual Studio Solution"},{"content":"\rThe Mike Nakis formula for calculating the impact of an incident:\nI = S × G × T\nWhere:\nI is the impact of the incident. S is the severity of the incident. G is the geographic pervasiveness of the incident. T is the temporal pervasiveness of the incident. Thus:\nAn incident of high severity does not have high impact if it happens rarely and in only a few places. An incident of low severity can have high impact if it is persistent and widespread. For example:\nThe sinking of the Titanic was certainly a disaster, but it was largely an isolated incident: it happened only once, on the 15th of April 1912, and only in one place, at 41°43'32''N 49°56'49''W; we have not had anything quite like that happening before, and we have been doing a decent job at avoiding similar incidents ever since, so in the big picture, it is not of particularly high impact.\nOn the other hand:\nA modern computer taking several long seconds to reboot, despite having a multi-core, multi-gigahertz, hyper-threaded and pipelined CPU with multi-level cache and solid-state storage, is something that affects everyone, everywhere, every day, so it does in fact have quite a high impact.\n","date":"2024-05-31T12:53:30.924Z","permalink":"https://blog.michael.gr/post/2024-05-incident-impact-calculation/","title":"Incident Impact Calculation Formula"},{"content":"\rI have a lot to say about the modern trend in graphical user interface design which aims to achieve an impossibly clean look at the expense of usability, but this is going to be the subject of another blog post. In this post, I want to talk about simplifying the user interface when the simplification is clearly a win, both from a usability point of view and, incidentally, from an aesthetics point of view. Specifically, I want to show how a yes/no/cancel prompt can be reduced to just a yes/cancel prompt.\n(Useful pre-reading: About these papers)\nA typical example of such a prompt is when an application asks the user what to do when the user tries to quit the application while a file is unsaved.\nWe have two boolean variables:\nTo save or not to save. To quit or not to quit. Since there are two boolean variables, there is a total of four conceivable options:\nSave and quit. Quit without saving. Save without quitting. Do not save and do not quit. The very first programmers of interactive applications did not fail to notice that saving without quitting is not particularly useful, so in fact we only need three options, and this has given us the traditional triple-choice yes / no / cancel prompt, variations of which you see in almost all applications out there. One of the variations is as follows:\nSave the file before quitting? [Yes] / [No] / [Cancel]\nNeedless to say, presenting the user with an application-modal prompt containing not one, not two, but three options is terrible. (If you think that \u0026quot;terrible\u0026quot; is a harsh word for such a low-impact problem, then please read Incident Impact Calculation Formula.)\nNote that the replacement of a generic \u0026quot;Yes\u0026quot; / \u0026quot;No\u0026quot; / \u0026quot;Cancel\u0026quot; prompt with a more specific \u0026quot;Save and exit\u0026quot; / \u0026quot;Exit without saving\u0026quot; / \u0026quot;Do not exit\u0026quot; prompt is probably an improvement, but this is not what I am discussing here. I would like to reduce the number of choices to less than three; once the number of choices has been reduced, finding some better wording for the remaining choices is all the better.\nAlso note that the best solution to multiple choice application-modal prompts is of course to restructure software, to rethink software from scratch if need be, so that they can be completely eliminated. For example, all prompts about saving before quitting could be a thing of the past if we were to abandon the notion of saving, or even the notion of quitting. However, such exotic approaches are off-topic in this discussion.\nA blog author who has also examined the problem of triple-choice prompts, and explains it better than me, is Martin Kleppmann in \u0026quot;Yes/No/Cancel causes Aspirin sales to soar\u0026quot;. Interestingly enough, Martin Kleppmann follows a thought process which is similar to mine, but does not present a proposal as to what to do instead.\nSo, here is my contribution to the subject:\nQuite often, a triple choice prompt can be simplified to a dual-choice prompt!\nWe can eliminate the option to save and quit because we offer the option to not quit, which, if chosen, makes saving or not saving irrelevant: for as long as the application is still running, the user can always achieve saving and quitting by simply saving, and then quitting. (Duh!)\nThus, we can offer the following simplified prompt:\nQuit without saving? [Yes] / [Cancel]\nThe only way in which this could be further simplified would be to tell the user that they cannot quit because they have unsaved changes, and only show an [OK] button, but that would be annoying.\nThe traditional triple-choice prompt may have been invented for the benefit of users who are in the habit of quitting while having unsaved changes that they intend to keep, but I hope that we can all agree that this is not a healthy habit worth facilitating, certainly not if facilitating it would add the slightest bit of inconvenience to other, more legitimate, (and I suspect more frequently occurring,) use cases:\nThe user quits intending to revert changes. The user attempts to quit while unaware that they have made changes. The user does not intend to quit, but issues the quit command accidentally. Special workflows can be facilitated by separate commands that specially target such workflows. For example, saving every single unmodified file can be accomplished with a \u0026quot;Save All\u0026quot; command, and this has the benefits of:\nBeing useful at any time, not only when quitting, and Sparing the user from having to do one click per unmodified file. To summarize:\nPresenting the user with only two choices is immensely better than presenting the user with three choices, when the missing choice can be trivially accomplished by the user, in a way which is intuitively obvious to the user.\n","date":"2024-05-31T09:53:03.713Z","permalink":"https://blog.michael.gr/post/2024-05-simplification-of-triple-choice-prompts/","title":"Simplification of triple-choice prompts to dual-choice"},{"content":" So, today I had the chance to observe an example of the relative accuracy of buienalarm.nl vs. buienradar.nl\nTime: ~13:00 Around 13:00 I wanted to know what the weather was going to be at 17:00, and I noticed that buienradar was predicting complete dryness, whereas buienalarm was predicting torrential rain.\nHere are the screenshots:\nBuienradar ~13:00 Buienalarm ~13:00 Time: ~16:00 When I checked again around 16:00, buienradar was showing more or less the same thing, while buienalarm had completely changed its mind!\nBuienradar ~16:00 Buienalarm ~16:00 Time: 17:00 Sure enough, at 17:00 the weather was completely dry. Here is the view from my office window:\nOf course this is just a single observation; in another observation, the roles might be reversed. It will take many more comparisons before I can tell if there is a pattern.\nCover image: \u0026quot;Relief map of the European Netherlands\u0026quot; from Wikipedia\n","date":"2024-05-16T14:00:09.882Z","permalink":"https://blog.michael.gr/post/2024-05-buienalarm-vs-buienradar/","title":"Buienalarm vs- Buienradar"},{"content":"\rWhile working on code in the context of a certain task, a programmer often discovers some preexisting quality issue. When this happens, there is a choice to be made:\nFix the quality issue on the spot, and commit the fix in the context of the task at hand; or: Only make changes that are strictly necessary for the task at hand, and introduce a separate task for fixing the quality issue. (Useful pre-reading: About these papers)\nThe practice of continuously improving the quality of the code-base is called Boy-Scouting. The term comes from a rule of the Boy Scouts of America that says: Always leave the campground cleaner than you found it. In the first chapter of Clean Code by Robert C. Martin there is a short section titled The Boy Scout Rule where the author gives an adaptation of that rule for software: Always leave the code you are editing a little better than you found it.\nOpinions vary as to exactly what constitutes Boy-Scouting, and whether it is a good thing or a bad thing. Some people believe that Boy-Scouting should be avoided; they say that when a quality issue is discovered while working on a certain task, a new ticket must always be created and handled separately.\nI once worked at a company where different development teams within the same department had different approaches: most frowned upon boy-scouting, but some embraced it. Since Boy-Scouting had a bad name overall within that company, one team that insisted on practicing it decided to rebrand it by inventing their own term for it: they called it Quality Osmosis. (TopDesk, Team Octarine 2016-2017.)\nThe problem Arguments raised against Boy-Scouting include the following:\nBoy-Scouting interferes with time tracking, because the hours it takes to complete a certain task get inflated by fixing quality issues, whereas the hours spent on fixing quality issues is unaccounted for. Boy-Scouting renders quality improvements untraceable, because each quality improvement is hidden inside a commit for some other task. Boy-Scouting adds extra burden to code review, because the reviewer has to examine not only the changes for the task at hand, but also additional changes that are unrelated to, or strictly speaking unnecessary for, or in any case not directly aiming to solve, the task at hand. Furthermore, Boy-Scouting is viewed as undesirable in some management-heavy environments because it is seen as giving programmers too much autonomy:\nBoy-Scouting implies that programmers are free to get sidetracked, instead of always being laser-focused on (what management perceives to be) their job. Boy-Scouting implies that programmers have carte blanche to be making any modifications they see fit to the code base without prior authorization, so who knows what else they might be doing. (Note that these concerns are rarely voiced, but often implied.)\nLet us look at the different kinds of quality issues that may be encountered while working on a task:\nLow-impact - e.g. a misspelled identifier or a violation of formatting guidelines. Medium-impact - e.g. a code construct which is more convoluted than necessary and could be simplified to make it more understandable. High-impact: - e.g. a previously undiscovered bug. Low-impact issues\nA low impact quality issue is by definition so trivial that it is not worth creating a new task for it; imagine a ticket with the title \u0026quot;fix spelling mistake in identifier such-and-such\u0026quot; or \u0026quot;abstract class so-and-so constructor is public, make it protected\u0026quot;; nobody wants to go through the motions of creating, grooming, and resolving such tickets; everyone is better off if no such tickets ever exist. So, unless the low-impact issue gets fixed the moment it is discovered, and committed as part of the task at hand, it will never get fixed. This means that we can safely establish the following rule:\nWhen a low-impact quality issue is discovered while working on a certain task, the quality issue should be fixed on the spot, and committed as part of the task at hand.\nNote that fixing a spelling mistake may involve modifying a large number of source files, which would not otherwise need to be modified in the context of the task at hand; this means that the code reviewer will have more work to do; we will address this problem later.\nMedium-impact and high-impact issues\nA medium-impact or high-impact quality issue does, in principle, deserve creating a separate ticket for, but first we need to ask whether it can actually be handled separately from the task at hand, because if not, then creating a separate ticket is moot point.\nIf it is known beyond a shadow of a doubt that the quality issue does not interfere at all with the task at hand, then the straightforward approach is in fact to create a separate ticket for the quality issue, and handle it after the task at hand has been completed; however, it is often very difficult to know for sure that a piece of code really does not affect another piece of code. There may be obscure mechanisms at play, through which a malfunction creeps through the system in non-obvious ways, and manifests in another place that nobody expected.\nWhen we come across a code construct which is convoluted or buggy, what we essentially have in our hands is code that works in mysterious ways, so we do not necessarily know exactly how it behaves; we could try to reason about that code, throw the debugger at it if necessary, get down to the bottom of it, and determine whether it does in fact interfere or not with the task at hand, but:\nNobody has time for that. It is futile, because the convoluted or buggy code has to be fixed anyway. Whatever conclusion you arrive to, can you actually be sure? Ascertaining that two different parts of a software system are completely isolated from each other is difficult, in the same way that it is difficult to ascertain pretty much anything when it comes to code. (See Halting Problem.) As a matter of fact, it is so difficult, that we usually prefer to not have to ascertain things ourselves, and to write tests instead, that will ascertain things for us. However, writing tests to check whether flawed code interferes or not with other code is an exercise in futility, especially if we consider that the flawed code has to be fixed, sooner or later, anyway.\nTherefore, when a quality issue is discovered while working on a certain task, and we suspect that it might be interfering with the task, it must be treated as if it does.\nThe bureaucracy Now, some will insist that even if the quality issue interferes, or is suspected to interfere, with the task at hand, and must therefore be resolved before the task can be completed, the quality issue must nonetheless be resolved as a separate ticket. Let us look at the workflow necessary for that:\nWork on the task at hand must be suspended. A new ticket must be created for the quality issue, and given immediate priority. A new branch must be created, from master, for fixing the quality issue. A fix for the quality issue must be devised, coded, tested, and committed. The changes made in the new branch must undergo code review. The reviewed branch must be merged into the master branch. The master branch must be merged into the branch of the task at hand. Merge conflicts must be resolved. Work on the task at hand can now resume. Note that merge conflicts are very likely to happen even if nobody else in the entire shop has touched the code in the mean time, because the quality issue was discovered while working on the task at hand, therefore the code affected by fixing the quality issue most likely overlaps with code that has already been modified in the context of the task at hand.\nAlso note that the programmer who is likely to fix the quality issue is the same programmer who was working on the original task, because they are probably free, since work on the original task has been suspended. When this programmer started working on the original task, they branched off from master; the code coming from master had a certain shape, and then they started making changes to it, giving it a new shape, which they are now intimately familiar with. By going back and branching off from master again in order to fix the quality issue, this programmer is now faced with the code in its original shape, which is in conflict with the shape that they have become intimately familiar with; this is brainfuck. When the programmer is done fixing the quality issue and returns to the branch of the task at hand, there is bound to be more brainfuck.\nApparently, there are no limits to the bureaucracy and the inconvenience that some people are willing to suffer in the name of some purist notion of \u0026quot;doing things right\u0026quot;. I prefer a more pragmatic approach.\nEnter Continuous Code Quality Improvement (CCQI) a.k.a. Boy-Scouting / Quality Osmosis.\nWhen a quality issue is discovered while working on a certain task, and the issue interferes, or it is suspected to interfere, with the task at hand, then the quality issue should be fixed on the spot, (while the programmer is in the flow,) and committed as part of the task at hand.\nIf someone wants to create a ticket, fine, but then a single commit will constitute a fix for multiple tickets.\nOf course, an exception to this rule is the case where fixing the quality issue is going to be a month-long project in and of itself, in which case another pragmatic approach is necessary: make do with the quality issue as it is for the time being, and handle it later, when a month is available to spare.\nWhat about time-tracking? As mentioned earlier, Boy-Scouting interferes with time tracking, because the hours it takes to complete a certain task get inflated by fixing quality issues, whereas the hours spent on fixing quality issues is unaccounted for.\nThe solution to this is simple: whoever is in charge of time-tracking should be using larger error bars. The work of the programmers is difficult enough as it stands, it will not be made more difficult for the convenience of those who do time-tracking.\nWhat about the commit history? As mentioned earlier, Boy-Scouting renders the history of quality improvements untraceable, because each quality improvement is hidden inside a commit for some other task.\nThe solution to this is also simple: there shall be no traceable history for quality improvements, because quality improvements are being continuously applied to the code-base as it is being worked on.\nWhat about code review? As mentioned earlier, Boy-Scouting adds extra burden to code review, because the reviewer has to examine not only changes pertinent to the task at hand, but also changes that are unrelated to, or strictly speaking unnecessary for, or in any case not directly aiming to solve, the task at hand.\nThe solution to this is the simplest of all: the reviewer needs to get used to it.\nAnd what about the management? As mentioned earlier, management tends to be skeptical of Boy-Scouting because it gives programmers too much autonomy.\nThe solution to this is very similar to the solution for code review: the management needs to get used to it. If programmers are competent enough to work on software development tasks, they are competent enough to decide exactly what needs to be done in the context of each task.\nCover image by michael.gr based on \u0026quot;Quality\u0026quot; by Sutriman (CC BY 3.0), \u0026quot;Thumb up\u0026quot; by Sewon Park (CC BY 3.0), and \u0026quot;Five Stars\u0026quot; by Tyler Gobberdiel (CC BY 3.0), from the Noun Project.\n","date":"2024-04-12T10:48:53.709Z","permalink":"https://blog.michael.gr/post/2024-09-boy-scouting/","title":"Continuous Code Quality Improvement"},{"content":"\rAbstract In this paper I put forth the proposition that contrary to popular belief, 100% code coverage can be a very advantageous thing to have, and I discuss a technique for achieving it without excessive effort.\n(Useful pre-reading: About these papers)\nThe problem Conventional wisdom says that 100% code coverage is unnecessary, or even undesirable, because achieving it requires an exceedingly large amount of effort not for the purpose of asserting correctness, but instead for the sole purpose of achieving coverage. In other words, it is often said that 100% code coverage has no business value.\nLet me tell you why this is wrong, and why 100% code coverage can indeed be a very good thing to have.\nIf you don't have 100% code coverage, then by definition, you have some lower percentage, like 87.2%, or 94.5%. The remaining 12.8%, or 5.5% is uncovered. I call this the worrisome percentage.\nAs you keep working on your code base, the worrisome percentage fluctuates:\none day you might add a test for some code that was previously uncovered, so the worrisome percentage decreases; another day you may add some code with no tests, so the percentage increases; yet another day you may add some more code along with tests, so even though the number of uncovered lines has not changed, it now represents a smaller percentage; ... and it goes on like that.\nIf the worrisome percentage is high, then you know for sure that you are doing a bad job, but if it is low, it does not mean that you are doing a good job, because some very important functionality may be left uncovered, and you just do not know. To make matters worse, modern programming languages offer constructs that achieve great terseness of code, meaning that a few uncovered lines may represent a considerable amount of uncovered functionality.\nSo, each time you look at the worrisome percentage, you have to wonder what is in there: are all the important lines covered? are the uncovered lines okay to be left uncovered?\nIn order to answer this question, you have to go over every single line of code in the worrisome percentage, and examine it to determine whether it is okay that it is being left uncovered. What you find is, more often than not, the usual suspects:\nSome ToString() function which is only used for diagnostics; Some Equals() and HashCode() functions of some value type which does not currently happen to be used as a key in a hash-map; Some default switch clause which can never be reached, and if it was to ever be reached it would throw; ... etc.\nSo, your curiosity is satisfied, your worries are allayed, and you go back to your usual software development tasks.\nA couple of weeks later, the worrisome percentage has changed again, prompting the same question: what is being left uncovered now?\nEach time you need to have this question answered, you have to re-examine every single line of code in the worrisome percentage. As you do this, you discover that in the vast majority of cases, the lines that you are examining now are the exact same lines that you were examining the previous time you were going through this exercise. After a while, this starts getting tedious. Eventually, you quit looking. Sooner or later, everyone in the shop quits looking.\nThe worrisome percentage has now become terra incognita: literally anything could be in there; nobody knows, and nobody wants to know, because finding out is such a dreary chore.\nThat is not a particularly nice situation to be in.\nThe solution So, here is a radical proposition: If you always keep your code coverage at 100%, then the worrisome percentage is always zero, so there is nothing to worry about!\nWhen the worrisome percentage is never zero, then no matter how it fluctuates, it never represents an appreciable change in the situation: it always goes from some non-zero number to some other non-zero number, meaning that we used to have some code uncovered, and we still have some code uncovered. No matter what happens, there is no actionable item.\nOn the other hand, if the worrisome percentage is normally zero, then each time it rises above zero it represents a definite change in the situation: you used to have everything covered, and now you have something uncovered. This signifies a clear call to action: the code that is now being left uncovered needs to be examined, and dealt with.\nBy dealing with uncovered code as soon as it gets introduced, you bring the worrisome percentage back to zero, thus achieving two things:\nYou ensure that next time the worrisome percentage becomes non-zero, it will represent a new call to action. You never find yourself in the unpleasant situation of re-examining code that has been examined before; so, the examination does not feel like a dreary chore. The conventional understanding of how to deal with uncovered code is to write a test for it, and that is why achieving 100% code coverage is regarded as onerous; however, there exist alternatives that are much easier. For any given piece of uncovered code, you have three options:\nOption #1: Write a test for the code.\nThis is of course the highest quality option, but it does not always represent the best value for money, and it is not even always possible. You only need to do it if the code is important enough to warrant testing, and you can only do it if the code is in fact testable. If you write a test, you can still minimize the effort of doing so, by utilizing certain techniques that I talk about in other posts, such as Audit Testing, Testing with Fakes instead of Mocks, and Incremental Integration Testing.\nOption #2: Exclude the code from code coverage.\nCode that is not testable, or not important enough to warrant testing, can be moved into a separate module which does not participate in coverage analysis. Alternatively, if your code coverage analysis tool supports it, you may be able to exclude individual methods without having to move them to another module. In the DotNet world, this can be accomplished by marking a method with the ExcludeFromCodeCoverage attribute, found in the System.Diagnostics.CodeAnalysis namespace. In the Java world, IntelliJ IDEA offers a setting for specifying what annotation we want to use for marking methods to be excluded from code coverage, so you can use any annotation you like. (See IntelliJ IDEA can now exclude methods from code coverage.) Various different code coverage analyzers support additional ways of excluding code from coverage.\nOption #3: Artificially cover the code.\nWith the previous two options you should be able to bring the worrisome percentage down to a very small number, like 1 or 2 percent. What remains is code which should really be excluded from coverage, but it cannot, due to limitations in available tooling: although code coverage analyzers generally allow excluding entire functions from coverage analysis, they generally do not offer any means of excluding individual lines of code, such as the unreachable default clause of some switch statement. You can try moving that line into a separate function, and excluding that function, but you cannot exclude the call to that function, so the problem remains.\nThe solution in these cases is to cause the uncovered code to be invoked during testing, not in order to test it, but simply in order to have it covered. This might sound like cheating, but it is not, because the stated objective was not to test the code, it was to exclude it from coverage. You would have excluded that line from coverage if the tooling supported doing so, but since it does not, the next best thing, (and the only option you are left with,) is to artificially include it in the code coverage.\nHere is a (hopefully exhaustive) list of all the different reasons due to which code might be left uncovered, and what to do in each case:\nThe code should really be covered, but you forgot to write tests for it, or you have plans to write tests in the future.\nGo with Option #1: write tests for it. Not in the future, now.\nThe code is not used and there is no plan to use it.\nThis is presumably code which exists for historical reasons, or for reference, or because it took some effort to write it and you do not want to admit that the effort was a waste by throwing away the code.\nGo with Option #2 and exclude it from coverage.\nThe code is only used for diagnostics.\nThe prime example of this is ToString() methods that are not normally invoked in a production environment, but give informative descriptions of our objects while debugging.\nGo with Option #2: Exclude such methods from coverage.\nThe code is not normally reachable, but it is there in case something unexpected happens.\nThe prime example of this is C# switch statements that cover all possible cases and yet also contain a default clause just in case an unexpected value somehow manages to creep in.\nGo with Option #3: Artificially cover such code. This may require a bit of refactoring to make it easier to cause the problematic switch statement to be invoked with an invalid value. The code most likely throws, so catch the exception and swallow it. You can also assert that the expected exception was thrown, in which case it becomes more like Option #1: a test.\nThe code is reachable but not currently being reached.\nThis is code which is necessary for completeness, and it just so happens that it is not currently being used, but nothing prevents it from being used at any moment. A prime example of this is the Equals() and HashCode() functions of value types: without those functions, a value type is incomplete; however, if the value type does not currently happen to be used as a key in a hash-map, then those functions are almost certainly unused.\nIn this case, you can go with any of the three options:\nYou can go with Option #1 and write a proper test. You can go with Option #2 and exclude the code. You can go with Option #3 and artificially cover the code. The code is not important enough to have a test for it.\nSay you have a function which takes a tree data structure and converts it to text using box-drawing characters so as to be able to print it nicely as a tree on the console. Since the function receives text and emits text, it is certainly testable, but is it really worth testing? If it ever draws something wrongly, you will probably notice, and if you do not notice, then maybe it did not matter anyway.\nIn this case you can go either with Option #2 and exclude such functions, or Option #3 and artificially cover them.\nThe code is literally or practically untestable.\nFor example:\nIf your application has a Graphical User Interface (GUI), you can write automated tests for all of your application logic, but the only practical way to ascertain the correctness of the GUI is to have human eyes staring at the screen. (There exist tools for testing GUIs, but I assess them as woefully impractical and acutely ineffective.)\nIf your application controls some hardware, you may have a hardware abstraction layer with two implementations, one which emulates the hardware, and one which interacts with the actual hardware. The emulator will enable you to test all of your application logic without having the actual hardware in place; however, the implementation which interacts with the actual hardware is practically untestable by software alone.\nIf you have a piece of code that queries the endianness of the hardware architecture and operates slightly differently depending on it, the only path you can truly cover is the one for the endianness of the hardware architecture you are actually using. (You can fake the endianness query, and pretend that your hardware has the opposite endianness, but you still have no guarantees as to whether the bit-juggling that you do in that path is right for the opposite endianness.)\nIn all of the above cases, and in all similar cases, we have no option but #2: exclude the code from coverage.\nConclusion If testing has business value, then 100% code coverage has business value, too.\nA code coverage percentage of 100% is very useful, not for bragging, but for maintaining certainty that everything that ought to be tested is in fact being tested.\nAchieving a code coverage percentage of 100% does require some effort, but with techniques such as Artificial Coverage the effort can be reduced to manageable levels.\nIdeally, Artificial Coverage should never be necessary, but it is a practical workaround for the inability of coverage tools to exclude individual lines of code from analysis.\nCover image by Patrick Robert Doyle from Unsplash\n","date":"2024-03-26T15:01:55.206Z","permalink":"https://blog.michael.gr/post/2024-03-codecoverage/","title":"Artificial Code Coverage"},{"content":"\rBiological anthropologists generally agree that humans evolved reasoning to facilitate hunting together as a group; however, there are many other species that hunt in packs, and yet reasoning is unique to humans. Therefore, in order to explain reasoning, it is not enough to consider how it was beneficial to us; we also need to consider what enabled reasoning to emerge specifically in humans as opposed to any other species.\nI have a hypothesis which attempts to explain how this happened.\n(Useful pre-reading: About these papers)\nExaptation of Neural Circuitry for Arboreal NavigationA hypothesis for the emergence of reasoning in early humans When our simian ancestors first stepped down from the canopy of the forest to the ground of the savanna, they were still pretty well adapted to arboreal life, so they still had with them all the machinery necessary for arboreal locomotion by means of brachiation.\nMost of that machinery was evident in their anatomy:\nStereoscopic vision for determining distances to branches. Opposable thumbs for grabbing and holding on to branches. Strong upper limbs for suspending the entire body weight from a branch. The machinery also included a very important element in the brain:\nNeural circuits dedicated to the planning and execution of arboreal navigation. Just as brachiation is a formidable feat of the simian anatomy, so is arboreal navigation a formidable feat of the simian brain.\nGetting from branch A to branch Z in the forest canopy involves examination of multiple alternative routes, each consisting of several successive leaps from branch to branch, where at each branch multiple choices are open, but some choices lead to dead-ends. Leaps often have to be evaluated in advance (planning) not only in order to determine the feasibility of the route as a whole, but also to find the most optimal route. The process involves some techniques which we have only started to grasp in the era of information processing, through the study of pathfinding algorithms:\nBack-tracking: when the next leap in the path is deemed as not feasible, an alternative route can be sought from an earlier branch in the same path instead of starting from the beginning. Information reuse: when it has been determined that from a certain branch X there is a path to reach branch Y, this information can be reused when an alternative way is found for arriving at branch X. Reverse pathfinding: to plan a path from branch A to branch Z one can start from A, consider leaps to branches near A, and so on; however, if most of those seem to lead to dead ends, one may alternatively plan a path by doing the reverse: starting from Z, considering feasible leaps to Z from branches near Z, and so on, until a branch is found which is known to be reachable. Brachiation is an activity that simians carry out effortlessly, at incredibly high speeds, and flawlessly, because a single mistake can easily result in a broken bone, which equates to certain death in the wild. Furthermore, (and I do not know this for sure, but I think it is a pretty safe guess,) simian babies do not have to learn how to brachiate, they are pretty much born with the ability. Therefore, the simian brain must have built-in neural circuits specifically dedicated to the task of arboreal navigation and the planning thereof.\nOnce our simian ancestors settled on the ground of the savanna, the neural circuits for arboreal navigation fell into disuse, essentially becoming vestigial. At the same time, new challenges and opportunities arose: food on the savanna was scarce, so humans had to hunt. The hunt was generally big, and humans did not have claws and teeth, so they had to coordinate in order to be successful.\nMy hypothesis is that via mechanisms that I cannot claim to know, (since I am not a bioanthropologist, nor a neurobiologist,) the neural circuitry for arboreal navigation was repurposed in the human brain for the task of reasoning. Repurposing of traits is known by evolutionary biology to be common, not only in anatomy but also in behavior; it is called exaptation.\nThe anatomical machinery that enabled simians to brachiate is still present in us today, so it is not a stretch to imagine that the neural machinery is also present. Human babies have a lot to learn until they can function autonomously in the world, but reasoning comes rather naturally to them, and education seems to build upon a preexisting foundation which innately supports reasoning. Therefore, reasoning seems to also be largely based on dedicated neural circuitry in the human brain.\nThe fundamental workflows of reasoning and arboreal navigation bear a striking resemblance to each other: from a certain set of premises, a number of potential inferences can be examined; some of those can be ignored due to being irrelevant to the desired conclusion; they correspond to leaps in directions away from the desired destination. Other potential inferences can be disregarded due to being false; these correspond to leaps that are impossible to make. Each of the remaining inferences represents a new set of premises, from which further potential inferences can be examined, thus bringing us closer and closer to the desired conclusion. Some chains of inferences lead to dead-ends, at which point we go back one or more steps to examine a different path; these correspond to back-tracking in path-finding. In all these cases, the neural circuitry we use for reasoning may be that very same neural circuitry that we used to use for arboreal navigation but fell into disuse when we came down from the trees.\nThe necessity to coordinate for hunting may have acted as a catalyst for the repurposing of neural circuitry from the task of arboreal navigation to the task of reasoning. The Gorilla is another great ape that descended from the trees to the ground, but they have remained vegetarian, so their food was free for the grabbing in the trees, and continued to be free for the grabbing on the ground. Therefore, Gorillas had no need to form packs and go hunting for food, and thus no evolutionary pressure to do any repurposing of neural circuitry.\nMore light might be shed on this hypothesis from research on the anatomical connectivity between the cerebellum, which is associated with motor control, and the frontal lobe, which is associated with problem solving.\nCover image: \u0026quot;Thinking Caveman\u0026quot;, AI-generated at canva.com from a prompt by michael.gr\n","date":"2024-03-10T11:24:24.444Z","permalink":"https://blog.michael.gr/post/2024-10-reasoning/","title":"On the evolutionary origin of reasoning"},{"content":"\rWhat is the most important quality of software?\nCorrectness, they say.\nAnd what is the second most important quality of software?\nReadability, they say.\nThat is right, but only in theory.\n(Useful pre-reading:About these papers)\nIf it was possible to write software just once, and never touch it again, then indeed, the most important thing, perhaps the only important thing, would be correctness. But in practice, this never happens.\nIn the real world, requirements change, execution environments change, interfaces change. When that happens, we have software which used to be correct, but it is not correct anymore, and needs to be fixed to make it correct.\nHowever, in order to fix software you have to be able to read it.\nThus, in practice, readability is a prerequisite for correctness, which brings us to the astounding realization that readability is even more important than correctness.\nCover image by Fotis Fotopoulos on Unsplash\n","date":"2024-03-07T18:12:01.077Z","permalink":"https://blog.michael.gr/post/2024-03-the-most-important-quality-of-software/","title":"The most important quality of software"},{"content":"\rAbstract An automated software testing technique is presented which spares us from having to stipulate our expectations in test code, and from having to go fixing test code each time our expectations change.\n(Useful pre-reading: About these papers)\nThe Problem The most common scenario in automated software testing is ensuring that given specific input, a component-under-test produces expected output. The conventional way of achieving this is by feeding the component-under-test with a set of predetermined parameters, obtaining the output of the component-under-test, comparing the output against an instance of known-good output which has been hard-coded within the test, and failing the test if the two are not equal.\nThis approach works, but it is inefficient, because during the development and evolution of a software system we often make changes to the production code fully anticipating the output of certain components to change. Unfortunately, each time we do this, the tests fail, because they are still expecting the old output. So, each change in the production code must be followed by a round of fixing tests to make them pass.\nNote that under Test-Driven Development things are not any better: first we modify the tests to start expecting the new output, then we observe them fail, then we modify the components to produce the new output, then we watch the tests pass. We still have to stipulate our expectations in test code, and we still have to change test code each time our expectations change, which is inefficient.\nThis imposes a considerable burden on the software development process. As a matter of fact, it often happens that programmers refrain from making needed changes to their software because they dread the prospect of having to fix all the tests that will break as a result of those changes.\nAudit Testing is a technique for automated software testing which aims to correct all this.\nThe Solution Under Audit Testing, the assertions that verify the correctness of the output of the component-under-test are abolished, and replaced with code that simply saves the output to a text file. This file is known as the Audit File.\nThe test may still fail if the component-under-test encounters an error while producing output, in which case we follow a conventional test-fix-repeat workflow, but if the component-under-test manages to produce output, then the output is saved in the Audit File and the test completes successfully without examining it.\nThe trick is that the Audit File is saved right next to the source code file of the test, which means that it is kept under Version Control. In the most common case, each test run produces the exact same audit output as the previous run, so nothing changes, meaning that all is good. If a test run produces different audit output from a previous test run, then the tooling alerts the developer to that effect, and the Version Control System additionally indicates that the Audit File has been modified and is in need of committing. Thus, the developer cannot fail to notice that the audit output has changed.\nThe developer can then utilize the \u0026quot;Compare with unmodified\u0026quot; feature of the Version Control System to see the differences between the audit output that was produced by the modified code, and the audit output of the last known-good test run. By visually inspecting these differences, the developer can decide whether they are as expected or not, according to the changes they made in the code.\nIf the observed differences are not as expected, then the developer needs to keep working on their code until they are. If the observed differences are as expected, then the developer can simply commit the new code, along with the new Audit File, and they are done. This way, we eliminate the following burdens:\nHaving to hard-code into the tests the output expected from the component-under-test.\nHaving to assert, in each test, that the output of the component-under-test matches the expected output.\nHaving to go fixing test code each time there is a (fully expected) change in the output of the component-under-test.\nThe eliminated burdens are traded for the following much simpler responsibilities:\nThe output of the component-under-test must be converted to text and written to an audit file.\nWhen the version control system shows that an audit file changed after a test run, the differences must be reviewed, and a decision must be made as to whether they are as expected or not.\nTests and production code must be written with some noise reduction concerns in mind. (More on that further down.)\nThis represents a considerable optimization of the software development process.\nNote that the arrangement is also convenient for the reviewer, who can see both the changes in the code and the resulting changes in the Audit Files.\nAs an added safety measure, the continuous build pipeline can deliberately fail the tests if an unclean working copy is detected after running the tests, because that would mean that the tests produced different results from what was expected, or that someone failed to commit some updated audit file.\nNoise reduction For Audit Testing to work effectively, all audit output must be completely free of noise. By noise we mean:\nTwo test runs of the exact same code producing different audit output. A single change in the code producing wildly different audit output. For example, if a test emits the username of the current user into the audit output, then the audit file generated by that test will be different for every user that runs it, even if the user does not modify any code.\nNoise is undesirable, because:\nNeedlessly modified audit files are a false cause of alarm. Examining changes in audit files only to discover that they are due to noise is a waste of time. A change that might be important to notice can be lost in the noise. Noise in audit files is most commonly caused by various sources of non-determinism, such as:\nWall-clock time.\nAs the saying goes, the arrow of time is always moving forward. This means that the \u0026quot;current\u0026quot; time coordinate is always different from test run to test run, and this in turn means that if any wall-clock timestamps find their way into the audit output, the resulting audit file will always be different from the previous run. So, for example, if your software generates a log, and you were thinking of using the log as your audit output, then you will have to either remove the timestamps from the log, or fake them. Faking the clock for the purpose of testing is a well-known best practice anyway, regardless of audit testing. To accomplish this, create a \u0026quot;Clock\u0026quot; interface, and propagate it to every place in your software that needs to know the current time. Create two implementations of that interface: one for production, which queries the actual wall-clock time from the operating environment, and one for testing, which starts from some fixed, known origin and increments by a fixed amount each time it is queried.\nRandom number generation.\nRandom number generators are usually pseudo-random, and we tend to make them practically random by seeding them with the wall-clock time. This can be easily fixed for the purpose of testing by seeding them with a known fixed value instead. Some pseudo-random generators seed themselves with the wall-clock time without allowing us to override this behavior; this is deplorable. Such generators must be faked in their entirety for the purpose of testing. This extends to any other constructs that employ random number generation, such as GUIDs/UUIDs: they must also be faked when testing, using deterministic generators.\nMulti-threading.\nMultiple threads running in parallel tend to exhibit unpredictable timing irregularities, and result in a chaotically changing order of events. If these threads affect audit output, then the ordering of the content of the audit file will be changing on every test run. For this reason, multi-threading must either be completely avoided when testing, or additional mechanisms (queuing, sorting, etc.) must be employed to guarantee a consistent ordering of audit output.\nFloating-point number imprecision.\nFloating-point calculations can produce slightly different results depending on whether optimizations are enabled or not. To ensure that the audit file is unaffected, any floating point values emitted to the audit file must be rounded to as few digits as necessary. At the very least, they must be rounded to one digit less than their full precision.\nOther external factors.\nUser names, computer names, file creation times, IP addresses resolved from DNS, etc must either be prevented from finding their way into the audit output, or they must be faked when running tests. Fake your file-system; fake The Internet if necessary. For more information about faking stuff, see Testing with Fakes instead of Mocks.\nIn short, anything that would cause flakiness in software tests will cause noisiness in Audit Testing.\nAdditionally, the content of audit files can be affected by some constructs that are fully deterministic in their nature. These constructs will never result in changed audit files without any changes in the code, but may produce drastically different audit files as a result of only minute changes in the code. For example:\nHash Table Rehashing.\nA hash table may decide to re-hash itself as a result of a single key addition, if that addition happens to cause some internal load factor threshold to be exceeded. Exactly when and how this happens depends on the implementation of the hash table and we usually have no control over it. After re-hashing, the order in which the hash table enumerates its keys is drastically different, and if the keys are emitted to audit output, then the audit file will be drastically different. To avoid this, replace plain hash tables with hash tables that retain the order of key insertion.\nInsufficient Sorting Keys.\nWhen sorting data, the order of items with identical keys is undefined. It is still deterministic, but the addition or removal of a single item can cause all items with the same sorting key to be arbitrarily rearranged. To avoid this, always use a full set of sorting keys when sorting data, so as to give every item a specific unique order. Introduce additional sorting keys if necessary, even if you would not normally have a use for them.\nNoise reduction aims to ensure that we will never see changes in the audit files unless there have been changes in the code, and that for every unique change in the code we will see a specific expected set of changes in the audit output, instead of a large number of irrelevant changes. This ensures that the single change that matters will not be lost in the noise, and makes it easier to determine that the modifications we made to the code have exactly the intended consequences and not any unintended consequences.\nNote that in some cases, noise reduction can be implemented in the tests rather than in the production code. For example, instead of replacing a plain hash table with an ordered hash table in production code, our test can obtain the contents of the plain hash table and sort them before writing them to the audit file. However, this may not be possible in cases where the hash table is several transformations away from the auditing. Thus, replacing a plain hash table with an ordered hash table may sometimes be necessary in production code.\nNoise reduction in production code can be either always enabled, or only enabled during testing. The most performant choice is to only have it enabled during testing, but the safest choice is to have it always enabled.\nFailure Testing Failure Testing is the practice of deliberately supplying the component-under-test with invalid input and ensuring that the component-under-test detects the error and throws an appropriate exception. Such scenarios can leverage Audit Testing by simply catching exceptions and serializing them, as text, into the audit output.\nApplicability Audit Testing is most readily useful when the Component Under Test produces results as text, or results that are directly translatable to text. With a bit of effort, any kind of output can be converted to text, so Audit Testing is universally applicable.\nMust Audit Files be committed? It is in theory possible to refrain from storing Audit Files in the source code repository, but doing so would have the following disadvantages:\nIt would deprive the code reviewer from the convenience of being able to see not only the changes in the code, but also the differences that these changes have introduced in the audit output of the test. It would require the developer to always remember to immediately run the tests each time they pull from the source code repository, so as to have the unmodified Audit Files produced locally, before proceeding to make modifications to the code which would further modify the Audit Files. It would make it more difficult for the developer to take notice when the Audit Files change. It would make it more difficult for the developer to see diffs between the modified Audit Files and the unmodified ones. Of course all of this could be taken care of with some extra tooling. What remains to be seen is whether the effort of developing such tooling can be justified by the mere benefit of not having to store Audit Files in the source code repository.\nConclusion Audit Testing is a universally applicable technique for automated software testing which can significantly reduce the effort of writing and maintaining tests by sparing us from having to stipulate our expectations in test code, and from having to go fixing test code each time our expectations change.\nCover image: \u0026quot;Audit Testing\u0026quot; by michael.gr.\n","date":"2024-02-09T14:55:40.808Z","permalink":"https://blog.michael.gr/post/2024-04-audit-testing/","title":"Audit Testing"},{"content":"\rThe term \u0026quot;dependency\u0026quot; is used very often in software engineering, but depending on context, it may mean slightly different things. To avoid confusion, here are the different meanings of the term, and their explanations.\n(Useful pre-reading: About these papers)\nCompile-time (static) dependency: When module A makes use of a symbol which is defined in module B, we say that A has a compile-time dependency on B. (Or that B is a compile-time dependency of A.) This happens not only when module A contains a hard-coded invocation to module B, but also when A makes use of some definition from B, such as referring to a constant, implementing an interface, or instantiating a type defined in B. Runtime (dynamic) dependency: When module A is given, at runtime, a reference to invoke module B, then we have a runtime dependency between A and B. Runtime dependencies can be further divided in two sub-categories: Assembly-time (semi-dynamic) dependency: This is a runtime dependency which is realized during system assembly, and remains unchanged throughout the lifetime of the system. Post-assembly-time (fully dynamic) dependency: This is a runtime dependency which may be realized or changed at any moment, by having one module programmatically pass a callback to another module. If we are to take the Dependency Inversion Principle (DIP) for granted in software architecture, (and we should,) then software architecture is not concerned with static dependencies. This is because the DIP states that concrete modules should never statically depend on other concrete modules; instead, concrete modules may statically depend only on abstractions. Thus, the DIP is advising us to build our concrete modules so that they have no knowledge of each other. Instead, they should be making outgoing invocations to interfaces, and these invocations should be wired to concrete modules implementing those interfaces. Interfaces are abstractions, so it is okay for a concrete module to have compile-time dependencies on modules defining such abstractions.\nAssembly-time dependencies are what software architecture is mostly concerned with. The architecture of a software system specifies how to wire interface invocations between components. The wiring prescribed by the design is normally performed during system assembly, which is part of system deployment. Thus, the wires constitute assembly-time dependencies, and the graph of these dependencies is essentially the call graph of the system as defined by the architecture.\nPost-assembly-time dependencies do not affect the topology of a design, because every post-assembly time dependency requires an existing assembly-time dependency through which the callback can be communicated. Thus, post-assembly-time dependencies constitute implementation details of the modules that supply and invoke callbacks. As such, they are of only limited interest in software architecture.\n","date":"2024-01-11T17:33:42.741Z","permalink":"https://blog.michael.gr/post/2024-01-types-of-dependencies/","title":"Types of dependencies"},{"content":"\rAbstract In technical design of software systems as conventionally practiced, call graphs often contain cycles. We show that cyclic call graphs are highly problematic for a number of reasons, the most important being that they require careful handling on a case-by-case basis by custom-written code, thus preventing the standardization, and therefore the automation, of system assembly. We discuss refactoring strategies for systematically eliminating call cycles, including a universally applicable technique for trivially eliminating a certain common type of call cycle. We conclude that since call cycles can be avoided or eliminated, they can be comprehensively disallowed, thus paving the way for the automation of system assembly.\n(Useful pre-reading: About these papers)\nWhat is a cycle in a call graph When component A invokes component B, and component B also invokes component A, we say that the call graph contains a direct cycle. If A invokes B, which invokes C, which in turn invokes A, we say that the call graph contains an indirect cycle. If A invokes itself, we say that the call graph contains a self-loop, or a buckle, which is a special case of a cycle. In general, if a component diagram contains any path of invocations starting at a certain component and arriving back at the same component, the call graph contains a cycle.\nAs an example, let us consider the simplest possible scenario, consisting of just two components:\nA temperature sensor component, whose job is to obtain a temperature value from some piece of hardware, and make that value available within the software system.\nA temperature indicator component, whose job is to display a temperature on the screen.\nIn this diagram, each component has inputs and outputs, collectively known as pins:\nAn input is an endpoint for receiving incoming interface invocations; it represents an interface exposed by a component for invocation by other components. It is signified by an arrow pointing into the component. An output is an endpoint through which a component places outgoing interface invocations; it represents an interface that a component wants to invoke. It is signified by an arrow pointing out of the component. For those familiar with UML component diagrams, an input is a provided interface in UML, and an output is a required interface in UML. In this paper we use arrows to show the direction of invocations from output to input, instead of the socket-and-lollipop notation of UML.\nEach input and output has a name and a type. Obviously, an output can be connected to an input only if their types match.\nThe temperature sensor:\nHas an input called Reading, of type ReadonlyFloat, which can be invoked by some other component to obtain the current value of the temperature. Has an output called Changed, of type Procedure0 (same thing as the \u0026quot;Runnable\u0026quot; of Java or the \u0026quot;Action\u0026quot; of C#) that it invokes in order to indicate that the value of the temperature has changed. The temperature indicator:\nHas an output which is called Reading, of type ReadonlyFloat, that it invokes in order to obtain the current value of the temperature. Has an input called Refresh, of type Procedure0, which can be invoked to cause the indicator to re-display the current temperature value. In the diagram, the Reading output of the indicator has been connected to the Reading input of the sensor, and the Changed output of the sensor has been connected to the Refresh input of the indicator. Note that this design has a call cycle in it: The indicator invokes the sensor to obtain the current temperature, but the sensor also invokes the indicator to tell it that the current temperature has changed.\nDependencies In Types of dependencies I differentiate between compile-time (static) dependencies, assembly-time (semi-dynamic) dependencies, and post-assembly-time (fully dynamic) dependencies.\nCompile-time dependencies have already been given a lot of consideration and the general consensus is that they better not be cyclic. Most build systems prohibit static dependency cycles between build modules; for example, in the Java world, Maven artifacts cannot circularly depend on each other; similarly, in the dotnet world, MSBuild projects cannot circularly depend on each other. However, programming languages usually allow compile-time dependency cycles within program code: if classes A and B are defined within the same build module, it is usually possible to have A contain a hard-coded invocation to a method of B, and for B to also contain a hard-coded invocation to a method of A. Nonetheless, software architecture is not concerned with hard-coded invocations; therefore, in this paper the term \u0026quot;dependency\u0026quot; does not refer to compile-time (static) dependencies.\nAssembly-time dependencies are what software architecture is mostly concerned with, and as such, this is the sense in which the term \u0026quot;dependency\u0026quot; is used in this paper.\nPost-assembly-time dependencies are useful, as we will see, in certain techniques for eliminating call cycles; however, they do not affect the topology of a design, (they are implementation details which are not representable in a component diagram,) and as such these are not the kind of dependencies that we are referring to when we speak of dependencies in this paper.\nAre call cycles common? In software design as conventionally practiced, cycles in the call graph are a frequent phenomenon. Software architects often have no qualms about producing a design where A calls B and B also calls A.\nIn the literature we find statements endorsing this practice. For example, in the seminal paper \u0026quot;A Laboratory For Teaching Object-Oriented Thinking\u0026quot;(1989) by Kent Beck and Ward Cunningham, the authors acknowledge that many components act as servers \u0026quot;with little regard or even awareness of [their] client\u0026quot;, but also find it perfectly normal for some components to be \u0026quot;near-equals\u0026quot; in a \u0026quot;symmetric relation\u0026quot;. That paper introduced the term \u0026quot;collaborator\u0026quot;, which became a staple term in the software engineering discipline, specifically in order to allow for bidirectional interaction between components, as opposed to the already-existing term \u0026quot;dependency\u0026quot;, which implies a one-way interaction. (For more on this, see Definition: Collaborator)\nThe problem with cyclic call graphs Cyclic call graphs constitute tight coupling. This in turn has a severe negative effect on the understandability and maintainability of software. Wikipedia lists some specific disadvantages of tight coupling:\nA change in one module usually forces a ripple effect of changes in other modules. Assembly of modules might require more effort and/or time due to the increased inter-module dependency. A particular module might be harder to reuse and/or test because dependent modules must be included. The second point might be a bit vague, but it is very important, so it is worth examining it in more depth. The term \u0026quot;assembly of modules\u0026quot; refers to the process of instantiating each component that makes up the system, and wiring the components together so that the system can start running. In this paper, we call this process system assembly.\nThe simplest approach to system assembly is to pass to each component all of its dependencies as constructor parameters when instantiating it, so that immediately upon construction the component is ready to start performing its duties. However, this approach is not viable if the call graph contains cycles, because circular dependencies introduce a chicken-and-egg problem: If each component requires all of its dependencies to be passed as constructor parameters, and if components A and B depend on each other, then A can only be constructed if B has already been constructed, but B cannot be constructed unless A has been constructed first.\nThis problem is pervasive, but it has not received much attention because we are resigned to software development being a largely unstandardized, labor-intensive process where copious amounts of custom-written code provide ad-hoc solutions to long-standing problems on a case by case basis. During system assembly, developers tend to wire as many components as they can during construction, and when a certain wire turns out to form a call cycle, they make a special case and refactor the components involved so as to postpone the wiring of that particular call until after construction. (This often leads to order-of-initialization bugs, which require painstaking effort to troubleshoot and fix.)\nAs the system evolves, and wires between components are added or removed, developers try to keep components unchanged by re-arranging the order in which they are instantiated, and when this is not enough, they further refactor components, turning more construction-time wiring into post-construction-time wiring, and vice versa. (Invariably resulting in more order-of-initialization bugs, and more painstaking effort to troubleshoot and fix them.)\nConventional software development practices often utilize dependency injection frameworks to handle the wiring of components. Such frameworks work as if by magic, which is by some schools of thought undesirable by definition; they also represent substantial runtime overhead, so they are unsuitable for certain classes of applications, e.g. for embedded systems. Some dependency injection frameworks do not solve the problem of circular dependencies, because they simply prohibit them, whereas others attempt to solve the problem by transparently creating proxy objects, which postpone wiring until some post-construction moment, and are therefore doubly magical. The problem with proxy objects is that they tend to fail if invoked from within a constructor, and when this happens it is extremely difficult to troubleshoot and fix. Most importantly, dependency injection frameworks tend to hide dependencies from view, while the goal of software architecture is precisely the opposite: to keep dependencies into view.\nThe promise of authoritative technical software design, where the end-system is automatically generated from the design with no human intervention, requires us to stop writing custom code which wires components together in ad-hoc ways, and to replace it with a universally applicable, fully automated mechanism for assembling a system. In order for this mechanism to be fully automated, it must be fully standardized. If we were to try to standardize system assembly while allowing call cycles, we might for a moment imagine that we could accomplish our goal with a three-phase approach:\n(Note: I am not actually recommending this! It will not work!)\nConstruction: All components are instantiated in an unconnected state. Wiring: Now that all components exist, each component receives its dependencies. Showtime: A special event is broadcast to all components, letting them know that wiring is complete, so they can now perform their initialization and start performing their duties. This three-phase approach imposes a number of bureaucratic requirements on each component:\nEach component must support some means of receiving references to its dependencies after construction. This constitutes incidental complexity. Each component must support some means of receiving the showtime event, so that it can perform the initialization that it would have otherwise performed in its constructor. This also constitutes incidental complexity. The requirement for components to be able to receive their dependencies after construction and to respond to the \u0026quot;showtime\u0026quot; event necessitates the introduction of some IComponent interface, which must be implemented by all components. This ties all components to the framework which defines IComponent and knows what to do with it. The member fields in which a component stores the references to its dependencies are initialized after construction, and therefore must be declared as mutable, even though in principle they ought to be immutable. Similarly, the initialization performed during the showtime event often generates information that needs to be stored in member fields for later use. These member fields are also initialized after construction, so they must also be declared as mutable, even though in principle many of them ought to be immutable. Thus, any notion of immutability goes out the window. Many components might still be effectively immutable, but all of them are very mutable as far as any code analysis tool can tell. Most importantly, the three-phase approach does not solve the chicken-and-egg problem that we mentioned earlier, it only postpones it in time: When an event is triggered, the order in which event handlers are invoked is undefined. This means that during the processing of the showtime event a component may attempt to invoke another component which has not yet received the event, and therefore has not yet performed its initialization.\nEven if we were to further complicate things by introducing some additional mechanism that would give programmers control over the order in which components process the showtime event, the problem still remains: The presence of cycles in the call graph always leaves open the possibility that some components will be invoked before they have been initialized. It should by now be evident that cyclic call graphs are highly problematic, and that if they are to be allowed then there is no way to standardize system assembly. It remains to be shown whether call cycles can be systematically avoided or eliminated, and thus disallowed.\nSolving the trivial case If our goal is to eliminate the cycle in the call graph of the earlier example with the temperature sensor and the temperature indicator, we can trivially accomplish it by applying the Observer Pattern, as shown in the following figure:\nNote that the sensor does not invoke the indicator anymore; instead, the indicator invokes the sensor not only to read the current temperature but also to register an observer for temperature change notifications. The fact that the sensor will then be invoking that observer is an implementation detail which does not affect the topology of the design; thus, this design is free of cycles.\nPractically, the use of the observer pattern means that the sensor cannot invoke the indicator before the indicator has registered its observable, and this in turn means that the indicator can never be invoked before its initialization is complete.\nBy eliminating the call cycle between the sensor and the indicator, we end up with a system that has a specific, computable order of initialization which is guaranteed to be free of problems: the sensor does not depend on the indicator anymore, so it can always be constructed first. The indicator, which depends on the sensor, can always be constructed after the sensor, so it can receive its dependencies as constructor parameters.\nAs a result, each component can store all of its dependencies in immutable member variables. The only interface that needs to be stored in a mutable member variable is the callback that the observable receives from the observer, and this is in line with the nature of the observer pattern, where registration and de-registration of callbacks necessarily involves mutation.\nPins of type Observable\u0026amp;lt;T\u0026amp;gt; are bound to occur so often in software designs, that they warrant some special notation in order to:\nSimplify their representation. Make them more conspicuous. Save space in the diagram. The following figure shows an example of what this notation could look like:\nIn the above diagram, the following changes have been made to pins of type Observable\u0026amp;lt;T\u0026amp;gt;:\nThe generics notation (Observable\u0026amp;lt;T\u0026amp;gt;) has been replaced with tilde notation (~T). The tilde indicates that the type of the pin is not really of type T, it is of type Observable\u0026amp;lt;T\u0026amp;gt;. The arrows of the observable pins have been replaced with slightly larger circles containing slightly smaller arrows pointing in the opposite direction. The encircled arrows are not pointing from the output to the input as normal arrows do; instead, the encircled arrows are showing the direction of callback invocations, so they are pointing from the input to the output. Essentially, a circle signifies inversion of the direction of invocations, so an encircled arrow corresponds to a normal arrow of the opposite direction.\nIf the callback interface does not contain any methods that return information, (as the case is with all notification interfaces,) the above design can be improved even more.\nFirst, note that the refactoring of an input-output pair to an observer-output-observable-input pair results in components that violate the Single Responsibility Principle (SRP):\nInstead of simply exposing a Changed output, the sensor now has to implement the functionality of an event manager, so as to offer the same notification as an observable input. Similarly, instead of simply exposing a Refresh input, the indicator now has to contain a few more lines of code to register a callback in order to receive the same notification as an observer. The above may not represent a lot of work, but it is nonetheless a refactoring which must be applied to the code in order to support the needs of the design; however, in a different design, the observer pattern might be unnecessary, so why should the components be hard-coded to support it?\nTo solve this problem, let us revisit our first design, where we had a temperature sensor with a simple Changed output and a temperature indicator with a simple Refresh input, and hence a call cycle. Let us us now introduce two new components into the design, where one is a General-Purpose Observable and the other is a General-Purpose Observer.\nThe job of the observer is simply to register a callback via its Registration output during construction, and from that moment on to keep echoing each invocation of the callback to its Trigger output.\nThe job of the observable is to receive an observer registration via its Registration input, and to keep echoing invocations coming into its Trigger input to the registered observer, if any.\nNote that with this arrangement, the call cycle is still eliminated, and at the same time we have managed to retain the sensor and indicator components in their original form, with no code refactoring necessary, since the refactoring has now been applied to the design. Also note that the SRP is being nicely upheld.\nI postulate that the combination of a general-purpose observer and a general-purpose observable will be occurring quite frequently, to the point where it might be worth simplifying their representation using some special notation. For this purpose, I propose an air-gap pseudo-component. With the use of an air-gap, the previous figure turns into the following:\nNote that the symbol for the air-gap component has been borrowed from electronics, where it stands for a capacitor. An electronic capacitor is also, in a sense, an air gap; however, the similarity is superficial, and it is only meant to serve as a mnemonic: In software, an air-gap pseudo-component does not maintain any charge, nor does it act as some kind of high-pass filter, etc.; it just allows us to pick a wire that takes part in a call cycle, and trivially make that wire break the cycle.\nAlso note that the air-gap is not a real component, it is a pseudo-component. This means that it is simply a notation, which represents an underlying pair of actual components: a general-purpose observer, and a general-purpose observable. This is necessary because these two components will invariably need to be constructed at different times during system assembly. In the sensor-indicator example, construction would take place in the following order:\nThe general-purpose observable is constructed first because it does not depend on anything else. Then, the sensor is constructed, which depends on the general-purpose observable. Then, the indicator is constructed, which depends on the sensor. Finally, the general-purpose observer is constructed, which depends on both the indicator and the general-purpose observable. So, during system assembly, each air-gap pseudo-component is decomposed into an observer and an observable, so that all components can be instantiated and wired in the order dictated by their dependency graph.\nSolving non-trivial cases The application of the observer pattern is a good first step in the direction of being able to express any software design acyclically; however, it does not cover all cases. Specifically, the observer pattern cannot be used under the following circumstances:\nThe observable expects information to be returned back from the observer, and it is incapable of handling the case where no observer is registered, and therefore no results can be returned. (For example, the observable needs to invoke the observer and receive information back from the observer during the observable's construction, at which point the observer cannot possibly have registered yet.)\nThe simple Changed notification of the temperature-sensor-and-indicator example does not fall under these circumstances because it is in the nature of notifications that they never return any information; however, in other scenarios, these circumstances can arise. Thus, it remains to be shown how call cycles can be eliminated when the observer pattern is inapplicable.\nStrategy: Fusion The presence of a call cycle between components A and B might indicate that perhaps they should not be separate components, and that we might be better off by fusing A and B into a single component. In doing so, we remove the cycle from the topology of the design by turning it into an implementation detail of the new component. The following figure illustrates this:\nClearly, the left diagram contains a cycle, whereas the right diagram does not.\nNeedless to say, this strategy is only marginally useful, and it should not be considered unless all else fails, because the goal of software architecture is to distribute functionality into as many components as possible, so that each component can be as simple as possible, instead of conglomerating functionality into monolithic components. The fusion strategy is mentioned here only for the sake of completeness.\nStrategy: Plain Fission The presence of a call cycle between components A and B might indicate that at least one of the two components violates the Single Responsibility Principle (SRP). In this case, one of the responsibilities can be extracted into a separate component which only exposes inputs, thus breaking the cycle, as the following figure illustrates:\nIn this diagram, component A has been split into A1 and A2. Both A1 and B invoke A2, but A2 does not invoke anything, so there is no cycle.\nThis can happen, for example, if component A contains both a data model and some logic acting upon the data model. If A is incapable of fully encapsulating the data model, it may have to expose not only an output for interacting with the rest of the system, but also an input for the rest of the system to interact with the data model. With the fission refactoring, component A has been split into one component for the logic (A1) and a separate component for the data model (A2).\nStrategy: Observable Fission The astute reader might notice that in the plain fission example, components A1 and A2 are not exactly equivalent to component A: In the original diagram A used to be able to take notice of incoming calls from B intended for the data model, and could therefore take action in response to those calls, whereas in the refactored diagram A1 is oblivious to any calls that B makes to A2.\nIf A1 needs to be aware of such calls, this can be very easily accomplished by having A2 issue change notifications, and wiring these notifications back to A1. This new wire does not introduce a call cycle, because as we have already shown when describing the trivial case, notifications can always be air-gapped.\nThe following figure illustrates the application of the observable fission strategy:\nConclusion We have shown that cyclic call graphs prevent the standardization, and therefore the automation, of system assembly.\nWe have discussed refactoring strategies for systematically eliminating call cycles, including a universally applicable technique for trivially eliminating the most common call cycles.\nWe conclude that since call cycles can be avoided or eliminated, they can be comprehensively disallowed, thus paving the way for the automation of system assembly.\nTO DO:\nChange the cover image to make the cycle look nicer. Show a diagram where the observable and observer interfaces are fully spelled out before showing the diagram in which the notation has been simplified. Redo the placement and wiring of A1 and A2 to more clearly show that they used to be A. Mention that the air-gap pseudo-component does not need to incorporate an actual multicast observable component; a unicast observable will suffice. Provide a better example of a situation where an air-gap cannot be used. Most importantly: Introduce a distinction between \u0026quot;early\u0026quot; outputs, which may be invoked during construction, and \u0026quot;late\u0026quot; outputs, which may only be invoked after construction. Show that late wires can be air-gapped too, even if they are two-way. Use a polarized capacitor symbol for one-way interface air-gaps and a non-polarized capacitor symbol for two-way interface air-gaps. Possibly introduce diode notation for one-way pins. Possibly introduce bar-plus-arrow notation for early pins. Possibly introduce not-gate notation for inverted pins. Possibly represent an observatory as a not-gate and an observer as the opposite (a triangle with a bubble on its flat side.) Note the following: One-way interfaces: can always be inverted. Two-way interfaces: can only be inverted if late, and one-to-one. Normal two-way interfaces: multiple outputs can connect to one input. Normal one-way interfaces: multiple outputs can connect to multiple inputs. (With the help of a distributor.) Inverted one-way interfaces: one output can connect to multiple inputs. Inverted two-way interfaces: one output connects to one input. (And must be late.) ","date":"2023-12-27T12:08:24.775Z","permalink":"https://blog.michael.gr/post/2023-12-27-call-graph-acyclicity/","title":"Call Graph Acyclicity"},{"content":"\rAbstract This paper examines the long-standing need within the software engineering discipline for technical design that is authoritative. A design process is authoritative if there exist technical means of materializing the design document as a working product, thus guaranteeing that the end result is indeed as described by the design. We notice the scarcity and inadequacy of existing solutions for software design, we look at solutions in other engineering disciplines, and we conclude with realizations on what it would take to come up with a solution that works for software.\n(Useful pre-reading: About these papers)\nPrior art Through the decades, plenty of tools and methodologies have been developed with the aim of aiding the software design process. A common pattern among them is that they try to make some aspect of development more visual rather than textual. They fall into one of the following categories:\nVisual Implementation tools (For example: Visual Programming Languages like Snap!, Scratch, EduBlocks, Blockly, etc.,) - They are indeed visual, and they do indeed produce runnable software, but their structure and level of detail is identical to the structure and level of detail of program code in the form of text, so they express implementations rather than designs. Visualization tools (For example: class diagrams, dependency diagrams, call trees, etc.) - They are restricted to the visualization, exploration, and documentation, but not the editing of existing software, nor the design of new software. As such, they are reverse engineering tools, not design tools. Niche tools (For example: Web Services Description Language (WSDL), *Business Process Execution Language (BPEL), etc.) - They are exclusively focused on specific domains such as web services, business processes, etc., and cannot be used for software design at large. \u0026quot;Look ma, no code\u0026quot; tools (For example: Rapid Application Development (RAD) tools, No-Code Development Platforms (NCDPs), and Low-Code Development Platforms (LCDPs)) - They impose limitations on what can be done; they impose the use of a massive vendor-specific platform; they do not scale; they are aimed at non-programmers, allowing easy creation of simple user-interface-centric applications to quickly (and usually haphazardly) meet specific narrow business needs. Modelling tools (For example: Microsoft Visio, Modelling Languages such as Unified Modeling Language (UML), The C4 model, etc.) - They are restricted to modelling, so they produce designs that bear no necessary relationship to reality. They aim to constrain what is supposed to be included in a design, but these constrains exist only in theory, because they are not enforced by any technical means. For a more detailed look at prior art, see The state of affairs in computer-aided software design.\nOf all the technologies listed above, only modelling tools can legitimately be said to be of any potential usefulness in the software design process at large.\nThe unsuitability of modelling Modelling tools allow designs that bear no relationship to reality: they are not informed via any technical means about the actual components available for incorporation in a design, nor about valid ways of interconnecting them. Consequently, modelling tools are nothing more than fancy whiteboards: they cannot guarantee, via any technical means, the feasibility of a design. (This is so by definition; otherwise, it would not be modelling, it would be engineering.)\nEssentially, modelling tools are non-authoritative: no matter how sophisticated the model is, the authoritative source of truth for the structure of the system remains the source code, not the model.\nThe source code should ideally constitute a faithful implementation of the model, but there are no technological safeguards to guarantee that it does, and as a matter of fact it usually cannot, because the model is almost never feasible as designed to begin with.\nFor these reasons, modelling is of severely limited value, and programmers largely regard it as loathsome double book-keeping.\nFor a list of ways in which modelling as a means of design fails the software engineering discipline, please see The perils of whiteboards.\nOther engineering disciplines In long-established engineering disciplines such as mechanical, electrical, civil, etc., for several decades now, design work has been facilitated by Computer-Aided Design (CAD) tools and Computer-Aided Engineering (CAE) tools.\nMechanical engineers use CAD tools to create documents describing complicated three-dimensional structures with detailed information about materials, dimensions, and tolerances. The tools perform various forms of analysis to verify the validity and feasibility of the design. Based on the results, the engineers can edit the design to optimize it, and repeat the analysis as necessary. Eventually, the design document is sent to a shop where CNC machining or 3D-printing is used to create the parts with minimal human intervention.\nIn electronic engineering, which is the discipline from which most parallels can be drawn to software engineering, virtually all design work since the 1980s is being done using Electronic Design Automation (EDA) / Electronic Computer-Aided Design (ECAD) tools. These tools have revolutionized electronic design by using a standardized notation to not only describe, analyze, and optimize products, but also to manufacture them.\nElectronic schematic diagrams use a standard notation which is understood by all electronic engineers. A new hire begins their first day at work by studying the schematics, and before the end of the day they are often able to pick up the soldering iron and start doing productive work. Contrast this with software engineering, where a new hire usually cannot be productive before spending weeks studying source code and documentation, and having numerous knowledge transfer meetings with senior engineers who know the system. Most importantly, ECAD tools bridge the gap from the physical world to the design, and from the design back to the physical world. The tools have libraries of electronic components available for inclusion in a design, and electronic manufacturing has long ago advanced to the point where an electronic design document can be turned into a functioning circuit board with nearly zero human intervention. Thus, electronic design documents today are authoritative: the end products are accurately described by their designs. The problem with software Unfortunately, thus far, the software engineering discipline has been following a very different path from other engineering disciplines: technical software design documents are scarce, and authoritative technical software design documents are completely non-existent.\nThis situation has been allowed to go on for so long, partly because in software we already have a certain other kind of document which is authoritative, and this is the source code. However, source code is an implementation, or at best a detailed technical description, but not a technical design. To say that the technical design of a software system is a listing of the lines of source code that make up that software system is equivalent to saying that the technical design of the Great Wall of China is a list of all the bricks that make up the Great Wall of China.\n(A tiny part of) the Great Wall of China by Hao Wei, CC BY 2.0 A technical design is supposed to list operative components, and to show how they are interconnected, but not to delve past the level of detail of the component. Unfortunately, we do not have that for software, at least not in an authoritative form.\nIt is a great paradox of our times that the software engineering discipline is bereft of authoritative design tools, when such tools are the bread and butter of the long-established engineering disciplines.\nIn lieu of authoritative tools, software design today is practiced using conventional, non-authoritative means, such as box-and-arrow drawing applications, which, as explained earlier, are only capable of modelling, and therefore amount to nothing more than fancy whiteboards.\nThe end-result of all this is the following:\nSoftware systems do not match their designs.\nEven if the technical design happens to describe a software system that could actually be built as described, there are no technological safeguards to guarantee that it will: the software engineers and the operations engineers are free to build and deploy a system that deviates from the design, and neither the architects, nor the management, have any way of knowing.\nSoftware systems diverge from their designs over time.\nEven if the deployed software system initially matches its design, the system is bound to evolve. The design should ideally evolve in tandem, but it rarely does, again because there are no technological safeguards to enforce this: the engineers are free to modify and redeploy the system without updating the design document, and in fact they usually do, because it saves them from double book-keeping. Thus, over time, the design bears less and less relationship to reality.\nIf, due to the above reasons, you suspect that your technical design document is counterfactual, and you would like to know exactly what it is that you have actually deployed and running out there, you have to begin by asking questions to the software engineers and the operations engineers.\nIn order to answer your questions, the engineers will in turn have to examine source code, version control histories, build scripts, configuration files, server provisioning scripts, and launch scripts, because the truth is scattered in all those places. In some cases they might even have to try and remember specific commands that were once typed on a terminal to bring the system to life.\nIf this sounds a bit like it is held together by shoestrings, it is because it is in fact held together by shoestrings.\nThus, the information that you will receive will hardly be usable, and even if you manage to collect it all, make sense out of it, and update the design document with it, by the time you are done, the deployed system may have already changed, which means that your design document is already obsolete.\nAs a result, it is generally impossible at any given moment to know the actual technical design of any non-trivial software system in existence.\nThis is a very sorry state of affairs for the entire software industry to be in.\nTowards a solution If we consider all the previously listed problems that plague software design as conventionally practiced, and if we look at how the corresponding problems have been solved in long-established engineering disciplines, we inescapably arrive at the following realization:\nThe technical design of a system can only be said to accurately describe that system if there exist technical means of having the system automatically created from the design.\nIn order to automatically create a system from its design, the design must be semantically valid. This brings us to a second realization:\nThe semantic validity of a technical design can only be guaranteed if there exist technical means of informing the design with components available for incorporation and restricting the design to only valid ways of interconnecting them.\nThe above statements define a design process as authoritative.\nAn authoritative software design document is an essential engineering instrument instead of an abstract work of art:\nThe design document contains all the information necessary for provisioning target environments with software components, instantiating the components, and wiring them together; this information not only need not, but in fact must not be encoded anywhere else in the source code; this eliminates double book-keeping, which is considered by developers as another layer of red tape which is preventing them from getting things done, and is the complaint most often heard from developers about conventional software design. The design document is the only means through which the system can be re-deployed after making a change to either the code, or the design, or both; this guarantees that the deployed system will always be exactly as described by the design, so there is no possibility of the design ever becoming outdated, which is the complaint most often heard from architects about programmers. Any attempt to introduce authoritative design in the software engineering discipline would necessarily have to borrow concepts from the electronic engineering discipline. This means that the solution must lie within the realm of Component-Based Software Engineering (CBSE), where systems consist of well-defined components, connectable via specific interfaces, using well-defined connectivity rules.\nWhat we need is a toolset that implements such a paradigm for software. The toolset must have knowledge of available components, knowledge of the interfaces exposed by each component, and rules specifying valid ways of connecting those interfaces. The toolset must then be capable of materializing the design into a running software system.\nThe toolset must not repeat the mistakes and suffer from the drawbacks of previous attempts at component-based software engineering. Thus, the toolset must meet the following goals:\nFacilitate any programming language. By this we do not mean that it should be possible to freely mix C++ components with Java components; what we mean is that it should be possible to express in one place a C++ subsystem containing C++ components interconnected via C++ interfaces, and in another place a Java subsystem containing Java components interconnected via Java interfaces, and at a higher scope to have each of these subsystems represented as an individual opaque component, where connections between the two components are made via language-agnostic interfaces (e.g. REST) or cross-language interfaces (e.g. JNI, JNA, etc.)\nFacilitate any level of scale, from embedded systems to network clouds. This means that the nature of a component and the nature of an interface must not be restricted, so that they can be realized in different ways at different levels of scale. For example, at the embedded/C++ level of scale, a component might be defined as a C++ class exposing C++ interfaces, whereas at the internet level of scale a component is likely to be defined as a (physical or virtualized) network host exposing TCP interfaces.\nGuarantee type-safety at any scale. Type safety can be carried across different levels of scale by means of parametric polymorphism (generic interfaces.) For example, a type-safe interface between a client and a server in a network can be described with a construct like Tcp\u0026lt;Rest\u0026lt;AcmeShopping\u0026gt;\u0026gt; which stands for a TCP connection through which we are exchanging REST transactions according to a schema which corresponds to some programmatic interface called \u0026quot;AcmeShopping\u0026quot;.\nRequire minimal extra baggage. Components should not be required to include a lot of extra overhead to facilitate their inclusion in a design. Especially at the embedded level, components should ideally include zero overhead.\nThis means that a C++ class which accepts as constructor parameters interfaces to invoke and exposes interfaces for invocation by virtue of simply implementing them should ideally be usable in s design as-is.\nThe extra functionality necessary for representing the component during design-time, provisioning a target environment with it, instantiating it, and wiring it should be provided by a separate companion module, which acts as a plugin to the design toolset, and exists only during design-time and deployment-time, but not during run-time**.**\nSupport automatic deployment. The toolset must be capable of deploying a software system of arbitrary complexity to a production environment of arbitrary complexity, and it must be capable of doing so with no human intervention other than the pressing of a \u0026quot;Deploy\u0026quot; button. To this end, toolset must support components representing various different kinds of environments such as network hosts, isolated devices, operating systems, virtual machines, etc. and each of these components must be configurable with everything necessary in order to provision a certain environment with the corresponding part of the design.\nSupport iterative development. Once a system has been designed, coded, and deployed, it is a fact of life that it will keep evolving. The design toolset must support re-deploying after modifying the code, or the design, or both.\nSupport automatic wiring. Once an execution environment has been provisioned with software components, the components must be wired together in order to start running. Traditionally, the wiring of freshly instantiated components is done by carefully hand-crafted code, to account for circular dependency issues between components. If we are to have fully automated deployment, the wiring cannot be done by hand-crafted code anymore; it must be automated, therefore it must be standardized. This in turn means that certain connectivity rules are necessary in order to guarantee that software designs do not suffer from circular dependency issues that would require custom handling. For more on this, see Call Graph Acyclicity.\nFacilitate incremental adoption. It should be possible to express, via an authoritative design document, the structure of a small subsystem within a larger system whose structure has not (yet) been expressed authoritatively.\nIn systems of medium scale and above, this may be handled by making the core deployment and wiring engine of the toolset available on demand, during runtime, to quickly materialize a small subsystem within the larger system. In embedded-scale systems, it should be possible to utilize code generation to do the instantiation and the wiring, so as to avoid having the core engine present in the target environment. Utilize a text-based document format. In software we make heavy use of version control systems, which work best with text files, so the design documents must be text-based. The text format would essentially be a system description language, so it must be programmer-friendly in order to facilitate editing using a text editor or an IDE. A graphical design tool would read text of this language into data structures, allow the visual editing of such data structures, and save them back as text.\nFacilitate dynamic software systems. Every non-trivial system has the ability to vary, at runtime, the number of instances of some components in response to changing computation needs, and to choose to instantiate different types of components to handle different needs. Therefore, a toolset aiming to be capable of expressing any kind of design must be capable of expressing, at a minimum, the following dynamic constructs:\nPlurality: Multiple instantiation of a certain component, where the number of instances is decided at runtime. Polymorphism: Fulfilling a certain role by instantiating one of several different types of components capable of fulfilling that role, where the choice of which component type to instantiate is made at runtime. Polymorphic plurality: A combination of the previous two: A runtime-variable array of components where each component can be of a different, runtime-decidable type. Facilitate multiple alternative configurations (layers). In virtually every software development endeavor there is a core system design which is materialized in a number of variations to cover different needs. For example:\nDebug vs. release Testing vs. production With instrumentation or without With hardware emulation vs. a targeting the actual hardware The bulk of the components and the wires of the design exist in all configurations, but some configurations prescribe additional components and slightly different wiring.\nTherefore, the toolset must facilitate the expression of alternative configurations so that each configuration can be defined authoritatively.\nTo facilitate this, the toolset must support design layers, similar to drawing layers found in drawing applications like Photoshop. Note that design layers are unrelated to the architectural layers found in layered architectures, although it is possible that people will figure out ways to represent architectural layers using design layers.\nThe details of how layers are going to work in order to support configurations are to be decided, but one preliminary idea is to have one or more base layers where the bulk of the components are laid out, and a few mutually exclusive configuration layers on top of them. A configuration layer combines with one or more base layers to form a complete system, and is deployable, whereas base layers do not describe complete systems and are therefore not deployable by themselves.\nBe extensible. The design document must support the inclusion of arbitrary metadata to be used by various tools, which can be either separate applications, or plugins to the graphical editor. Examples of metadata:\nKeeping track of documentation of interest to different stakeholders, for example Architectural Decisions 1.\nKeeping track of Team Architecture, i.e. which development teams are responsible for building and/or maintaining different parts of the design. 2\nRecording various technical characteristics, such as data flow. (Every interface can be associated with a direction of data flow with respect to the direction of invocation: when invoked, some interfaces only pull data, some only push data, and some perform bi-directional transfer of data.)\nRecording, either manually or automatically, various metrics such as:\nTechnical debt estimations Threat modelling Compliance considerations and responsibilities Test code coverage results Performance statistics Frequency of change statistics Using such metadata and plugins, the graphical editor may allow switching between views to visualize various aspects of the system overlaid on the component diagram, such as, for example, data flow instead of control flow, a heat map of technical debt, a heat map of test code coverage, a heat map of frequency of change, etc.\nBe accessible and attractive. The extent and speed by which a new software development technology is adopted greatly depends on how accessible and attractive the technology is. To this end:\nThe core toolset must be free and open source software. (Profit may be made from additional, optional tools, such as a visual editor.) This also means that the toolset must be a cross-platform, installable software package rather than a cloud offering. A clear distance must be kept from unattractive technologies like UML, XML, etc. The literature around the toolset must avoid wooden language and alienating terms such as \u0026quot;enterprise architecture\u0026quot;, \u0026quot;standards committee\u0026quot;, \u0026quot;industry specifications consortium\u0026quot;, etc Efficiently manage complexity. Software designs can become formidably complicated. One of the major goals of a design methodology is to manage complexity and to reduce clutter. Therefore, the toolset must support the following constructs:\nContainers\nSome systems are so large that expressing them in a single diagram may be inconvenient to the point of being unworkable. To address this, the toolset must facilitate hierarchical system composition by means of container components. A container encapsulates an entire separately-editable diagram and exposes some of the interfaces of the contained components as interfaces of its own. Thus, containers can be used to abstract away entire sub-designs into opaque black-boxes within greater designs. Container components moust be boundlessly nestable.\nVias\nLarge numbers of wires traveling long distances within a diagram can have a detrimental effect on the intelligibility of the diagram. For this reason, the concept of the \u0026quot;via\u0026quot; will be borrowed from electronic design. A via is a named circle into which a wire may terminate and thus vanish from view. All vias with the same name are implicitly connected without having to show the wires between them. This is especially useful for wires of interfaces representing cross-cutting concerns, which are ubiquitous, and therefore do not need to be shown everywhere.\nA via is strongly typed like any pin; when the first pin is wired to a via, the via implicitly takes the type of that pin. Vias are to be drawn as little circles.\nRibbons\nSometimes there may be multiple parallel wires that travel over long distances on a diagram. Some of them might even go in opposite directions.\nTo reduce clutter, the toolset must make it possible to group such wires together in a ribbon. At each end of a ribbon is a connector, which breaks the ribbon into individual pins and shows the name and type of each pin, so that individual wires can be drawn from there to component pins.\nRibbons and connectors are pseudo-elements, in the sense that they only exist in the design diagram and have no counterpart in code. Ribbons are to be drawn as two parallel hairlines with a slanted hash between them. The shape of connectors is to be determined, but it will probably be borrowed from electronic design. Ribbons can also be routed in and out of vias. Ribbon vias are to be slightly bigger than single-wire vias.\nEstablish a universal notation. To ensure that every developer can easily understand a design document that they see for the first time, the toolset must standardize the notation used in software diagrams, the same way that electronic schematic diagrams follow a standard notation which is universally understood by all electronic engineers.\nThe details are to be decided, but some preliminary ideas about styling and conventions are as follows:\n(Need to show an illustration here.)\nDiagrams are drawn using nothing but monochrome lines. (Black lines on a white background, or white lines on a blue background, etc.) This is because color opens up too many possibilities for distractions and for non-standard representations. The use of color should be reserved for:\nDistinguishing between different layers when multiple layers are drawn superimposed. Transient concepts such as: Mouse-over in the graphical editor Selection in the graphical editor Validation errors Visualization of statistics (especially heat maps) Nonetheless, people will probably figure out that they can present a design in a colorful way by placing different components on different layers, choosing a different color for each layer, and having all layers displayed simultaneously. However, should they decide to do that, they are on their own: the toolset will not offer any features specifically intended to facilitate this.\nWires are to be drawn using hairlines.\nPins are also to be drawn using hairlines. Outputs will be triangular arrows pointing out of a component, inputs will be triangular arrows pointing into a component. The name and type of each pin is to be drawn outside the shape of the component, allowing components to be relatively small and requiring a lot of empty space around them to fit the names of the pins. The pin name is to be drawn with a bigger font than for the pin type.\nWires may bend only in right angles. When two wires cross, this means that they are isolated from each other. When multiple outputs converge into a single input, a small but discernible dot at the point of convergence indicates that the wires are connected.\nAt various points along a wire there can be tiny skinny arrows to remind the viewer of the direction of the wire (always from the output to the input.)\nComponent shapes are to be drawn using thick lines. The default shape for every component type is a plain rectangle, with the name and type of the component rendered in the center. The component name is to be drawn using a bigger font than the component type.\nSome component types perform simple and standard functions, which can usually be inferred from their pins, for example adapters from one interface to another, or converters that transform data from one form to another. For such simple components, there is merit in refraining from displaying their name and type, and instead displaying them with special shape, thereby making them occupy less space in the design, and making the design more expressive. The toolset will initially offer a few special shapes:\nA triangular component shape intended for component types that act as converters. An AND-gate component shape for component types that play the role of adapters. Over time, more component types that perform simple and standard functions will inevitably be identified. This will lead to a demand to introduce additional component shapes, bearing some resemblance to electronic or flowchart symbols, to represent those components; however, the intention is to be conservative in this, and only introduce new shapes if the demand for them is strong and widespread.\nThe preferred placement of pins on the perimeter of a component shall be:\nInputs along the left and top edges Outputs along the right and bottom edges The convention for pin placement shall be:\nGeneral-purpose and cross-cutting concern interfaces: inputs along the top edge outputs along the bottom edge. Application-specific interfaces: inputs along the left edge. outputs along the right edge. This arrangement is analogous to electronic design, where the convention is that signals flow from left to right and voltages from top to bottom.\nSee Architectural Decision Records by Michael Nygard-\u0026gt; link is dead, new link here. For ADRs as a vehicle of engagement between architects and developers instead of documentation, see Mark Richards - The Intersection of Architecture and Implementation - DDD Europe.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee the concept of \u0026quot;Team Architecture\u0026quot; in Practical (a.k.a. Actually Useful) Architecture by Stefan Tilkov, GOTO 2023, section 2, \u0026quot;Explicitly architect your team setup\u0026quot; -- Related term: Team Topologies.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-12-09T19:16:05.156Z","permalink":"https://blog.michael.gr/post/2023-12-09-authoritative-technical-design/","title":"Towards Authoritative Software Design"},{"content":"\rI recently did this at work, and I decided to document the process here in the form of a how-to guide. Please note that I am not an expert, I am learning as I go along, so there may be mistakes.\nSdk-style project files have existed since net5, but when they were introduced they were made compatible with earlier versions of dotnet, such as dotnet framework 4.7.2. The kind of project files we were using before can now be called legacy-style project files.\nLegacy-style project files begin with \u0026lt;Project ToolsVersion=\u0026quot;... Sdk-style project files begin with \u0026lt;Project Sdk=\u0026quot;Microsoft.NET.Sdk\u0026quot;\u0026gt;. Sdk-style project files are necessary if you want to:\nStart using the dotnet command-line utility and all the functionality that it provides. Eventually migrate to a modern version of dotnet. Note: If your legacy project files are using packages.config, they first need to be converted to PackageReference-style. We live in the 3rd millennium, we should act like it. Converting from packages.config to PackageReference is beyond the scope of this guide.\nHere are the steps I followed:\nI replaced the \u0026lt;Project ToolsVersion=... tag with \u0026lt;Project Sdk=\u0026quot;Microsoft.NET.Sdk\u0026quot;\u0026gt;.\nI replaced the \u0026lt;TargetFrameworkVersion\u0026gt;v4.7.2\u0026lt;/... tag with \u0026lt;TargetFramework\u0026gt;net472\u0026lt;/...\nI removed the following tags:\n\u0026lt;ProjectGuid\u0026gt;\n\u0026lt;TargetFrameworkProfile\u0026gt;\n\u0026lt;FileAlignment\u0026gt;\n\u0026lt;AutoGenerateBindingRedirects\u0026gt;\n\u0026lt;Deterministic\u0026gt;\n\u0026lt;NuGetPackageImportStamp\u0026gt;\n\u0026lt;AssemblyName\u0026gt;\n\u0026lt;AppDesignerFolder\u0026gt;\n\u0026lt;ProjectTypeGuids\u0026gt;\n\u0026lt;XamlDebuggingInformation\u0026gt;\n\u0026lt;Prefer32Bit\u0026gt;\n\u0026lt;ErrorReport\u0026gt;\nI left the following tags as they were:\n\u0026lt;RootNamespace\u0026gt; (This is only necessary if the name of the project file does not exactly match the name of the root namespace.)\n\u0026lt;OutputType\u0026gt;\n\u0026lt;StartupObject\u0026gt; (if any)\n\u0026lt;ApplicationIcon\u0026gt; (if any)\n\u0026lt;PlatformTarget\u0026gt;\nI added the following tags:\n\u0026lt;Platforms\u0026gt;AnyCPU;x64\u0026lt;/...\n\u0026lt;ImplicitUsings\u0026gt;disable\u0026lt;/...\n\u0026lt;Nullable\u0026gt;enable\u0026lt;/...\n\u0026lt;TreatWarningsAsErrors\u0026gt;True\u0026lt;/...\n\u0026lt;NoWarn\u0026gt;NU1701;NU1702\u0026lt;/...' \u0026lt;UseWPF\u0026gt;true\u0026lt;/... (for a WPF project)\n\u0026lt;UseWindowsForms\u0026gt;True\u0026lt;/... (for a WPF project -- don't ask.)\nThen, I arrived at the most enjoyable part:\nDozens upon dozens of \u0026lt;Reference Include=...\u0026gt; items for things like \u0026quot;System\u0026quot;, \u0026quot;System.Data\u0026quot;, \u0026quot;System.Xml\u0026quot; etc. were removed. A few had to stay, for example: System.Printing ReachFramework System.IO.Compression Microsoft.VisualBasic System.ServiceProcess All \u0026lt;ProjectReference...\u0026gt; items became one-liners since neither project guid nor name is necessary anymore. All \u0026lt;PackageReference...\u0026gt; items also became one-liners since the version does not have to be a nested tag, it can be an XML attribute. Hundreds of lines of XML that reference individual .cs and .xaml files, as well as the associations between them, were removed. In our case this resulted in a 12:1 reduction in project file size. References to included resources stayed of course, as well as references to anything else that needs special handling. The importing of Microsoft.common.props and Microsoft.CSharp.targets was removed. Once this is done, or even while doing it, various problems popped up, which I had to address. For example:\nAt some point Visual Studio started skipping the building of a project, so the projects that depended on it would fail. If I tried to clean that project, Visual Studio would again skip that project, so it would not do any cleaning. Forcibly cleaning by deleting all the bin and obj directories had no effect; restarting Visual Studio had no effect; enabling more verbose build output (even diagnostic-level) did not reveal the slightest hint as to why Visual Studio was skipping the project. That was very frustrating. After some googling around, gathering a list of magical incantations, and trying them one after the other, the one that worked for me was unloading the project and then reloading it. At some point I was receiving an error telling me that one of my WPF applications was missing a \u0026quot;Main\u0026quot; entry point. However, its project file was for all practical purposes identical to the project file of another WPF application that was building just fine, and suffice it to say, neither of the two applications had a \u0026quot;Main\u0026quot; entry point. As it turns out, the application object must be called \u0026quot;App.xaml.cs\u0026quot; and \u0026quot;App.xaml\u0026quot;; if you rename it, the magic does not work anymore, or perhaps it needs an \u0026lt;ApplicationDefinition Include=\u0026quot;MyCustomApp.xaml\u0026quot; /\u0026gt; in order to work. At some point Visual Studio was launching one of my console applications passing it all of its command-line arguments twice. Visual Studio stopped doing that after it was restarted. You may encounter different problems, or even if you encounter problems that seem similar, you may need different magical incantations to overcome them. Once the above was done, it was time to try building. I did not really expect it to build, and in fact it did not build. There were a number of problems that needed to addressed on a case-by-case basis. Here are the build problems that I encountered, and how I solved them.\nBuild Problem: Referencing standard assemblies.\nIt turns out that in SDK-style projects not only we do not have to reference standard assemblies anymore, but we must actually refrain from referencing them. In our case this was fixed by editing our project file and removing the entire \u0026lt;ItemGroup\u0026gt; with items like \u0026lt;Reference Include=\u0026quot;System\u0026quot; /\u0026gt; and \u0026lt;Reference Include=\u0026quot;System.Xml\u0026quot; /\u0026gt; and the like.\nBuild Problem: Duplicate attributes in AssemblyInfo.cs\nMost of the assembly attributes defined in AssemblyInfo.cs were causing duplicate attribute errors, because in SDK-style projects these are automagically generated for us. The solution was to remove those attributes.\nBuild Problem: PresentationUI assembly not found\nThis is a very strange problem which I was unable to either understand or properly solve. It may be related to the following discussions:\ngithub/dotnet/wpf: \u0026quot;PresentationUI ref-assembly missing: Build fails because cannot find type PresentationUIStyleResources\u0026quot;\ngithub/dotnet/runtime: \u0026quot;WPF has removed PresentationUI ref assembly\u0026quot;\nLuckily, the assembly was not necessary, so I was able to remove it without losing any functionality.\nBuild Problem: Other assemblies not found\nThis is another strange thing which I was also unable to understand. Examples of assemblies that could not be found anymore: Microsoft.Bcl.HashCode, System.Collections.Immutable, and JetBrains.Annotations.\nI solved this problem by simply avoiding the use of those assemblies and either forfeiting their functionality or implementing it by myself.\nSystem.HashCode was very easy to re-implement. System.Collections.Immutable turned out to be unnecessary. JetBrains.Annotations turned out to also be unnecessary. Once the build problems were resolved, it was time to try running. Again, I did not expect the application to run, and in fact it did not run. Here are the runtime problems I encountered, and how I resolved them.\nRuntime Problem: Accessing native DLLs\nIn our project we have a few natives DLLs which would fail to load under the SDK-style project.\nIn one case, the solution was to modify the code that loads the DLL to look for it not only in bin\\x64\\Debug but also in bin\\x64\\Debug\\runtimes\\win-x64\\native and in bin\\x64\\Debug\\runtimes\\win-x64.\nIn another case, the solution was to add \u0026lt;AppendTargetFrameworkToOutputPath\u0026gt;False\u0026lt;/... to the project file.\nIn another case, the solution was to add the following tags to the project file:\n\u0026lt;RuntimeIdentifiers\u0026gt;win-x64\u0026lt;/... \u0026lt;RuntimeIdentifier\u0026gt;win-x64\u0026lt;/...\nAnd in an especially difficult case, the solution was to add a post-build step which copies everything from bin\\x64\\Debug\\runtimes\\win-x64\\native to bin\\x64\\Debug.\nRuntime Problem: Custom-built resources\nIn my application I have icons in SVG format. WPF has no built-in support for SVG, so conversion of SVG to XAML is necessary. A long time ago I decided to handle this as follows:\nI added a custom build target that would convert the SVG files to XAML during build, then these XAML files would be included as resources into my application in a kind of mysterious way which I did not quite understand myself, and then my application would have access to the icons as XAML.\nIt is no surprise that clunky tricks like this break when you try to make a significant change, such as change the style of the project files. In our case, the custom build target did in fact run, but the XAML files that it generated were not being magically included as resources in our application anymore, so all of those icons failed to load, and they were completely blank on the screen. Furthermore, I had no idea how to fix this, and becoming an expert in this monstrosity known as MSBuild was not in my immediate or even long-term goals.\nThe solution was to ditch the svg-to-xaml build target, to include the original SVG files as resources into the application, and to do the necessary conversions from SVG to XAML at runtime.\nRuntime Problem: no splash-screen\nThe solution to this problem, (which is described here: https://stackoverflow.com/a/62141464/773113) was to write a couple of lines of code in Application.OnStartup() to create the splash-screen myself.\nOnce all of the runtime problems were resolved, I was able to perform the following additional improvements:\nReplaced a whole bunch of resource-include statements like \u0026lt;Resource Include=\u0026quot;Art/Icon/Checkmark.svg\u0026quot; /\u0026gt; with a single resource include statement: \u0026lt;Resource Include=\u0026quot;Art/Icon/\\*.svg\u0026quot; /\u0026gt;. (And if I wanted to include all SVG files under Art, I could have used \u0026lt;Resource Include=\u0026quot;Art/**/*.svg\u0026quot; /\u0026gt;)\n","date":"2023-09-05T10:45:31.634Z","permalink":"https://blog.michael.gr/post/2023-09-05-sdk-style-msbuild-projects/","title":"Converting MSBuild project files from legacy-style to SDK-style"},{"content":"\rI am giving this tool a try at work, and I am encountering a great many problems with it. I decided to publicly document my findings.\nYou launch their GUI application by going to the \u0026quot;Tools\u0026quot; menu of Visual Studio and selecting \u0026quot;Preemptive Protection - Dotfuscator Community\u0026quot;. At first it seems like nothing happens, but the application does appear a couple of incredibly long seconds later.\nEvery single time you launch their GUI application you are presented with their \u0026quot;Dotfuscator Community Registration\u0026quot; dialog, which you have to cancel in order to proceed. Every. Single. Time.\nWhile looking at the front page of their GUI, there are no fewer than 3 nags to buy visible:\nOne that says \u0026quot;Try Dotfuscator Professional\u0026quot; Another that says \u0026quot;Evaluate Dotfuscator Professional now\u0026quot; And one more which says \u0026quot;A new version of Dotfuscator is available. Upgrade Now\u0026quot;, which, as I will show, is a lie to trick you into visiting their web site. Upon every single startup of either the GUI or the command-line it says \u0026quot;a newer version is available, please download it from the downloads page of our website\u0026quot;. So, if you want to ignore the newer version, you can't, you will always be pestered to download the newer version.\nObviously, it calls home, but it did not first ask for permission to call home.\nIf you go to the downloads page of their website to download the latest version, the only downloads available are for versions of Visual Studio that are older than the latest version, which is 2022, which is what I am using, and which is what Dotfuscator came bundled in.\nThey do not say what the latest version number is on their web site, so you cannot compare it against the one you already have.\nI downloaded the latest version they offered on their web-site, and when I tried to install it, it said that it cannot find any compatible version of Visual Studio to install itself into. So, the \u0026quot;newer version is available\u0026quot; message is just a damned lie to lure you into visiting their website.\nEach time you launch their GUI application, it says \u0026quot;Dotfuscator1.xml\u0026quot; on its titlebar, which is the filename of the configuration file I created, but it has not loaded that file, because the \u0026quot;Inputs\u0026quot; page is empty. Furthermore, it shows an asterisk next to the filename, meaning that the file has been modified, even before I have performed any actions that would have modified it. (And if I exit their GUI application, it does not ask whether I want to save any changes.)\nAs it turns out, this \u0026quot;Dotfuscator1.xml\u0026quot; is just the default settings filename that it uses so that it does not start completely empty, and it is just a coincidence that it has the same name as my actual configuration file. This explains a lot of the observed behavior, but the fact still remains that this is the default settings filename, so my first settings file is likely to have this name, and then things are bound to get mighty confusing, because \u0026quot;Dotfuscator1.xml\u0026quot; will sometimes refer to the default unsaved settings file, and sometimes it will refer to my actual settings file.\nTheir GUI application remembers the size and position of its main window only on the primary monitor; if you move it to another monitor, next time it starts it will appear on the primary monitor again.\nThe user interface of their GUI application is clunky, inelegant, nonsensical, and results in a very poor user experience. As a small example, on the \u0026quot;Inputs\u0026quot; tab they show a red exclamation mark next to every single one of my DLLs, but they don't give the slightest hint as to what the exclamation mark means or why it is being shown.\nThe Dotfuscator1.xaml file generated by their GUI application is completely unusable because it contains absolute pathnames.\nYou have to manually edit the file to convert them to pathnames relative to the root of the solution. Of course, in doing so, you will be blatantly disregarding the auto-generated comment at the top of the file which says that to edit this file, you supposedly have to use their GUI application. But it is okay, we live in the 3rd millenium, we do our builds on continuous build servers, so all of our build tools are command-line tools, and nobody cares about their crappy GUI application.\nSo, let's use the command-line tool, shall we?\nIf you try to use the command-line tool, it says:\n\u0026quot;You must register Dotfuscator Community in order to execute command line builds. Run the Dotfuscator GUI which will explain how to register.\u0026quot;\nSo, the \u0026quot;community edition\u0026quot; product name is marketing deceit; this is not a community edition, this is a completely useless advertisement of a product. It becomes an evaluation version once you have completed registration, where \u0026quot;registration\u0026quot; is a euphemism for personal information phishing.\nSo, in order to proceed we have no option but to register.\nSo, let's register, shall we?\nTheir \u0026quot;Dotfuscator Community Registration\u0026quot; dialog says \u0026quot;PreEmptive Solutions will notify you by email with news, updated products and services (you may opt-out of being contacted).\u0026quot; There is no checkbox to control whether this will happen; you are just being informed that it will happen. So, beware, you are being opted-in. (I think this violates a bunch of European Union regulations, doesn't it?) Their \u0026quot;Dotfuscator Community Registration\u0026quot; dialog has a \u0026quot;Read our online privacy policy\u0026quot; link. If you click that link, it takes you to some \u0026quot;Policies \u0026amp; Procedures\u0026quot; page, it does not take you to their privacy policy page. On that \u0026quot;Policies \u0026amp; Procedures\u0026quot; page, if you search among the many links that are completely unrelated to privacy, you might find a link to the page they have about their privacy policy. The title of that page is \u0026quot;Privacy Policy under Privacy Shield\u0026quot;. The contemptible disgrace known as the \u0026quot;EU-US Privacy Shield\u0026quot; was declared invalid by the European Court of Justice on 16 July 2020. Their software shows me a \u0026quot;Serial Number\u0026quot; but in the e-mail that they sent me they call the same thing \u0026quot;license key\u0026quot; instead. Their software asks me for a \u0026quot;Confirmation Code\u0026quot; but in the e-mail that they sent me they call the same thing \u0026quot;confirmation number\u0026quot; instead. After registration, each time you launch their command-line tool, it displays the following message: \u0026quot;For personal use only. Please refer to the EULA distributed with The Software for details\u0026quot; Indeed, on their \u0026quot;Command Line Interface\u0026quot; page they have a \u0026quot;License Restrictions\u0026quot; section which states: The Dotfuscator Community license expressly prohibits the use of the product by commercial organizations for anything other than personal research and education. If you would like to use Dotfuscator on commercial projects, please consider evaluating Dotfuscator Professional.\u0026quot;\nAgain, this is even further proof that the \u0026quot;Dotfuscator Community\u0026quot; product title is nothing but a euphemism for \u0026quot;limited evaluation version\u0026quot;; in other words, marketing deceit.\nBoth the \u0026quot;for personal use only\u0026quot; and the \u0026quot;newer version available\u0026quot; messages keep appearing even if you supply the /q (quiet) flag, so there is no way to suppress them. When the command-line tool encounters an error, including normal usage error, it often displays not just an error message, but also a stack trace. This tells me that the tool is still at a highly experimental and immature stage of development. Furthermore, the class names and method names in the stack trace are obfuscated, so they are doubly useless. The command-line tool may also fail with error messages that are extremely cryptic and completely unhelpful. For example, on one occasion where a nuget package sources server was unavailable, the dotfuscator command-line tool failed with the following: Metadata Root has bad signature at 219f8\nIn their Dotfuscator.xml configuration file they make use of guids and other hashes, which render it extremely hostile to humans. For example: \u0026lt;inputassembly refid=\u0026quot;1639ab18-0eb8-4c8c-ba6c-9eab6d8a740d\u0026quot;\u0026gt; \u0026lt;referencerule rulekey=\u0026quot;{6655B10A-FD58-462d-8D4F-5B1316DFF0FF}\u0026quot; /\u0026gt; While running, the commandline tool spews out an incredible amount of messages. A few of those messages are useful, for example the ones about each output file that it creates; the rest, which is 99.9% of them, are entirely useless. If you use the /q (quiet) flag, the only messages that are suppressed are those few useful ones; the 99.9% of entirely useless messages are still spewed. A great many lines of output spewed out by the command line tool are prefixed with the string \u0026quot;[Build Output]\u0026quot;, which is a statement in direct conflict with fact: this is definitely not build output, this is obfuscator output. If you run the command-line tool without any options, it gives usage information. The usage information says that it supports a /p=outdir=\u0026lt;directory\u0026gt; option. This option has absolutely no effect. If you run the command-line tool with the /?? option to see the \u0026quot;extended\u0026quot; options, there is another /out:\u0026lt;directory\u0026gt; option, and that one works. The tool systematically utilizes silent failure. As a result, it either issues no error messages where it should, or it issues misleading error messages. Both of these behaviors constitute sabotage against the developer. For example: If it cannot find one or more of the input assemblies specified in the configuration, it will not complain at all. This means that I may be under the impression that a certain assembly is being obfuscated, while in fact it is being shipped to customers completely unobfuscated, due to a simple spelling mistake, and the tool did not give me the slightest warning or hint that this is happening. If all input assemblies are missing, then it says \u0026quot;There are no assemblies to process. Stopping the build.\u0026quot; The problem here is that the message suggests that I invoked the tool without giving it any work to do, while in fact I did invoke the tool with specific work to do, which the tool did not do. Within the torrent of output lines that are prefixed with \u0026quot;[Build Output]\u0026quot; no distinction is made between lines that are frivolous spam which has to be filtered away and lines that contain error messages, such as \u0026quot;no assemblies found to process\u0026quot;. Thus, in order to avoid missing any error messages, we are forced to see all the frivolous spam every single time we launch the tool. So, after all this, I could verify that I can use the tool from my build server, and that name mangling works. But what about code mangling, otherwise known as flow control obfuscation? Well, as it turns out, that is beyond the scope of the \u0026quot;Community\u0026quot; edition; you have to buy the \u0026quot;Professional\u0026quot; edition if you want to have that. So, let's give \u0026quot;Dotfuscator professional\u0026quot; a try, shall we?\nIn the confirmation e-mail that I received when I registered my \u0026quot;Community\u0026quot; edition there was a link to \u0026quot;Visit My Account\u0026quot;. When I click on this link, it takes me to a page which asks for a user-name and password. However, when I registered, they did not say anything about any user-name nor password; I just received a \u0026quot;confirmation code\u0026quot;. This is mighty confusing, annoying, and frustrating. So, it appears that I am going to have to register again, this time with a user-name and password. After registering again, and clicking on \u0026quot;Try Dotfuscator Professional\u0026quot;, and filling in their mandatory survey form, and receiving the installer, and installing it, I am presented with a \u0026quot;Click here to activate Dotfuscator\u0026quot; dialog. So, besides \u0026quot;registering\u0026quot; in order to try their useless advertisement, and then \u0026quot;registering\u0026quot; again in order to start a free trial of their actual product, I now have to \u0026quot;activate\u0026quot; the free trial. This was done by entering a key that they sent me by e-mail. From the looks of it, if I decide to continue using this product after the trial, I am going to have to purchase what they call a \u0026quot;Build License\u0026quot;. This build license will have to somehow be added to the build server, which I am not in control of, and will have to somehow be updated each time the build server changes; we will see about that when we get there. Neither on their web-site, nor during the entire process of starting the free trial, do they seem to mention how long this free trial lasts. It might be one month, it might be one week, it might be one day. They just don't say. It is a secret. Nowhere in their communications do they seem to mention how much their product costs. It might be 100 bucks, it might be 1000 bucks, it might be 10000 bucks. The only thing they say is \u0026quot;Request a Quote\u0026quot;, which to me means a few things: They are going to be eyeballing me and tailoring their price according to how deep they estimate my pocket to be. Someone else will get a different price, which is unfair. I might get a different price if I bargain, which I hate to do. They are an inefficiently run company that relies on salesperson labor. On their \u0026quot;Request a Quote\u0026quot; page they have a spelling mistake, \u0026quot;Xamarian\u0026quot; instead of \u0026quot;Xamarin\u0026quot;. Their \u0026quot;Request-a-quote\u0026quot; page does not work. When I click the \u0026quot;submit\u0026quot; button, the button becomes slightly faded out, and nothing else happens. Of course I had to waste my time reloading the page, re-filling all of my information, re-solving the captcha, and re-submitting the form 3 times before deciding that it just does not work. So, PreEmptive Solutions is proving to be very good at one thing: wasting my time. They called me. On the phone. Just at the exact moment that their request-a-quote page was proving to be a fiasco, my phone rang. It was a guy speaking with a thick French accent, allegedly making a follow-up call after my registration. I would bet that their seemingly out-of-order request-a-quote page contained just enough functionality to alert him that I tried to use it. I asked him how much it costs to have a license for one team, one application. He said 4250 euros per year. I also asked how long the evaluation lasts, he said that it is 14 days. Since they are not posting this information on their web site, I am taking the liberty to post what they said to me in person. You might get different answers. The date today is 2023-08-16. When I run the tool, it says \u0026quot;Your subscription expires in 15 days.\u0026quot; During the evaluation I was able to determine that the tool does actually work; if used correctly, it does those things that I would want from such a tool: It can be used as a command-line tool on a build server. It provides code mangling that is so strong that the reverse-engineered code produced by ILSpy does not compile, and even if it did compile, my guess is that it would probably not run. It provides name mangling, not only within individual assemblies, but also across assemblies if requested. (Mangling of public identifiers.) The tool also has many other features, for example string encryption. While working with this tool I encountered the following additional problems:\nTheir command-line tool requires an internet connection in order to work. If your internet connection is out of order while trying to use it, the tool will fail with the following error messages:\nError: Could not contact the activation server. An error occurred while sending the request. The remote name could not be resolved: 'licensing.preemptive.com'\nRetrying the connection in 30 seconds.\nRetrying connection...\nRetry failed.\nError: Could not contact the activation server. An error occurred while sending the request. The remote name could not be resolved: 'licensing.preemptive.com'\n","date":"2023-08-15T12:29:37.658Z","permalink":"https://blog.michael.gr/post/2023-08-15-on-dotfuscator-by-preemptive/","title":"On Dotfuscator by PreEmptive Solutions"},{"content":"So, I decided to adopt my covid look as my post-covid look.\nThis means that I should be updating my avatar.\nSince my pre-covid avatar will not be used anymore, I thought I should post it here for posterity.\nFor reference, if you wish.\n","date":"2023-08-06T11:47:52.089Z","permalink":"https://blog.michael.gr/post/2023-08-06-my-pre-covid-look/","title":"My pre-covid look"},{"content":"\rI recently did this at work, and I decided to document the process here in the form of a how-to guide. Please note that I am not an expert, I am learning as I go along, so there may be mistakes.\nConvert all projects to sdk-style. This is necessary for net7, and also a very useful thing to do even if we were staying in net472. I cover it in another post: Converting MSBuild project files from legacy-style to SDK-style and it actually represents most of the work needed to migrate to net7.\nChange the actual version. You might want to start migrating the projects one at a time, so that you do not migrate the entire solution at once. This will allow you to keep ensuring at each step that the entire solution still works.\nA dotnet project may depend on dotnet-framework projects, but a dotnet-framework project may not depend on dotnet projects; therefore, if we want to migrate projects one at a time instead of all of them at the same time, then the first project that we migrate must be one which constitutes a root of a project dependency tree.\nIn our case, we are making a WPF application; so, in our solution we have one project which is a windows executable, and a multitude of other projects that are class libraries. The executable project directly or indirectly depends on the class libraries, but no class library depends on the executable project; therefore, the executable project is a root in the project dependency tree. So, that's the first project to migrate.\nIn your project file, replace the following:\n\u0026lt;TargetFramework\u0026gt;net472\u0026lt;/TargetFramework\u0026gt;\nwith the following:\n\u0026lt;TargetFramework\u0026gt;net7.0\u0026lt;TargetFramework\u0026gt;\nor, for a WPF project:\n\u0026lt;TargetFramework\u0026gt;net7.0-windows\u0026lt;/TargetFramework\u0026gt;\nThat's it, you can now build. Of course, it will not build. There is a number of issues that will need to be fixed.\nResolve build problems. The issues that I encountered and had to fix are as follows:\nBuild Problem: More nullability issues\nNet7 complains about nullability issues there were net472 did not. For example:\nIn a class you may have had public override string Equals( object other ) and it may have worked fine, but you can't do that anymore in net7: the base Equals() method accepts a nullable parameter, and you cannot just waive the nullability of the original parameter in an override. So, it will now have to be public override string Equals( object? other ).\nAnnoyingly, the same applies to the Equals method of IEquatable\u0026lt;T\u0026gt;, but in this case for absolutely no good reason. That's just how it is, and we have to make do.\nAt some place I was invoking new System.Threading.Thread( threadProcedure ); where threadProcedure was defined as void threadProcedure( object data ). The error was:\nerror CS8622: Nullability of reference types in type of parameter 'data' of 'void ServerThread.threadProcedure(object data)' doesn't match the target delegate 'ParameterizedThreadStart' (possibly because of nullability attributes).\nAs you can see, the error message even includes a hint which points to the exact problem, and the fix is to simply declare data as nullable: void threadProcedure( object? data ).\nWhen declaring a new dictionary type of \u0026lt;K,V\u0026gt; you have to add where K: notnull.\nMethods like Dictionary.TryGetValue ( key, out T value ) need to be changed to Dictionary.TryGetValue( key, out T? value ).\nBuild Problem: GlobalSuppressions\nI used to have a GlobalSuppressions.cs file with a bunch of [assembly: SuppressMessage( ... )] attributes for things like \u0026quot;ENC1003\u0026quot;, \u0026quot;IDE0063\u0026quot;, \u0026quot;IDE1006\u0026quot;, etc. I did not know what to pass as \u0026quot;category\u0026quot;, so I used to pass null. This does not work anymore, due to global nullability checking.\nTo resolve this problem, there are three options:\nFind the proper values to pass for category. (#AintNoBodyGotNoTimeFoDat) Specify #nullable disable for this particular file. (Meh.) Just delete this file, since we can now start making use of EditorConfig. Needless to say, I picked the last option.\nBuild Problem: Types \u0026quot;forwarded\u0026quot; to nuget assemblies\nI had a piece of code which was obtaining a windows service in order to restart it, with a line like this:\nvar serviceController = new SysServiceProcess.ServiceController( serviceName );\nFor this line, MSBuild started giving me the following error:\nerror CS1069: The type name 'ServiceController' could not be found in the namespace 'System.ServiceProcess'. This type has been forwarded to assembly 'System.ServiceProcess.ServiceController, Version=0.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a' Consider adding a reference to that assembly.\nAs you can see, the error message is quite descriptive, and even suggests a fix, which is almost correct. In this case, I had to add the following to my project file:\n\u0026lt;PackageReference Include=\u0026quot;System.ServiceProcess.ServiceController\u0026quot; Version=\u0026quot;4.1.0\u0026quot;/\u0026gt;\nIn another instance, I had a piece of code that played a sound, with a line like this:\nvar player = new System.Media.SoundPlayer( soundPathName );\nFor this line, MSBuild started giving me the following error:\nerror CS1069: The type name 'SoundPlayer' could not be found in the namespace 'System.Media'. This type has been forwarded to assembly 'System.Windows.Extensions, Version=0.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51' Consider adding a reference to that assembly.\nThe fix was to add the following to my project file:\n\u0026lt;PackageReference Include=\u0026quot;System.Windows.Extensions\u0026quot;/\u0026gt;\nIn some other piece of code, MSBuild started complaining that there exists no \u0026quot;Bitmap\u0026quot; type, even though it was entirely unclear why it was looking for type \u0026quot;Bitmap\u0026quot;. In any case, again it suggested to reference a particular assembly, and the problem went away.\nBuild Problem: System.Range\nIn our solution we used to have a type called Range. In modern dotnet a new type called System.Range has been introduced, and this caused ambiguous reference errors.\nThis can be solved either by renaming our own types, or by never directly importing external namespaces, and always using aliases instead. In other words, using Sys = System; instead of using System;\nBuild Problem: warnings about assembly conflicts\nNot really an error, but I like my build to be issuing no warnings.\nMSBuild started complaining the following:\nwarning MSB3243: No way to resolve conflict between \u0026quot;System.IO.Compression, Version=7.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\u0026quot; and \u0026quot;System.IO.Compression\u0026quot;. Choosing \u0026quot;System.IO.Compression, Version=7.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\u0026quot; arbitrarily.\nOne way to solve this problem is to find all occurrences of the following:\n\u0026lt;Reference Include=\u0026quot;System.IO.Compression\u0026quot; /\u0026gt;\nand replace them with the following:\n\u0026lt;Reference Include=\u0026quot;System.IO.Compression\u0026quot; Version=\u0026quot;7.0.0.0\u0026quot; /\u0026gt;\nHowever, there is a better way to solve this problem: Just remove the reference! The build system is reporting a conflict between the assembly as referenced in the project file and the already-existing assembly in net7, so obviously, the assembly already exists, so the project does not need to explicitly reference it anymore.\nBuild Problem: System.Diagnostics.Debug.Listeners\nI had a line like this:\nSystem.Diagnostics.TraceListener listener = System.Diagnostics.Debug.Listeners[0];\nMSBuild started complaining as follows:\nerror CS0117: 'Debug' does not contain a definition for 'Listeners'\nThe solution was to replace the above line with the following:\nSystem.Diagnostics.TraceListener listener = System.Diagnostics.Trace.Listeners[0];\nBuild Problem: Thread.Abort() is obsolete\nCode that makes use of 'Thread.Abort()' started giving the following warning:\nwarning SYSLIB0006: 'Thread.Abort()' is obsolete: 'Thread.Abort is not supported and throws PlatformNotSupportedException.' https://aka.ms/dotnet-warnings/SYSLIB0006\nThe solution was to fix the code so that it does not use Thread.Abort(). (It was a bad idea anyway.)\nBuild Problem: Empty macros in Post-Build-Step\nMy post-build-step was failing, because the macro $(ProjectDir) was empty. There are two possible solutions to this:\nIn the post-build-step, the correct magical incantation to use is $(MSBuildProjectDirectory) instead of $(ProjectDir). Better yet, drop post-build steps, and instead go to project settings, and add a post-build target, which is the new dotnet way of specifying post-build steps. Resolve runtime problems. Once the build problems were resolved, it was time to try running. Again, I did not expect the application to run, and in fact it did not run.\nHere are the runtime problems I encountered, and how I resolved them.\nRuntime Problem: App.config/system.data\nMy application would fail during startup with a \u0026quot;System.Windows.Data Error 17\u0026quot; saying that it could not get some value from some settings file. The stack trace was followed by the good old familiar nonsense: TargetInvocationException:'System.Reflection.TargetInvocationException: Exception has been thrown by the target of an invocation which essentially means \u0026quot;please keep reading\u0026quot;. The next line was System.Configuration.ConfigurationErrorsException: Configuration system failed to initialize, which again says pretty much nothing, bringing us, finally, to the next line that mentions the actual problem: `System.Configuration.ConfigurationErrorsException: Unrecognized configuration section system.data. (my-application.config line 12)\nSo, it turns out that modern dotnet does not like the \u0026lt;system.data\u0026gt; section in App.config. In my case this section was empty, so all I had to do was remove the section.\nRuntime Problem: non-null EventArgs\nI am a control freak, so my System.AppDomain.CurrentDomain.ProcessExit event handler contained an assertion that the eventArgs parameter of that event is null, because I had observed it to be null under dotnet-framework.\nAs it turns out, in modern dotnet this parameter is not null anymore; it is a default instance of EventArgs.\nThe fix for this was to change the assertion to expect a non-null eventArgs from now on.\nRuntime Problem: Accessing native DLLs\nIn our project we have a native DLL, which used to be placed in the same directory as the executable, but under dotnet-framework native DLLs are, by default, placed in special locations. In our case, our DLL was placed in \u0026lt;executable-location\u0026gt;/runtimes/win-x64/native, so it could not be loaded.\nI could update the code to go looking for the DLL in that new location, but I decided to do something more simple; I added the following line to the project file:\n\u0026lt;AppendTargetFrameworkToOutputPath\u0026gt;False\u0026lt;;/AppendTargetFrameworkToOutputPath\u0026amp;gt\u0026gt;\nRuntime Problem: Splash-screen weirdness\nAs soon as I managed to get my WPF application to run, I noticed something weird with the splash-screen: as our application was loading, the splash-screen would first appear stretched (in an ugly way) to a size that was larger than normal, and then it would shrink to its normal size but it would move to a location slightly to the left, and slightly above the center of the screen, where it would stay until our application would finally complete loading and the splash-screen would disappear.\nAs it turns out, this is a known bug in WPF see github dotnet wpf issue 947 and github dotnet wpf issue 5070.\nThe solution, (which is described here: https://stackoverflow.com/a/62141464/773113) was to write a couple of lines of code in Application.OnStartup() to create the splash-screen myself.\nResolve release build problems. Once all of the above was done, it was time to try building the release version of our application. Here, I ran into the biggest problem:\nRelease Build Problem: ConfuserEx does not work anymore\nIn our application we use obfuscation. So far, this has caused us a lot of trouble: first we used to employ a tool called Confuser, which was later abandoned and re-incarnated as ConfuserEx, so we had to start using that one. Then, that one was abandoned too, so we had to find a fork of it that was still being maintained by someone.\nNow with DotNet 7, ConfuserEx does not work for us anymore: it fails with a message saying that it does not know the executable file format. There is a question-and-answer where someone asks the author for a solution, and the author points them to an alpha build of some version 2.0 of that tool, but that was a long time ago, and the artifacts of that alpha build do not exist anymore.\nThere is a new tool called Dotfuscator, which is bundled with Visual Studio, but there are all kinds of serious problems with it, see another post of mine about that: On Dotfuscator by PreEmptive Solutions. I am currently working on this, when I find a solution I will update this article with my findings.\nThis guide will be updated during the following days as I complete the migration of the entire application from DotNet Framework 4.6.2 to DotNet 7.\n","date":"2023-07-21T11:33:13.653Z","permalink":"https://blog.michael.gr/post/2023-07-21-migrating-project-from-dotnet-framework/","title":"Migrating a project from DotNet Framework 4-7-2 to DotNet 7"},{"content":"It has been more than a year since I created this question on GitHub Community; a couple of days after that I found the solution by myself, so I answered my own question, and to this date comments keep being added by people who were helped by my post.\nWhen I look at it today, I notice that my answer has this particular style, this grumpy indignation which has become so characteristic of me, after a lifetime of battling with lame software, and even worse, with lame error messages.\nI thought I should share this on my blog for posterity.\nHere is the link:\nhttps://github.com/orgs/community/discussions/23474\nAnd here is the question:\nGitHub Community\nMaven deploy fails with status 422 unprocessable entity #23474\nAsked by Mikenakis on May 27, 2022\nI have this repository: GitHub - mikenakis/Bathyscaphe: Deep immutability and thread-safety assessment for Java objects in which both my CI-Workflow and my Release-Workflow work fine.\nNow I am trying to extract a sub-module from that repository into a new repository.\nSo, I created this repository: GitHub - mikenakis/BathyscapheClaims\nThe CI-Workflow works, but when I try to launch the Relase-Workflow, I get the following error:\nFailed to execute goal org.apache.maven.plugins:maven-deploy-plugin:3.0.0-M2:deploy (default-deploy) on project bathyscaphe-claims: ArtifactDeployerException: Failed to deploy artifacts: Could not transfer artifact io.github.mikenakis:bathyscaphe-claims:jar:1.3 from/to github (https://maven.pkg.github.com/mikenakis/BathyscapheClaims): transfer failed for https://maven.pkg.github.com/mikenakis/BathyscapheClaims/io/github/mikenakis/bathyscaphe-claims/1.3/bathyscaphe-claims-1.3.jar, status: 422 Unprocessable Entity → [Help 1]\nI have already seen the issues that appear to be similar; most of them have no answer, one of them (Maven deploy fails with HTTP 422 Unprocessable Entity) has an extensive discussion, but it does not seem to apply in my case.\nCan someone please help me? It is probably something silly, but I only have one pair of eyes, while this obviously needs two! (-:=\nCheers!\nAnd here is the answer:\nAnswered by mikenakis on May 29, 2022\nI figured this one out too by myself. (Hey community, you are not helping much!)\nWhat happened is that GitHub apparently has some monstrous bug which causes artifact deployment to fail if some other repository (only by the same owner, I hope!) has already deployed an artifact with the exact same name.\nIn this case, one repository was called Bathyscaphe, the other was called BathyscapheClaims, but each repository was trying to deploy an artifact called bathyscaphe-claims.\nI changed the name of the artifact in the second repository from bathyscaphe-claims to bathyscaphe-claims2 and it worked.\nThe situation was certainly made worse by this contemptible monstrosity that goes by the name of “maven”, which never gives any meaningful error message whatsoever when the slightest thing goes wrong. I mean, “unprocessable entity”? Really? It must have been some very special kind of idiot who came up with this error message.\n","date":"2023-07-06T09:21:20.537Z","permalink":"https://blog.michael.gr/post/2023-07-06-solved-maven-deploy-fails-with-status/","title":"[SOLVED] Maven deploy fails with status 422 unprocessable entity"},{"content":"\rThe XAML Hot Reload feature of WPF is extremely useful because GUI work often involves tweaking visual aspects of an application, so being able to modify XAML, save it, and immediately see the changes on the screen saves a huge amount of time as opposed to having to terminate the application, modify the code, re-compile, re-run, and go clickety-clickety-click to navigate to the same page and finally see your changes.\nUnfortunately, as a WPF project grows, the XAML Hot Reload feature inevitably one day stops working: You modify your XAML, you save the XAML file, and yet nothing changes on the screen. The message \u0026quot;No changes were found\u0026quot; appears in the Hot Reload tab of the Visual Studio Output Window, but it is a damned lie, because you just made changes. This can really be a problem.\nWhen you find yourself in this extremely unpleasant situation, here is a list of things to try:\nFirst of all, the usual: Exit Visual Studio and re-launch Visual Studio. Then, the all too familiar: Exit Visual Studio, delete the .vs directory in your solution, and re-launch Visual Studio. Exit Visual Studio, delete the .vs directory, clean all output directories, and re-launch Visual Studio. No, you cannot just go to the Build menu and select \u0026quot;Clean Solution\u0026quot;; that would make too much sense. The \u0026quot;Clean Solution\u0026quot; option is a joke that never fixes anything; instead, you have to have a script that actually visits every bin and obj directory and deletes its contents, and you also have to make sure that Visual Studio is not running when you run this script. And then, the arcane: Make sure all your dependency properties are done properly. What constitutes \u0026quot;properly\u0026quot; for Microsoft is actually quite counter-intuitive and quite preposterous: every dependency property must:\nbe backed by a field of type System.Windows.DependencyProperty which must be public static, and whose name must start with the name of the corresponding C# property, and must end with the suffix Property. All this magic must be done exactly right every single time, or else all sorts of other magic do not work anymore, for example the XAML Hot Reload magic.\nIf you have any styles where you had to specify the type of the data context so as to enjoy type safety and auto-completion when editing, (and to avoid ReSharper warings,) make sure that you do not use the d:DataContext=\u0026quot;{d:DesignInstance ... magical incantation suggested by some folks out there. If something like that appears outside of the root XAML element, XAML Hot Reload will stop working. Instead, use the following magical incantation:\n\u0026lt;Style TargetType=\u0026#34;{x:Type ...}\u0026#34;\u0026gt; \u0026lt;d:Style.DataContext\u0026gt; \u0026lt;x:Type Type=\u0026#34;SomeViewModel\u0026#34; /\u0026gt; \u0026lt;/d:Style.DataContext\u0026gt; Ultimately, all problems with XAML Hot Reload can be traced down to the fact that it embraces silent failure. In my experience any feature that involves silent failure is a failure as a whole, because:\nIn general, anything that can break will at some point break;\nif it gives you some hint as to why it broke,\nor even just a hint that it just now broke,\nthen you can fix it;\nbut if it gives no hint, then you can't fix it.\n","date":"2023-06-12T06:11:58.25Z","permalink":"https://blog.michael.gr/post/2023-06-12-the-trouble-with-xaml-hot-reload-in-wpf/","title":"The Trouble with XAML Hot Reload in WPF"},{"content":"\rThere are some words in English that are uncountable. For example: cheese, furniture, music, evidence, research, knowledge, information, etc. When we speak of those things in plural, we still use the singular form: \u0026quot;I would like to order a four-cheese pizza\u0026quot;, \u0026quot;Let me give you some of my furniture\u0026quot;, \u0026quot;We need to consider all the evidence\u0026quot;, etc.\nAnother such word is code, in the context of programming.\nWhen referring to program code, the word code is uncountable. \u0026quot;I wrote the code for all those apps\u0026quot;, \u0026quot;You have a lot of code to review\u0026quot;, \u0026quot;We write new code every day\u0026quot;.\nThere are certain other meanings of the word code that do have a plural form; for example, \u0026quot;Give me the access codes\u0026quot; or \u0026quot;See the list of HTTP status codes\u0026quot;; but these meanings refer to numbers, or ciphers, in other words data, not program code. Program code is always uncountable.\nReference: Oxford Learner's Dictionaries - English - Code (Scroll down to meaning 3, \u0026quot;programming\u0026quot;.)\nNow, we live in free societies where everyone enjoys freedom of speech, and more generally, freedom of expression. You can choose to say whatever you like, just as you can choose to dress in whatever way you like.\nBut when you say \u0026quot;codes\u0026quot; referring to program code, please do keep in mind that you come across like this complete idiot:\nOr like this complete idiot:\nDo you want to come across like a complete idiot?\nThe choice is yours.\n(Grumpy cat meme unnecessary; this was grumpy enough by itself.)\n","date":"2023-05-26T07:44:24.071Z","permalink":"https://blog.michael.gr/post/2023-05-26-code-is-uncountable/","title":"Program Code is Uncountable"},{"content":" With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody.\nHyrum's Law\n(From https://www.hyrumslaw.com/)\n","date":"2023-05-12T22:42:54.061Z","permalink":"https://blog.michael.gr/post/2023-05-12-hyrums-law/","title":"Hyrum's Law"},{"content":"Among the answers that I have given to thousands of different questions on stackoverflow.com and softwareengineering.stackexchange.com, some have been vehemently down-voted.\nSometimes I make mistakes; when that is the case, I fix or delete my answer; however, in other cases, the down-votes represent opinion which is in disagreement with my opinion, and in those cases I let my down-voted answers be, since I stand by my own convictions.\nI suppose that this is the price you have to pay for:\nhaving your own opinions; and daring to voice them. There will always be some folks who will take offense.\nHere is a list of my severely down-voted answers, so that you too can take offense and down-vote them even further:\nStack Overflow: api design - Which HTTP code is most suitable for when an endpoint is \u0026quot;full\u0026quot;?\nSoftware Engineering: api design - Should a REST API return a 500 Internal Server Error to indicate that a query references an object that does not exist?\nSoftware Engineering: grammar - Does it make sense to use \u0026quot;ys\u0026quot; instead of \u0026quot;ies\u0026quot; in identifiers to ease find-and-replace functionality?\nSoftware Engineering: java - Is it okay to have objects that cast themselves, even if it pollutes the API of their subclasses?\n","date":"2023-04-23T15:59:28.478Z","permalink":"https://blog.michael.gr/post/2023-04-23-notable-severely-down-voted-answers-of/","title":"Notable severely down-voted answers of mine on Stack Exchange"},{"content":"\rThis band has been haunting me for the past days. I feel compelled to write about it. This post will be completely different from the kind of posts you normally see on this blog.\nWhile going through some random playlist on YouTube I stumbled upon this band that I immediately took a liking to, which is something that does not happen often. When I tried to find out a bit more information about them, what I discovered wrecked me.\nThe name of the band is Trees of Eternity, and they are said to belong to the Doom/Gothic Metal genre, which is not exactly the kind of music that I listen to, but in the case of this particular band, their sound matches my taste very well.\nI was intending to listen to them while working on my computer, but there was something about their sound that grabbed hold of me and I could not help but switch to watch the videos of their songs.\nA couple of their official videos feature their singer, a beautiful lady in her thirties.\nShe has an extraordinarily soft, breathy voice the like of which I have never heard before. It has been described as delicate, angelic, ethereal. It is feminine, but not girly; I would actually call it solemn. Because she sings so softly, she keeps her mouth very close to the microphone, so you can hear her inhaling between sentences, and this creates a feeling of presence, as if she is singing right next to you. Their instruments sound like Symphonic/Goth Metal, but without excitement or fanfare; they play masterfully, but they stay in the background so as to keep her beautiful voice verily in the foreground.\nThe grand sum of all this is definitely grave and gloomy, but mature, and completely free from pretentiousness and hyperbole, which are so common in the genre. It constitutes the most cohesive sound I have heard in many years.\nIn the videos, the background is nature; a Nordic forest; barren tree branches in darkness; gray sea waves crashing on shore; empty fields of tall grass with a somewhat withered look. In both videos she is wearing the same black sun necklace. In one video (Sinking Ships) she appears with flowing long black hair; in the other, (Broken Mirror) her head is covered in a veil.\nSo far, so good. The trouble started during a scene in the Broken Mirrors video where the wind momentarily lifts the veil from her face and she looks at the viewer with a distinctly eerie gaze that I found somewhat unsettling.\nAt first, I was tempted to dismiss it as just another goth acting before the camera as is befitting to a goth, but as the scene faded into the next, I was left with the impression that her gaze had been completely free from the overstatement which is characteristic of pretense; there was something inexplicably genuine about it, but I could not tell what it was. Since I was not sure what was going on, I could not assign much weight to it at that moment, but in retrospect, I would say that it was the kind of gaze that would make the blood freeze in your veins.\nSeeking to dissolve the mystery, my eyes fell upon the comments below the video. I read the first one.\nThe commenter had written that they were extremely sad, because they had just discovered this band, (like me,) and they were completely blown away by the beauty of their sound, (like me,) and they had been hoping to hear more from them in the future, (like me,) but they had just found out that the singer had died.\nI decided to look them up on Wikipedia, in part because I like fact-checking things, in part because I wished this to turn out to be false, and in part out of curiosity: common causes of death for rock singers are drugs, alcohol, suicide, or combinations thereof; so, what was it going to be for that angelic voice?\nIndeed, Wikipedia begins by stating that Trees of Eternity was a musical collaboration between certain individuals which ended with the death of their singer, Aleah Stanbridge, in 2016. It goes on to mention that the band members and guest contributors were all acclaimed musicians, having previously played with well-known bands such as Katatonia, Wintersun, and Paradise Lost, which even I have heard of, despite them belonging to a genre that I don't normally listen to; their drummer had even played on occasion with Nightwish, which used to be one of my favorite bands. This explained the masterfulness of the instrumentation. I kept reading.\nAs it turns out, Aleah Stanbridge died at the age of 39, from cancer.\nThis fact, along with her unsettling gaze in the video, made me curious, so I looked for information about the timeline of the band.\nThey formed in 2013. They mostly worked remotely, so progress must have been slow. At the time of her death in April of 2016 the band had not released anything yet. Their one and only record, The Hour Of The Nightingale was released half a year later, in November of 2016.\nNow, cancer is not something that kills you one night in your sleep while nobody expected it; it takes time. This is perhaps the greatest tragedy with this disease: you know well in advance. So, I thought that this timeline was a bit odd. That is when I begun to suspect the nature of the musical collaboration that I had stumbled upon.\nI went back to the track list looking for clues to support my hypothesis, and sure enough, the title of the very first song served as an immediate confirmation:\n01. My Requiem\nThe lyrics are also in line with that, in this and most other songs.\nThat was when I realized just what it was about the lady's gaze in the Broken Mirror video that made it so unsettling: this woman standing before the camera was not acting; she was dying.\nThat was when I realized that the Sinking Ships video, showing her with long flowing hair, must have been shot early on, while the Broken Mirror video, where her head was covered in a veil, must have been from later on.\nThat was when I realized that in the Broken Mirror video she appears standing, or walking, but she is not actually singing; maybe she had no voice anymore at that point.\nThat was when I started experiencing in full the gut-wrenching effect of many of the verses, like my season has come to an end, or embrace this as a Nightingale in song, or perhaps the most gut-wrenching of all: ...of a fate worse than death, condemned to silence. There are many other verses that have an immensely powerful meaning in light of the circumstances, I will leave them for you do discover.\nNone of this has been publicized; you have no way of suspecting anything unless you pay attention to hints, and no way of knowing unless you connect the dots. If you do not know, they are just your usual morbid gothic lyrics; but if you know, they are heart-felt devastation.\nThat was also when I realized how truly superb the instrumentation and the mastering had been. These were all such accomplished musicians that in the associated literature someone at some point described them as a super-group. Whatever; the thing is, accomplishment builds egos, and with ego usually comes an extravagant style of playing, a prime example of which is, say, Joe Satriani. Yet, these musicians produced music of excellent quality while keeping their tone down so as to keep that lady's ethereal voice always at the front: in the entire record there is not a single guitar solo; not a single attention-catching shred; not a single roaring drum roll; at the same time, there is absolutely nothing inadequate about it: from start to finish, the music is rich, sophisticated heavy metal, mostly in ballad style, at times in epic style, but always serving as a bed for her voice.\nThe music is precisely what it had to be for the occasion: masterful, but intentionally understated.\nBecause the record was not about them; it was about her.\nHow often does it happen that after receiving a medical death sentence someone will continue to make music instead of withdrawing and withering away? I do not know, but I would be willing to guess that it must be exceedingly rare. I used to suspect that Roy Orbinson's comeback at the end of the eighties with the re-make of Pretty Woman and with new hits like The End of the Line was such a case, because he died very soon afterwards, but as it turns out he died from heart attack, so that was probably not it. But even if someone received the bad news and decided to keep on keeping on, how likely would it be that they would be singing about death? And how likely would it be that they would be singing about their own death?\nIt may well be that Trees of Eternity is a project unprecedented in the history of music-making.\nSo, before us lies an interesting phenomenon: we have the entire doom/gothic genre of music, a huge death cult obsessed with all things morbid, but actually full of pretentiousness, hyperbole, and copious amounts of black make-up; and sitting right at the fringe of this genre we have this one little gem of a record, which contains in it more death than the entire genre combined. Is it not ironic?\nThese realizations appear to be completely lost on critics who have reviewed The Hour Of The Nightingale. They begin by acknowledging that it is tragic that the singer has died, and then they proceed to evaluate the record as they would evaluate any other record. At best, they find it very good music, but the circumstances under which the record was made seem to escape them, so they fail to appreciate that it does in fact sound precisely as it ought to sound, let alone that no one alive is entitled to find fault with it. One critic complains that most songs are in ballad style while he would have preferred more variation; whatever; another complains that overall, the record strays too much in the direction of pop, whereas I suppose he would have wanted it to sound more gothic; whatever.\nHere is the thing: in the face of actual death, the entire gothic genre suddenly begins to look like a highly inappropriate funny little circus. Death in itself is morbid enough; no need for black lipstick.\nArtist: Trees of Eternity\nAlbum: The Hour of the Nightingale (2016)\nSong: My Requiem\nLight\nBlistering, so bright\nRealign my core\nLenses open wide\nComing into form\nThe imprint of a life\n[Chorus]\nToo late you're calling out my name\nTo raise me up out of my grave\nAlive in memory I'll stay\nIf you shun these waters where I lay\nIf you shun these waters where I lay\nNight\nCurtains coming down\nInto the shade I stray\nLosing all I've found\nTo my own dismay\nMy season has come to an end\n[Chorus]\nToo late you're calling out my name\nTo raise me up out of my grave\nAlive in memory I'll stay\nIf you shun these waters where I lay\nIf you shun these waters where I lay\nLeave me to rest\nToo drained to rise\nTo try to detect\nAny light\nat the end of the tunnel\n[Chorus]\nToo late you're calling out my name\nTo raise me up out of my grave\nAlive in memory I'll stay\nIf you shun these waters where I lay\nIf you shun these waters where I lay\n","date":"2023-03-21T15:55:13.311Z","permalink":"https://blog.michael.gr/post/2023-03-21-trees-of-eternity/","title":"Trees of Eternity"},{"content":"So today I started encountering a very weird audio issue: When I play music, it sounds normal in the beginning, but then after about a second the sound gets distorted, as if it is muffled, or as if it is undergoing severe lossy compression. If I stop and resume the music, it goes through the same.\nNormally I would know what to do in this situation, but as the years pass Microsoft keeps changing Windows, in the direction of making them dumber and dumber, so in Windows 10 I cannot find the old sound options dialog that I used to use to fix this problem.\nLooking around the interwebz for a solution was not easy, so I decided to document the solution that I found. If you are a power user, you can skim through the text and only look at the words in bold-italics.\nType Win+R and type mmsys.cpl. This will bring up the good old sound options, which Microsoft recently made inaccessible by any means other than this magical incantation, because obviously, nobody should ever want to fix any sound problems, because obviously, Windows has no sound problems. In the Playback tab, locate your Speakers. If you are playing music at that moment, they are easy to locate from the live volume meter right next to them. Select Properties on your speakers. (Not Configure, but Properties.) This should open a Speakers Properties dialog. On the Speaker Properties dialog, select the Advanced tab. Near the bottom of the Advanced tab there is an Enable Fucked Up Sound checkbox, which is actually labelled Enable Enhancements. (Euphemism has always been Microsoft's forte.) Uncheck that checkbox; you are done. Afterword\nI probably started experiencing this problem right after pairing a new bluetooth sound device. Apparently when Windows detects a new sound device it sets some things up for it, and while doing that it entirely arbitrarily also goes and resets some settings for existing devices, e.g. it enables this \u0026quot;Enhancements\u0026quot; setting for my existing speakers.\n","date":"2023-03-15T19:53:01.942Z","permalink":"https://blog.michael.gr/post/2023-03-15-solved-windows-sound-becomes-distorted/","title":"[SOLVED] Windows: Sound becomes distorted after 1 second of playback"},{"content":"I found them at a place called Simplethread while randomly browsing.\nI thought I'd post links here for posterity.\nTaming Names in Software Development by Joseph Glass (2022)\nAgile at 20: The Failed Rebellion by Al Tenhundfeld (2021)\n20 Things I’ve Learned in my 20 Years as a Software Engineer by Justin Etheredge (2021)\n","date":"2023-02-21T00:08:48.557Z","permalink":"https://blog.michael.gr/post/2023-02-21-a-few-good-reads-from-simplethread/","title":"A few good reads from Simplethread"},{"content":"\rSpeeds up Visual Studio debug output by orders of magnitude.\nOn GitHub: https://github.com/mikenakis/VsDebugLogger\n","date":"2023-02-09T10:41:59.462Z","permalink":"https://blog.michael.gr/post/2023-02-09-vs-debug-logger/","title":"New GitHub project: VsDebugLogger"},{"content":" Note: This post is a draft; work-in-progress.\nIf you have ever been in the job market looking for the next move on your career, you cannot have failed to notice that job advertisements on various job boards fall in two distinctly different categories: those that disclose the identity of the employer, and those that do not.\nAs a rule, a job advertisement will not fail to state exactly who the employer is when the employer is doing their own hiring, either direcrly or via an exclusive partnership with a hiring agency. On the other hand, when the job advertisement keeps the identity of the employer a secret, referring to them as \u0026quot;my client\u0026quot;, or utilizing subterfuges such as \u0026quot;a well-established company\u0026quot;, \u0026quot;a leader in the field\u0026quot;, etc., this means that it has been posted by an independently acting recruiter (henceforth simply \u0026quot;recruiter\u0026quot;) who does not have an exclusive agreement with the employer. (And the term \u0026quot;my client\u0026quot; is almost always a lie.)\nThe reason for the secrecy is not understood by most candidates; a common misconception is that some employers wish to remain unidentified when hiring. This is true in such an exceedingly small percentage of cases that it is almost mythological. The true reasons for secrecy in job advertisement are the following:\nTo prevent candidates from bypassing the recruiter and directly contacting the employer. To prevent other recruiters from finding out about the job and creating their own competing job advertisements for it. To post advertisements about jobs that do not actually exist. (You might say, huh? -- I will explain, keep reading.) Secrecy towards candidates is only half of the story; recruiters also utilize secrecy towards employers. Before submitting the CV of a candidate to an employer, any information that would identify the candidate is redacted, and the employer is only told who the candidate is once the recruiter's fee has been paid. This bizarre policy of secrecy gives recruiters a terrific advantage over all parties that they interact with: they are free to lie, trick and deceive everyone so as to maximize their own gain.\nRecruitment is a field which is largely unregulated. This means that if you choose to interact with a recruiter, there is nothing to protect you from being lied to, being manipulated, being treated without any courtesy or decency. In the UK there is some \u0026quot;Employment Agency Standards Inspectorate\u0026quot; which exists purely for the appearances, and does not really have any practical effect. Most other countries do not even pretend to have a regulatory authority. Thus, the job market is like the far west: anything goes, no holds barred, dog eat dog, fastest gun wins.\nAt the same time, recruitment is a job that can be done by anyone. Of course those with many years of experience know how to do it much better than the newbies, but the fact remains that you do not need any studies to start doing this job, no degree or certification is required. All it takes is the ability to talk to people, use a phone, and maybe a computer. As a result, the job of the recruiter attracts those who do not have any particular skills to speak of, could not accomplish anything useful with their lives, and are looking for a way to make quick and easy money. When asked \u0026quot;what do you want to be when you grow up?\u0026quot; no child ever answered \u0026quot;a recruiter\u0026quot;. Usually, people doing this job are doing it after having tried other things and failed.\nFurthermore, the particular subset of people skills needed for this job is not exactly the most virtuous set of skills imaginable: what counts mostly is the ability to cold-read people, to quickly build fake trust with them, to keep your cards hidden while persuading them to reveal theirs, etc.; in short, to manipulate people. As such, it is not a wonder that the job of the recruiter attracts individuals of questionable character: if only they had passing grades, they would have become lawyers.\nSo, think about it: this is a profession that anyone can take a stab at without any qualifications; it is unregulated; it systematically and unpretentiously involves secrecy towards all parties involved; and its daily routine involves manipulating people.\nWhy would anyone ever want to have anything to do with this profession?\nBefore the time of the Internet, recruiters were somewhat useful because the process of finding candidates involved a lot of work. Nowadays, with all these job boards a few mouse clicks away, one would think that the job of the recruiter should be dead, and everyone should be doing their own hiring; and yet, if you go to a job board, what you see is mostly advertisements by recruiters. The reason why this is happening is because technology has also made the recruiter's job a lot easier: candidates are at any moment just a few clicks away. At the same time, their customary commission of one salary of the candidate they place has not changed.\nSince most jobs are posted by recruiters, most people see no option but to interact with recruiters. Once your CV has fallen in the hands of recruiters, it may and may not be used for the job you applied for, but one thing is certain: it will be reused for other jobs without you even knowing it. Not only they spam any company that does any hiring with unsolicited CVs, they also often do this without the knowledge or consent of the candidates.\nFrom all the CVs that they see, companies tend to express interest in only a few, so it does not hurt the recruiter to throw an overabundant number of CVs at the company, including CVs of candidates who are unaware of the fact that their CV is being circulated; it just makes the recruiter look big and well-connected, and nobody will ever discover the deceit because the names are redacted. If a company expresses interest in a CV, the recruiter contacts the candidate and asks them if it would be okay to send their CV to that company, omitting to mention that they have already sent it. If the candidate is uninterested because they happen to be happily employed, the recruiter just tells the company that in the last moment the candidate chose to go work for a different customer of theirs. What is wrong with a little white lie, right?\nMost people include on their CV their personal e-mail address and telephone number. This is a big mistake. Once your e-mail address and phone number have fallen in the hands of recruiters, you will keep receiving spam e-mail and cold calls forever. You will be pestered at work, you will be pestered at home, you will be pestered while going out with friends. Even if you tell a recruiter to never call you again, your phone number will stay in their records, so each new colleague of theirs will try their luck with you, and they will be calling from different phones, so there is no single number that you can block. Also, recruiters keep moving from agency to agency, often taking large amounts of data with them, so your contact information will keep spreading, even into neighboring countries.\nI hold it as a self-evident axiom that the use of secrecy is in and of itself an unscrupulous practice, and I maintain that it alone suffices to render the entire profession of recruiters immoral. One might object that this is the only way this profession could work, but here is the thing: this profession does not have to work; it does not have to exist; the job market can work just fine without them. Even if recruiters were somehow necessary, if the job market cannot work through any means other than through unscrupulous practices, then we might as well throw in the towel, call this the end of the civilized world, and go back to being hunters-gatherers, because this is not civilized either way.\nBesides redacting the name of the employer, recruiters will almost always tamper with the job description itself, in order to make the job more appealing and to remove other details through which one might be able to guess the identity of the employer. When they do this, it is called \u0026quot;sexing up the job description\u0026quot;.\nThat is how an embarrassingly junior position might be transformed into a position with high growth potential, how a fledgling startup might be presented as a rapidly growing company, and how Accountancy, Payroll, and HR Systems might be described as an exciting job opportunity. This is also the reason why many job descriptions are nothing but generic filler that does not really give any useful information about what the job actually entails. That's how you get job descriptions about \u0026quot;working together with stakeholders to implement our business objectives and pursue our strategic plans\u0026quot;\nQuite often recruiters make drastic changes to the list of required skills and competencies so that a web search for the keywords will not match the original job advertisement at the employer's website. Of course, very few recruiters know enough about the jobs as to do this tampering intelligently; as a result, a job that may have initially appeared suitable to you may prove to be entirely unsuitable on closer examination.\nSometimes recruiters even change certain hard facts about job positions; for example, recruiters will routinely advertise a job as being located in a certain city, while in fact the job is in a nearby city. When I asked a recruiter why he did this, he said that they do this \u0026quot;to throw off the competition\u0026quot;.\nIf falsifying job descriptions was not unscrupulous enough, recruiters quite often post jobs that do not exist at all. This practice serves them very well in a number of ways:\nIt gathers large numbers of CVs which they use in the following months or even years to bid for real jobs that require similar competencies. (When they contact you they never say \u0026quot;hey, we know you sent your CV for that awesome job, but here is this mediocre job\u0026quot;; they just say \u0026quot;here is this job\u0026quot;. If years have passed in the mean time, they might add \u0026quot;please send updated CV.\u0026quot;) By posting fake jobs they run no risk of hinting their competitors about the real jobs that they know of, if any. It acts as constant advertisement for their firm on the job boards. (Think about it: they do not advertise anywhere else; their postings on the job boards are virtually all of the exposure they receive; so, the more postings, the better.) These honey-pot job advertisements usually have fantastically good descriptions along with fantastically high salary indications. The word \u0026quot;fantastically\u0026quot; here is used in its literal sense: belonging to the realm of fantasy.\nWhat is especially problematic with honey-pot job posts is that they will never land you a job interview when you need it; they might land you a job interview when it suits the recruiter.\nSeasoned recruiters know all the tricks in the book, so they may be able to make the honey-pot scam work in real time. Or at least try to. I once responded to a very interesting-looking advertisement about a job with a well-known and reputable employer; I spoke with the recruiter on the phone, we went through all the small talk, everything was fine, then she said that the employer was looking to hire someone within the next two weeks, and asked me for my availability. Of course, I had to tell her that I was not available on a two weeks' notice, to which she responded with \u0026quot;oh, that's a pity,\u0026quot; immediately followed by \u0026quot;but here are some other jobs that I have --\u0026quot; at which point I had to interrupt her and end the call. You see, no reputable employer would be looking to hire someone on a two weeks' notice, and also, no reputable employer would want to hire a candidate who claims to be available on a two weeks' notice. That was a fake, honey-pot job advertisement.\nOf course, there is nothing to prevent a recruiter who is into the habit of falsifying jobs, from falsifying CVs too. The amount of intelligence they apply in this process is about the same as when falsifying job advertisements, (extremely low,) so they are perfectly capable of introducing into your CV various discrepancies that will disqualify you in the eyes of the employer whereas your original unmodified CV might have been fine. Even if they do not make any glaring mistakes, the mere fact that they changed the original means that there is now a danger of misrepresentation; therefore, in the event that a recruiter arranges a job interview between you and an employer, the first thing you must do upon arriving at the interview is to hand the employer a copy of your unmodified CV and ask them to read it right there, in front of you, while you wait. (Also, a very fun thing to do: ask the employer to hand you a copy of your CV as the recruiter gave it to them; chances are, you will be amused. Also a fun thing to ponder about: you are probably legally entitled to demand that it be handed to you.)\nEmployers that deal with recruiters are not to be trusted. The fact that they choose to handle their recruitment this way gives an idea of how they view human resources and what kind of treatment you can expect if you end up being hired by them. Here is an incident that actually happened to me: I responded to a job advertisement placed by a recruiter; I spoke with her on the phone; after I passed her filters and what not, she revealed the identity of the employer and asked me if I was interested. I said that I was. A couple of days passed without hearing back, so while waiting I decided to directly e-mail the employer asking one very simple thing: to confirm to me that they were in fact working with that particular recruiter. Instead of responding to my request with a simple \u0026quot;yes\u0026quot; or \u0026quot;no\u0026quot; answer, the employer forwarded my e-mail to the recruiter. Then, the recruiter forwarded to me the forwarded e-mail to show me that it had ended up in her hands, and contacted me with an attitude like \u0026quot;what were you trying to accomplish with this?\u0026quot; So, I found myself in the very awkward position of having to appease her, while defend myself and maintaining that I did nothing wrong. Since money is god, at the end of this very unpleasant interaction she was still willing to move forward with my application, but I disappointed her by telling her that I was not interested anymore. You see, I would not want to work for a company that treats prospective candidates like that. Even if it was just a secretarial mistake, I would not want to work for a company that hires airheads as secretaries.\nRecruiters do not care about your best interest, nor about the employer's best interest, they only care about their own interest. You might say that this is how this world runs, everyone looks after their own interest, but here is the thing about recruiters: they quite often lack the basic intelligence nessessary to know that their own interest will only be fulfilled if the other two interests are fulfilled first.\nChoosing to interact with recruiters is like knowing very little about poker and choosing to try your luck in a game against those doing it for a living.\nWhat can you do?\nNever react to any job posting that utilizes any kind of secrecy. Never react to any job posting that is vaguely worded. Before reacting to a job posting, visit the employer's web site, go to the careers section, and confirm that the job actually exists. State on your CV that it is your intellectual property and that unauthorized modification and circulation are prohibited. Include a throw-away e-mail address on your CV. As soon as you find a job, throw away the address; next time you find yourself looking for a job, create a new throw-away e-mail address. Do not include your phone number in your CV; if someone wants to contact you they can e-mail you. Mandatory grumpy cat meme -- \u0026quot;Recruiters - I hate them\u0026quot; Cover image from Freepik\n","date":"2023-01-21T16:17:27.771Z","permalink":"https://blog.michael.gr/post/2023-01-21-on-recruiters/","title":"On Recruiters"},{"content":"\rStackoverflow and the whole Stackexchange network is good for asking very narrowly-scoped questions that can receive objective and preferably authoritative answers that cite documentation or definitions. Any kind of question which is subject of opinion, or liable to elicit debate, is off-topic there. This means that stackoverflow is only good for asking strictly technical questions, and there is an upper limit on how valuable this can be. Sure it can be very helpful when you are trying to solve a specific technical problem, but in the grand scheme of things, it is irrelevant; from a philosophical point of view, it is trivial.\nI have been looking for ways to discuss with other software engineers (preferably experts) issues that are related to software engineering but are in fact very much subject of opinion. These are the interesting questions. I do of course already have my own opinions, which tend to either deviate or be diametrically opposite from the prevailing industry trends, so it would be very useful to me to debate these issues with others to see what they have to say. Clearly, either I am wrong, or the entire industry is wrong; wouldn't it be nice if we could debate this and have it settled?\nTo this effect, I decided to give a few forums a try, to see if it is possible to have debates in any of them. As it turns out, there seem to be very few options available, and things are rather quiet in each one of them; most people seem to be doing nothing but consuming content generated by influencers instead of participating in discussions. In this post I am listing my findings so far. I will be amending it as I gather more information.\nthecodingforums.com:\nThreads: 472,788 Messages: 2,566,483 Members: 44,019 Disqualified because it only offers categories for specific languages, platforms, and technologies, it does not have a category for discussing software architecture or software engineering in general. These folks seem to be completely oblivious to the fact that for these things we already have stackoverflow. forums.codeguru.com:\nThreads: 531,210 Posts: 2,127,952 Members: 374,149 Seems to be backed by a relatively large concern (TechnologyAdvice / developer.com) Seems to be implemented using vBulletin, so its looks are severely outdated. Does not support dark mode, and has several contrast problems when dark mode is forced with the darkreader extension. Registration is quick, (though it would be shorter without annoying irrelevant questions,) and you are immediately registered. Their anti-spam mechanism is just a trivial arithmetic question spelled out in English. However, the first time I tried to post, (a post which included one link,) a message briefly appeared which said something about moderation, and it very annoyingly disappeared before I had a chance to read it. Then, I was back at the home page, while my text was nowhere to be found, as if I never wrote it. That's an awful first-time experience. Describes me as \u0026quot;Junior Member\u0026quot;, which I guess is okay, though \u0026quot;New Member\u0026quot; would be better. My first post: https://forums.codeguru.com/showthread.php?566024-Debate-reactive-programming-with-me Does not seem to allow editing a post. Result of my post: not a single answer was posted for several days. codeforum.org:\nThreads: 3,637 Messages: 18,342 Members: 3,939 It is unclear who backs it. Seems to be implemented using some XenForo®© platform, which looks good. Supports dark mode, and as a matter of fact the page header seems to have been designed with dark mode in mind. Registration is quick, but their anti-spam mechanism is the standard annoying image identification routine, and once you are done with it you are not immediately registered; instead, you have to wait for some human to approve your registration. (At least the message informing you about this stays put, so you can read it.) My account was approved within less than 24 hours. Describes me as a \u0026quot;New Coder\u0026quot;, which is insulting. My first post: https://codeforum.org/threads/debate-reactive-programming-with-me.5991/ Allows the editing of posts, but does not seem to indicate anywhere that the post has been edited. Its link insertion dialog is nonsensical, and does not work correctly. Links always appear as text instead of as web addresses, and the text seems to match the title of the linked page, but it is unclear how this works. It seems impossible to create a link which appears as an address and does not get replaced with some text. Result of my post: it was deleted within a few hours, and before anyone had the chance to respond. I received a message saying that they decided that they will not allow debates \u0026quot;as it often brings users who do not accept opposing views, along with their often rude and unwelcomed behaviour.\u0026quot; So, clearly, they do not understand that for agreeable discussions the world already has stackoverflow, so the world does not need them. dreamincode.net:\nDead since 2021. codeproject.com\nStatistics are unavailable, but it has existed since 1999, and it has always been fairly well known, so it is very big. Some improvised calculations seem to indicate that it has some 15 million members and 3,718,733 threads. It is unclear who it is backed by, but it smells very Microsoft. Their entire web application including the forums seems to be home-grown using aspx. It is very clunky and also buggy. Its looks are severely outdated, and some of its aesthetic choices are bad even for the early nineties. It does not support dark mode; when dark mode is forced using the darkreader extension, it is usable, but ugly. I registered a long time ago, so I do not remember how the experience was back then, but when I try to modify my account information today, like change my avatar, it does not work due to bugs. It has a \u0026quot;discussions\u0026quot; section, but the button for creating a new post is labelled \u0026quot;Ask a question\u0026quot;, so these guys are clearly confused. Luckily, once you start editing the post, there is a radio button for indicating whether it is a question or something else like general, news, suggestion, etc. The first time I tried to create a post, (which included one link,) it gave me a message saying that my post has been flagged as potential spam and is awaiting moderation, which is insulting and unnecessarily technical. The post was approved within a few hours. My first post: https://www.codeproject.com/Messages/5921375/Debate-reactive-programming-with-me Allows editing of posts, but when I added a single character to the title, the post went back into the moderation queue for a couple of hours. These folks appear to have a severe problem with spammers. Result of my post: several people posted answers, but each of them was a self-contained conclusive statement largely agreeing with me and therefore not constituting debate. groups.google.com/g/archforum\nLast post was in 2012. developerfusion.com/t/architecture/forum\nLast post was in 2012. quora.com\nDisqualified due to extremely annoying format: each question consists of a single headline and presented impersonally, so it could be generated by a bot; question-and-answer formula stifles debate; advertisements cannot be blocked by an ad-blocker; advertisements and irrelevant posts embedded in the flow of the page without sufficient visual clues to differentiate them from the relevant elements; and html specifically engineered to prevent the use of content filters to separate the wheat from the chaff. A fast-food style forum. reddit.com\nSimilar fast-food style forum as quora.com, also extremely annoying, due to a different set of equally serious reasons. (For example, their retarded, chaotic, non-hierarchical format is completely unsuitable for a website aiming to cover everything imaginable.) I had an extremely bad experience with them in the past, when I tried to post something which was suppressed without an explanation, so I do not intend to waste my time with it for a second time. Old comments\nCarl-Erik 2023-01-27 09:14:09 UTC\nP.S. Followed you here from your profile on SO after seeing a great answer given by you on inter-thread communication via events and message passing.\nmichael.gr 2023-01-27 19:09:04 UTC\nThanks, Carl-Erik! I did give dev.to a try some time ago, but I did not continue with it because the content seemed very junior. But now that you reminded me of it, I will give it another try.\nCarl-Erik 2023-01-27 09:13:18 UTC\nHi, Michael. I feel the pain :-) Finding such forums is generally not easy. For answerable questions the StackExchange network is a gem; StackOverflow for the technical details and the Software Engineering for more high-level stuff.\nBut they both eschew open-ended discussions, which is what I find that you are seeking. One surprising newcomer might be DEV. For a while, I thought this was a curated site where one found low-quality articles on \u0026quot;This is how you do X in React\u0026quot;, etc. It was only when I critiqued the quality of one such article that I found out that it is basically Medium for programmers: a self-publication platform with comments. So that answered the quality bit: it depends on the author.\nBut recently, I have found that more senior software developers write here as well and some of these resulting discussions have been really, really good! One such example is the discussion that spiralled out of this innocuous looking piece called \u0026quot;Whatever happened to components being just a visual thing\u0026quot;: https://dev.to/redbar0n/what-happened-to-components-being-just-a-visual-thing-22hc\nSuper interesting tidbits in a day where frontend architecture talks usually are super shallow. Turns out what I have been missing is discussion of \u0026quot;app-centric\u0026quot; architectures vs the nowadays more common \u0026quot;component-centric\u0026quot; architecture (what we are mostly doing). Loads of great stuff, even though some of the stuff went over my head.\nKeywords from the discussion that spanned tens of comments, some spanning thousands of words: Steve Krug (of \u0026quot;Don't make me think\u0026quot;), Martin Fowler (of everything software design related), Pete Hunt (of the original React team), Pete Heard, reactive programming, MVC, MVVM, Robert C. Martin's, Trygve Reenskaug, Smalltalk, Active Record, template languages vs JS, Niladic Components, ...\nThe discussion essentially touches everything one ever learned about software architecture and design mixed into a greater discussion on how frontends can, have and should be designed. Puh!\nIf you start a discussion (essentially write a piece) that people find interesting, eventually interested parties will find it and write back (and might follow you for more).\nmichael.gr 2024-01-30 06:20:56 UTC\nFor reference, the answer that Carl-Erik was referring to was probably this one: Stack Overflow: How should I unit test multithreaded code? https://stackoverflow.com/a/74574386/773113\n","date":"2023-01-17T20:03:20.474Z","permalink":"https://blog.michael.gr/post/2023-01-17-debating-with-other-software-engineers/","title":"Debating with other Software Engineers"},{"content":"\rI have been coming across the term collaborator in software literature, and I have been using it too in my own writings, but without having seen it defined. I tried searching for its definition, but could not find any. In UML the term collaboration is vaguely described, but not the term collaborator. After asking on Software Engineering Stack Exchange I was pointed to what is in almost all certainty the original definition, but it turns out that it is very old, and slightly problematic, so I thought I should provide a modern definition here, at the very least for use in my own writings.\nHere it goes:\n**A *collaborator* is a component invoked by another component to do a job.** (And since the context is software, these are, of course, software components.)\nOrigin of the term (Useful pre-reading: About these papers)\nCollaborators were introduced as a concept in a paper by Kent Beck and Ward Cunningham in 1989 (See http://c2.com/doc/oopsla89/paper.html) and defined as \u0026quot;objects which will send or be sent messages in the course of satisfying responsibilities\u0026quot;.\nProblems with the original definition The original paper was written in the context of Smalltalk, which relied on message-passing, but more generally, collaborators exchange invocations. The \u0026quot;in the course of satisfying responsibilities\u0026quot; part seems to convey no information, despite the fact that it comprises about half of the definition; it is just filler and it needs to go. The paper allowed a collaborator to be either \u0026quot;a service with little regard or even awareness of its client\u0026quot; or a \u0026quot;near-equal\u0026quot; in a \u0026quot;symmetric relation\u0026quot;; however, nowadays we tend to put emphasis on loose coupling, so collaborators are generally services: a component has knowledge of collaborators that it employs, but a collaborator has no specific knowledge of components that it is employed by. Components vs. Interfaces Depending on the scope of the discussion, the term collaborator may mean two things of different nature:\nWhen discussing a component and its collaborators, the term collaborator usually refers to an interface, because a component is supposed to be invoking interfaces, not actual implementations. When discussing a system and the components in it, the term collaborator refers to an actual component implementing one or more interfaces, because components is what systems consist of. When there is a need to differentiate between the two, the terms Collaborator Interface and Collaborator Component can be used.\nAvailability The term does not imply any particular mechanism for making a collaborator component available to a component that wants to use it. The collaborator might be hard-coded, might be supplied as a parameter to an interface method call, might be injected, etc. If the mechanism by which a collaborator is being made available is unclear, and if it matters, then it should be explicitly stated.\nHowever, in software architecture we are usually discussing collaborators that may appear in architectural diagrams, and these tend to be injectable. Collaborators that are hard-coded or supplied as parameters tend to be small-scale implementation details that generally do not appear in architectural diagrams.\nDelivery of invocations The term does not imply any particular mechanism for placing invocations; one collaborator might be invokable via programmatic interface method calls, while another might be invokable via message-passing, and yet another might be invokable via REST request-response pairs; if the mechanism of placing invocations is unclear, and if it matters, then it should be explicitly stated.\nHaving said that, I should add that in most cases it should not matter, because writing software using any invocation mechanism other than programmatic interface method calls is misguided; things like message-passing or REST requests and responses are:\na) Invocation delivery details, and\nb) System deployment and wiring concerns.\nTherefore, they should always be abstracted away, ideally in a completely automatic and transparent way, so that we never have to deal with them in any way whatsoever when writing code. Thus, when I speak of invocations between collaborators, and unless I explicitly state otherwise, I mean programmatic interface method calls, with the provision that some automatic and transparent conversion between such calls and some other invocation delivery mechanism might be taking place under the hood if necessary.\nSimilarities and differences from dependencies From an architectural point of view, where any components worth discussing are injectable, collaborator components are never dependencies, because no component depends on any particular implementation of another component. However, the interfaces of the collaborators are dependencies of the components that invoke them, because a component needs to import an interface in order to make invocations to it, otherwise it will not compile. (Assuming we are using a real programming language, meaning a strongly typed programming language.) Collaborator components that are available via hard-coding are of course dependencies, because when a component is hard-coded to make use of a certain component, it explicitly depends on that particular concrete implementation. Reference: softwareengineering.stackexchange.com - Definition of \u0026quot;collaborators\u0026quot; (of an object) in Software Design?\nCover image: \u0026quot;Collaboration\u0026quot; by michael.gr, using 'Gear' by Lluisa Iborra and 'hands making a circle' by Oleksandr Panasovskyi from the Noun Project.\n","date":"2023-01-16T12:58:35.537Z","permalink":"https://blog.michael.gr/post/2023-01-16-collaborator/","title":"Definition: Collaborator"},{"content":"\rAbstract: The practice of using Mock Objects in automated software testing is examined from a critical point of view and found to be highly problematic. Opinions of some well known industry speakers are cited. The supposed benefits of Mock Objects are shown to be either no real benefits, or achievable via alternative means.\n(Useful pre-reading: About these papers)\nIntroduction The automated software testing technique which is predominant in the industry today is Unit Testing. The goal of Unit Testing is to achieve defect localization, and to this effect it requires each component to be tested in strict isolation from its collaborators.\nTesting components in isolation from each other poses certain challenges:\nWhile being tested, the component-under-test makes invocations to collaborator interfaces; since the collaborator components are not present, some kind of substitute must be there to implement the collaborator interfaces and receive those invocations. For each invocation that the component-under-test makes to a collaborator, it expects to receive back some result; therefore, the substitute receiving the invocation must be capable of generating a result that matches the result that would be generated by the real collaborator. The technique which is predominant in the industry today for providing the component-under-test with substitutes of its collaborators is Mock Objects, or just mocks.\nHow do mocks work? Mocks are based on the premise that the real work done by collaborators in a production environment is irrelevant during testing, and all that the component-under-test really needs from them is the results that they return when invoked. A test exercises the component-under-test in a specific way, therefore the component-under-test is expected to invoke its collaborators in ways which are known in advance; thus, regardless of how the real collaborators would work, the mocks which replace them do not need to contain any functionality; all they need to do is to yield the same results that the real collaborators would have returned, which are also known in advance.\nTo this effect, each test dynamically creates and configures as many mocks as necessary to substitute each one of the collaborators of the component-under-test, with the help of some mocking framework. These frameworks are so popular that there exists a proliferation of them: JMock, EasyMock, Mockito, NMock, Moq, JustMock, and the list goes on.\nA mock object is configured to expose the same interface as the real collaborator that it substitutes, and to expect specific methods of this interface to be invoked, with specific argument values, sometimes even in a specific order of invocation. If anything goes wrong, such as an unexpected method being invoked, or a parameter having an unexpected value, the mock fails the test. A very common practice is to also fail the test if an expected method is not invoked.\nFor each one of the expected methods, the mock is configured to yield a prefabricated result which is intended to match the result that the real collaborator would have produced if it was being used, and if it was working exactly according to its specification.\nOr at least, that is the intention.\nDrawbacks of Mocks Complex and laborious In each test it is not enough to invoke the component-under-test to perform a computation and check the results; we also have to configure a mock for each one of the collaborators of the component, to anticipate every single call that the component will be making to them while performing the computation, and for each call to fabricate a result which matches the result that the real collaborator would have returned from that call. Luckily, mocking frameworks lessen the amount of code necessary to accomplish this, but no matter how terse the mocking code is, the fact still remains that it constitutes substantial additional functionality which represents considerable additional complexity. One of the well-known caveats of software testing is that a test failure does not necessarily indicate a defect in the production code; it always indicates a defect either in the production code or in the test itself, and the only way to know is to troubleshoot. Thus, the more code we put in tests, and the more complex this code is, the more time we end up wasting in chasing and fixing bugs in the tests themselves rather than in the code that they are meant to test. Over-specified By anticipating every single call that the component-under-test makes to its collaborators, we are claiming to have detailed knowledge of the inner workings of the component-under-test, and we are concerned not only with what it accomplishes, but also with every little detail about how it goes on about accomplishing it. Essentially, we are implementing all of our application logic twice: once with production code expressing the logic in imperative mode, and once more with testing code expressing the same logic in expectational mode. In both cases, we write copious amounts of code describing what should happen in excruciatingly meticulous detail. Note that over-specification might not even be a goal in and of itself in some cases, but with mocking it is unavoidable in all cases: Each request that the component-under-test sends to its collaborators could conceivably be ignored, but the component-under-test still needs to receive some meaningful result in response to that request, so as to continue functioning during the remainder of the test; unfortunately, the only way that mocks can fabricate individual responses is by anticipating individual requests, even if the intention of the test is not to verify whether the requests are being made. Presumptuous When using mocks we are claiming to not only have detailed knowledge of the calls that the component-under-test makes to its collaborators, but also detailed knowledge of the results that would be returned by the real collaborators in a production environment. Furthermore, the results returned by a collaborator depend on the state that the collaborator is in, which in turn depends on previous calls made to it, but a mock is by its nature incapable of emulating state, so when using mocks we are also claiming to have knowledge of the state transitions that the real collaborators undergo in a production environment, and of the effect that these state transitions have on the results that they return. Such exorbitant presumptuousness might be okay if we are building high-criticality software, where each collaborator is likely to have requirements and specification that are well-defined and unlikely to change; however, in all other software, which is regular, commercial, non-high-criticality software, things are a lot less strict: not only the requirements and specifications change all the time, but also, by established practice, both the requirements, and the specification, and even the documentation, tend to be the code itself, and the code changes every time a new commit is made to the source code repository. Thus, the only way to know exactly how a collaborator behaves tends to be to actually invoke it and see what it does, while the mechanism which ensures that it does what it is supposed to do is the tests of that collaborator itself, which are unrelated to the tests of components that invoke it. As a result of all this, the practice of mocking often places us in the all too familiar situation where our Unit Tests all pass with flying colors, but our Integration Tests miserably fail because the behavior of the real collaborators turns out to be different from what the mocks assumed it would be. Fragile By its nature, a mock object has no option but to fail the test if the interactions between the component under test and its collaborators deviate from what it expects. However, these interactions may legitimately change as software evolves. This may happen due to the application of a bug-fix, due to refactoring, or simply because as we write new code we invariably have to also modify existing code to interact with the new code that we are adding. Thus, when using mocks, every time we change the behavior of production code, we also have to fix tests to expect the new behavior. (Not only do we have to write all of our application logic twice, we also have to perform all of its maintenance twice.) The original promise of Automated Software Testing was to enable us to continuously evolve our software without fear of breaking it. The idea is that whenever you modify the production code, you can re-run the tests to ensure that everything still works. When using mocks this does not work, because every time you change the slightest thing in the production code, the tests break. As a result, many programmers are hesitant to make needed changes to production code because of all the changes in testing code that would be required. The understanding is growing within the software engineering community that mock objects actually hinder software development instead of facilitating it. Non-reusable Mocks exercise the implementation of a component rather than its interface. Thus, when using mocks, it is impossible to reuse the same testing code to validate multiple different components that implement the same public interface but employ different collaborators. For example: It is impossible to completely rewrite the component and reuse the old tests to make sure that the new implementation works exactly as the old one did. It is impossible to use a single test suite to exercise both a real component and its fake. Unenlightening Ideally, a set of tests for a certain component should act as sample code demonstrating usage scenarios of that component. A programmer who is not familiar with a particular component should be able to read the tests of that component and gain a fairly good idea of what it can do, what it cannot do, and how to write production code that interacts with it. Unfortunately, when using mocks, the tests are full of cryptic mock-related jabber, which obscures the actual usage of the component-under-test, and so the enlightening bits are lost in the noise. What do others say? I am certainly not the only one to voice dissatisfaction with mocks. People have been noticing that although automated software testing is intended to facilitate refactoring by ensuring that the code still works after each change that we make, the use of mocks often hinders refactoring, because the tests are so tied to the implementation that you cannot change anything without breaking the tests.\nIn the video Thoughtworks - TW Hangouts: Is TDD dead? (youtube, text digest) at 21':10'' Kent Beck states \u0026quot;My personal practice is I mock almost nothing.\u0026quot; In the same video, at 23':56'' Martin Fowler adds \u0026quot;I'm with Kent, I hardly ever use mocks.\u0026quot; In the Fragile Test section of his book xUnit Test Patterns: Refactoring Test Code (xunitpatterns.com) author Gerard Meszaros admits that \u0026quot;extensive use of Mock Objects causes overcoupled tests.\u0026quot; In his presentation TDD, where did it all go wrong? (InfoQ, YouTube) at 49':32'' Ian Cooper states \u0026quot;I argue quite heavily against mocks because they are overspecified.\u0026quot; Note that in an attempt to avoid sounding too blasphemous, these people refrain from suggesting that mocks should be abolished; however, it is evident that 3 out of 4 of them are strongly against mocks, and we do not need to read much between the lines to figure out that they would probably be calling for the complete abolition of mocks if they had a viable and universally applicable alternative to propose.\nSo, if not mocking, then what? Mocking has been such a great hit with the software industry because it achieves multiple different goals at once. Here is a list of the supposed benefits of mocking, and for each one of them an explanation of why it is not really a benefit, or how it can be achieved without mocking:\nMocking achieves defect localization by eliminating collaborators from the picture and allowing components to be tested in strict isolation from each other. Defect localization is useful, but it is not an absolute necessity, and it does not have to be done to absolute perfection as mocking aims to do; we can achieve more than good enough defect localization by testing each component in integration with its collaborators, simply by arranging the order in which tests are executed to ensure that by the time a component gets tested, all of its collaborators have already passed their tests. See Incremental Integration Testing. Mocking allows a component to be tested without the performance overhead of instantiating and invoking its real collaborators. The performance overhead of instantiating and invoking the real collaborators is not always prohibitive, or even noticeable, so in many cases it is perfectly fine to test a component in integration with its real collaborators. See Incremental Integration Testing. In the limited number of cases where the performance overhead is indeed prohibitive, it can be avoided with the use of Fakes instead of Mocks. See Testing with Fakes instead of Mocks. Mocking allows us to examine invocations being made by the component-under-test to its collaborators, to ensure that they are issued exactly as expected. In most cases, examining the invocations made by the component-under-test to its collaborators is in fact bad practice, because it constitutes white-box testing. The only reason why this is being widely practiced in the industry is because mocking does not work otherwise, so in this regard mocking contains a certain element of a self-serving paradigm. In those rare cases where examining the invocations is in fact necessary, it is still bad practice to do so programmatically, because it results in tests that are over-specified and fragile. What we can do instead is to record the interactions during each test run, visually compare the latest recording with that of the last known good run, and decide whether the differences match our expectations; if they do not match, then we must keep working on our code; but if they do match, then we are done without the need to go fixing any tests. See Audit Testing and Collaboration Monitoring. Mocking allows us to fabricate the results returned from a collaborator to the component-under-test, so as to guarantee that they are free from defects that could be caused by bugs in the implementation of the real collaborator. Fabricating the results that would have been returned by a real collaborator is in fact bad practice, because it will not magically make any bugs go away, (in this sense it can be likened to ostrich policy,) and because as I have already explained, it is highly presumptuous. The definitive authority on what results are returned by a certain collaborator is the real implementation of that collaborator, or a fake thereof, which in turn necessitates integration testing. See Incremental Integration Testing. Mocking allows us to verify the correctness of components that generate their output by means of forwarding results to collaborators rather than by returning results from invocations. Even in this case, Collaboration Monitoring can be used instead of mocking, to verify that the results are generated as expected without having to programmatically describe what the results should be and without having to go fixing tests each time we modify the component under test and deliberately change something about the results it generates. See Audit Testing and Collaboration Monitoring. Mocking allows us to start testing a component while one or more of its collaborators are not ready yet for integration because they are still in development, and no fakes of them are available either. This is true, but once the collaborators (or fakes thereof) become available, it is best to integrate them in the tests, and to unceremoniously throw away the mocks. See Incremental Integration Testing. Mocking allows us to develop a component without depending on factors that we have no control over, such as the time of delivery of collaborators, the quality of their implementation, and the quality of their testing. With the use of Mocks we can claim that our component is complete and fully tested, based on nothing but the specification of its collaborators, and we can claim that it should work fine in integration with its collaborators when they happen to be delivered, and if they happen to work according to spec. True, but this implies a very bureaucratic way of working, and utter lack of trust towards the developers of the collaborators; it is best if it never comes to that. We can still avoid the use of mocks by creating fakes of the collaborators ourselves. See Testing with Fakes instead of Mocks. To summarize, mocks can always be replaced with one or more of the following:\nFakes (see Testing with Fakes instead of Mocks) Incremental Integration Testing (see Incremental Integration Testing) Audit Testing (see Audit Testing) and Collaboration Monitoring (see Collaboration Monitoring) Conclusion As we have shown, the practice of using Mock Objects in automated software testing is laborious, over-specified, presumptuous, and leads to tests that are fragile and non-reusable, while each of the alleged benefits of using mocks is either not a real benefit, or can be realized by other means, which we have named.\nMandatory grumpy cat meme - \u0026quot;Mock objects - they are horrible\u0026quot; Cover image: \u0026quot;Mocking\u0026quot; by michael.gr, based on 'mock' by 'Iconbox' from the noun project.\n","date":"2023-01-14T14:13:37.79Z","permalink":"https://blog.michael.gr/post/2023-01-14-mocking/","title":"If you are using mock objects you are doing it wrong"},{"content":"\rAbstract An automated software testing technique is presented which solves the fragile test problem of white-box testing by allowing us to ensure that the component-under-test interacts with its collaborators according to our expectations without having to stipulate our expectations as test code, without having the tests fail each time our expectations change, and without having to go fixing test code each time this happens.\n(Useful pre-reading: About these papers)\nSummary In automated software testing it is sometimes necessary to ensure not only that given specific input, the component-under-test produces correct output, (Black-Box Testing,) but also that while doing so, it interacts with its collaborators in certain expected ways. (White-Box Testing.) The prevailing technique for achieving white-box testing (Mock Objects) requires copious amounts of additional code in the tests to describe the interaction that are expected to happen, and fails the tests if the actual interactions deviate from the expected ones.\nUnfortunately, the interactions often change due to various reasons, for example applying a bug fix, performing refactoring, or modifying existing code in order to accommodate the addition of new code intended to introduce new functionality; so, tests keep breaking all the time, (the Fragile Test problem,) requiring constant maintenance, which imposes a heavy burden on the Software Development process.\nCollaboration Monitoring is a technique for white-box testing where during a test run we record detailed information about the interactions between collaborators, we compare the recording against that of a previous test run, and we visually examine the differences to determine whether the changes observed in the interactions are as expected according to the changes that were made in the code. Thus, no code has to be written to describe in advance how collaborators are expected to interact, and no tests have to be fixed each time the expectations change.\nThe problem Most software testing as conventionally practiced all over the world today consists of two parts:\nResult Validation: ascertaining that given specific input, the component-under-test produces specific expected output. Collaboration Validation: ensuring that while performing a certain computation, the component-under-test interacts with its collaborators in specific expected ways. As I argue elsewhere, in the vast majority of cases, Collaboration Validation is ill-advised, because it constitutes white-box testing; however, there are some cases where it is necessary, for example:\nIn high-criticality software, which is all about safety, not only the requirements must be met, but also nothing must be left to chance. Thus, the cost of white-box testing is justified, and the goal is in fact to ensure that the component-under-test not only produces correct results, but also that while doing so, it interacts with its collaborators as expected. In reactive programming, the component-under-test does not produce output by returning results from function calls; instead, it produces output by forwarding results to collaborators. Thus, even if all we want to do is to ascertain the correctness of the component's output, we have to examine how it interacts with its collaborators, because that is the only way to observe its output. The prevalent mechanism by which the Software Industry achieves Collaboration Validation today is Mock Objects. As I argue elsewhere, (see If you are using mock objects you are doing it wrong) the use of mocks is generally ill-advised due to various reasons, but with respect to Collaboration Validation in specific, the problem with mocks is that their use is extremely laborious:\nWhen we write a test for a certain component, it is counter-productive to have to stipulate in code exactly how we expect it to interact with its collaborators. When we revise the implementation of a component, the component may now legitimately start interacting with its collaborators in a different way; when this happens, it is counter-productive to have the tests fail, and to have to go fix them so that they stop expecting the old interactions and start expecting the new interactions. The original promise of automated software testing was to allow us to modify code without the fear of breaking it, but with the use of mocks the slightest modification to the code causes the tests to fail, so the code always looks broken, and the tests always require fixing.\nThis is particularly problematic in light of the fact that there is nothing about the concept of Collaboration Validation which requires that the interactions between collaborators must be stipulated in advance, nor that the tests must fail each time the interactions change; all that is required is that we must be able to tell whether the interactions between collaborators are as expected or not. Thus, Collaboration Validation does not necessitate the use of mocks; it could conceivably be achieved by some entirely different means.\nThe Solution If we want to ensure that given specific input, a component produces expected results, we do of course have to write some test code to exercise the component as a black-box. If we also want to ensure that the component-under-test interacts with its collaborators in specific ways while it is being exercised, this would be white-box testing, so it would be best if it does not have to also be written in code. To achieve this without code, all we need is the ability to somehow capture the interactions so that we can visually examine them and decide whether they are in agreement with our expectations:\nIf they are not as expected, then we have to keep working on the production code and/or the black-box testing code. If they are as expected, then we are done: we can commit our code, and call it a day, without having to modify any white-box tests! The trick is to do so in a convenient, iterative, and fail-safe way, meaning that the following must hold true:\nWhen a change in the code causes a change in the interactions, there should be some kind of indication telling us that the interactions have now changed, and this indication should be so clear that we cannot possibly miss it. Each time we modify some code and run the tests, we want to be able to see what has changed in the interactions as a result of only those modifications, so that we do not have to pore through long lists of irrelevant interactions, and so that no information gets lost in the noise. To achieve this, I use a technique that I call Collaboration Monitoring.\nCollaboration Monitoring is based on another testing technique that I call Audit Testing, so it might be a good idea to read the related paper before proceeding: Audit Testing.\nLet us assume that we have a component that we want to test, which invokes interface T as part of its job. In order to test the component, we have to wire it with a collaborator that implements T. For this, we can use either the real collaborator that would be wired in the production environment, or a Fake thereof. Regardless of what we choose, we have a very simple picture which looks like this:\nNote that with this setup we can exercise the component-under-test as a black-box, but we cannot yet observe how it interacts with its collaborator.\nIn order to observe how the component-under-test interacts with its collaborator, we interject between the two of them a new component, called a Collaboration Monitor, which is a decorator of T. The purpose of this Collaboration Monitor is to record into a text file information about each function call that passes through it. The text file is called a Snoop File, and it is a special form of Audit File. (See Audit Testing.)\nThus, we now have the following picture:\nThe information that the Collaboration Monitor saves for each function call includes:\nThe name of the function. A serialization of the value of each parameter that was passed to the function. A serialization of the return value of the function. As per Audit Testing, the Snoop File is saved in the source code tree, right next to the source code file of the test that generated it, and gets committed into the Source Code Repository / Version Control System along with the source code. For example, if we have SuchAndSuchTest.java, then after running the tests for the first time we will find a SuchAndSuchTest.snoop file right next to it. We can examine this file to ensure that the component-under-test interacted with the collaborator exactly as expected.\nAs we continue developing our system, the modifications that we make to the code will sometimes have no effect on how collaborators interact with each other, and sometimes will cause the collaborators to start interacting differently. Thus, as we continue running our tests while developing our system, we will be observing the following:\nFor as long as the collaborations continue in exactly the same way, the contents of the Snoop Files remain unchanged, despite the fact that the files are re-generated on each test run. As soon as some collaborations change, the contents of some Snoop Files will change. As per Audit Testing, we can then leverage our Version Control System and our Integrated Development Environment to take care of the rest of the workflow, as follows:\nWhen we make a revision in the production code or in the testing code, and as a result of this revision the interactions between the component-under-test and its collaborators are now even slightly different, we will not fail to take notice because our Version Control System will show the corresponding Snoop File as modified and in need of committing. By asking our Integrated Development Environment to show us a \u0026quot;diff\u0026quot; between the current snoop file and the unmodified version, we can see precisely what has changed without having to pore through the entire snoop file. If the observed interactions are not exactly what we expected them to be according to the revisions we just made, we keep working on our revision. When we are confident that the differences in the interactions are exactly as expected according to the changes that we made to the code, we commit our revision, along with the Snoop Files. What about code review? As per Audit Testing, the reviewer is able to see both the changes in the code, and the corresponding changes in the Snoop Files, and vouch for them, or not, as the case might be.\nRequirements For Collaboration Monitoring to work, snoop files must be free from non-deterministic noise, and it is best if they are also free from deterministic noise. For more information about these types of noise and what you can do about them, see Audit Testing.\nAutomation When using languages like Java and C# which support reflection and intermediate code generation, we do not have to write Collaboration Monitors by hand; we can instead create a facility which will be automatically generating them for us on demand, at runtime. Such a facility can be very easily written with the help of Intertwine (see Intertwine.)\nUsing Intertwine, we can create a Collaboration Monitor for any interface T. Such a Collaboration Monitor works as follows:\nContains an Entwiner of T so that it can expose interface T without any hand-written code implementing interface T. The Entwiner delegates to an instance of AnyCall, which expresses each invocation in a general-purpose form. Contains an implementation of AnyCall which serializes all necessary information about the invocation into the Snoop File. Contains an untwiner of T, so that it can convert each invocation from AnyCall back to an instance of T, without any hand-written code for invoking interface T. Comparison of Workflows Here is a step-by-step comparison of the software development process when using mocks, and when using collaboration monitoring.\nWorkflow using Mock Objects:\nModify the production code and/or the black-box part of the tests. Run the tests. If the tests pass: Done. If the tests fail: Troubleshoot why this is happening. If either the production code or the black-box part of the tests is wrong: Go to step 1. If the white-box part of the tests is wrong: Modify the white-box part of the tests (the mocking code) to stop expecting the old interactions and start expecting the new interactions. Go to step 2. Workflow using Collaboration Monitoring:\nModify the production code and/or the tests. Run the tests. If the tests pass: If the interactions have remained unchanged: Done. If the interactions have changed: Visually inspect the changes. If the interactions agree with our expectations: Done. If the interactions differ from our expectations: Go to step 1. If the tests fail: Go to step 1. Conclusion Collaboration Monitoring is an adaptation of Audit Testing which allows the developer to write black-box tests which only exercise the public interface of the component-under-test, while remaining confident that the component interacts with its collaborators inside the black box according to their expectations, without having to write white-box testing code to stipulate the expectations, and without having to modify white-box testing code each time the expectations change.\nCover image: \u0026quot;Collaboration Monitoring\u0026quot; by michael.gr based on original work 'monitoring' by Arif Arisandi and 'Gears' by Free Fair \u0026amp; Healthy from the Noun Project.\n","date":"2023-01-06T13:03:22.83Z","permalink":"https://blog.michael.gr/post/2023-01-06-collaboration-monitoring/","title":"Collaboration Monitoring"},{"content":"\rAbstract What are fakes, what are their benefits, and why they are incontestably preferable over mocks. Also, how to create fakes if needed.\nIntroduction (Useful pre-reading: About these papers)\nWhen testing a component it is often necessary to refrain from connecting it with the real collaborators that it would be connected with in a production environment, and instead to connect it with special substitutes of its collaborators, also known as test doubles, which are more suitable for testing than the real ones.\nOne book that names and describes various kinds of test doubles is xUnit Test Patterns: Refactoring Test Code by Gerard Meszaros, (xunitpatterns.com) though I first read about them from martinfowler.com - TestDouble, which refers to Meszaros as the original source.\nThere exist a few different kinds of test doubles; by far the most commonly used kind is mocks, which, as I explain elsewhere, are a very bad idea and should be avoided like COVID-19. (See If you are using mock objects you are doing it wrong.) Another kind of test double, which does not suffer from the disadvantages of mocks, is Fake Objects, or simply fakes.\nWhat are fakes In just one word, a fake is an emulator.\nIn a bit more detail, a fake is a component that fully implements the interface of the real component that it substitutes, or at any rate the subset of that interface that we have a use for; it maintains state which is equivalent to the state of the real component, and based on this state it provides the full functionality of the real component, or a very convincing illusion thereof; to achieve this, it makes some compromises which either do not matter during testing, or are actually desirable during testing. Examples of such compromises are:\nHaving limited capacity.\nNot being scalable.\nNot being distributed.\nNot remembering any state from run to run.\nPretending to interact, but not actually interacting, with the physical world.\nGenerating fake data that would be unusable in a real production scenario.\nA fake can be more suitable for testing than the real thing in the following ways:\nBy performing much better than the real thing; for example:\nby keeping state in-memory instead of persisting to the file-system. by working locally instead of over the network. by pretending that the time has come for the next timer to fire instead of having to wait for that timer to fire. etc. By being deterministic; for example:\nby fabricating time-stamps instead of querying the system clock. by fabricating entities such as GUIDs, that would otherwise introduce randomness. by utilizing a single thread, or forcing threads to work in a lock-step fashion. etc. By avoiding undesirable interactions with the real world; for example:\nby pretending that a mass e-mail was sent instead of actually sending it. by pretending that an application-modal message box popped up, and that the user picked one of the available choices, instead of allowing an actual modal message box to block the running of tests on the developer's computer, or, worse yet, on some continuous build server in some data center out there. by pretending that an industrial robot made a certain movement, instead of causing an actual robot to move on a factory floor. etc. A few examples of frequently used fakes:\nVarious in-memory file-system libraries exist for various platforms, which can be used in place of the actual file-systems on those platforms. HSQLDB and H2 for Java, in-memory DbContext for DotNet EntityFramework, etc. are in-memory database systems that can be used in place of actual Relational Database Management Systems when testing. EmbeddedKafka can be used in place of an actual pair of Kafka + Zookeeper instances. A pseudo-random number generator seeded with a known constant value acts as a fake of the same pseudo-random number generator seeded with a practically random value such as the current time coordinate. To recap:\nFakes refrain from performing the actual operations that the real thing would perform, (e.g. when a file is created while using an in-memory file-system, no file gets created on disk,) but: They do go through all the motions, (e.g. attempting to create a file using an invalid filename will cause an error just as in a real file-system,) and: They do maintain the same state, (e.g. reading a file from an in-memory file-system will yield the exact same data that were previously written to it,) so: They do fully behave as if the operations were actually performed as far as the component-under-test is concerned, while: The compromises that they make in order to achieve this are inconsequential or even desirable when testing. (e.g. during a test run it does not matter if files created during a previous test run do not exist anymore, and as a matter of fact it is better if they do not exist.) Note that the terminology is a bit unfortunate: fakes are not nearly as fake as mocks.\nMocks are the ultimate in fakery because: They only respond to invocations that we prescribe in each test, based on our assumptions as to how the component-under-test would invoke the real thing. They maintain no state. They contain no functionality. They only return results that we prefabricate in each test, based on our assumptions as to how the real thing would respond. Fakes are not quite as fake as their name suggests, because: They expose the same interface as the real thing. They maintain an equivalent state as the real thing. They implement equivalent functionality as the real thing. They return the exact same results as the real thing. Benefits of fakes By using a fake instead of the real thing: We achieve better performance, so that our tests run quickly. We avoid non-determinism during testing, so our tests are repeatable. We avoid undesirable interactions with the real world, so nobody gets hurt. We have less code to write, since a fake is usually simpler to set up than the real thing. By using a fake instead of a mock: We save ourselves from having to write complicated mocking code in each test. We do not need to claim any knowledge as to how the component under test invokes its collaborators. We do not have to make assumptions about the state in which the collaborators are at any given moment. We do not have to make assumptions as to what results would be returned by each collaborator in each invocation. In both cases: We are incorporating in our tests a collaborator which has already been tested and can be reasonably assumed to be free of defects. Thus, in the event of a test failure we can be fairly confident that the defect lies in the component-under-test, (or in the test itself,) but not in one of the collaborators, so we achieve defect localization, which is the aim of Unit Testing. Creating fakes of our own components In some cases we may want to create a fake ourselves, as a substitute of one of our own components. Not only will this allow other components to start their testing as early as possible without the need for mocks, but also, a non-negligible part of the effort invested in the creation of the fake will be reusable in the creation of the real thing, while the process of creating the fake is likely to yield valuable lessons which can guide the creation of the real thing. Thus, any effort that goes into creating a fake of a certain component represents a much better investment than the effort of creating a multitude of throw-away mocks for various isolated operations on that component.\nOne might argue that keeping a fake side-by-side with the real thing may represent a considerable additional maintenance overhead, but in my experience the overhead of doing so is nowhere near the overhead of maintaining a proliferation of mocks for the real thing.\nEach time the implementation of the real thing changes without any change to its specification, (such as, for example, when applying some refactoring, or a bug fix,) some mocks must be modified, some must even be rewritten, while the fake usually does not have to be touched at all. When the specification of the real thing changes, the mocks have to be rewritten, and the fake has to be modified, but the beauty of the fake is that it is a self-contained module which implements a known abstraction, so it is easy to maintain, whereas every single snippet of mocking code is nothing but incidental complexity, and thus hard to maintain. In either case, a single change in the real thing will generally require a single corresponding change in the fake, whereas if we are using mocks we invariably have to go changing an arbitrary number of mocking snippets scattered throughout the tests. Furthermore, the use of fakes instead of mocks promotes the creation of black-box tests instead of white-box tests. Once we get into the habit of writing all of our tests as black-box tests, new possibilities open up which greatly ease the development of fakes: we can now write a test for a certain module, and then reuse that test in order to test its fake. The test can be reused because it is a black-box test, so it does not care how the module works internally, therefore it can test the real thing just as well as the fake of the real thing. Once we run the test on the real thing, we run the same test on the fake, and if both pass, then from that moment on we can continue using the fake in place of the real thing in all other tests.\nThe tests that exercise the real thing will be slow, but the real thing does not change very often, (if ever,) so here is where a testing tool like Testana shines: by using Testana we ensure that the tests exercising the real thing will only run in the rare event that the real thing actually changes. For more information about Testana, see Testana: A better way of running tests.\nCreating fakes of external components If we are using an external component for which no fake is available, we may wish to create a fake for it ourselves. First, we write a test suite which exercises the external component, not really looking for defects in it, but instead using its behavior as reference for writing the tests. Once we have built our test suite to specifically pass the behavior of the external component, we can reuse it against the fake, and if it also passes, then we have sufficient reasons to believe that the behavior of the fake matches the behavior of the external component. A similar technique is described by Martin Fowler in his Contract Test post.\nIn an ideal world where everyone would be practicing Black-Box testing, we should even be able to obtain from the creators of the external component the test suite that they have already built for testing their creation, and use it to test our fake.\nIn an even more ideal world, anyone who develops a component for others to use would be shipping it together with its fake, so that nobody needs to get dirty with its test suite.\nConclusion Despite widespread practices in the industry, fakes are the preferred alternative to mocks. Even though they might at first seem laborious, they are actually very convenient to use, and on the long run far less expensive than mocks.\nCover image: fake moustache by michael.gr based on art by Claire Jones from the Noun Project.\n","date":"2022-12-30T14:01:26.478Z","permalink":"https://blog.michael.gr/post/2022-10-testing-with-fakes/","title":"Testing with Fakes instead of Mocks"},{"content":"\rOver the decades, numerous software system architectures have emerged which require invocations across subsystems to be done via message-passing instead of programmatic interface method calls. Such architectures are so common that many programmers have come to regard message-passing as an end in and of itself, oblivious of the fact that it is nothing but a (poor) technical mechanism for accomplishing a certain architectural goal.\n(Useful pre-reading: About these papers)\nThe architectural goal is to be able to perform general-purpose operations on invocations, for example routing the invocations according to configuration, or queuing the invocations for delivery on a different thread. In order to be able to do things like that, the invocations must first be expressed in a general-purpose form.\nMessage-passing is simply the only general-purpose form that could be imagined by the pioneers who built the first asynchronous event-driven systems, or perhaps the only form that could readily be implemented using the programming languages available back then. However, in succeeding decades our thinking and our tools have advanced considerably, to the point where we now have much better ways of achieving things technically, so it might be worth taking a moment to re-examine the concept of message-passing.\nHere is a list of problems with message passing:\nCustom message classes have to be written and maintained, usually in large numbers, constituting nothing but incidental complexity which steers focus away from the class hierarchy of the problem domain, and towards the class hierarchy of the overelaborate inter-module communication apparatus. For each invocation, a message class needs to be instantiated, filled, and submitted, requiring several lines of custom-written code. This is also nothing but incidental complexity, diverting the attention of programmers from solving the problem at hand to negotiating the trifling technicalities of placing invocations. On the receiving end, each message must be examined in order to determine what kind of message it is, usually by means of an unwieldy switch statement, and its contents have to be extracted before any useful work can be done. Again, this is all incidental complexity, contributing nothing towards the end-goal of the software system; its sole purpose is to serve the message-passing bureaucracy. In order to reduce the total number of different message classes that need to be defined, programmers often reuse message classes for different purposes, filling different parts according to each purpose. This habit further increases the total amount of incidental complexity both at the sending and at the receiving end, and very often leads to bugs due to wrongly packed or wrongly unpacked messages. So, message-passing exists for the sole purpose of expressing invocations in a general-purpose form, but as it turns out, its use is laborious, and it tends to flood systems with debilitating amounts of incidental complexity.\nThe most natural, simple, convenient, straightforward, robust, maintainable, and self-documenting paradigm for making and receiving invocations, which facilitates problem-solving instead of hindering it, is programmatic interface method calls. Unfortunately, interfaces are not general-purpose in and of themselves, because each interface constitutes a unique type, requiring custom-written code to place calls to it and custom-written code to receive calls for it, thus preventing us from applying general-purpose operations on it. So, we have two separate and seemingly conflicting concerns:\nHow to express invocations in the most convenient way How to perform general-purpose operations on the invocations Ideally, separate concerns should not be mixed; the need to somehow apply general-purpose operations on invocations should not be dictating how we write code, and it should certainly not be making our job harder. Therein lies perhaps the biggest objection to message-passing: they are an onerous contrivance that programmers by themselves would never opt to use out of their own free will, but usually gets imposed on them by software architects who do not actually have to write code using this contrivance.\nMessage-passing has enjoyed widespread use mainly due to the historical inability of application programmers to think in terms of abstractions: it is always possible, even in systems that require message-passing, to write all application code so that it never deals with any messages at all, and uses nothing but application-specific programmatic interfaces instead; the trick is to create packaging and unpackaging adaptors, where on the sending side we are simply invoking a programmatic interface which is implemented by a packaging adaptor that creates messages, packs them, and sends them off to be enqueued, while on the receiving side a corresponding unpackaging adaptor is fed with messages from the queue, unpacks them, and calls the corresponding implementation of the interface. Alas, this arrangement requires a modicum of abstract thinking, and application programmers are generally not into that sort of thing.\nFurthermore, if we bother creating such packaging and unpackaging adaptors, the realization quickly starts to sink-in that all the message classes are irrelevant; there is no need to define a special message class containing a separate field for each parameter of each method, because the only code that would ever deal with such a class would be the corresponding pair of packaging and unpackaging adaptors; so, the adaptors might as well use a single universal message class which simply stores all parameters in an array of object, and voila, the entire menagerie of message classes becomes entirely unnecessary.\nThus, it becomes evident that what we are really after is not message-passing per se; it is some general-purpose form of expressing invocations, so that general-purpose operations can be performed on them, and some mechanism for converting back and forth between this general-purpose form and the natural form, which is programmatic interface method calls, so that we can write code naturally. Ideally, the conversion mechanism would be automatic and transparent, so that we do not even have to write those adaptors. Messages have only existed due to the historical absence of such an automatic and transparent mechanism.\nFortunately, with modern reflecting, intermediate-code-based, just-in-time compiled programming languages, today we have at our disposal all that is necessary to build such mechanisms. For more information see Intertwine.\nMandatory grumpy cat meme: \u0026quot;Message-Passing: it's awful\u0026quot; by michael.gr Cover image: Conceptual illustration of message-passing, by michael.gr, based on original art by Youmena and Made from the Noun Project.\n","date":"2022-12-18T08:59:25.089Z","permalink":"https://blog.michael.gr/post/2022-12-messages-and-message-passing/","title":"On messages and message-passing"},{"content":"\rLet me get one thing out of the way first: I am open to Test-Driven Development (TDD). I am not currently practicing it, because when I gave it a try some time ago it did not seem to resonate with me, but I do not have any objections to it in principle, so I might give it another try in the future. Let us just say that it was not love at first sight, but then again some relationships do take some time to warm up.\nHaving said that, let me now express a few reasons why I am skeptical of TDD. The previous paragraph should have established that I am not trashing TDD, I am just expressing some reservations.\n(Useful pre-reading: About these papers)\nReasons why I am skeptical of TDD:\nThe religion effect\nAdvocates of TDD say that it is the only proper way of developing software, and any other way is just plain wrong. If you do not like TDD, it is because you do not understand TDD. If you don't practice TDD, you are being unprofessional. In other words, TDD seems to have gained religion status. Its disciples are saying that their way is the one true virtuous way, and if you are not following it then you should repent and change your evil ways. As a civilization we have been there, tried that, it did not work well.\nThe Life of Brian effect\nIn Thoughtworks - TW Hangouts: Is TDD dead? (youtube) Kent Beck (the inventor of TDD) says starting at 13'19'' that there exist problems which are not amenable to solving via TDD, and when he comes across such problems, he does not use TDD. Martin Fowler adds that for him, the most important thing is to deliver properly tested code, and whether you write the tests before the production code or the other way around is secondary, and a matter of personal preference. Note how these statements constitute blasphemy among hard-core practitioners of TDD. It appears that the prophets do not endorse the creed as fervently as the adherents.\nThe stealth effect\nObviously you must never commit failing tests; this means that others should never be able to tell, by looking at the commit history, whether you wrote the tests first or the production code first. This in turn means that TDD is not observable, and therefore not enforceable, so perhaps we should not be too worried about something which is, by its nature, each developer's private business.\nThe envy effect\nEven though Test-Driven Development is a way of developing software which is based on a special way of doing testing, people seem to feel compelled to use the TDD buzzword with every opportunity, so the term is quite often used as nothing but a synonym for just plain testing. I come across articles which mention TDD in the title, but when you read the text you discover that absolutely nothing in there applies to writing the tests before the production code; therefore, the article was not about TDD, it was about testing at large. This in turn is an indication that TDD is being mentioned more often than it is actually being practiced.\nAre these observations damning about TDD? No; I am just saying.\nP.S. (2025-08-08)\nSo, I just discovered a debate between John Ousterhout and Robert Martin (Uncle Bob) about differences between John's book \u0026quot;A Philosophy of Software Design\u0026quot; and Bob's book \u0026quot;Clean Code\u0026quot;. The two men decided to hold the discussion mostly in the form of writing, and they posted it as a markdown document on GitHub.\nThe debate is somewhat lengthy, and it is up to you to decide whether it is worth reading in its entirety, but here is a direct link into the TDD section of the debate:\nhttps://github.com/johnousterhout/aposd-vs-clean-code?tab=readme-ov-file#test-driven-development\n(This is, by the way, a very interesting format for holding a debate.)\nIt is worth noting that there is no overlap between that debate and this post of mine about TDD; the two men did not touch upon the issues I go over in my post, and my post does not touch upon the issues that were debated by the two men.\nCover image: Conceptual illustration of Test-Driven Development, by michael.gr\n","date":"2022-12-15T17:16:45.672Z","permalink":"https://blog.michael.gr/post/2022-12-on-test-driven-development-tdd/","title":"On Test-Driven Development (TDD)"},{"content":"\rAbstract A mechanism is described for automatically converting method invocations of any programmatic interface into a single-method normal form and converting back to invocations of the original interface, so that general-purpose operations can be performed on the normal form without explicit knowledge of the interface being invoked. Implementations are provided for C# and for Java.\n(Useful pre-reading: About these papers)\nThe Problem When creating software systems of nontrivial complexity we often need to be able to apply certain operations on the invocations that are being made between certain components. Examples of such operations are:\nLogging: Recording information about each invocation being made. Multicasting: Delivering a single invocation to multiple recipients. Remoting: Placing invocations across machine boundaries. Desynchronization: Queuing invocations for later execution, possibly on a different thread. Synchronization: Obtaining and holding a lock for the duration of the invocation. Transformation: Converting between invocation formats, e.g. method calls to REST and back. Ordinarily, the components doing the invocations are application-specific, and the interfaces between them are also application-specific, but the operators that we want to interject between them are general-purpose, so they need to remain agnostic of the application-specific details of the invocations, in a way analogous to how a general-purpose sorting algorithm is agnostic of the application-specific format of the data it sorts.\nTherefore, we need some way of expressing application-specific invocations in a general-purpose form.\nPrior Art Messages and message-passing: The mechanism historically used for expressing invocations in a general-purpose form is message-passing. Unfortunately, its use is laborious, and it floods systems with debilitating amounts of incidental complexity. For details, see On messages and message-passing.\nParameterless lambdas: Application-specific method calls can be wrapped inside parameterless lambdas, and since all parameterless lambdas look the same, they can be handled by general-purpose code which may for example add them to a queue, and later dequeue and invoke them. Unfortunately:\nThe wrapping of each application-specific method call inside a parameterless lambda must happen at each call site, which is cumbersome and reveals details about the underlying invocation delivery mechanism. The evaluation of the parameters that are passed to the application-specific method happens at the moment that the lambda makes the call, not at the moment that the lambda is constructed, which can lead to insidious bugs even if the evaluations have no side-effects. (And woe to you on earth and sea if they do have side-effects.) The parameterless lambda completely hides the values of the parameters that are being passed to the application-specific method, as well as the identity of the method being invoked. Thus, parameterless lambdas cannot be used in scenarios that require information about each call being made. Dynamic Proxies: Both in Java and in C# there exist mechanisms that can be used to convert application-specific invocations to a general-purpose form, but not the other way around. These are java.lang.reflect.Proxy for Java, and various libraries like Castle's and LinFu for C#. The reverse operation can be achieved using reflection, but this involves a round-trip to native-land, which incurs a heavy performance penalty. Furthermore, these mechanisms suffer from additional issues, such as messing with exceptions, doing more work than necessary, etc.\nThe Solution In order to be able to perform general-purpose operations on application-specific invocations we need a mechanism for converting application-specific invocations into a general-purpose form and back, so that the operators can act upon the general-purpose form. What follows is a description of such a mechanism, which I call Intertwine.\nIntertwine introduces a general-purpose form for expressing invocations, which is called the normal form of invocations, and is represented by a single method of the following signature:\nObject AnyCall( MethodKey key, Object[] arguments ); In C#, AnyCall would be a delegate. In Java, AnyCall would be a single-method interface, otherwise known as a functional interface. This method signature represents the fact that every conceivable interface method call can be fully described in terms of:\nA return value, of the common denominator type Object. A unique key which identifies which method of the interface is being invoked. An array containing arguments, of the common denominator type Object. Please note that the method identifier is MethodKey in the Java implementation, but int selector in the C# implementation. This is because the Java implementation was made a considerable time after the C# implementation, and is therefore a bit more advanced.\nThe MethodKey used in the Java implementation allows the caller and the callee to unambiguously identify methods even in situations where binary compatibility between the caller and the callee is not guaranteed, and therefore an integer method index does not necessarily refer to the same method on both the caller and the callee.\nThe Java implementation of intertwine provides efficient means of converting back and forth between a MethodKey and any of the following:\nThe reflection \u0026quot;Method\u0026quot; object of the method. (This is java.lang.reflect.Method in Java, or System.Reflection.MethodInfo in C#.) The string representation of the prototype of the method. The zero-based method index of the method. The sample code that follows was written for C#, so it uses an int selector instead of MethodKey key.\nNote:\nFor methods of void return type the value returned by AnyCall is unspecified. (It will in all likelihood be null, but nobody should rely on this.) Value types (primitives) are boxed and unboxed as necessary. Certain features such as the ref and out parameters in C#, receive special handling. Other features such as properties, indexers, virtual events, etc. are nothing but syntactic sugar which is implemented using regular method calls under the hood, so they require no special handling. So, the problem can now be restated as follows:\nHow to convert any interface method invocation to an invocation of an AnyCall method, and how to convert back from an invocation of an AnyCall method to an invocation of the original interface method.\nFor this, Intertwine introduces two new concepts: Entwiners and Untwiners.\nAn Entwiner of interface T is a class which exposes (implements) interface T and delegates to an instance of AnyCall. It can also be thought of as a normalizer or generalizer or multiplexer. An Untwiner of interface T is a class which exposes an AnyCall method and delegates to an instance of T. It can also be thought of as a denormalizer or specializer or demultiplexer. More specifically:\nThe entwiner of T does the following: Accepts an instance of Anycall as a constructor parameter and stores it in a final/readonly field. Implements each method of T as follows: Packs the parameters that were passed to the method into an array of Object, performing any boxing necessary. Invokes anyCall passing it a key that uniquely identifies the method, and the array of parameters. Returns, possibly after unboxing, whatever was returned by the invocation of anyCall. The untwiner of T performs the opposite and complementary operation of the entwiner, namely: Accepts an instance of T as a constructor parameter and stores it in a final/readonly field. Implements the anycall method of the Anycall interface as follows: It uses the supplied MethodKey to determine which method of T is being invoked, and for each method it does the following: Unpacks the parameters from the array of Object, performing any unboxing necessary. Invokes the method of T, passing it the unpacked parameters. Returns, possibly after boxing, whatever was returned by the method, or null if the method was of void return type. A hand-crafted implementation Before we look at the automatic creation of entwiners and untwiners, let us take a look at an example of how we would implement an entwiner and untwiner for a certain interface if we were to do it by hand.\nLet us consider the following interface:\n1 2 3 4 5 interface IFooable { void Moo( int i ); void Boo( string s, bool b ); } And let us consider the following class implementing that interface:\n1 2 3 4 5 class FooImplementation: IFooable { void IFooable.Moo( int i ) { Console.WriteLine( \u0026#34;i: \u0026#34; + i ); } void IFooable.Boo( string s, bool b ) { Console.WriteLine( \u0026#34;s: \u0026#34; + s + \u0026#34;, b: \u0026#34; + b ); } } And then let us consider the following method which invokes the interface:\n1 2 3 4 5 void InvokeFoo( IFooable fooable ) { fooable.Moo( 42 ); fooable.Boo( \u0026#34;fubar!\u0026#34;, true ); } The InvokeFoo method can be directly hooked up to an instance of the implementing class in a completely conventional way as follows:\n1 2 3 4 5 void Run1() { IFooable fooable = new FooImplementation(); InvokeFoo( fooable ); } Now, an entwiner for our IFooable interface could be hand-crafted as follows:\n1 2 3 4 5 6 7 class EntwinerForFooable: IFooable { private readonly AnyCall AnyCall; Constructor( AnyCall anycall ) { AnyCall = anycall; } void IFooable.Moo( int i ) { AnyCall( 0, new object[]{ i } ); } void IFooable.Boo( string s, bool b ) { AnyCall( 1, new object[]{ s, b } ); } } Whereas an untwiner for IFooable could be hand-crafted as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class UntwinerForFooable { public readonly IFooable Target; public Constructor( IFooable target ) { Target = target; } public object AnyCall( int selector, object[] args ) { switch( selector ) { case 0: Target.Moo( (int)args[0] ); break; case 1: Target.Boo( (string)args[0], (bool)args[1] ); break; default: throw new System.InvalidOperationException(); } return null; } } With the above classes, we can now write the following piece of awesomeness:\n1 2 3 4 5 6 7 void Run2() { IFooable fooable = new FooImplementation(); var untwiner = new UntwinerForFooable( fooable ); var entwiner = new EntwinerForFooable( untwiner.AnyCall ); InvokeFoo( entwiner ); } Note that Run2() has exactly the same end-result as Run1(), but there is a big difference in what goes on under the hood: all outbound interface method calls from the InvokeFoo function are now arriving at the entwiner, which converts them to AnyCall invocations, which are then forwarded to the untwiner, which converts them back to IFooable calls, which are then forwarded to our FooImplementation object. This means that if we wanted to, we could interject a chain of objects between the entwiner and the untwiner, each one of these objects implementing an AnyCall delegate and invoking another AnyCall delegate, thus enabling us to perform any conceivable operation upon those invocations without having any built-in knowledge of the IFooable interface.\nAs the complexity of the interface increases, and as additional subtleties come into the picture, such as parameters passed with ref or out, coding entwiners and untwiners by hand can become very tedious and error-prone, so, obviously, we would like to have it automated.\nAutomating it with reflection It is possible to write a general-purpose untwiner that does its job using reflection, but reflection is slow, so the result is going to suffer performance-wise. For the sake of completeness, here is a possible implementation for a general-purpose reflecting untwiner using reflection:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class ReflectingUntwiner //WARNING: SLOW AS MOLASSES { private readonly object Target; private readonly System.Reflection.MethodInfo[] Methodinfos; public Constructor( Type twinee, object target ) { Target = target; Methodinfos = twinee.GetMethods( BindingFlags.Public | BindingFlags.NonPublic | BindingFlags.Instance ); } public object AnyCall( int selector, object[] arguments ) { return Methodinfos[selector].Invoke( Target, arguments ); } } Note that untwiner creation could be optimized by caching the MethodInfos of frequently used types, but that's not the problem; the real bottleneck is the MethodInfo.Invoke() call. If you put a breakpoint on the target and examine the stack, you will see that between the AnyCall frame and the target frame there will be a managed-to-native transition and a native-to-managed transition, which is something to be avoided at all costs.\nAlso note: it is impossible to write a reflecting entwiner.\nAutomating it with Intertwine The Intertwine library will automatically generate for us a pair of optimally-performing entwiner and untwiner classes for any interface. These classes are generated at runtime, so no extra build step is needed. To accomplish this, the C# implementation of Intertwine generates MSIL and creates assemblies from it; the Java Implementation generates bytecode and creates classes from it.\nThe following method of the Intertwine.Factory class creates an entwiner:\npublic static T NewEntwiner\u0026lt;T\u0026gt;( AnyCall anycall ); For T we give the type of our interface, and for anycall we give a delegate of ours that will be receiving calls. This method returns a reference to an implementation of our interface, provided by an Entwiner-derived class that has been dynamically generated specifically for our interface, and instantiated to work with the given AnyCall instance. For every call received through a method of our interface, this special entwiner will be marshalling the arguments and forwarding the call to our AnyCall delegate.\nThe following method of the Intertwine.Factory class creates an untwiner:\npublic static AnyCall NewUntwiner\u0026lt;T\u0026gt;( T target ); For target we give an implementation of our interface, and what we get is a reference to an AnyCall delegate implemented by an Untwiner-derived class that was dynamically generated specifically for our interface, and instantiated to work with the given target instance. For every call received through the AnyCall delegate, this special untwiner will be unmarshalling the arguments and forwarding the call to the appropriate method of our target interface.\nSo, with the dynamically generated entwiners and untwiners we can now do the following epicness:\n1 2 3 4 5 6 7 void Run3() { IFooable fooable = new FooImplementation(); AnyCall untwiner = Intertwine.Factory.NewUntwiner\u0026lt;IFooable\u0026gt;( fooable ); IFooable entwiner = Intertwine.Factory.NewEntwiner\u0026lt;IFooable\u0026gt;( untwiner ); InvokeFoo( entwiner ); } The actual implementation of Intertwine.Factory is pretty straightforward, so there is not much to talk about. As one might expect, the generated types are cached. A static factory method is generated with each generated type, for instantiating the type, so as to avoid having to call Activator.CreateInstance(), because that method uses reflection. The static factory method is invoked using Delegate.Invoke(), which does not use reflection. You will find the code-generating code choke-full of comments, explaining exactly what each emitted opcode does.\nIntertwine for C#: https://github.com/mikenakis/IntertwineCSharp\nIntertwine for Java: https://github.com/mikenakis/Public/tree/master/intertwine\nAppendix: An example: Interface multicasts (events) in C# If you are still with me you may be thinking that it is about time for a demonstration. What follows is not just an example, but actually a complete and useful application of intertwine which you may be able to start utilizing in your projects right away.\nThe C# language has built-in support for multicasts (events) but only delegates can be used as event observers. There are many cases, however, where interfaces would be more suitable. Java does not even have built-in support for multicasts, so programmers generally have to write their own, using single-method (functional) interfaces. In either language, if you want to achieve multicasting on multi-method interfaces, you have to rewrite the multicasting code for every single method of every single interface.\nConsider the following interface:\n1 2 3 4 5 6 interface ITableNotification { void RowInserted( Fields fields ); void RowDeleted( Key key ); void RowUpdated( Key key, Fields fields ); } And consider the following hypothetical (not actually possible) way of using it:\n1 2 3 event ITableNotification tableNotificationEvent; tableNotificationEvent += my_observer; tableNotificationEvent.RowUpdated( key, fields ); The above does not work because events in C# work only with delegates, not with interfaces. However, with Intertwine, the next best thing is actually possible:\n1 2 3 var tableNotificationEventManager = new InterfaceEventManager\u0026lt;ITableNotifcation\u0026gt;(); tableNotificationEventManager.Source.RegisterObserver( my_observer ); tableNotificationEventManager.Trigger.RowUpdated( key, fields ); This approach is self-explanatory, and the amount of code you have to write in order to use it is optimal; you do not need to deal with anything more than what is necessary, and if you ever add a notification, it will be a new interface method, so all existing implementations of that interface will automatically be flagged by the compiler as incomplete. With the help of Intertwine, this event manager is implemented in just 150 lines of code, including extensive comments.\nEnd-notes Back in 2011 I posted a question on stackoverflow.com, titled Multiplexing interface method calls into a single delegate and demultiplexing asking if anyone knows of anything like Intertwine, but nobody did, so I built it myself.\nThis post supersedes the original post from 2011: Intertwine: Normalizing Interface Invocations\nCover image: The Intertwine Logo, by michael.gr\n","date":"2022-12-11T16:18:00Z","permalink":"https://blog.michael.gr/post/2022-12-intertwine/","title":"Intertwine"},{"content":"\rThe most awesome Stack Overflow question ever posted was later deemed unsuitable for the site, so it was not just closed; it was deleted. It does not exist anymore. The title of the question was: \u0026quot;New programming jargon you coined?\u0026quot; and as you might imagine, it received hundreds of answers. Most of the answers would make you laugh; some would make you laugh hard; some would have you in stitches.\nJeff Atwood saved the top 30 or so of those answers in his blog: Coding Horror - New Programming Jargon, though they are not even the funniest ones. As for the rest of the answers, it seems like they are lost forever.\nOr are they?\nWell, actually, thanks to archive.org, humanity will continue to have access to this treasure of mirth for all eternity. Here is the link: archive.org - stackoverflow.com - New programming jargon you coined?\nTip: First read the question, then go to the last answer and start reading the answers back-to-front after skipping those with scores of -1, 0, and 1. Make sure you have an hour to spare.\nHere is some programming jargon which I have coined:\nMagical Incantation Any statement, argument, command, formulation, or construct that a) is necessary to get something to work as intended, b) one could have never guessed, and c) absolutely no hint for it was provided. Also, any construct that one could never infer what it accomplishes, either after a quick look, or after in-depth study and extensive reasoning.\nPEARL A keyword that I add to source code comments that explain weird, unexpected, usually lame, sometimes incredibly lame behavior of other people's software that my software has to account for and work around.\nBlasphemy Any choice which, although technically valid, nonetheless feels wrong, such as a buffer size which is not a power of two, a raster image when a vector image would have served equally well, a for-loop index variable which is called a instead of i, etc.\nHail-Mary Initialization The cargo-cult programming practice of pre-initializing a local variable with some meaningless value which is later overwritten with some other value, in all execution paths, before it is ever read. See Hail-Mary Initialization.\nUndocumented adverb: documented in a way that does not really explain anything. For example: // A value of 'null' is undocumented to signify that the file is \u0026quot;not directly accessible\u0026quot;, whatever they mean by that. The word is useful for attaching to a comment to succinctly indicate that the comment reflects the official documentation, but the official documentation is nonesense. Often the result of documentation written with the intent of not being wrong rather than the intent of explaining anything. Often used together with PEARL.\nUndocumentedly adverb: attested despite lack of documentation. For example: zork.close(); // undocumentedly also flushes the zork before closing it. The word is useful for attaching to a comment to succinctly indicate that the described behavior was not found in documentation, and was instead divined through guesswork and trial-and-error. Often used together with PEARL.\nAnd here is some jargon which I have picked up and doing my best to increase its popularity:\nGet run over by a crocodile Refers to the possibility that the programmer who has written all of the software that a company depends on might one day quit working for that company or otherwise become incapable of continuing with their duties. Combination of \u0026quot;get run over by a bus\u0026quot; and \u0026quot;get eaten by a crocodile\u0026quot;, similar to \u0026quot;rocket surgery\u0026quot;.\nSearch and destroy Refers to a botched \u0026quot;search and replace\u0026quot; operation that seriously fouls up the source code. Common in scripting languages, where lack of types means that refactoring is practically impossible and has to be faked by means of search-and-replace, and where lack of a compilation step means that the fouled up code cannot be discovered unless an attempt is made to execute that code.\nWTFPM What-The-Fucks-Per-Minute -- a measure of how smoothly (or not) code review is progressing.\nCargo Cult Programming Doing something in a certain way not because there is any actual benefit in that way, but because some folks proclaim that way to be \u0026quot;common knowledge\u0026quot;, or a \u0026quot;pattern\u0026quot;, or even \u0026quot;best practice\u0026quot;, based on reasons that are either dubious, or severely outdated, or outright fictitious. For an example, see Hail-Mary Initialization.\n","date":"2022-12-05T10:48:33.532Z","permalink":"https://blog.michael.gr/post/2022-12-05-jargon/","title":"Jargon"},{"content":" This is only of interest to people who are into Stack Overflow.\nHave you ever wondered why Stack Overflow is exactly the way it is? Here are some insights.\n(Note: this presentation violates one of the cardinal rules of presentations, which is to avoid long texts, so here is a tip: ignore every screen that contains a long text, just listen to what Jeff Atwood is saying.)\nNote: as I look at him, I can't help but think that all he is missing is the upward curved tie, i.e. he has to be the real-life person after whom Dilbert was fashioned.\n","date":"2022-12-05T09:55:21.628Z","permalink":"https://blog.michael.gr/post/2022-12-05-jeff-atwood-building-social-software/","title":"Jeff Atwood: Building Social Software for the Antisocial"},{"content":"\rThe latest version of IntelliJ IDEA supports a new option for excluding certain methods from code coverage by annotation, and it is the result of a feature request posted by me!\nNote that the wording in the screenshot is a bit goofy, presumably because they have non-programmers in charge of the \u0026quot;What's New in IntelliJ IDEA\u0026quot; page, or perhaps non-java-programmers. The new option does not allow us to control which annotations will be excluded, it allows us to control which annotations will cause code to be excluded. Actually, the news is not even the fact that there is now an option that allows us to control those annotations; the news is that the IntelliJ IDEA code-coverage analyzer can now exclude code by annotation, and by the way, it even allows us to specify which annotations will achieve this.\nIn my original feature request I had asked for something a bit more complex and more useful that what they ended up delivering, but this is still a good step in the right direction. The feature request was IDEA-292401 \u0026quot;Exclude specific methods from coverage\u0026quot; and can be found at:\nhttps://youtrack.jetbrains.com/issue/IDEA-292401/Exclude-specific-methods-from-coverage\nCover image: Screenshot from \u0026quot;What's New in IntelliJ IDEA 2022.3\u0026quot;\n","date":"2022-12-03T15:50:58.002Z","permalink":"https://blog.michael.gr/post/2022-12-intellij-idea-can-now-exclude-methods/","title":"IntelliJ IDEA can now exclude methods from code coverage"},{"content":"\rInstructions for future reference.\nEnable \u0026quot;Developer Mode\u0026quot;.\nSet up user environment variables\nDIRCMD = /OGNE TEMP = %UserProfile%\\AppData\\Local\\Temp TMP = %UserProfile%\\AppData\\Local\\Temp TMPDIR = %UserProfile%\\AppData\\Local\\Temp Kill \u0026quot;RegisteredOrganization\u0026quot;\nLocate the following registry keys and make sure they are empty: HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion\\RegisteredOrganization HKLM\\Software\\Wow6432Node\\Microsoft\\Windows NT\\CurrentVersion\\RegisteredOrganization (Win11) Kill \u0026quot;let's finish setting up your device\u0026quot;.\nGo to \u0026quot;Settings\u0026quot; -\u0026gt; \u0026quot;System\u0026quot; -\u0026gt; \u0026quot;Notifications\u0026quot; -\u0026gt; \u0026quot;Additional Settings\u0026quot; \u0026quot;Show the Windows welcome experience after updates and when signed in to show what's new and suggested\u0026quot; -\u0026gt; DISABLE \u0026quot;Suggest ways to get the most out of Windows and finish setting up this device\u0026quot; -\u0026gt; DISABLE \u0026quot;Get tips and suggestions when using Windows\u0026quot; -\u0026gt; DISABLE Disable \u0026quot;Windows Error Reporting\u0026quot;\nOpen \u0026quot;Edit group policy\u0026quot; (gpedit.msc) Go to Local computer Policy -\u0026gt; Computer Configuration -\u0026gt; Administrative Templates -\u0026gt; Windows Components -\u0026gt; Windows Error Reporting Set \u0026quot;Disable Windows Error Reporting\u0026quot; to \u0026quot;Enabled\u0026quot;. Taskbar:\nUnpin everything. (Microsoft edge, Windows explorer, Microsoft Store, \u0026quot;Mail\u0026quot;.) \u0026quot;Toolbars\u0026quot; -\u0026gt; \u0026quot;HP Support Assistant Quick Access\u0026quot; -\u0026gt; DISABLE \u0026quot;Settings\u0026quot; -\u0026gt; \u0026quot;Taskbar\u0026quot; (Win10) \u0026quot;Select which icons appear on the taskbar\u0026quot; \u0026quot;Always show all icons in the notification area\u0026quot; -\u0026gt; ENABLE (Win11) \u0026quot;Taskbar items:\u0026quot; \u0026quot;Widgets:\u0026quot; DISABLE \u0026quot;Chat:\u0026quot; DISABLE (Win11) \u0026quot;Taskbar behaviors:\u0026quot; \u0026quot;Taskbar alignment:\u0026quot; SELECT \u0026quot;Left\u0026quot; (Win11) Open the \u0026quot;Run Prompt\u0026quot; (with Win+R) Execute this nonsense: explorer shell:::{05d7b0f4-2121-4eff-bf6b-ed3f69b894d9} \u0026quot;Always show all icons and notifications on the taskbar\u0026quot; -\u0026gt; ENABLE \u0026quot;Replace Command Prompt with Windows PowerShell in the menu when I right-click the start button or press Windows logo key + X\u0026quot; -\u0026gt; DISABLE \u0026quot;Show taskbar buttons on\u0026quot; -\u0026gt; SELECT \u0026quot;Main taskbar and taskbar where window is open\u0026quot; \u0026quot;Show news and interests on the taskbar\u0026quot; -\u0026gt; DISABLE Uninstall crapware:\nClipchamp - Video Editor (Windows 11) Cortana (Windows 10) Feedback Hub Mail and Calendar Maps Microsoft News (Windows 11) Microsoft OneDrive Microsoft To Do (Windows 11) Movies \u0026amp; TV Office People (Windows 10) Power Automate (Windows 11) Quick Assist (Windows 11) Solitaire Collection / Microsoft Solitaire Collection Tips (Windows 11) Weather (On Windows 10, certain crapware like \u0026quot;Game Bar\u0026quot;, \u0026quot;Mixed Reality Portal\u0026quot;, \u0026quot;Phone Link\u0026quot;, \u0026quot;Tips\u0026quot; etc. cannot be uninstalled. FUCK YOU MICROSOFT.)\n(On Windows 11, certain crapware like \u0026quot;Microsoft Store\u0026quot;, \u0026quot;Microsoft Edge\u0026quot;, \u0026quot;Cortana\u0026quot;, \u0026quot;Get Help\u0026quot;, \u0026quot;People\u0026quot;, \u0026quot;Windows Security\u0026quot;, and \u0026quot;Your Phone\u0026quot; cannot be uninstalled. FUCK YOU MICROSOFT.)\nConfigure Windows File Explorer\nFolder Options: General: \u0026quot;Open File Explorer to:\u0026quot; -\u0026gt; SELECT \u0026quot;This PC\u0026quot; View: Files and Folders: \u0026quot;Decrease space between items (compact view)\u0026quot; -\u0026gt; ENABLE (Win11?) \u0026quot;Display the full path in the title bar\u0026quot; -\u0026gt; DISABLE \u0026quot;Hidden files and folders\u0026quot; -\u0026gt; SELECT \u0026quot;Show hidden files, folders and drives\u0026quot; \u0026quot;Hide extensions for known file types\u0026quot; -\u0026gt; DISABLE \u0026quot;Hide protected operating system files (Recommended)\u0026quot; -\u0026gt; DISABLE \u0026quot;Launch folder windows in a separate process\u0026quot; -\u0026gt; ENABLE \u0026quot;Restore previous folder windows at log-on\u0026quot; -\u0026gt; ENABLE Navigation pane: \u0026quot;Expand to open folder\u0026quot; -\u0026gt; ENABLE \u0026quot;Show all folders\u0026quot; -\u0026gt; ENABLE \u0026quot;Show Network\u0026quot; -\u0026gt; ENABLE (Win11?) \u0026quot;Show This PC\u0026quot; -\u0026gt; ENABLE (Win11?) \u0026quot;Apply to Folders\u0026quot; -\u0026gt; CLICK Configure Language\n\u0026quot;Settings\u0026quot; -\u0026gt; \u0026quot;Language\u0026quot; \u0026quot;Preferred languages\u0026quot; \u0026quot;Add a language\u0026quot; \u0026quot;English (United States)\u0026quot; -\u0026gt; ADD \u0026quot;Install Language Pack\u0026quot; -\u0026gt; ENABLE Add a language \u0026quot;Ελληνικά (Greek)\u0026quot; -\u0026gt; ADD \u0026quot;Install language pack\u0026quot; -\u0026gt; DISABLE \u0026quot;Text-to-speech\u0026quot; -\u0026gt; DISABLE \u0026quot;Handwriting\u0026quot; -\u0026gt; DISABLE \u0026quot;Windows display language\u0026quot; -\u0026gt; SELECT \u0026quot;English (United States)\u0026quot; \u0026quot;Related settings\u0026quot; \u0026quot;Administrative language settings\u0026quot; -\u0026gt; \u0026quot;Formats\u0026quot; \u0026quot;Format:\u0026quot; -\u0026gt; SELECT \u0026quot;English (United States)\u0026quot; \u0026quot;Short date:\u0026quot; -\u0026gt; SELECT \u0026quot;yyyy-MM-dd\u0026quot; \u0026quot;Long date:\u0026quot; -\u0026gt; SELECT \u0026quot;dddd, MMMM, d, yyyy\u0026quot; \u0026quot;Short time:\u0026quot; -\u0026gt; SELECT \u0026quot;HH:mm\u0026quot; \u0026quot;Long time:\u0026quot; -\u0026gt; SELECT \u0026quot;HH:mm:ss\u0026quot; \u0026quot;Typing\u0026quot; (\u0026quot;Spelling, typing, and keyboard settings\u0026quot;) \u0026quot;Autocorrect misspelled words\u0026quot; -\u0026gt;DISABLE Configure Accessibility\n\u0026quot;Settings\u0026quot; -\u0026gt; \u0026quot;Accessibility\u0026quot; -\u0026gt; \u0026quot;Text cursor\u0026quot; \u0026quot;Text cursor thickness\u0026quot; -\u0026gt; SELECT 2 \u0026quot;Keyboard\u0026quot; \u0026quot;Use Sticky Keys\u0026quot; \u0026quot;Allow the shortcut key to start Sticky Keys\u0026quot; -\u0026gt; DISABLE \u0026quot;Use Toggle Keys\u0026quot; \u0026quot;Allow the shortcut key to start Toggle Keys\u0026quot; -\u0026gt; DISABLE \u0026quot;Use Filter Keys\u0026quot; \u0026quot;Allow the shortcut key to start Filter Keys\u0026quot; -\u0026gt; DISABLE \u0026quot;Change how keyboard shortcuts work\u0026quot; \u0026quot;Underline access keys when available\u0026quot; -\u0026gt; ENABLE Personalize\n\u0026quot;Settings\u0026quot; -\u0026gt; \u0026quot;Accounts\u0026quot; \u0026quot;Your Info\u0026quot; -\u0026gt; \u0026quot;Create your picture\u0026quot; -\u0026gt; \u0026quot;Browse for one\u0026quot; Save and browse for this picture: \u0026quot;Settings\u0026quot; -\u0026gt; \u0026quot;Personalization\u0026quot;\n\u0026quot;Background\u0026quot; \u0026quot;Βackground\u0026quot; -\u0026gt; SELECT \u0026quot;Picture\u0026quot; \u0026quot;Choose your picture\u0026quot; -\u0026gt; Download, browse to, and select this image: \u0026quot;Choose a fit\u0026quot; -\u0026gt; SELECT \u0026quot;Tile\u0026quot; - \u0026quot;Colors\u0026quot; - \u0026quot;Choose your color\u0026quot; -\u0026gt; SELECT \u0026quot;Dark\u0026quot; - \u0026quot;Choose your accent color\u0026quot; -\u0026gt; SELECT \u0026quot;Custom color\u0026quot;, make it orange, medium brightness. - \u0026quot;Show accent color on the following surfaces\u0026quot; - \u0026quot;Start, taskbar, and action center\u0026quot; -\u0026gt; ENABLE - \u0026quot;Title bars and window borders\u0026quot; -\u0026gt; ENABLE Fix the title-bar color of inactive windows Using RegEdit, set the DWORD value AccentColorInactive in HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\DWM to 2f2f2f. (Add the value if missing.) \u0026quot;Start\u0026quot; \u0026quot;Show suggestions occasionally in Start\u0026quot; -\u0026gt; DISABLE - \u0026quot;System\u0026quot; -\u0026gt; \u0026quot;Multi-tasking\u0026quot; -\u0026gt; \u0026quot;Alt + Tab\u0026quot; \u0026quot;Pressing Alt + Tab shows\u0026quot; -\u0026gt; SELECT \u0026quot;Open windows only\u0026quot; \u0026quot;Shared experiences\u0026quot; \u0026quot;Share across devices\u0026quot; -\u0026gt; DISABLE - \u0026quot;Clipboard\u0026quot; -\u0026gt; \u0026quot;Clipboard history\u0026quot; -\u0026gt; DISABLE - Create an Administrator Command Prompt shortcut Search for \u0026quot;Command Prompt\u0026quot; Create a shortcut on the desktop Edit properties Select \u0026quot;Advanced\u0026quot; Select \u0026quot;Run as Administrator\u0026quot; Pin to start - Create an Administrator Powershell shortcut Search for \u0026quot;Powershell\u0026quot; Create a shortcut on the desktop Edit properties Select \u0026quot;Advanced\u0026quot; Select \u0026quot;Run as Administrator\u0026quot; Pin to start - Create a shortcut to Services Search for \u0026quot;Services\u0026quot; Pin to start - Create a shortcut to RegEdit Search for \u0026quot;RegEdit\u0026quot; Pin to start - Create a shortcut to CharMap Search for \u0026quot;CharMap\u0026quot; Pin to start Enable natural mouse wheel scrolling\nFollow the instructions at Mouse Wheel Natural Scrolling in Windows Install applications:\nDitto (because Windows built-in \u0026quot;Clipboard History\u0026quot; is lame beyond words.) From https://ditto-cp.sourceforge.io/ \u0026quot;...\u0026quot; -\u0026gt; \u0026quot;Quick Options\u0026quot; \u0026quot;Lines per clip\u0026quot; -\u0026gt; SELECT \u0026quot;1\u0026quot; \u0026quot;View Caption Bar On\u0026quot; -\u0026gt; SELECT \u0026quot;Top\u0026quot; \u0026quot;Options\u0026quot; \u0026quot;Keyboard Shortcuts\u0026quot; \u0026quot;Activate Ditto\u0026quot; (2) -\u0026gt; SET Win+V This will not work because starting with Windows 10 Microsoft has reserved all \u0026quot;Win\u0026quot; keys for its own use -- FUCK YOU MICROSOFT. \u0026quot;Activate Ditto\u0026quot; (2) -\u0026gt; SET Ctrl+` \u0026quot;Move clips to the top of the list on paste\u0026quot; -\u0026gt; ENABLE \u0026quot;7-Zip\u0026quot; From https://www.7-zip.org \u0026quot;Options\u0026quot; \u0026quot;7-Zip\u0026quot; tab: \u0026quot;Icons in context menu\u0026quot; -\u0026gt; ENABLE \u0026quot;Context menu items:\u0026quot; DISABLE ALL, then: \u0026quot;Open Archive\u0026quot; -\u0026gt; ENABLE \u0026quot;Extract to \u0026lt;Folder\u0026gt;\u0026quot; -\u0026gt; ENABLE \u0026quot;Add to \u0026lt;Archive\u0026gt;.zip -\u0026gt; ENABLE \u0026quot;Settings\u0026quot; tab: \u0026quot;Show real file icons\u0026quot; -\u0026gt; ENABLE \u0026quot;Firefox\u0026quot; From https://www.mozilla.org/en-US/ Visit \u0026quot;about:config\u0026quot; and set \u0026quot;browser.tabs.tabMinWidth\u0026quot; to 50 install \u0026quot;Dark Reader\u0026quot; extension install \u0026quot;UBlock Origin\u0026quot; extension install \u0026quot;Markdown Viewer\u0026quot; extension by \u0026quot;simov\u0026quot; Options -\u0026gt; allow all file access, allow all site access Theme -\u0026gt; Water-Dark \u0026quot;Notepad++\u0026quot; From https://notepad-plus-plus.org/ \u0026quot;AutoHotkey\u0026quot; From https://www.autohotkey.com/ In case of antivirus trouble, download as a zip file. Git From https://git-scm.com/downloads/win Under \u0026quot;Select Components\u0026quot; check \u0026quot;Windows Explorer integration\u0026quot; / \u0026quot;Open Git Bash here\u0026quot; Under \u0026quot;Choosing the default editor used by Git\u0026quot; select \u0026quot;Use Notepad++ as Git's default editor\u0026quot;. \u0026quot;TortoiseGit\u0026quot; From https://tortoisegit.org/ Select all defaults, except: \u0026quot;English (GB) dictionary\u0026quot; -\u0026gt; DISABLE \u0026quot;SSH\u0026quot; -\u0026gt; SELECT \u0026quot;OpenSSH\u0026quot; (Alternatively, after installing, go to \u0026quot;Options\u0026quot; -\u0026gt; \u0026quot;Network\u0026quot; -\u0026gt; \u0026quot;SSH\u0026quot; -\u0026gt; \u0026quot;SSH client:\u0026quot; and enter C:\\Windows\\System32\\OpenSSH\\ssh.exe\u0026quot;) \u0026quot;Settings\u0026quot; -\u0026gt; \u0026quot;Colors 1\u0026quot; -\u0026gt; \u0026quot;Dark Theme\u0026quot; -\u0026gt; ENABLE Select any two files in Windows File Explorer Right-click on the files -\u0026gt; \u0026quot;TortoiseGit\u0026quot; -\u0026gt; \u0026quot;Diff\u0026quot; -\u0026gt; \u0026quot;File\u0026quot; -\u0026gt; \u0026quot;Settings\u0026quot; -\u0026gt; \u0026quot;Colors\u0026quot; -\u0026gt; \u0026quot;Use dark mode\u0026quot; -\u0026gt; ENABLE \u0026quot;Visual Studio Community Edition\u0026quot; From https://visualstudio.microsoft.com \u0026quot;SysInternals Process Explorer\u0026quot; Download into \u0026quot;%UserProfile%\\bin\\ProcessExplorer\u0026quot; From https://learn.microsoft.com/en-us/sysinternals/downloads/process-explorer \u0026quot;Options\u0026quot; \u0026quot;Run At Logon\u0026quot; -\u0026gt; ENABLE Hide When Minimized\u0026quot; -\u0026gt; ENABLE \u0026quot;Allow Only One Instance\u0026quot; -\u0026gt; ENABLE \u0026quot;Confirm Kill\u0026quot; -\u0026gt; DISABLE \u0026quot;Tray Icons\u0026quot; -\u0026gt; \u0026quot;Commit History\u0026quot; -\u0026gt; ENABLE \u0026quot;SVG Viewer Extension for Windows Explorer\u0026quot; by tibold From https://github.com/tibold/svg-explorer-extension/releases Configure Visual Studio\n\u0026quot;Tools\u0026quot; -\u0026gt; \u0026quot;Import and Export Settings...\u0026quot; -\u0026gt; \u0026quot;Import and Export Settings Wizard\u0026quot; \u0026quot;Import selected environment settings\u0026quot; -\u0026gt; (Next) \u0026quot;Yes, save my current settings\u0026quot; -\u0026gt; Save as YYYY-MM-DD.defaults.vssettings in \u0026lt;home\u0026gt;\\Personal\\Settings\\VisualStudio -\u0026gt; (Next) \u0026quot;Choose a Collection of Settings to Import\u0026quot; -\u0026gt; (Browse...) Select settings file from previous installation \u0026quot;Tools\u0026quot; -\u0026gt; \u0026quot;Options\u0026quot; -\u0026gt; NuGet Package Manager\u0026quot; Click on the camouflaged \u0026quot;+\u0026quot; button at the top-right corner of the dialog Fill-in: \u0026quot;Name:\u0026quot; -\u0026gt; \u0026quot;nuget.org\u0026quot; \u0026quot;Source:\u0026quot; \u0026quot;https://api.nuget.org/v3/index.json\u0026quot; \u0026quot;Tools\u0026quot; -\u0026gt; \u0026quot;Options\u0026quot; -\u0026gt; \u0026quot;Source Control\u0026quot; -\u0026gt; \u0026quot;Git Global Settings\u0026quot; \u0026quot;Cryptographic network provider\u0026quot; -\u0026gt; SELECT \u0026quot;Unset\u0026quot; \u0026quot;Credential helper\u0026quot; -\u0026gt; SELECT \u0026quot;Unset\u0026quot; \u0026quot;Extensions\u0026quot; -\u0026gt; \u0026quot;Manage Extensions\u0026quot; -\u0026gt; Online Install \u0026quot;ReSharper\u0026quot; Install \u0026quot;VsColorOutput64\u0026quot; Install \u0026quot;EditorConfig Guidelines\u0026quot; (by \u0026quot;Ivan.Z\u0026quot;) If dealing with Nazi admins from hell who have taken control over your HOMEDRIVE and HOMEPATH variables and forced them to point to some network drive, thus preventing you from using SSH unless connected to their stupid network:\nAdd the following environment variable: HOME = %UserProfile% Configure \u0026quot;Memory Compression\u0026quot;\nIn an ELEVATED PowerShell prompt, execute the following: Disable-MMAgent -mc Configure UAC\nGo to Settings, search for UAC, select \u0026quot;User Account Control Settings\u0026quot;, set slider to \u0026quot;Never notify\u0026quot;. Justification: In principle the UAC notification is a good thing to have, but Windows is so fucked up that the UAC notification often starts minimized in the taskbar instead of full screen, so it does not get noticed. Configure ZoomPlusU\nIn Windows File Explorer, right-click on the executable, select properties. On the \u0026quot;Compatibility\u0026quot; tab: Check \u0026quot;Run this program in compatibility mode for:\u0026quot; Select \u0026quot;Windows Vista (Service Pack 2)\u0026quot; Click \u0026quot;Change high DPI settings\u0026quot;; in the new dialog: Check \u0026quot;Override high DPI scaling behavior.\u0026quot; Select \u0026quot;Application\u0026quot;. Install Sumatra PDF\nFrom https://www.sumatrapdfreader.org Install InkScape\nFrom https://inkscape.org Install Foobar2000\nFrom https://www.foobar2000.org Also install the free encoder pack from https://www.foobar2000.org/encoderpack Install VLC\nFrom https://www.videolan.org Personal configuration (Must be done by me)\nConfigure AutoHotkey\nGo to %USERPROFILE%\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup In that folder, create a shortcut to %USERPROFILE%\\Personal\\myStandardAutoHotKeyScript\\MyStandardAutoHotKeyScript.ahk Configure Firefox Sync\nConfigure Notepad++\nGo to \u0026quot;Settings\u0026quot; -\u0026gt; \u0026quot;Style Configurator...\u0026quot; and select \u0026quot;Choco\u0026quot; Unzip the contents of: %UserProfile%\\Personal\\settings\\Notepad++.zip into: %UserProfile%\\AppData\\Roaming\\Notepad++\\ Configure Microsoft Edge\nDelete the Microsoft Edge icon from the desktop and empty the recycle bin. Launch Microsoft Edge, go to settings Go to \u0026quot;Settings / Profiles / Microsoft Rewards / Earn Microsoft Rewards in Microsoft Edge\u0026quot; --\u0026gt; DISABLE (FUCK YOU MOTHERFUCKERS!) Restart Microsoft Edge. (FUCK YOU MOTHERFUCKERS!) If the misfortune hath befallen thee of being employed by a company that has configured \u0026quot;Microsoft Edge\u0026quot; to automatically launch on every login: Open C:\\Program Files (x86)\\Microsoft\\Edge\\Application in Windows Explorer Rename msedge.exe to new_msedge.exe \u0026quot;Pin to Start\u0026quot; new_msedge.exe Create a directory called \u0026quot;msedge.exe\u0026quot; Windows has been observed to create a directory called \u0026quot;msedge_exe\u0026quot; instead, if that happens, rename it to \u0026quot;msedge.exe\u0026quot; (FUCK YOU MICROSOFT) Go to \u0026quot;Properties\u0026quot;, make it read-only. (As it turns out, this is not enough, keep reading) Go to \u0026quot;Properties\u0026quot; -\u0026gt; \u0026quot;Security\u0026quot; -\u0026gt; \u0026quot;Advanced\u0026quot;, take ownership and disable inheritance so that nothing has access to it. Launch \u0026quot;Microsoft Edge\u0026quot;. Right-click on the icon of the running application on the taskbar. In the menu that appears, right-click on \u0026quot;Microsoft Edge\u0026quot; and select \u0026quot;Properties\u0026quot;. Shortcut tab: Change the target from \u0026quot;msedge.exe\u0026quot; to \u0026quot;new_msedge.exe\u0026quot; Click \u0026quot;Change Icon...\u0026quot; Under \u0026quot;Look for icons in this file\u0026quot; replace \u0026quot;msedge.exe\u0026quot; with \u0026quot;new_msedge.exe\u0026quot; and select the first icon. Add SSH keys\nIf the command \u0026quot;ssh-add -l\u0026quot; fails with \u0026quot;error connecting to agent\u0026quot; then temporarily start the \u0026quot;OpenSSH Authentication Agent\u0026quot; service (may require setting its \u0026quot;Startup Type\u0026quot; to \u0026quot;Manual\u0026quot;.) Execute the following: ssh-add \u0026lt;HOME\u0026gt;.ssh\u0026lt;personal-key-file\u0026gt;_rsa ssh-add \u0026lt;HOME\u0026gt;.ssh\u0026lt;employer-key-file\u0026gt;_rsa Install Jasc Paint Shop Pro\nInstall \u0026quot;Paint Shop Pro 9.01 EN Trial (psp901entr).exe\u0026quot; In the \u0026quot;Save files in folder\u0026quot; box enter \u0026quot;C:\\temp\\psp\u0026quot; When selecting features: \u0026quot;Browse with Right Click Menu\u0026quot; -\u0026gt; DISABLE \u0026quot;Jasc Thumbnail Shell Extension\u0026quot; -\u0026gt; DISABLE On the \u0026quot;InstallShield Wizard Completed\u0026quot; page: \u0026quot;Show me important information about Jasc Paint Shop Pro 9\u0026quot; -\u0026gt; DISABLE \u0026quot;Place an Icon for Jasc Paint Shop Pro 9 on the Desktop\u0026quot; -\u0026gt; DISABLE \u0026quot;Place an Icon for Jasc Paint Shop Pro 9 in your QuickLaunch bar\u0026quot; -\u0026gt; DISABLE On the registration dialog: Skip Delete \u0026quot;C:\\temp\\psp\u0026quot; Copy \u0026quot;Jasc PaintShop Pro 9.01- Bidjan.exe\u0026quot; to \u0026quot;C:\\Program Files (x86)\\Jasc Software Inc\\Paint Shop Pro 9\u0026quot; Select \u0026quot;Run As Administrator\u0026quot; on \u0026quot;Jasc PaintShop Pro 9.01- Bidjan.exe\u0026quot; On the \u0026quot;Jasc Update\u0026quot; dialog: \u0026quot;Never Check Automatically\u0026quot; -\u0026gt; ENABLE Install Cool Edit Pro 2.1\nFollowing the instructions in the README.txt file. Configure \u0026quot;VsColorOutput64\u0026quot;\nCopy the file vscoloroutput.json from %UserProfile%\\Personal\\Settings\\VSColorOutput to %UserProfile%\\AppData\\Roaming\\VsColorOutput64\\ More software to install:\nTry Dotz Software SVG Explorer Extension\nMPC-HC\nqbittorrent\nobsidian\n","date":"2022-12-01T13:34:00.326Z","permalink":"https://blog.michael.gr/post/2022-12-setting-up-new-windows-11-machine/","title":"Setting up Windows"},{"content":"To always show all system tray icons in Windows 11:\nOpen the Run prompt (Win+R) and execute the following nonsense:\nexplorer shell:::{05d7b0f4-2121-4eff-bf6b-ed3f69b894d9}\nIn the dialog that appears, there will be a checkbox to always show all icons and notifications on the taskbar.\nOriginal source of information: TheWindowsClub\n\u0026lt;rant-mode\u0026gt;\nAs always, with Windows 11 Microsoft is continuing its long tradition of trying to force upon everyone their own very specific ways of doing things, which range from the fully retarded to the outrageously retarded. Casual users and new users might not know the difference, but every single old power user on the planet is likely to get pissed off.\nAnd as usual, when they change things, they follow the creeping approach:\nWindows 95 supported taskbar icons: if an application had a taskbar icon, then the icon would appear on the taskbar, simple as that. Windows XP allowed you to choose to hide some icons. Okay, whatever. Windows 7 started only showing the taskbar icons of Microsoft applications and hiding icons from anyone else by default 1, but at least the option to always show all taskbar icons was easy to reach. Windows 10 required you to go digging into settings to find the option to show taskbar icons from others. Now, with Windows 11 this option is not available anymore, at least not without magical incantations like the one shown here. Judging by the obvious trend, the next version of windows will require launching RegEdit to fix this. \u0026lt;/rant-mode\u0026gt;\nNote that for that move they should have been sued to non-existence, but they were not, because 'murica, and because complacency.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2022-12-01T11:19:36.255Z","permalink":"https://blog.michael.gr/post/2022-12-01-how-to-always-show-all-icons-in-windows/","title":"How to ALWAYS show all icons on the Windows 11 taskbar"},{"content":"\rI am one of those people who choose to publish their ideas on a blog.\nThe practical reason behind doing this is so that in a conversation I can refer my interlocutor to a text which elucidates my points better than I could conversationally. Admittedly, the opportunity to do this does not arise as often as I wish it did, and even when it does happen, about half the time the interlocutor appears to be reluctant to go and actually read the post, so let's just say that I publish my ideas mainly because I like doing it.\nThere is an old quote which says:\nIt is better to remain silent and be thought of as a fool than to speak and remove all doubt. (Quote Investigator.)\nWhen you publish your ideas, you are making them available for scrutiny, so you are running the danger that the tiniest mistake will be discovered, (or that the slightest semantic disparity will cause something to be interpreted as a mistake,) and thus negative opinion may be formed about your person. Furthermore, the negative opinion will be formed in your absence, so you will not be given the opportunity to discuss it and defend yourself.\nMost people do not write about their ideas, so they do not run any risks. I deliberately choose to run the risks, because for a person who a) has ideas, b) is capable of writing about them, and c) likes writing, to not write them lest he be thought of as a fool would be cowardice. So, I have no option but to write.\nUnpopular ideas greatly exacerbate the problem. Statistically, most readers are likely to be following established practices, but many of my ideas go against established practices, so readers might find the content of this blog disagreeable.\nThe opposite of this would be re-iterating known facts and established practices: many people like writing for the benefit of those who might be unaware of what the facts are, or what the established practices are; I guess there is some usefulness in this, but for me an idea is worth writing about if it has something different to say, i.e. showing how a widely popular established practice or understanding is wrong, and how we can do better. Most ideas of that kind are at least a bit blasphemous to at least a sizable portion of the population.\nI am an engineer who is passionate about his engineering discipline and about finding ways of advancing it. Now, engineering tends to advance by trying new things, not by re-iterating old things. New Things decidedly means Not the old things. Plus, if you are trying to make an omelet, then some eggs will necessarily be broken.\nIt goes without saying that everything I write falls under the category of Strong opinions, weakly held. (Coding Horror.) Everything I write on this blog is subject to revision in light of new evidence. If you find a mistake in something I have written, please point it out to me, and if you disagree with some opinion I have expressed here, then please either debate it with me, or let us agree to disagree.\n","date":"2022-11-19T11:38:13.233Z","permalink":"https://blog.michael.gr/post/2022-11-about-these-papers/","title":"About these papers"},{"content":"\rGames industry veteran Mike Acton gave talk/rant at Game Developers' Conference (GDC) 2019 where he listed 50 things he expects of developers: https://www.youtube.com/watch?v=cV5HArLYajE This list was transcribed by Adam Johnson and posted here: https://adamj.eu/tech/2022/06/17/mike-actons-expectations-of-professional-software-engineers/ and I am copying it here for posterity.\nI found this list useful as reference material; some of the items on this list do not apply to my job because I rarely do anything especially performance-oriented nowadays, and some of the items on the list are good to always have in mind but subject to the programmer's own judgement, on a case by case basis, whether they should be practiced or not.\nHere it is:\nI can articulate precisely what problem I am trying to solve. It's all too easy to get stuck in the weeds and lose track of why you're doing what you're doing. Keep top of mind what the actual end goal is do, and you might spot an alternative path. I have articulated precisely what problem I am trying to solve. Communicate the problem \u0026quot;out loud\u0026quot; to other team members, your product manager, etc. I have confirmed that someone else can articulate what problem I am trying to solve. Communication! Ensure your team is all on the same page. Make sure your understanding of the problem is complete. I can articulate why my problem is important to solve. If you solve the problem you're working on, who benefits, and how much? I can articulate how much my problem is worth solving. If you say it's worth \u0026quot;as long as it takes\u0026quot;… Mike does not have friendly words for you. For any problem there's a maximum amount of time and effort worth investing in solving it. At least have some idea of the upper bound. I have a Plan B in case my solution to my current problem doesn't work. Imagine you're days or hours before the deadline, and you can tell that completing Plan A will be impossible. What do you do instead? Maybe you have a simplified algorithm, or you can disable a certain subsystem. Have more than one plan. I have already implemented my Plan B in case my solution to my current problem doesn't work. Mitigate risk by writing the backup version first. This means you always have a safety net and you can learn more about the problem space in order to iterate on Plan A. I can articulate the steps required to solve my current problem. Programming only works when you break down problems into manageable chunks. Have a sketch of the steps to the end state before you begin. I can clearly articulate unknowns and risks associated with my current problem. There are always going to be things you don't know. You should know where they are in your plan, so you can manage them. I have not thought or said \u0026quot;I can just make up the time\u0026quot; without immediately talking to someone. Say it's Wednesday, you have a project due on Friday, and you get some new task dropped on your lap. You think \u0026quot;I'll do the new thing now, and make up the time for the original task by Friday\u0026quot;… mistake! Communicate about the conflict on Wednesday. Your product manager will help manage the timing and risk. I write a \u0026quot;framework\u0026quot; and have used it multiple times to actually solve a problem it was intended to solve. If you're writing a tool of some kind, you should verify it works in practice. Too often people create something in isolation and it doesn't end up delivering in the real world. (This is how Django came to be: from a real team making real websites, on deadlines!) I can articulate what the test for completion of my current problem is. If you don't know when to stop, you might find yourself going down rabbit holes, chasing unimportant marginal gains. I can articulate the hypothesis related to my problem and how I could falsify it. If a hypothesis cannot be proven wrong, there's no knowledge to be gained. As Karl Popper showed, science only works through falsification. I can articulate the (various) latency requirements for my current problem. Any time you write code, you should consider when the output data is required. Not every caller needs output data instantly, and nor do you have an unbounded amount of time to perform everything. At least get an idea of the sensible bounds. I can articulate the (various) throughput requirements for my current problem. How much data needs to come through the system? How many bytes, requests, or frames per second? I can articulate the most common concrete use case of the system I am developing. You should know what actual users of your system will actually be doing most of the time. Having a vague idea doesn't help, since knowing which patterns are common informs which way to write a given piece of code. I know the most common actual, real-life values of the data I am transforming. Beyond use cases, you should know the data inside the system. For example, if your function works with integers, you'd probably write it quite differently if 99% of the values are 0. I know the acceptable ranges of values of all the data I am transforming. Computer systems always have limits. Know the ranges for the data types you're working with (and enforce them). I can articulate what will happen when (somehow) data outside that range enters the system. Murphy's Law says \u0026quot;anything that can go wrong will go wrong\u0026quot;. Know how your system will behave in such cases, and handle such problems if necessary. I can articulate a list of input data into my system roughly sorted by likelihood. Have an idea of the space of possible data, what's most likely, second most likely, etc. Code appropriately, for example checking for common error conditions first. I know the frequency of change of the actual, real-life values of the data I am transforming. Reason about the frequency of change and figure out how often you'll want to calculate derived values. I have (at least partially) read the (available) documentation for the hardware, platform, and tools I use most commonly. Read the friendly manual! Go a step beyond day-to-day reference, and try reading the full documentation to gain a deep understanding. (Jens Oliver Meiert calls reading the HTML specification the Web Developer's Pilgrimage.) I have sat and watched an actual user of my system. Watching users can massively break shift your view of how your software works. Do it! I know the slowest part of the users of my system's workflow with high confidence. Any workflow has a bottleneck. Make sure you know what it is so you can focus efforts there, if need be. I know what information users of my system will need to make effective use of the solution. Think about what documentation or data users need to understand and use your solution. I can articulate the finite set of hardware I am designing my solution to work for. Software requires hardware. Know what hardware your program targets, such as: CPU architectures Minimum requirements for memory, CPU speed, and network bandwidth Input devices Output devices The environment the hardware runs in (e.g. data centre or living room) I can articulate how that set of hardware specifically affects the design of my system. If you're targeting low end devices, how do you ensure you don't exhaust memory? If some users don't have pointing devices, how do you accommodate them? I have recently profiled the performance of my system. If you're developing a local app, run profiling tools regularly to gain an idea of performance over time. With server based programs, you can install an APM (Application Performance Monitoring) tool in production and have continuous profiling data. I have recently profiled memory usage of my system. Make sure you aren't wasting memory. I have used multiple different profiling methods to measure the performance of my system. There's no perfect profiling tool, so know how to use more than one. For example, some great Python profilers are cProfile, py-spy, Austin, Scalene, Fil, and memray. They all have different characteristics and complement each other. I know how to significantly improve the performance of my system without changing the input/output interface of the system. Do you know the next step to optimize your system? You don't have to do it right now, as it may not be worth it, but you should have an idea what you'd do next to make your code faster. For example, use a faster but less convenient data structure, or convert a hot function into a faster language (such as Python to C with Cython). This should also guide you to designing interfaces that are optimizable in the first place. For example, don't commit to returning expensive-to-compute results immediately, but instead return a promise. I know specifically how I can and will debug live release builds of my work when they fail. Bugs are inevitable. You should know the tools that will let you work through those problems in production, such as logs, debuggers, or a live shell. I know what data I am reading as part of my solution and where it comes from. Know where the data comes from, in what format, and how you can read it. I know how often I am reading data I do not need as part of my solution. Data access is rarely optimal. You'll often be moving data that's not required for your solution, such as unnecessary fields or wrapper objects. If you don't know about this waste, you can't reason about whether it's worth the overhead or not. I know what data I am writing as part of my solution and where it is used. All output data is intended for use by a human or another program. Be organized enough to know what the downstream consumers of your output are. I know how often I am writing data I do not need to as part of my solution. Data output is also rarely optimal. Are you frequently writing out data that hasn't changed? Are you writing many fields when only one is used downstream? Again, know about it so you can reason about it. I can articulate how all the data I use is laid out in memory. Many programming languages and frameworks can handle memory for you, but that doesn't abdicate you of responsibility. Know how your tools lay out memory, so you can tell when another approach makes sense. For example, in Python most objects are based on dictionaries, so you should have a solid understanding of how they work, and alternatives like slotted classes or arrays. I never use the phrase \u0026quot;platform independent\u0026quot; when referring to my work. Any system depends on many things below it. Know what they are. I never use the phrase \u0026quot;future proof\u0026quot; when referring to my work. Future-proofing is \u0026quot;100% a fool's errand\u0026quot;. \u0026quot;You can't pre-solve problems you have no information of.\u0026quot; I can schedule my own time well. \u0026quot;You're an adult person, just use a calendar.\u0026quot; I am vigilant about not wasting others' time. Don't waste time asking questions that you can google in five seconds. But also don't waste loads of time struggling for days alone when you could get help from a team member in minutes! Find the balance. I actively seek constructive feedback and take it seriously. Ask for feedback and do something about it. I am not actively avoiding any uncomfortable (professional) conversations. If there's something wrong at work, don't put off talking about it. I am not actively avoiding any (professional) conflicts. If you've noticed something is going wrong, whether technically or communication wise, get those conflicts out in the open. Letting them stew never helps. I consistently interact with other professionals, professionally. Be courteous and professional! Mike jokes about setting an incredibly low bar: no yelling, no hitting, … I can articulate what I believe others should expect from me. Have a standard for yourself and be ready to tell your co-workers what it is. I do not require multiple reminders to respond to a request or complete work. \u0026quot;Waiting for someone else to poke you is not an effective way to get your job done.\u0026quot; I pursue opportunities to return value to the commons (when appropriate). All our work builds on top of the work of countless others. At some point, you'll have opportunities to give back to the community at large. For example, talking at meetups, making open source contributions, or even just discussing topics with your team to boost everyone's skills. I actively work to bring value to the people I work with. You're part of a team, so work to help them. I actively work to ensure under-represented voices are heard. Don't stand by leaving this to be someone else's problem. Do something to make sure that minorities are heard. This might mean ensuring that the minority person at work gets a chance to speak, that your hiring process is unbiased, or that your website is accessible for users who rely on screen readers. ","date":"2022-10-12T05:54:06.726Z","permalink":"https://blog.michael.gr/post/2022-10-fifty-things-expected-of-developers/","title":"50 things expected of developers"},{"content":"\rThis post is intended as support material for another post of mine; see Towards Authoritative Software Design.\nOne day back in the early nineties, when people were using Windows 3.0 and the Microsoft C/C++ Compiler, a colleague showed me a software design that for the first time he had done not on whiteboard, nor on paper, but on a computer screen, using a new drawing tool called Visio.\n(Useful pre-reading: About these papers)\nScreenshots of Visio 1.0 running under Windows 3.1.\nHe showed me interconnected components laid out on a canvas, and as he moved one of the components, the drawing tool re-routed the lines to maintain the connections to other components. This meant that Visio was not just a pixel drawing utility like Microsoft Paint; it had some understanding of the structure of the information that was being displayed.\nWe both knew that the next logical thing to ask from such a tool would be to automatically produce an actual running software system according to that design; alas, Visio could do no such thing. In our eyes, the product embodied a latent promise for such functionality, but no such functionality was there.\nBack then, Visio was not yet owned by Microsoft, but the two companies were obviously in talks, because the first pre-release version of Visio had been distributed by Microsoft in a floppy disc containing other Microsoft Software. (For more information about the early relationship between Microsoft and Visio, read The Early Days of Visio Corporation - Recollections by Ted Johnson, Visio Co-founder.)\nThen, in 1993 Microsoft announced the successor to their Microsoft C/C++ compiler, and the name of the new product was going to be Microsoft Visual C++.\nEven before we could get our hands on the new compiler, we could not help but speculate what Microsoft might mean by \u0026quot;visual\u0026quot; in the product name, and our hopes were high that they would have made good on the promise of visual tools for software design.\nAlas, nothing of that sort happened; Microsoft Visual C++ was just another command-line toolset, and the term \u0026quot;visual\u0026quot; in the title was nothing but marketing deceit.\nThen a few more years passed, and in 1997 Microsoft announced their first true Integrated Development Environment (IDE), which was going to be called Microsoft Visual Studio starting with version 5.0, a.k.a. '97.\nAgain, we hoped that this time they would deliver visual software design tools, and again were disappointed: sure, Microsoft was finally providing programmers with an IDE, and an IDE is admittedly a visual sort of thing, but there was still no sign of any actual visual software design tools.\nA few more years passed, and in 2000 Visio was acquired by Microsoft and became Microsoft Visio (see Wikipedia.)\nWhen we saw that acquisition happening, we thought that perhaps it was the one thing that was missing for that long unfulfilled promise to finally become a reality; surely, the next release of Visual Studio would have Visio built-in, allowing us to create our software designs and then launch them, right?\nSo, a couple of years later, in 2002 another major release of Visual Studio was announced, which was to be named Visual Studio Dot Net.\nAs it turned out, that new version was nothing but the exact same old version, with bundled support for the Dot Net platform, and a tacky product name slapped onto it. Visio was not in any way connected to Visual Studio, and instead it had become part of the offerings around Microsoft Office.\nSo, by that time, we finally accepted the realization that Microsoft's plan for world domination was not so much about actual software development breakthroughs but more about tacky product names.\nCover image: The logo from Visio version 1.0.\n","date":"2022-08-16T11:35:17.407Z","permalink":"https://blog.michael.gr/post/2022-08-on-microsoft-visual-products/","title":"On Microsoft 'Visual' products"},{"content":"\rThis post is intended as support material for another post of mine; see Towards Authoritative Software Design.\nThe idea of creating software using visual tools has existed ever since the first aspiring programmer was bitterly disillusioned by discovering that programming almost exclusively entails writing lots of little text files containing nothing but boring and cryptic text.\n(Useful pre-reading: About these papers)\nGiven the relative abundance of people who engage in programming, and the relative scarcity of people who engage in software design, it should come as no surprise that visual software development has largely been regarded as an issue of visual programming rather than an issue of visual software design.\nThus, there has been a multitude of attempts to create so-called Visual Programming Languages (see Wikipedia) such as Snap!, Scratch, EduBlocks, Blockly, etc. where code is expressed not as boring text, but instead as colorful blocks on a canvas.\nThe following example is borrowed from Computer Science with Snap! by Eckart Modrow from University of Goettingen, 2018.\nUnfortunately, the blocks used in these visual programming languages tend to faithfully mirror the structure and concepts of textual code, so even though at first glance the two might seem different, a closer examination reveals that they are actually equivalent: it is still all just variables, expressions, flow control statements, function invocations, and even word-wrapping when an expression is too long to fit in the page; there is no real paradigm shift.\nThe usefulness of these languages goes as far as teaching the basic principles of programming to kids, using elements that are colorful and can be manipulated with the mouse, but no further, because as it turns out, code expressed as text is far more expressive, terse, and malleable than blocks.\n(Not to mention, less distracting and less likely to cause epileptic seizures due to excessive application of color.)\nCover image: Logos of various visual programming languages.\n","date":"2022-08-16T10:48:21.08Z","permalink":"https://blog.michael.gr/post/2022-08-on-visual-programming-languages/","title":"On Visual Programming Languages"},{"content":"\rThis post is intended as support material for another post of mine; see Towards Authoritative Software Design.\nThe Universal Modeling Language (UML) (Wikipedia) was intended to be a standard notation for expressing software designs, and to replace the multitude of ad-hoc notations that software architects have been using on various mediums such as whiteboard, paper, and general-purpose box-and-arrow diagram-drawing software. The idea was that by following a standard notation which prescribes a specific way of expressing each concept, every diagram would be readily and unambiguously understood by everyone.\nIt has miserably failed.\n(Useful pre-reading: About these papers)\nUML is probably very close to the top of the list of things that everyone mentions, but nobody uses, and this is due to a number of good reasons:\nIt is incredibly comprehensive, to the point where its sheer size acts as a very strong deterrent to most people attempting to learn it. There are about 20 different types of diagrams for different purposes, each with its own complete set of meticulously detailed notation and rules. UML actually begins to make sense once you realize that it has mostly been an effort to catalogue every imaginable type of diagram used in software development, and standardize the notation used in it, while most of these diagram types are actually irrelevant, or very seldom relevant, to our daily job. However, even if you pick a single diagram type that you happen to have some use for, and decide to learn just that one, the notation is still so comprehensive that the task is daunting. Most of UML is so rarely useful that it is not worth the learning effort. In the extremely rare event that a software development team is to have a meeting in which they could benefit from having an Interaction Overview Diagram to point at, it will be a lot easier to use some ad-hoc but intuitive notation to get the point across, than to only schedule the meeting after every single one of the attendees has completed a UML course to refresh upon the intricacies of the UML Interaction Overview Diagram. The type of UML diagram that has received most attention in the software engineering profession is the UML Class Diagram (see Wikipedia) which deals with representing a class, the structure of a class, and its relationships with other classes. Unfortunately: The UML Class Diagram insists on prescribing a very specific type of notation for everything about a class, and this notation is not always intuitive, thus posing the same obstacles to understanding as posed by program code written in apocryphal syntax and convoluted structure: in both cases, it is all jargon. This might not be an issue for those who have already gone through the trouble of learning the jargon, but the uninitiated are bound to question the usefulness of the entire exercise. The UML Class Diagram prescribes its notation in excruciatingly meticulous detail, so there is no information hiding, and no abstraction: the amount of information contained in a UML Class Diagram is roughly the same as the amount of information contained in a C or C++ header file, or in a Java Interface, so there is virtually nothing to be gained by looking at one vs. looking at the other, which in turn seriously begs the question of why should we be doing double book-keeping. The UML Class Diagram is much too low-level and too finely detailed to be pertinent to software systems design, where the unit of interest is the system component, corresponding to an entire module, rather than to individual classes within a module. It is also becoming even less pertinent as classes are becoming less important in programming due to the modern shift towards functional rather than object-oriented programming. UML is mostly used as documentation, meaning that its role tends to be indicative or suggestive, and usually non-enforceable and non-materializable. This means that mistakes made in the use of the meticulously detailed notation generally go undetected, or might be detected by colleagues, but not by automated validation tools, because for most types of UML diagrams, there exist no such tools. UML is trying to solve problems which do not exist: When a human needs to communicate something to a machine, this has to be done in a perfectly inambiguous fashion, which makes special notation necessary, i.e. jargon. However, when there is no machine involved, and a human simply needs to communicate something to other humans, what matters most is to get the point across, so jargon is actually undesirable, despite the unambiguousness that it would bring. That is okay, because humans thrive in ambiguity. In other words, UML is an attempt to apply a rigid engineering discipline to a form of communication which is fine as it is: free and fluid. (One of the \u0026quot;Three Amigos\u0026quot; that created UML had a military background; coincidence? maybe.) In an attempt to make UML more pertinent to the software development process, some UML tools offer some automatic code generation features. Unfortunately, automatic code generation is almost always a bad idea, because each time the design changes, code generation must be re-applied, but this invariably results in the following bad things happening to code that has already been hand-written by programmers: Hand-written code is overwritten with auto-generated code and thus forever lost, or Hand-written code does not compile anymore due to dependencies on automatically generated definitions which have now changed, or, more often, Both of the above. The idea that you can apply automatic code generation once and never repeat it stems from the \u0026quot;all design up-front\u0026quot; doctrine, which may have been strong back in the 1990s when the foundations of UML were laid down, but the doctrine died soon thereafter, and it has been dead for decades now. From the plethora of diagram types offered by UML, the only one that could perhaps be useful in our daily jobs is the UML Component Diagram (see Wikipedia) but there exist no tools that I am aware of that are capable of either guiding the composition of such a diagram from existing software components, or materializing such a diagram into a running system. Furthermore, if any such tools were to be introduced, they are unlikely to be well-received, because by now people have developed a distaste towards UML and anything associated with it. UML literature follows a lofty standardspeak writing style which is incomprehensible. I tried looking up the term \u0026quot;collaboration\u0026quot; and here is what I found in IBM literature: In UML diagrams, a collaboration is a type of structured classifier in which roles and attributes co-operate to define the internal structure of a classifier.\nThere are two problems with this:\nThe definition depends on other definitions. This happens everywhere in UML. So, in order to understand a certain term you first have to understand other terms, and quite often the definitions make circles, so in order to understand anything you have to have superpowers.\nThis kind of looks like a recursive definition. They may be implying that there is something hierarchical in the nature of the concept, but they are not saying it. Definitions are written with the goal of being correct, not with the goal of being understood. (And since we do not understand them, we cannot tell whether they are correct.)\nOkay, let's look at the next sentence:\nYou use a collaboration when you want to define only the roles and connections that are required to accomplish a specific goal of the collaboration.\nSurprise! Recursion again. Sorry, but now it makes absolutely no sense. And that's how it goes with UML.\nTo summarize:\nUML is insufferably baroque.\nIt should have never been, and it should cease to be.\nIt should be let go into the good night.\nMandatory Grumpy Cat Meme. \u0026quot;UML: I hate it.\u0026quot;\nCover image: The UML logo, by Object Management Group®, Inc. from uml.org; Public Domain.\n","date":"2022-08-16T09:20:46.871Z","permalink":"https://blog.michael.gr/post/2022-08-uml/","title":"On UML"},{"content":"\rThe origins of the debate go so far back that they are lost in the mists of time, but a good starting point (which contains references to prior debate) is an Internet Draft from 2018 titled Terminology, Power, and Inclusive Language in Internet-Drafts and RFCs. Some especially woke communities like the Python community had already started applying some of the recommendations in this draft as early as 2019, but things really picked up steam in 2020, with the murder of George Floyd.\n(Useful pre-reading: About these papers)\nProgrammers all over the world, the overwhelming majority of whom are white boys, wanted to feel like they are doing something about the whole Black Lives Matter thing, but killing cops is a bit difficult, let alone messy, not to mention risky, and what if there is one good cop in the USA and we kill him? -- so they resorted to the next best thing: using more sensitive language.\nNaturally, every company that caters to wokeys needed to show that they are more woke than the next one, and changing terminology is such a cheap and easy thing to do compared to the amount of good publicity it generates that it is actually a bargain; thus, all mega-corporations were suddenly competing on who will revise more politically incorrect terms faster. This involved the identification of politically incorrect terms that we were previously unaware of, and in some cases even the invention of some.\nThe 2018 Internet Draft about inclusive language says nothing about the master branch; it suggests, among other things, to rename the term master/slave to something else, e.g. primary/secondary. This change is arguably worth making; not so much because of its inherent merit, (it has very little of that,) but because we have to acknowledge the possibility that we are unable to put ourselves in the shoes of people who might be hurt by the use of the term. Rumor has it that if you ask actual black people about this issue, they are likely to tell you that they don't give a damn, but this is destined to slide by. Making this change also presupposes that we feel compelled to go out of our way to ease the pain of people who for whatever reasons feel hurt by various things, but that can also arguably be regarded as a reasonable thing to do.\nI am worried that one day people might start feeling hurt by the fact that I am sporting a beard, due to the unbearably toxic masculinity that it exudes and what not, but I guess I will deal with that when the day comes.\nNow, if the abolition of master/slave hardly had any real grounds to stand on, the abolition of the master branch is absolutely groundless, because in this case the word \u0026quot;master\u0026quot; is used in the sense of \u0026quot;original\u0026quot;, as in \u0026quot;master recording\u0026quot;. (See\u0026quot;master\u0026quot; in wiktionary.org.) However, if we acknowledge someone the right to be offended by master/slave, then who are we to take away their right to also be offended by master branch? After all, don't forget that we have already established that we are incapable of putting ourselves in their shoes, right?\nSo, once the abolition of \u0026quot;master/slave\u0026quot; was unanimously agreed upon, the master branch was naturally next. It was just a matter of the innocent paying along with the guilty.\nAfter some debate, (see Git Rev News: Edition 65) both git and GitHub announced in 2020 that they were moving in the direction of renaming the default branch from \u0026quot;master\u0026quot; to main. In October of that year, GitHub proceeded with the change. Lots of other mega-corporations followed suit. One article which skips the whys and the why-nots and just talks about the technical aspects of this transition is Of Git and GitHub, Master and Main by Matt Neuburg, 2021.\nSo, we shall all rename our \u0026quot;master\u0026quot; branches to \u0026quot;main\u0026quot;. You might think that you can get away with using \u0026quot;main\u0026quot; only for new projects and leaving old projects alone, but that will not quite cut it, because then you will be left with eternal confusion since you will have different projects with different default branch names and you will have to always remember which is which. Trying to remember things is the stuff mistakes are made of, and mistakes with branches tend to have very severe consequences. So, all master branches will have to be renamed to main everywhere.\nIs it silly? Yes. Is it a waste of time? Yes. Is it giving in to wokeism? Yes. But you have to pick your battles. You have to question whether you want to engage in an argument with a broader cause that you are already in alignment with anyway. As a manager, it is easier to suffer the small technical pain of transitioning from \u0026quot;master\u0026quot; to main\u0026quot; than to spend valuable time debating the whole silly thing, and running the risk of appearing as a bigot in the process.\nHaving said all that, let me state that on the broad picture, I am completely with Bill Maher on this: the liberal world is going mad with political correctness, sense of entitlement, snowflakeism, and wokeness.\nMandatory grumpy cat meme: Wokeness? How About NO. ","date":"2022-05-27T09:12:37.646Z","permalink":"https://blog.michael.gr/post/2022-12-master-branch-not-kosher/","title":"So the \"master\" branch is not kosher anymore"},{"content":" I knew I was definitely going to watch this one, from just the title. It turns out that he comes across a bit annoying due to attitude; nonetheless, the talk is definitely worth watching.\n","date":"2022-05-25T19:23:27.983Z","permalink":"https://blog.michael.gr/post/2022-05-why-majority-is-always-wrong-paul/","title":"Why the majority is always wrong | Paul Rulkens | TEDxMaastricht"},{"content":" Interestingly enough, in his code snippets he is using JavaScript, whereas one of my personal guiding principles is thou shalt not suffer an error to go undetected, which means that no scripting language should be used under any circumstances, for anything at all, by anyone, anywhere, ever. But I digress. Excellent presentation.\n","date":"2022-05-23T13:00:35.248Z","permalink":"https://blog.michael.gr/post/2022-05-bret-victor-inventing-on-principle/","title":"Bret Victor - Inventing on Principle"},{"content":"\rAbstract This article introduces Bathyscaphe, an open-source java library that you can use to assert that your objects are immutable and/or thread-safe.\nThe problem Programmers all over the world are embracing immutability more and more; however, mutation is still a thing, and in all likelihood will continue being a thing for as long as there will be programmers. In a world where both mutable and immutable objects exist side by side, there is often a need to ascertain that an object is of the immutable variety before proceeding to use it for certain purposes. For example, when an object is used as a key in a hash map, it better be immutable, or else the hash code of the key may change, causing the map to severely malfunction.\nFurthermore, even when an object is mutable, there is often the need to ascertain that it is at least thread-safe before sharing it between threads, otherwise there will be race conditions, with catastrophic results.\nNote that when any of the above goes wrong, it tends to be a bug which is very difficult to troubleshoot.\nUnfortunately, assessment of thread safety and immutability is not an easy task. Most don't even consider it, few talk about it, even fewer attempt it. Programmers all over the world are accustomed to routinely using objects in situations where thread-safety and/or immutability are absolute requirements, but without ever ascertaining them, essentially praying that the objects be thread-safe and/or immutable.\nAs far as I can tell, in the world of the JVM there exist no libraries that will ascertain thread-safety. As for immutability, there are some that purport to do so, but Judging by how marginal status these libraries have in the greater technology landscape, they are not being put into much use. This is not surprising, because they rely exclusively on static analysis, which does not really solve the problem, as I will show.\nIntroducing Bathyscaphe Bathyscaphe aims to give the Java world another chance at addressing the problem of thread-safety and immutability assessment instead of letting it linger on like a chronic ailment. Bathyscaphe is really easy to use, and produces correct and useful results. It is also very small:\nThe JAR file is only about 100 kilobytes. Setting aside the test module, which necessarily depends on JUnit, Bathyscaphe does not have any dependencies outside the Java Runtime Environment. Let me repeat this: Bathyscaphe. Has. No. Dependencies. It depends on nothing. When you include Bathyscaphe in a project, you are including its tiny JAR file and nothing else. Why existing solutions do not work Oftentimes we can tell whether an object is mutable or immutable just by looking at its class, and indeed there exist static analysis tools that examine classes and classify them as either mutable or immutable. The widespread understanding is that once a class has been classified, all instances of that class can receive the same classification. However, in many cases it is not enough to just look at the class to determine immutability; instead, it is necessary to examine each and every instance of the class at runtime. When static analysis tools assess such classes, they yield results that are erroneous, or in the best case useless.\nExamples where static analysis does not work and cannot work:\nStatic analysis does not work when a class contains a field which is final, receives its value from a constructor parameter, and the type of the field is an interface or a non-final class. Static analysis can determine that the field itself will not mutate, but has no way of knowing whether the value referenced by the field can mutate. In order to err on the safe side, static analysis tools tend to assess classes containing such fields as mutable, but this is arbitrary, and it constitutes a false negative when the field is in fact initialized with an immutable value. Static analysis does not work when a class is an unmodifiable collection of elements, where the elements can be of any type. The most famous examples in this category are the JDK-internal classes java.util.ListN and java.util.List12, instances of which are returned by java.util.List.of() and its overloads. Some static analysis tools assess such classes as immutable, which can be a false positive, e.g. in the case of List.of( new StringBuilder() ). Some static analysis tools assess such classes as mutable, which can be a false negative, e.g. in the case of List.of( 1 ). Static analysis does not work when a class is freezable. By this we mean a class whose instances begin life as mutable, and are at some point instructed to transition from being mutable to being immutable. For an explanation as to why freezable classes are important, see related appendix. From the above it follows that in many cases, examining a class is not enough; in these cases, we need to examine each and every instance of the class at runtime. Furthermore, we need to examine not only the instance at hand, but the entire object graph referenced by that instance. In other words, we must not just assess shallow (superficial) immutability, we must assess deep immutability. That's what Bathyscaphe does. And that's why it is called Bathyscaphe.\nHow Bathyscaphe Works In a nutshell, Bathyscaphe uses reflection to examine each field of a class, and recursively the type of each field. If all fields of a class can be conclusively assessed as immutable, then all instances of that class will be assessed as immutable. However, if the actual type of the runtime value of a certain field cannot be known by a static examination of the class, then for each instance of the class at runtime, Bathyscaphe will read the value of the field, obtain the actual type of the value, and recursively assess the type of that value.\nIn most cases Bathyscaphe can determine by itself whether a field is immutable or not; however, in some cases, things are not what they seem to be. For example, lazily initialized fields look mutable, but they are effectively immutable. Bathyscaphe does not attempt to analyze bytecode and detect how a certain field is used; that kind of detective work belongs to the realm of static analysis tools. In such cases, it is necessary to guide Bathyscaphe by using annotations to mark fields that might look mutable but should be considered as immutable.\nThese annotations are essentially claims made by the programmer: Bathyscaphe does not, and cannot, verify the truthfulness of these claims. In this sense, Bathyscaphe does not provide a 100% fool-proof solution, because the programmer may code these annotations wrongly. In the future some synergy between Bathyscaphe and static analysis tools might be achieved, so as to provide 100% fool-proof results, but the benefit of using Bathyscaphe now lies in the fact that given correct annotations, Bathyscaphe will yield correct and usable results in all cases, whereas static analysis does not work in all cases and by its nature cannot work in all cases.\nWhere to find Bathyscaphe Bathyscaphe is hosted on GitHub; see https://github.com/mikenakis/Bathyscaphe\nAppendix: Goals of Bathyscaphe I decided to write my own immutability assessment facility with the following goals in mind:\nI want to be able to write framework-level code such as the following: A hash-map which asserts that any and all keys added to it are immutable. A message-passing framework which asserts that every single message that it is asked to deliver is either immutable or at the very least thread-safe. I want results that are always accurate, meaning that there must be no false positives or false negatives, no compromises, no \u0026quot;aiming to cover the majority of use cases\u0026quot;. All use cases should be covered, and they should be covered correctly. I want to assess the immutability of objects, not classes, because I have observed that from a certain class we can sometimes construct instances that are mutable, and sometimes construct instances that are immutable. For example, both of the following method calls yield instances of the exact same class, and yet one instance is mutable, while the other instance is immutable: List.of( 1 ) (immutable) List.of( new StringBuilder() ) (mutable) I want to assess the immutability of the entire graph of objects referenced by a certain object, not the immutability of that object alone. In other words, I want deep immutability assessment, as opposed to shallow or superficial immutability assessment. When assessment cannot be achieved in an entirely automatic fashion, (as the case is, for example, with classes that perform lazy initialization,) I want to be able to achieve it by either: adding special annotations to certain fields, or adding a manual preassessment (assessment override) for that specific class. I want the immutability assessment facility to account for freezable classes. This necessitates the introduction of a special self-assessment interface, so that instances can be asked whether they are immutable or not. When an immutability assertion fails, meaning that an object which I had intended to be immutable has been found to actually be mutable, I want to receive extensive diagnostics in human-readable form, explaining precisely why this happened. I want the immutability assessment library which achieves all this to be attractive to programmers, by being: very easy to integrate very easy to use very small having no dependencies. Appendix: Non-goals of Bathyscaphe Predicting what code will do. That is the job of static analysis tools. Bathyscaphe is meant to issue accurate and useful assessments assuming correctly annotated classes. The correctness of the annotations is a lesser, and largely different problem, which is suitable as the focus of static analysis tools. Dealing with untrustworthy classes. Immutability can always be compromised via reflection, so trying to assess immutability in an environment which is not completely trustworthy is a hopeless endeavor. Therefore, assessment is to be done on a full-trust basis. Dealing with buggy classes. If a class promises, either by means of annotations or the self-assessment interface, that it will behave immutably, but in fact it does not, the fault is with that class, not with the immutability assessment facility. Dealing with inaccessible classes. Due to security restrictions, the inner workings of certain JDK classes are inaccessible. Since every single one of those classes can receive a manual preassessment, this is not an issue. Dealing with farce. If we create a subclass of a mutable class and override each mutation method to always throw an exception, do we have a mutable or immutable class in our hands? Some say it is mutable; others say it is immutable; I say it is a farce, and not worth considering. Performance. Immutability assessment can be computationally expensive, but it is only meant to be performed through assertions, so its overhead is to be suffered only on development runs. On production runs, where assertions are supposed to be disabled, the performance penalty of using Bathyscaphe is to be zero. Therefore, performance is not an issue. Non-assertive assessment. Non-assertive assessment means yielding an assessment result object which can then be further examined, as opposed to assertive assessment which means either passing the immutability check or throwing an exception. Non-assertive assessment would require publicly exposing the entire assessment hierarchy of Bathyscaphe. One that is done, people would inevitably start writing code that makes use of it, and from that moment on it would be impossible for me to continue refactoring and evolving Bathyscaphe without breaking all that code. Therefore, non-assertive assessment is not a goal. Static analysis. While it is indeed possible in many cases to conclusively assess a class as mutable or immutable by just looking at the class, in many other cases (and certainly in all interesting cases) examining the class is not enough, as the example of List.of( 1 ) vs. List.of( new StringBuilder()) demonstrates. Thus, the use of Bathyscaphe as a static analysis tool is not a goal. If you need a static immutability analysis tool for Java, please see MutabilityDetector on GitHub: https://github.com/MutabilityDetector Appendix: A note on reference types If you decide to incorporate Bathyscaphe in a project, the first thing you are likely to do is what I did: introduce your own HashMap class which asserts that every key added to it is immutable. In doing so you might discover some bugs in your code, but you will also notice something seemingly strange: Bathyscaphe is preventing you from using reference types as keys, which kind of makes sense because they are in fact mutable, but you have never had any issues with that before, so why is it becoming a problem now?\nWhat is happening is that your reference types refrain from overriding equals() and hashCode(), so they inherit the reference-equals function and the identity hash-code function from Object. A reference to an object remains the same regardless of mutations that the object undergoes during its lifetime, and the same holds true for its identity hash-code. This has been allowing you to use reference types as keys in in hash maps, despite the fact that they undergo mutations, but it has only been working by coincidence.\nAnother word for \u0026quot;coincidence\u0026quot; is \u0026quot;accident\u0026quot;, and Bathyscaphe is meant to be used precisely in order to avoid accidents, so you cannot keep doing this anymore. From now on, you will have to be using IdentityHashMap for reference types, and HashMap for value types.\nAppendix: A note on so-called immutable collections When Java 9 introduced the new java.util.List.of() method and its various overloads, the documentation referred to the objects returned by that method as immutable lists. Specifically, in the Java 9 API docs we read \u0026quot;Returns an immutable list containing one element.\u0026quot; Later, the Java people realized that this is inaccurate, so in JDK issue 8191517 they decided among other things to \u0026quot;Adjust terminology to prefer 'unmodifiable' over 'immutable'.\u0026quot; Thus, if we look at the documentation today, (for example, in the Java 18 API documentation,) it reads \u0026quot;Returns an unmodifiable list containing one element.\u0026quot;\nJava 9 API docs: https://docs.oracle.com/javase/9/docs/api/java/util/List.html JDK issue 8191517: https://bugs.openjdk.java.net/browse/JDK-8191517 Java 18 API docs: https://docs.oracle.com/en/java/javase/18/docs/api/java.base/java/util/List.html Dropping the word \u0026quot;immutable\u0026quot; was the right thing to do, because there is no such thing as an immutable collection, at least when type erasure is involved. That's because a collection contains elements, the immutability of which it is in no position to vouch for.\nUnfortunately, the term \u0026quot;unmodifiable\u0026quot; is also problematic for describing these collections, because that term already had a meaning before List.of() was introduced, and the meaning was \u0026quot;an unmodifiable-to-you view of my collection, which is still very mutable, and any mutations I make will be visible to you.\u0026quot;\nLuckily, List.of() does better than that: it returns a list that cannot be modified by anyone. So, I would rather call it \u0026quot;unchangeable\u0026quot; or \u0026quot;superficially immutable\u0026quot; to indicate that it falls short of achieving true immutability only in the sense that it cannot guarantee deep immutability.\nAppendix: A note on assessment overrides An assessment override on an effectively immutable class (for example, on a class which contains a lazily initialized field) is a drastic measure which should be used as seldom as possible. That's because an assessment override is also a blanket measure: it will prevent the immutability assessment facility from ascertaining the immutability of not only the lazily initialized field, but also of all other fields in the class, and in so doing it may hide errors. Assessment overrides should only be used on classes whose source code we do not control, and therefore we cannot annotate on a field-per-field basis.\nAppendix: Freezable classes As a rule, immutable objects tend to be immutable-upon-construction, meaning that any and all objects that they reference must be supplied as constructor parameters. There is, however, an exception: there is a category of objects called \u0026quot;freezable\u0026quot; which begin their life as mutable, (so that they can undergo complex initialization,) and are at some later moment instructed to transition to being immutable, that is, to \u0026quot;freeze\u0026quot;.\nFreezing happens in-place, it is permanent from the moment it is applied, and it is trivial to implement: all it takes is to set a frozen field to true. (A few more things are nice to have, for example assertions ensuring that no mutation methods are invoked once frozen, and splitting the interface of the object in two separate interfaces for mutable and immutable functionality respectively, so that once the object has been frozen, we can forget our reference to the mutable interface and only retain the immutable one.)\nFreezing is useful for performance: Creating a mutable object, initializing it, and then freezing it performs much better than creating a mutable object, initializing it, and then copying its contents into a freshly allocated immutable object. Freezing can achieve things that are otherwise hard, or impossible: The creation of immutable cyclic graphs requires objects to be mutable while the graph is being constructed, and to become immutable in-place once construction is complete. This problem cannot be solved using the builder pattern, because the builder is bound to run into the same problem: how to construct A with a reference to B when B must be constructed with a reference to A. To accommodate freezable classes, Bathyscaphe introduces the ImmutabilitySelfAssessable interface. If a class implements this interface, then Bathyscaphe will be invoking instances of this class to ask them whether they are immutable or not.\nCover image: the Bathyscaphe logo, a line drawing of bathyscaphe Trieste by michael.gr, based on art found at bertrandpiccard.com\n","date":"2022-04-19T10:54:08.44Z","permalink":"https://blog.michael.gr/post/2022-05-bathyscaphe/","title":"Bathyscaphe"},{"content":"Quite often, when I rename a maven module, IntellijIdea gets confused and keeps showing the old module name in the project view.\nRe-importing maven projects does not help. Clearing the IntellijIdea caches and restarting does not help. Even deleting the .idea directory does not help. I discovered that this is happening because:\nIntellijIdea keeps information about a project outside of the .idea directory IntellijIdea fails to delete that information when you invalidate caches and restart. The solution I have found to this problem is:\nExit IntellijIdea. Go to the application data directory of IntellijIdea (on my Windows machine, this currently is %USERPROFILE%\\AppData\\Local\\JetBrains\\IdeaIC2022.1) Delete the external_build_system subdirectory Start IntellijIdea. Re-import all maven projects. Update:\nShortly after I discovered this, a new release of IntelliJ IDEA came out, (IdeaIC2021.3) and there appears to be no \u0026quot;external_build_system\u0026quot; subdirectory in its directory. Miraculously, it makes use of the \u0026quot;external_build_system\u0026quot; subdirectory of the IdeaIC2021.1 directory!\nAlso, with this new version, when I apply the above steps, IntellijIdea for some reason starts to believe that I have chosen to \u0026quot;ignore\u0026quot; all of my pom.xml files. So, I have to open up maven settings, go to \u0026quot;Ignored Files\u0026quot;, and uncheck every single one of my pom.xml files.\n","date":"2022-04-17T12:10:53.601Z","permalink":"https://blog.michael.gr/post/2022-04-fixing-broken-module-names-in/","title":"Fixing broken module names in IntellijIdea "},{"content":"\rSome time ago and for about a year I used a Mac, which had natural scrolling by default. I decided to resist the urge to immediately configure it to work like Windows, and instead I made it a point to give natural scrolling a try for at least a while before making up my mind as to whether to keep it or lose it. While giving it a try, I was surprised to see that it was very easy for me to adjust to it, despite the fact that I have been using the unnatural Windows scrolling mode for nearly 30 years. (Ever since the mouse wheel became a thing.) I found that natural scrolling was indeed... natural. So, I kept using it, and I became addicted to it. Ever since then, I always have to configure natural scrolling on any Windows machine that I get my hands on before I can start using that machine.\nWindows is so poor that it does not offer any user interface through which a novice user can change the mouse wheel scrolling mode. To do that you have to edit the registry, and the setting you are going to be modifying is a machine setting, so you will be affecting the mouse wheel mode for all users, not just for yourself. This is unbelievably lame, but hey, that's Windows, we are totally used to lame.\nThere exists a procedure for finding a specific mouse device and modifying the wheel mode of only that device, but it is a hassle, and it does not even guarantee that you will be modifying the settings for the right device, so we will be skipping that procedure and modifying the setting for all mouse devices that are currently connected or have ever been connected. If a new mouse is connected to the system, the procedure will need to be repeated.\nLuckily, I do not have any contractual obligations to give warnings regarding the alleged dangers of modifying the registry; so, without further ado, here is what you need to do in order to configure natural scrolling under Windows:\nOpen up RegEdit, and navigate to the following key: Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Enum\\HID\nSearch for values named FlipFlopWheel For each value you find, if the data is 0, change the data to 1. You are done when Find Next (F3) starts yielding results that are outside of the above key. (i.e. when you reach Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\DriverDatabase.) Restart Windows. (Logging out and back in will not work, because remember, this is a Windows-wide setting, not a per-user setting.) Cover image: Artwork by michael.gr based on original mouse-15887 by Arkthus from the Noun Project\n","date":"2022-03-16T09:55:12.728Z","permalink":"https://blog.michael.gr/post/2022-03-mouse-natural-scrolling/","title":"Mouse Wheel Natural Scrolling in Windows"},{"content":"\rOne of our customers tried installing our Windows Application on one of their computers and their antivirus software complained. They in turn complained to us, so we had to find a solution. I am documenting the solution here.\n(Useful pre-reading: About these papers)\nPossible Causes Antivirus engines may find a certain file suspicious for many different reasons. The most obvious reason is the file containing a byte pattern which has been encountered in some known malware. Unfortunately, this does not necessarily mean that the file is infected; it may simply be a coincidence. Furthermore, many other checks performed by antivirus engines tend to fall well within the spectrum of paranoia. It appears that some particularly paranoid antivirus engines will flag an executable as malware simply because it has not been code-signed or strong-name-signed, or because it has undergone obfuscation. Our application checked both of those boxes. Your mileage may vary.\nHow to Troubleshoot The first problem that you are likely to run into when troubleshooting issues of this kind is how to tell whether your application is passing antivirus checks or not. Obviously, the antivirus suite at the company that I work for was not finding anything suspicious with our application, or else we would have certainly noticed; however, a couple of antivirus suites that our client was running were taking issue with it. I did not have access to the antivirus suites that our client had, and even of I could somehow get my hands on them, I would not be able to install them on my machine, because the antivirus suite which had already been installed on my machine by our IT department played god and did not allow me to touch it.\nLuckily, there is a website that provides a solution to the problem. It is called virustotal.com, it aggregates a large number of antivirus engines, and it allows you to upload an executable and see what all these engines think of it. It is quite famous in the security community.\nUnfortunately, checking an application with virustotal.com is not an exact science, for many reasons.\nFor one thing, virustotal.com said that out of more than 60 antivirus engines that they are aggregating, only four of them took issue with our application. As far as I am concerned, this is a pretty good indication that our application is fine, and these four engines are just being paranoid, but try telling that to the customer.\nComplicating matters even worse was the fact that the same antivirus engine on virustotal.com that was being used by the antivirus suite that we were using at the company I work for was in fact flagging our application as suspicious, whereas the antivirus engines on virustotal.com that belong to the corresponding suites that our customer was using did not take issue with it, so go figure.\nNonetheless, it stands to reason that if you manage to produce your application in such a way that none of the antivirus engines on virustotal.com take issue with it, then chances are that the antivirus suites of customers will also not take issue with it. Even if this turns out to not be true, you will at the very least be able to tell your customer to go to virustotal.com and see for themselves and then just trust you.\nAnother interesting thing about virustotal.com is that it does not seem to care very much about DLLs used by an application. There is of course a chance that I have not quite understood how the whole thing works yet, but for now it seems like I could pretty much run any malicious code I want out there, as long as I package that code in a DLL which is loaded by an otherwise clean and innocent executable which passes the antivirus checks. More about this as I understand more about how things work.\nStrong-name Signing The first approach that I tried in the direction of solving the problem involved what is known as \u0026quot;strong-name signing\u0026quot;. The problem with this approach is that it requires that every single DLL that is loaded by the application must also be strong-name signed. From a security stand-point this is of course absolutely correct, but here is the problem: even though we can see to it that all DLLs produced by us are also strong-name signed, we are additionally using many DLLs produced by third parties, over which we have no control, and which are not strong-name signed.\nThere appears to be some solution out there that will automatically strong-name sign DLLs used by an application, but it works like magic, and of course, as the case usually is with magic, it miserably fails with certain DLLs that use some loading strategy that is off the beaten path, for example with the MathNet Numerics library, and possibly with others.\nI tried hard to get strong-name signing to work, but eventually I gave up.\nCode Signing The next approach that I tried was just plain \u0026quot;signing\u0026quot;, or \u0026quot;code signing\u0026quot;, as soon as I realized that it is not the same thing as \u0026quot;strong-name signing\u0026quot;.\nCode signing did not completely solve the problem, but at least it took us halfway there. With code signing we were able to reduce the number of antivirus engines that take issue with our application from four to two. As it turns out, the remaining two would only be appeased if we disabled the code obfuscation step in our build. I would like to take the opportunity to point out that code obfuscation is a perfectly legitimate technique, and that antivirus engines that take issue with it are junk.\nA secure and proper application of code-signing involves obtaining a signing certificate from a certification authority, at a cost of a few hundred Euros per year. Of course, when you are researching or troubleshooting the matter you are likely to skip that expense, and self-sign your files, until you are sure that this is the way you want to go. This is absolutely not secure, because there is no chain of authority to guarantee the authenticity of your certificate. It is worth pointing out that the two antivirus engines on virustotal.com that were appeased when they saw a code-signed version of our application did not bother checking the authenticity of our certificate! The self-signed certificate was enough. So, these two antivirus engines are junk, too.\nWhat follows is all the voodoo magic that is necessary to get signing, and specifically self-signing, to work:\nThe original source of information is this stackoverflow question and accepted answer: https://stackoverflow.com/questions/84847/how-do-i-create-a-self-signed-certificate-for-code-signing-on-windows\n(Ignore the claim that makecert is deprecated, and the suggestion to use some Powershell command instead. Makecert works just fine, and if you have VisualStudio then you already have it, which is not the case with the recommended powershell command.)\nFirst, we need to create what is known as a \u0026quot;self-signed certificate authority (CA)\u0026quot;, using the following command:\nmakecert -r -pe -n \u0026#34;CN=My CA\u0026#34; -ss CA -sr CurrentUser -a sha512 -cy authority -sky signature -sv MyCA.pvk MyCA.cer This will create MyCA.pvk and MyCA.cer in the current directory.\nAlthough makecert is a command-line utility, it will pop up a graphical dialog asking us to create a password.\nYou can just hit [Enter] to use no password, or you can create a password of your choice, but be sure to copy it to the clipboard, because you are going to need it a lot in the following steps, when you are going to be seeing dialogs like this:\nThen, we need to do what is known as \u0026quot;Importing the CA certificate\u0026quot;, using the following command:\ncertutil -user -addstore Root MyCA.cer This will pop up a big dialog saying that it does not know where this certificate came from, and it cannot verify its authenticity, (of course, since we just created the Certification Authority out of thin air,) and asking us whether we are sure that we want to proceed installing it.\nJust answer \u0026quot;yes\u0026quot;.\nThen, we need to create what is known as a \u0026quot;Code-signing Certificate\u0026quot; and is for some reason referred to by the acronym \u0026quot;SPC\u0026quot;, using the following command:\nmakecert -pe -n \u0026#34;CN=My SPC\u0026#34; -a sha512 -cy end -sky signature -ic MyCA.cer -iv MyCA.pvk -sv MySPC.pvk MySPC.cer Then, we need to convert the generated certificate and key into a PFX file, using the following command:\npvk2pfx -pvk MySPC.pvk -spc MySPC.cer -pfx MySPC.pfx The above steps only have to be carried out once. From the moment that we have that MySPC.pfx file, we can keep using it to sign any number of files, as often as we want, using the following command:\nsigntool sign /v /t http://timestamp.comodoca.com/authenticode /f MySPC.pfx myexecutable.exe The /t parameter followed by a URL allows us to timestamp the signature. A timestamped signature is required for performing a full authenticity check, but as I have already explained, the antivirus engines at virustotal.com do not seem to perform even a rudimentary authenticity check, so the timestamp is probably unnecessary in our case. Nonetheless, I have included it for the sake of completeness. There are various timestamp services out there that you can use, and you can find a list of them in the original stackoverflow article, however the available services inevitably change over time. I tried the globalsign.com URL but it appeared dead, so then I tried the comodoca.com URL and it worked, so comodoca.com it shall be, at least for now.\nVerifying the results Using the Windows File Explorer to look at the properties of the signed executable, we see that there is now a new property tab called \u0026quot;Digital Signatures\u0026quot;:\nViewing \u0026quot;Details\u0026quot; on the signature yields this property page:\nAnd clicking \u0026quot;View Certificate\u0026quot; yields this dialog:\nOf course, Windows is finding the digital signature to be \u0026quot;OK\u0026quot; only because I have installed this self-created and fictitious \u0026quot;My CA\u0026quot; Certification Authority. On a customer's computer it will most probably not show \u0026quot;OK\u0026quot; unless you purchase a certificate from a valid Certification Authority, but for the purpose of passing the silly virus checks on virustotal.com, it is okay.\nConclusion After self-signing my executable, (and disabling obfuscation,) I uploaded it to virustotal.com, and it passed the checks of all antivirus engines.\nThere is a chance that the antivirus solution on our client will perform a complete authenticity check, which will fail due to self-signing, in which case we will have to purchase a real certificate.\nThere is also a chance that individual DLLs used by our application will not pass this test, which will necessitate signing each one of them, using the same command.\nCover image: \u0026quot;Glass shield anti virus\u0026quot; from vecteezy.com.\n","date":"2022-03-05T15:32:00.678Z","permalink":"https://blog.michael.gr/post/2022-03-getting-antivirus-software-to-trust/","title":"Getting AntiVirus software to trust a Windows Application"},{"content":"\rConsistently every year for many years now, the Dutch have been ranking #1 in the world in English-as-a-foreign-language proficiency, according to yearly reports by the Education First (EF) organization. (Wikipedia.) I can attest that at least in the Randstad area, almost everyone speaks English, and many do so very fluently.\nNonetheless, there are certain mistakes that the Dutch are somewhat prone to make in English, due to interference from peculiarities of the Dutch language. When this happens, it is called Dutchlish.\nHere is a collection of examples of Dutchlish that I have collected over the course of several years of living in The Netherlands.\nI will learn you how to skate. Learn instead of teach\nFrom Dutch leren, which means either to learn or to teach, depending on context.\nWhen you want, we also have it in white. When instead of if\nApparently because mixing als (=if) with waneer (=when) is also a common mistake in Dutch.\nLet's meet at sex. Sex instead of six\nFrom Dutch zes, which means six. This is not really a common mistake, but I have personally heard it, and I think it is funny enough to deserve including. (Freudian slip? LOL!)\nDear colleagues, hereby the schedule. Hereby instead of Here is or Hereby I give you.\nBecause in Dutch, herebij (which means the same as \u0026quot;hereby\u0026quot;, and is pronounced very similarly,) can in fact be used alone like that. However, in English, it can't.\nWe have lot's of bicycle's In the written word, using apostrophe-'s' to signify plural, whereas in English plural is signified with -'s' alone, and apostrophe-'s' is only used to signify genitive (possessive) case.\nBecause in the Dutch language, that is in fact how you signify plural. Different language, different rules. As a result, the Dutchies often confuse the Dutch way of writing plural with the English way of writing plural.\nI will meet you at the busstop In the written word, concatenating words that are not normally concatenated in English.\nIn Dutch it is very common to concatenate words to form new words. That is, for example, how come the Dutch word for Multiple personality disorder is Meervoudigepersoonlijkheidsstoornis. So, to the Dutch, the term bus stop as two separate words looks unnatura; when they see this, they are seized with an overwhelming desire to concatenate the two words into one: busstop. However, this is not a valid thing to do in English.\nI swear I can sometimes hear them using concatenated terms like \u0026quot;busstop\u0026quot; even in the spoken word. (Or maybe I am just imagining it?)\nLet's have a telco Telco instead of teleconference.\nThe Dutch words for telephone and conference are telefoon and conferentie, so the Dutch have every right to create the word telco in their language. The mistake that they sometimes make is using this word when speaking English, because to English speakers, telco generally does not mean anything, and if it was to mean something, that would perhaps be telecommunications company.\nTurn on the airco Airco instead of air-conditioner.\nJudging by the fact that the Dutch word for air sounds nothing like air, (it is lucht,) the word airco is probably the result of the Dutch trying to forge a word that was intended to sound English. However, for English-speakers, airco does not mean anything, and if it was to mean something, that would perhaps be airline company. Apparently, the Dutch do not make the connection between \u0026quot;co\u0026quot; and \u0026quot;company\u0026quot;, perhaps because in their language the word for company sounds nothing like company; it is bedrijf.\nEstimate repair laptop In titles, word ordering that makes no sense in English.\nThe Dutch apparently have certain special rules dictating how to write signs, titles, and e-mail subjects: the sentence is first written in its fully spelled out form, and then all the small words are removed. So, the fully spelled-out sentence \u0026quot;the estimate for the repair of the laptop\u0026quot; is \u0026quot;de schatting voor de reparatie van de laptop\u0026quot;, and then the title \u0026quot;Laptop repair estimate\u0026quot; is formed by removing all the small words to yield \u0026quot;Schatting reparatie laptop\u0026quot;. Unfortunately, the Dutch are prone to follow the same rule when translating to English, yielding \u0026quot;Estimate repair laptop\u0026quot; (or \u0026quot;Estimate laptop repair\u0026quot; at best) which either makes no sense or means something different.\nMy favorite example of this is a sign that you can see in every single train in the entire little Kingdom of The Netherlands, which urges the passengers to refrain from obstructing the emergency exit of the train driver. The Dutch text says \u0026quot;Nooduitgang machinist vrijhouden\u0026quot; and the English translation says \u0026quot;Emergency exit train driver, do not block\u0026quot;. (What?)\nA sign in Dutch trains, with Dutch text and Dutchlish translation. Note how the sentence would be valid not only in Dutch but also in English if the small words had not been omitted:\nNooduitgang (van de) machinist vrijhouden Emergency exit (of the) train driver, do not block. How long are you? Long instead of tall.\nThe Dutch word for high is hoog, and they have no unnecessary extra word like tall. When speaking of a person's height, they do not use the word hoog; but lacking a word like tall, they use the word lang instead, which means long. So, when a Dutch person wants to ask you how tall you are in English, they may inadvertently ask you how long you are instead. (Another Freudian slip? LOL!)\nLet's make a photo Make instead of take for photos.\nFrom Dutch foto maken.\nCan I lend your umbrella? Lend instead of borrow.\nFrom Dutch lenen which means either to lend or to borrow, depending on context.\nYou want just the bread? Bread instead of Hamburger.\nNo, the McDonald's employee is not seriously considering that you might want nothing but a plain piece of bread; what they mean to ask instead is whether you want the hamburger alone, or the entire combo consisting of the hamburger plus fries plus drink. They say \u0026quot;bread\u0026quot; because in their head they are translating word for word from Dutch, where every edible item that is served on bread is referred to as a broodje, which means bread.\nFodafone In the spoken word, pronouncing the letter V as if it was an F.\nDutch pronunciation rules tend to cause some weird things to happen; for example, when you call Vodafone in The Netherlands, the recorded greeting welcomes you to \u0026quot;Fodafone\u0026quot;. Here I have recorded two such greetings, one from sales, one from billing:\nThis is due to some weirdness in how the letter V sounds in the Dutch language. A similar weirdness also exists in German; if you are an English speaker, chances are you have always pronounced Volkswagen wrongly; the correct pronunciation is folks-vaa-gen.\nHowever, the use of Dutch pronunciation rules is questionable when it comes to the names of companies originating outside of The Netherlands, such as Vodafone, which hails from the UK.\nTax for Taxi Using the word \u0026quot;tax\u0026quot; in reference to \u0026quot;taxi\u0026quot;, which does not quite work in English.\nNaturally, taxi firms all over the world tend to want to do their best to communicate in English, because their clients are often tourists, and the international language of tourism is English. It is surprising then, that many taxi companies in The Netherlands use \u0026quot;tax\u0026quot; to mean \u0026quot;taxi\u0026quot;.\nHere is an indicative list of Dutch domain names that belong to taxi firms: my-tax.nl; atax.nl; btax.nl; t-tax.nl; v-tax.nl ahrotax.nl; cosi-tax.nl; ataxeindhoven.nl; eagletax.nl; star-tax.nl; and my personal favorite, city-tax.nl. The list is practically endless. Apparently, many Dutchies are under the impression that the word \u0026quot;tax\u0026quot; evokes the notion of \u0026quot;taxi\u0026quot; in English, but that is not so; in the English-speaking world, all these domain names would belong to accounting firms. (And fun fact: in Europe, another name for \u0026quot;city tax\u0026quot; is \u0026quot;tourist tax\u0026quot;; probably not an association you want to be making if you are a taxi service.)\nI used to think that perhaps this is happening because the Dutch word for tax sounds nothing like \u0026quot;tax\u0026quot;, it is belasting, so maybe they do not associate \u0026quot;tax\u0026quot; with taxation, but according to the Dutch dictionary, the word tax also exists in the language, and is indeed a financial term, and is in fact related to taxation. So, how come the Dutch feel free to use \u0026quot;tax\u0026quot; to stand for taxi, is beyond me.\nAnother fun fact: The word taxi is a shortening of the word taxicab. The word taxicab is a contraction of the term taximeter cabriolet. Cabriolet is a type of light, horse-drawn carriage. Taximeter is a blend of Greek metron (measure) and Latin taxa, which means fee, or charge, or charging a fee. So, albeit very remotely, \u0026quot;taxi\u0026quot; is in fact related to \u0026quot;tax\u0026quot; in English. The Dutch are not entirely wrong on this one, after all!\nCover image: \u0026quot;Dutch Tongue\u0026quot; by michael.gr, based on the logo of The Rolling Stones and the Flag of the Kingdom of the Netherlands.\n","date":"2022-01-13T12:17:42.188Z","permalink":"https://blog.michael.gr/post/2022-01-dutchlish/","title":"Common mistakes Dutchies make in the use of English"},{"content":"\rAbstract A new method for Automated Software Testing is presented as an alternative to Unit Testing. The new method retains the benefit of Unit Testing, which is Defect Localization, but eliminates white-box testing and mocking, thus greatly lessening the effort of writing and maintaining tests.\n(Useful pre-reading: About these papers)\nSummary Unit Testing aims to achieve Defect Localization by replacing the collaborators of the Component Under Test with Mocks. As we will show, the use of Mocks is laborious, complicated, over-specified, presumptuous, and constitutes testing against the implementation, not against the interface, thus leading to brittle tests that hinder refactoring rather than facilitating it.\nTo avoid these problems, Incremental Integration Testing allows each component to be tested in integration with its collaborators, (or with Fakes thereof,) thus completely abolishing Mocks. Defect Localization is achieved by arranging the order in which tests are executed so that the collaborators of a component get tested before the component gets tested, and stopping as soon as a defect is encountered.\nThus, when a test discovers a defect, we can be sufficiently confident that the defect lies in the component being tested, and not in any of its collaborators, because by that time, the collaborators have passed their tests.\nThe problem The goal of automated software testing in general, regardless of what kind of testing it is, is to exercise a software system under various usage scenarios to ensure that it meets its requirements and that it is free from defects. The most simple and straightforward way to achieve this is to set up some input, invoke the system to perform a certain job, and then examine the output to ensure that it is what it is expected to be.\nUnfortunately, this approach only really works in the \u0026quot;sunny day\u0026quot; scenario: if no defects are discovered by the tests, then everything is fine; however, if defects are discovered, we are faced with a problem: the system consists of a large network of collaborating software components, and the test is telling us that there is a defect somewhere, but it is unclear in which component the problem lies. Even if we divide the system into subsystems and try to test each subsystem separately, each subsystem may still consist of many components, so the problem remains.\nWhat it ultimately boils down to is that each time we test a component, and a defect is discovered, it is unclear whether the defect lies in the component being tested, or in one or more of its collaborators.\nIdeally, we would like each test to be conducted in such a way as to detect defects specifically in the component that is being tested, instead of extraneous defects in its collaborators; in other words, we would like to achieve Defect Localization.\nThe existing solution: Unit Testing Unit Testing was invented specifically in order to achieve defect localization. It takes an extremely drastic approach: if the use of collaborators introduces uncertainties, one way to eliminate those uncertainties is to eliminate the collaborators. Thus, Unit Testing aims to test each component in strict isolation. Hence, its name.\nTo achieve this remarkably ambitious goal, Unit Testing refrains from supplying the component under test with the actual collaborators that it would normally receive in a production environment; instead, it supplies the component under test with specially crafted substitutes of its collaborators, otherwise known as test doubles. There exist a few different kinds of substitutes, but by far the most widely used kind is Mocks.\nEach Mock must be hand-written for every individual test that is performed; it exposes the same interface as the real collaborator that it substitutes, and it expects specific methods of that interface to be invoked by the component-under-test, with specific argument values, sometimes even in a specific order of invocation. If anything goes wrong, such as an unexpected method being invoked, an expected method not being invoked, or a parameter having an unexpected value, the Mock fails the test. When the component-under-test invokes one of the methods that the Mock expects to be invoked, the Mock does nothing of the sort that the real collaborator would do; instead, the Mock is hard-coded to yield a fabricated response which is intended to exactly match the response that the real collaborator would have produced if it was being used, and if it was working exactly according to its specification.\nOr at least, that is the intention.\nDrawbacks of Unit Testing Complex and laborious In each test it is not enough to simply set up the input, invoke the component, and examine the output; we also have to anticipate every single call that the component will make to its collaborators, and for each call we have to set up a mock, expecting specific parameter values, and producing a specific response aiming to emulate the real collaborator under the same circumstances. Luckily, mocking frameworks lessen the amount of code necessary to accomplish this, but no matter how terse the mocking code is, the fact still remains that it implements a substantial amount of functionality which represents considerable complexity. One of the well-known caveats of software testing at large (regardless of what kind of testing it is) is that a test failure does not necessarily indicate a defect in the production code; it always indicates a defect either in the production code, or in the test itself. The only way to know is to troubleshoot. Thus, the more code we put in tests, and the more complex this code is, the more time we end up wasting in chasing and fixing bugs in the tests themselves rather than in the code that they are meant to test. Over-specified Unit Testing is concerned not only with what a component accomplishes, but also with every little detail about how the component goes on about accomplishing it. This means that when we engage in Unit Testing we are essentially expressing all of our application logic twice: once with production code expressing the logic in imperative mode, and once more with testing code expressing the same logic in expectational mode. In both cases, we write copious amounts of code describing what should happen in excruciatingly meticulous detail. Note that with Unit Testing, over-specification might not even be goal in and of itself in some cases, but it is unavoidable in all cases. This is due to the elimination of the collaborators: the requests that the component under test sends to its collaborators could conceivably be routed into a black hole and ignored, but in order for the component under test to continue working so as to be tested, it still needs to receive a meaningful response to each request; thus, the test has to expect each request in order to produce each needed response, even if the intention of the test was not to know how, or even whether, the request is made. Presumptuous Each Unit Test claims to have detailed knowledge of not only how the component-under-test invokes its collaborators, but also how each real collaborator would respond to each invocation in a production environment, which is a highly presumptuous thing to do. Such presumptuousness might be okay if we are building high-criticality software, where each collaborator is likely to have requirements and specification that are well-defined and unlikely to change; however, in all other software, which is regular, commercial, non-high-criticality software, things are a lot less strict: not only the requirements and specifications change all the time, but also quite often, the requirements, the specification, even the documentation, is the code itself, and the code changes every time a new commit is made to the source code repository. This might not be ideal, but it is pragmatic, and it is established practice. Thus, the only way to know exactly how a component behaves tends to be to actually invoke the latest version of that component and see how it responds, while the mechanism which ensures that these responses are what they are supposed to be is the tests of that component itself, which are unrelated to the tests of components that depend on it. As a result of this, Unit Testing often places us in the all too familiar situation where our Unit Tests all pass with flying colors, but our Integration Tests miserably fail because the behavior of the real collaborators turns out to be different from what the mocks assumed it would be. Fragile During Unit Testing, if the interactions between the component under test and its collaborators deviate even slightly from our expectations, the test fails. However, these interactions may legitimately change as software evolves. This may happen due to the application of a bug-fix, due to refactoring, or due to the fact that whenever new code is added to implement new functionality, preexisting code must almost always be modified to accommodate the new code. With Unit Testing, every time we change the inner workings of production code, we have to go fixing all related tests to expect the new inner workings of that code. The original promise of Automated Software Testing was to enable us to continuously evolve software without fear of breaking it. The idea is that whenever you make a modification to the software, you can re-run the tests to ensure that everything still works as before. With Unit Testing this does not work, because every time you change the slightest thing in the production code you have to also change the tests, and you have to do this even for changes that are only internal. The understanding is growing within the software engineering community that Unit Testing with mocks actually hinders refactoring instead of facilitating it. Non-reusable Unit Testing exercises the implementation of a component rather than its interface. As such, the Unit Test of a certain component can only be used to test that component and nothing else. Thus, with Unit Testing the following things are impossible: Completely rewrite a piece of production code and then reuse the old tests to make sure that the new implementation works exactly as the old one did. Reuse the same test to test multiple different components that implement the same interface. Use a single test to test multiple different implementations of a certain component, created by independently working development teams taking different approaches to solving the same problem. The above disadvantages of Unit Testing are direct consequences of the fact that it is White-Box Testing by nature. What we need to be doing instead is Black-Box testing, which means that Unit Testing should be avoided, despite the entire Software Industry's addiction to it.\nNote that I am not the only one to voice dissatisfaction with Unit Testing with Mocks. People have been noticing that although tests are intended to facilitate refactoring by ensuring that the code still works after refactoring, tests often end up hindering refactoring, because they are so tied to the implementation that you can't refactor anything without breaking the tests. This problem has been identified by renowned personalities such as Martin Fowler and Ian Cooper, and even by Ken Beck, the inventor of Test-Driven Development (TDD).\nIn the video Thoughtworks - TW Hangouts: Is TDD dead? (youtube) at 21':10'' Kent Beck says \u0026quot;My personal practice is I mock almost nothing\u0026quot; and at 23':56'' Martin Fowler says \u0026quot;I'm with Kent, I hardly ever use mocks\u0026quot;.\nIn the Fragile Test section of his book xUnit Test Patterns: Refactoring Test Code (xunitpatterns.com) author Gerard Meszaros states that extensive use of Mock Objects causes overcoupled tests.\nIn his presentation TDD, where did it all go wrong? (InfoQ, YouTube) at 49':32'' Ian Cooper says \u0026quot;I argue quite heavily against mocks because they are overspecified.\u0026quot;\nNote that in an attempt to avoid sounding too blasphemous, none of these people calls for the complete abolition of mocks, they only warn against the excessive use of mocks. Furthermore, do not seem to be isolating the components under test, and yet they seem to have little, if anything, to say about any alternative means of achieving defect localization.\nA new solution: Incremental Integration Testing If we were to abandon Unit Testing with mocks, then one might ask what should we be doing instead. Obviously, we must somehow continue testing our software, and it would be nice if we can continue to be enjoying the benefits of defect localization.\nAs it turns out, eliminating the collaborators is just one way of achieving defect localization; another, more pragmatic approach is as follows:\nAllow each component to be tested in integration with its collaborators, but only after each of the collaborators has undergone its own testing, and has successfully passed it.\nThus, any observed malfunction can be attributed with a high level of confidence to the component being tested, and not to any of its collaborators, because the collaborators have already been tested.\nI call this Incremental Integration Testing.\nAn alternative way of arriving at the idea of Incremental Integration Testing begins with the philosophical observation that strictly speaking, there is no such thing as a Unit Test; there always exist collaborators which by established practice we never mock and invariably integrate in Unit Tests without blinking an eye; these are, for example:\nMany of the external libraries that we use. Most of the functionality provided by the Runtime Environment in which our software runs. Virtually all of the functionality provided by the Runtime Library of the language we are using. Nobody mocks standard collections such as array-lists, linked-lists, hash-sets, and hash-maps; very few people bother with mocking filesystems; nobody would mock a math library, a serialization library, and the like; even if one was so paranoid as to mock those, at the extreme end, nobody mocks the MUL and DIV instructions of the CPU; so clearly, there are always some things that we take for granted, and we allow ourselves the luxury of taking these things for granted because we believe that they have been sufficiently tested by their respective creators and can be reasonably assumed to be free of defects.\nSo, why not also take our own creations for granted once we have tested them? Are we testing them sufficiently or not?\nPrior Art An internet search for \u0026quot;Incremental Integration Testing\u0026quot; does yield some results. An examination of those results reveals that they refer to some strategy for integration testing which is meant to be performed manually by human testers, constitutes an alternative to big-bang integration testing, and requires full Unit Testing of the traditional kind to have already taken place. I am hereby appropriating this term, so from now on it shall mean what I intend it to mean. If a context ever arises where disambiguation is needed, the terms \u0026quot;automated\u0026quot; vs. \u0026quot;manual\u0026quot; can be used.\nThe first hints to Incremental Integration Testing can actually be found in the classic 1979 book The Art of Software Testing by Glenford Myers. In chapter 5 \u0026quot;Module (Unit) Testing\u0026quot; the author plants the seeds of what later became white-box testing with mocks by writing:\n[…] since module B calls module E, something must be present to receive control when B calls E. A stub module, a special module given the name \u0026quot;E\u0026quot; that must be coded to simulate the function of module E, accomplishes this.\nthen, the author proceeds to write:\nThe alternative approach is incremental testing. Rather than testing each module in isolation, the next module to be tested is first combined with the set of modules that have already been tested.\n(emphasis mine.)\nBack in 1979, Glen Myers envisioned these approaches to testing as being carried out by human testers, manually launching tests and receiving printouts of results to examine. He even envisioned employing multiple human testers to perform multiple tests in parallel. In the last several decades we have much better ways of doing all of that.\nImplementing the solution: the poor man's approach As explained earlier, Incremental Integration Testing requires that when we test a component, all of its collaborators must have already been tested. Thus, Incremental Integration Testing necessitates exercising control over the order in which tests are executed.\nMost testing frameworks execute tests in alphanumeric order, so if we want to change the order of execution all we have to do is to appropriately name the tests, and the directories in which they reside.\nFor example:\nLet us suppose that we have the following modules:\ncom.acme.alpha_depends_on_bravo\ncom.acme.bravo_depends_on_nothing\ncom.acme.charlie_depends_on_alpha\nNote how the modules are listed alphanumerically, but they are not listed in order of dependency.\nLet us also suppose that we have one test suite for each module. By default, the names of the test suites follow the names of the modules that they test, so again, a listing of the test suites in alphanumeric order does not match the order of dependency of the modules that they test:\ncom.acme.alpha_depends_on_bravo_tests\ncom.acme.bravo_depends_on_nothing_tests\ncom.acme.charlie_depends_on_alpha_tests\nTo achieve Incremental Integration Testing, we add a suitably chosen prefix to the name of each test suite, as follows:\ncom.acme.T02_alpha_depends_on_bravo_tests\ncom.acme.T01_bravo_depends_on_nothing_tests\ncom.acme.T03_charlie_depends_on_alpha_tests\nNote how the prefixes have been chosen in such a way as to establish a new alphanumerical order for the tests. Thus, an alphanumeric listing of the test suites now lists them in order of dependency of the modules that they test:\ncom.acme.T01_bravo_depends_on_nothing_tests\ncom.acme.T02_alpha_depends_on_bravo_tests\ncom.acme.T03_charlie_depends_on_alpha_tests\nAt this point Java developers might object that this is impossible, because in Java, the tests always go in the same module as the production code, directory names must match package names, and test package names always match production package names. Well, I have news for you: they don't have to. The practice of doing things this way is very widespread in the Java world, but there are no rules that require it: the tests do not in fact have to be in the same module, nor in the same package as the production code. The only inviolable rule is that directory names must match package names, but you can call your test packages whatever you like, and your test directories accordingly.\nJava developers tend to place tests in the same module as the production code simply because the tools (maven) have a built-in provision for this, without ever questioning whether there is any actual benefit in doing so. Spoiler: there isn't. As a matter of fact, in the DotNet world there is no such provision, and nobody complains. Furthermore, Java developers tend to place tests in the same package as the production code for no purpose other than to make package-private entities of their production code accessible from their tests, but this is testing against the implementation, not against the interface, and therefore, as I have already explained, it is misguided.\nSo, I know that this is a very hard thing to ask from most Java developers, but trust me, if you would only dare to take a tiny step off the beaten path, if you would for once do something in a certain way for reasons other than \u0026quot;everyone else does it this way\u0026quot;, you can very well do the renaming necessary to achieve Incremental Integration Testing.\nNow, admittedly, renaming tests in order to achieve a certain order of execution is not an ideal solution. It is awkward, it is thought-intensive since we have to figure out the right order of execution by ourselves, and it is error-prone because there is nothing to guarantee that we will get the order right. That's why I call it \u0026quot;the poor man's approach\u0026quot;. Let us now see how all of this could be automated.\nImplementing the solution: the automated approach Here is an algorithm to automate Incremental Integration Testing:\nBegin by building a model of the dependency graph of the entire software system. This requires system-wide static analysis to discover all components in our system, and all dependencies of each component. I did not say it was going to be easy. The graph should not include external dependencies, since they are presumed to have already been tested by their respective creators. Test each leaf node in the model. A leaf node in the dependency graph is a node which has no dependencies; at this level, a Unit Test is indistinguishable from an Integration Test, because there are no dependencies to either integrate or mock. If any malfunction is discovered during step 2, then stop as soon as step 2 is complete. If a certain component fails to pass its test, it is counter-productive to proceed with the tests of components that depend on it. Unit Testing seems to be completely oblivious to this little fact; Incremental Integration Testing fixes this. Remove the leaf nodes from the model of the dependency graph. Thus removing the nodes that were previously tested in step 2, and obtaining a new, smaller graph, where a different set of nodes are now the leaf nodes. The dependencies of the new set of leaf nodes have already been successfully tested, so they are of no interest anymore: they are as good as external dependencies now. Repeat starting from step 2, until there are no more nodes left in the model. Allowing each component to be tested in integration with its collaborators, since they have already been tested. No testing framework that I know of (JUnit, MSTest, etc.) is capable of doing the above; for this reason, I have developed a utility which I call Testana, that does exactly that.\nTestana will analyze a system to discover its structure, will analyze modules to discover dependencies and tests, and will run the tests in the right order so as to achieve Incremental Integration Testing. It will also do a few other nice things, like keep track of last successful test runs, and examine timestamps, so as to refrain from running tests whose dependencies have not changed since the last successful test run. For more information, see Testana: A better way of running tests.\nWhat if my dependencies are not discoverable? Some very trendy practices of our modern day and age include:\nUsing scripting languages, where there is no notion of types, and therefore no way of discovering dependencies via static analysis. Breaking up systems into disparate source code repositories, so there is no single system on which to perform system-wide static analysis to discover dependencies. Incorporating multiple different programming languages in a single system, (following the polyglot craze,) thus hindering system-wide static analysis, since it now needs to be performed on multiple languages and across language barriers. Making modules interoperate not via normal programmatic interfaces, but instead via various byzantine mechanisms such as REST, whose modus operandi is binding by name, thus making dependencies undiscoverable. If you are following any of the above trendy practices, then you cannot programmatically discover dependencies, so you have no way of automating Incremental Integration Testing, so you will have to manually specify the order in which your tests will run, and you will have to keep maintaining this order manually.\nSorry, but silly architectural choices do come with consequences.\nWhat about performance? One might argue that Incremental Integration Testing does not address one very important issue which is nicely taken care of by Unit Testing with Mocks, and that issue is performance:\nWhen collaborators are replaced with Mocks, the tests tend to be fast. When actual collaborators are integrated, such as file systems, relational database management systems, messaging queues, and what not, the tests can become very slow. To address the performance issue I recommend the use of Fakes, not Mocks. For an explanation of what Fakes are, and why they are incontestably preferable over Mocks, please read Testing with Fakes instead of Mocks.\nBy supplying a component under test with a Fake instead of a Mock we benefit from great performance, while utilizing a collaborator which has already been tested by its creators and can be reasonably assumed to be free of defects. In doing so, we continue to avoid White-Box Testing and we keep defects localized.\nFurthermore, nothing prevents us from having our CI/CD server run the test of each component twice:\nOnce in integration with Fakes Once in integration with the actual collaborators This will be slow, but CI/CD servers generally do not mind. The benefit of doing this is that it gives further guarantees that everything works as intended.\nBenefits of Incremental Integration Testing It greatly reduces the effort of writing and maintaining tests, by eliminating the need for mocking code in each test. It allows our tests to engage in Black-Box Testing instead of White-Box Testing. For an in-depth discussion of what is wrong with White-Box Testing, see White-Box vs. Black-Box Testing. It makes tests more effective and accurate, by eliminating assumptions about the behavior of the real collaborators. It simplifies our testing operations by eliminating the need for two separate testing phases, one for Unit Testing and one for Integration Testing. It is unobtrusive, since it does not dictate how to construct the tests, it only dictates the order in which the tests should be executed. Arguments and counter-arguments Argument: Incremental Integration Testing assumes that a component which has been tested is free of defects.\nA well-known caveat of software testing is that it cannot actually prove that software is free from defects, because it necessarily only checks for defects that we have anticipated and tested for. As Edsger W. Dijkstra famously put it, \u0026quot;program testing can be used to show the presence of bugs, but never to show their absence!\u0026quot;\nCounter-arguments:\nI am not claiming that once a component has been tested, it has been proven to be free from defects; all I am saying is that it can reasonably be assumed to be free from defects. Incremental Integration Testing is not meant to be a perfect solution; it is meant to be a pragmatic solution. The fact that testing cannot prove the absence of bugs does not mean that everything is futile in this vain world, and that we should abandon all hope in despair: testing might be imperfect, but it is what we can do, and it is in fact what we do, and practical, real-world observations show that it is quite effective. Most importantly: Any defects in an insufficiently tested component will not magically disappear if we mock that component in the tests of its dependents. In this sense, the practice of mocking collaborators can arguably be likened to Ostrich policy. On the contrary, continuing to integrate that component in subsequent tests gives us incrementally more opportunities to discover defects in it. Argument: Incremental Integration Testing fails to achieve complete defect localization.\nIf a certain component has defects which were not detected when it was being tested, these defects may cause tests of collaborators of that component to fail, in which case it will be unclear where the defect lies.\nCounter-arguments:\nIt is true that Incremental Integration Testing may fall short of achieving defect localization when collaborators have defects despite having already been tested. It is also true that Unit Testing with Mocks does not suffer from that problem when collaborators have defects; but then again, neither does it detect those defects. For that, it is necessary to always follow a round of Unit Testing with a round of Integration Testing. However, when the malfunction is finally observed during Integration Testing, we are facing the exact same problem that we would have faced if we had done a single round of Incremental Integration Testing instead: a malfunction is being observed which is not due to a defect in the root component of the integration, but instead due to a defect in some unknown collaborator. The difference is that Incremental Integration Testing gets us there faster. Let us not forget that the primary goal of software testing is to guarantee that software works as intended, and that defect localization is an important but nonetheless secondary goal. Incremental Integration Testing goes a long way towards achieving defect localization, but it may not achieve it perfectly, in favor of other conveniences, such as making it far more easy to write and maintain tests. So, it all boils down to whether Unit Testing represents overall more or less convenience than Incremental Integration Testing. I assert that Incremental Integration Testing is unquestionably far more convenient than Unit Testing. Argument: Incremental Integration Testing only tests behavior; it does not check what is going on under the hood.\nWith Unit Testing, you can ensure that a certain module not only produces the right results, but also that it follows an expected sequence of steps to produce those results. With Incremental Integration Testing you cannot observe the steps, you can only check the results. Thus, the internal workings of a component might be slightly wrong, or less than ideal, and you would never know.\nCounter-arguments:\nThis is true, and this is why Incremental Integration Testing might be unsuitable for high-criticality software, where White-Box Testing is the explicit intention, since it is necessary to ensure not only that the software produces correct results, but also that its internals are working exactly according to plan. However, Incremental Integration Testing is not being proposed as a perfect solution, it is being proposed as a pragmatic solution: the vast majority of software being developed in the world is regular, commercial-grade, non-high-criticality software, where Black-Box Testing is appropriate and sufficient, since all that matters is that the requirements be met. Essentially, Incremental Integration Testing represents the realization that in the general case, tests which worry not only about the behavior, but also about the inner workings of a component, constitute over-engineering. For a more in-depth discussion about this, please read White-Box vs. Black-Box Testing. In order to make sure that everything is happening as expected under the hood, you do not have to stipulate in excruciating detail what should be happening, you do not have to fail the tests at the slightest sign of deviation from what was expected, and you do not have to go fixing tests each time the expectations change. Another way of ensuring the same thing is to simply: Gain visibility into what is happening under the hood. Be notified when something different starts happening. Visually examine what is now different. Vouch for the differences being as expected. For more details about this, see Collaboration Monitoring. Argument: Incremental Integration Testing prevents us from picking a single test and running it.\nWith Unit Testing, we can pick any individual test and run it. With Incremental Integration Testing, running an individual test of a certain component is meaningless unless we first run the tests of the collaborators of that component.\nCounter-arguments:\nPicking an individual test and running it is meaningless under all scenarios. It is usually done in the interest of saving time, but it is based on the assumption that we know what tests have been affected by the changes we just made to the source code. This is never a safe assumption to make. Instead of picking an individual test and running it, we need a way to automatically run all tests that have been affected by the changes we just made, which requires knowledge of the dependency graph of the system. If you are unsure as to exactly what you just changed, and exactly what depends on it, then consider using a tool like Testana, which figures all this out for you. See Testana: A better way of running tests. Argument: Incremental Integration Testing requires additional tools.\nIncremental Integration Testing is not supported by any of the popular testing frameworks, which means that in order to start practicing it, new tools are necessary. Obtaining such tools might be very difficult, if not impossible, and creating such tools might be difficult, because they would have to do advanced stuff like system-wide static analysis to discover the dependency graph of a system.\nCounter-arguments:\nMy intention is to show the way; if people see the way, the tools will come. I have already built such a tool which is compatible with some combinations of programming languages, build systems, and testing frameworks; see Testana: A better way of running tests. Even in lack of tools, it is possible to start experimenting with Incremental Integration Testing today by following the poor-man's approach, which consists of simply naming the tests, and the directories in which they reside, in such a way that your existing testing framework will run them in the right order. This is described in the \u0026quot;poor man's approach\u0026quot; section of this paper. Conclusion Unit Testing was invented in order to achieve defect localization, but as we have shown, it constitutes White-Box Testing, so it is laborious, over-complicated, over-specified, and presumptuous. Furthermore, it is not even, strictly speaking, necessary. Incremental Integration Testing is a pragmatic approach which achieves almost the same degree of defect localization but without the use of mocks, and in so doing it greatly reduces the effort of developing and maintaining tests.\nCover image: Incremental Integration Testing by michael.gr\n","date":"2021-12-14T09:07:09.161Z","permalink":"https://blog.michael.gr/post/2022-10-incremental-integration-testing/","title":"Incremental Integration Testing"},{"content":"This was written on 2021-11-26 but it is retro-dated so as to not appear among my recent posts, and thus avoid embarrassing certain unnamed entities. It is written in past tense even though a few paragraphs down the page it begins describing my current experience, because in the near future I intend this to become my past experience.\nIn 2015 I decided to leave Greece and its destroyed economy, and to go live and work elsewhere in Europe. I started an international job search, and within a couple of months I had a few offers to choose from. I picked the one from a company called Topdesk, in the nice little university town of Delft, in The Netherlands, mainly because of tax benefits available to expats in that country, but also, and in no small part, because The Netherlands has the reputation of being one of the most foreigner-friendly countries in Europe. The Netherlands achieves this reputation in a number of ways, one of which is the fact that the Dutch rank number one in the world (1) in English-as-a-foreign language proficiency, making it possible to live in The Netherlands without ever having to learn Dutch.\nSo, Topdesk brought me to Delft with all expenses covered, and even though as a foreigner I represented a trifling minority of maybe 2% of the employees in the office, my colleagues did an excellent job of helping me integrate, which is definitely something I am grateful for.\nFor example, as I entered a room, they would all switch from speaking Dutch to speaking English among themselves, even if some of them were just discussing how their weekend was. That's how sensitive they were towards having a foreigner among them and making that foreigner feel included.\nLater I worked for another company, also in Delft, where more than 50% of the employees were non-Dutch, so obviously, virtually all spoken and written communication in that place was in English. The impression that I formed was that when the Dutchies had a rare chance to speak Dutch among themselves, the foreigners were likely to think \u0026quot;let the poor folks speak their own native language for a change!\u0026quot;.\nThen later, the seemingly impossible happened: I somehow managed to find myself in a workplace which, despite also being located in Delft, did not have a sufficiently foreigner-friendly culture. I was amazed by how sharply I felt the difference; it really made me appreciate the international culture of the previous workplaces.\nI suppose that the difference can be fully attributed to the fact that this particular workplace was a branch of a larger company which was headquartered outside of The Randstad area of The Netherlands. I have delved outside the Randstad a few times, and there is indeed a noticeable change in the varieties of Dutchies observable out there: things tend to be less international, people who struggle with English are actually common, and there is a decidedly more rural look and feel to everything, including the mindsets.\nIn this particular company I did once communicate with a headquarters employee who was in fact struggling with English, and mind you, I am talking about a person holding a high position in the high technology sector. How this is possible, I do not know. Apparently, the company is accommodating towards employees who are not comfortable with English, and as a result they attract employees that are not comfortable with English, so it is a vicious cycle. This culture appears to overflow to other branches, even in areas of The Netherlands where English fluency is the norm.\nHere is an example of a very foreigner-unfriendly situation that I came across once, in the Delft branch:\nMy colleague next to me tells me that our code is causing something unexpected to appear on his screen, and asks me to come take a look. So, I roll my chair from my desk to his desk, I take a look at it, and we start discussing it. Another colleague overhears the discussion, and joins. The three of us continue discussing in English for a few more minutes or so, and then suddenly the two of them switch to Dutch. I am left sitting there waiting to see if they will switch back to English, but no such thing seems to be happening. After some more wondering what my purpose in life is, I quietly roll my chair back to my desk, while they are still discussing the problem in Dutch.\nNow, I am not saying that either one of them had the intention of offending me, nor am I blaming them for not realizing that their behavior would be offending to me; this is the type of thing that may easily go unnoticed by everyone but the foreigner. However, from an organizational point of view this lack of anyone to blame means that situations like this cannot be corrected by expecting individual employees to take the initiative to adjust their own behavior and their own sensitivities; it can only be corrected by having a company culture which promotes international workplace awareness. That particular company decidedly had no such culture.\nOf course, that was one of the most glaring examples of things going wrong, and it was not happening very often. There were, however, many other things which were less glaring, but they were occurring more frequently, some even on a daily basis, contributing to making the overall experience of working in that place unpleasant.\nFor example, in that company there were talks and presentations every once in a while on various subjects related to our work, most of them physically taking place in the headquarters but available in Delft via videoconferencing. Many of them were being conducted in Dutch so I was excluded. Some were announced in English, but when I would join I would invariably discover that they were also being conducted in Dutch. In theory I could interrupt them and ask them to switch to English, but I was unwilling to do so, because the company culture gave me no such mandate. When the head of the company gives his yearly talk in Dutch, he sets a precedent. When many of the communications that arrive every week in my inbox are in Dutch only, this sets a precedent. When e-mails contain an English-translation but their subject lines are nonetheless in Dutch, this sets a precedent. When non-Dutch speakers are being treated as second-class employees, not by salary or by nature of work, but simply by communication, this sets a precedent. That is how company culture is formed.\nWorking for that company made me realize that my earlier colleagues at Topdesk were doing above and beyond what was necessary to make me feel included, and in that sense they may have actually been spoiling me. Of course, I do not expect anyone to switch to English just because I happen to be within hearing distance. As a matter of fact, not being able to understand Dutch can be considered as an advantage, because it allows my ears to easily cancel the environmental noise and focus on my work. However, when I am invited, or otherwise included, or in any other way entitled to participate in a discussion, to deny me participation by speaking Dutch it is highly unprofessional.\nHere is a checklist of things to look out for with respect to international employees in the workplace:\nIf you post a job advertisement in English, and in the list of requirements you refrain from including knowledge of the Dutch language, and an international candidate shows up, and you hire them, you are essentially promising a job that can be performed in English. So, this is an obligation you have now picked up, which you must fulfill. There is very little that the employee can do to foul up their end of the deal; it is not like they can start submitting their work in Swahili tomorrow, and expect you to cope with it. It is only the employer's end of the deal that can be either done right or fouled up. In the high technology sector, these individuals have not come to The Netherlands asking for favors or \u0026quot;willing to cope with adversities\u0026quot;. They are highly valued professionals fulfilling important roles. These jobs are available not because Dutchies are unwilling to do them, but because there are not enough Dutchies around to do them. So, it is best if these individuals are not made to feel like fruit-pickers. Any and all verbal communication directed to a group of employees which may potentially contain an international employee should be in English. If the speaker is unsure about the composition of the group: The speaker should either assume that the group contains international employees, or ask. The speaker should not assume that the group does not contain international employees, and expect them to interrupt and request that their presence be taken into consideration. Words spoken in the presence of an international employee, and in the context of work, (such as in a live or virtual meeting,) should be in English, regardless of: Whether these particular words are work-related or not. Whether the words are deemed relevant to the international employee or not. Whether the meeting has started or we are still waiting for everyone to join, etc. The international employee is present in the meeting as part of their job, so professional rules of engagement apply.\nAny and all written communication directed to a group of employees should be in English, not just for the narrow reason that the group may potentially contain an international employee, but more broadly, because that is what having an international company culture means. If Dutch must also be included:\nThe Dutch text may of course precede the English text, but it should not fail to begin with \u0026quot;=== ENGLISH FOLLOWS ===\u0026quot;. Both texts should have an equivalent status in all other respects, including formatting, pictures, etc. For e-mail, subject lines should always be in English. Mistakes that will inevitably be made need to be detected, corrected, and prevented from being repeated. For example, a newly hired employee may write an e-mail introducing themselves in Dutch only, and send it to the entire branch.\nThe correction in this case consists of management asking that employee to re-send the e-mail, this time translated in English. For this to happen, management must be on the lookout for such mistakes. For this to not be repeated with the next fresh hire, management must examine what it is about the hiring process that fails to get the message across that there are non-Dutch speakers working in the company. (Hint: simple measures, like changing the predefined e-mail signature from \u0026quot;Met vriendelijke groet,\u0026quot; to \u0026quot;Kind regards,\u0026quot; can work miracles.) Any and all written communication that for whatever reason must be in Dutch should be in a format which facilitates single-click machine translation, because it may fall in the hands of an international employee, (even at some point in the distant future,) who must be able to very easily make sense out of it even if only to discover that it is irrelevant to them.\nAny web sites or applications that international employees must regularly use should be 100% in English, without requiring machine translation, not even single-click machine translation.\nThis includes not only the messages displayed by the application itself, but also all verbiage added by means of data entry. For example, it is not of much use to have the header of a table translated from \u0026quot;type velof\u0026quot; to \u0026quot;type of leave\u0026quot; if the entries in the table still contain words like \u0026quot;bovenwettelijk\u0026quot;. Doing this right might require purchasing world-class software and investing in its customization; this is a price that must be paid in exchange of the benefit of hiring non-Dutch speakers. Doing this right may also require consultation with labor law experts, taxation experts, etc., and this is also a price that must be paid in exchange of the benefit of hiring non-Dutch speakers. If the employee must choose the English language from a menu, they should not have to do that more often than maybe once every couple of years. Tools that offer a language choice only for demonstration purposes, without actually remembering the user's preference, are unacceptable. Sometimes fixing such a tool to make it behave correctly is seen as a feature request which gets filed along with other feature requests, prioritized according to how many users need it, and acted upon accordingly, meaning, never. Having an international company culture means giving such feature requests a very high priority.\nEmployment contracts should ideally be in English. If a company finds it hard to provide international employees with contracts in English, then each printed contract in Dutch should be accompanied by a copy in electronic form, along with a statement affirming that the electronic copy is identical to the printed copy, so that the employee can at the very least have it machine-translated so as to make sense out of it.\n(1): English-as-a-second-language proficiency world-wide top rank in The Netherlands: See https://www.ef.nl/epi/\n","date":"2021-11-26T10:04:57.914Z","permalink":"https://blog.michael.gr/post/2021-11-on-international-company-culture-in/","title":"On International Company Culture in The Netherlands"},{"content":"\r(Useful pre-reading: About these papers)\nThis assumes that you have previously established a wi-fi connection, so windows has created what it calls a \u0026quot;profile\u0026quot;.\nIn short, the commands are:\nnetsh wlan connect ssid=\u0026amp;lt;ssid\u0026amp;gt; name=\u0026amp;lt;name\u0026amp;gt; and\nnetsh wlan disconnect To obtain ssid and name, use:\nnetsh wlan show profile This should display all existing profile names, and by default, the \u0026lt;name\u0026gt; is the same as the \u0026lt;ssid\u0026gt;.\nThings can get more complicated if you have multiple wi-fi adapters, or an ssid that differs from the profile name, but the above should cover the general case.\n","date":"2021-10-27T11:58:21.691Z","permalink":"https://blog.michael.gr/post/2021-10-windows-how-to-connectdisconnect-wi-fi/","title":"Windows: how to connect/disconnect wi-fi from command-line"},{"content":"If you are a recruiter wishing to contact me with regards to some job opportunity, please read this.\nIf you have a deal with an employer: I expect you to disclose the name and website of that employer within the first sentence of your initial message to me. If they seem interesting, I will contact them to verify that they are in fact working with you, before I respond to you. How well they handle this communication is part of my evaluation process for a prospective employer. If you want to help the process, you can point me to the person within the company to whom I should address my inquiry. If you do not have a deal with an employer: (Or if you are otherwise unwilling to do as the first paragraph says)\nYou are still free to try and push my C.V. / Resume to any employers you wish, and sell it for whatever you can get. I cannot prevent you from doing so. However: ***I expect to hear directly from those employers, not from you.*** Let me expound on this, in case it was not clear: I do not care to hear how awesome the job is; the fact that you utilize secrecy as part of your process means that absolutely nothing of what you say can be trusted. An employer working with random recruiters instead of doing their own hiring already has one big minus in my book; my interest is quite low to begin with. I do not care whether it is necessary for you to contact me first so as to appear to the employer as if you have filtered me; you will not contact me. Sorry, not sorry. If you nonetheless do contact me: your e-mails will go directly to spam (and behind my custom e-mail address I use gmail, which learns) your cold calls will go unanswered, and if I mistakenly answer, I will hang-up on your face. Not only you are generally a waste of time, but many of you are annoying, indiscrete, inconsiderate, and are known to have very low morals. As far as I am concerned, you do not exist. Please stay like that. Thanks. ","date":"2021-10-21T17:21:21.335Z","permalink":"https://blog.michael.gr/post/2021-10-dear-recruiter/","title":"Dear recruiter-"},{"content":"This article attempts to shed some light on what a microservice really is; it is meant as support material for other posts of mine that discuss microservices, mainly The Stateful Microservice.\nWhat is a microservice? If you go looking for information on what a microservice is, you will find many different descriptions, exhibiting considerable difference of opinion. Most claims about microservices are non-technical rather than technical, for example the allegedly \u0026quot;independent\u0026quot; software development style around microservices, or some alleged organization of microservices \u0026quot;around business capabilities\u0026quot;. Even when the claims do stick within the technical realm, they are often unwarranted; for example, I have seen statements to the effect that a microservice is supposed to live in its very own source code repository, that microservices must communicate with each other via nothing but REST, etc. My favorite one is that they must necessarily be stateless. This paper is a first step in dispelling the statelessness myth.\n(Useful pre-reading: About these papers)\nTo clear up the confusion a little bit, I would like to propose a purely technical definition of a microservice which is brief and to the point:\nFrom a purely technical standpoint, a microservice is a scalable module.\nSo, what I am proposing here is that the only fundamental technical requirement for a microservice is scalability, and that all other purported characteristics mentioned in the literature are either non-technical, or they are byproducts of this fundamental technical requirement.\nYou see, there was a time back in the late 1990s to early 2000s when users were joining websites at exponential rates, and servers running monolithic web apps were reaching capacity and could not deliver service anymore; the business people were asking the technologists to fix this, because they were losing money, and the technologists were saying that they could not do anything, because they already had the biggest, most expensive server that money could buy. The business people would na?vely say \u0026quot;well, add more servers!\u0026quot; to which the technologists would (equally na?vely) reply \u0026quot;you don't understand, that's impossible!\u0026quot; Later on the technologists started realizing that it is in fact possible, it just requires a radical change in their way of thinking, and a radical redesign of all their systems. When the sums of money at stake were high enough to justify redoing everything from scratch, scalable systems started appearing, and are commonplace today.\nThe new software development paradigm that was allowing web sites to achieve scalability received a name quite some time after it first started being put to use; the naming happened some time in the mid 2000s, and it was microservices.\nThen of course came the evangelists. Unfortunately, when people become salespersons of a cause, for some reason they never seem to be satisfied with simply mentioning the one real, game-changing advantage that their product has over the alternatives; instead, they feel compelled to throw as much as possible at the customer, inventing advantages if possible. That's how all the fictitious characteristics of microservices came to be. However, the truth remains that there was one and only one thing that business needed which could not be achieved otherwise, and therefore business was willing to pay for it limitlessly, and that one thing was scalability, nothing else. (1)\nBusiness could not care less whether the software gets deployed in pieces or in a big bang; business could not care less whether development is done by autonomous teams or by all the programmers shouting at each other in one big room; business could not care less whether the software communicates via REST or via pheromone secretion. Things were getting done before, and things would continue getting done, regardless of those alleged \u0026quot;advantages\u0026quot; of microservices. Scalability was the only thing that was impossible before microservices and was made possible by microservices.\nOf course, you might not agree with this definition; if not, then please take it as nothing but a working definition, and only for the purpose of these papers about statelessness.\nThere is one more characteristic of microservices which is not really fundamental, because it is a direct consequence of the first, but it is so important that it deserves mentioning as a requirement, and this is resilience.\nYou see, scalable architectures are not magically capable of performing better than monolithic ones; in fact, quite the opposite is true: in terms of raw throughput per unit of hardware, scalable architectures perform much worse than monolithic ones. In order to accommodate the same workload that you used to have with a single server running a monolithic application, you might need several servers running microservices. The benefit of the scalable architecture is that you can in fact now throw more hardware at the problem, instead of being stuck with a single piece of hardware. So, given enough money to buy enough hardware you can end up with a higher sum of throughput despite the worse throughput per unit of hardware. Thus, when we are talking about scalability, we are usually talking about a lot of hardware. And by this I mean an awful lot of hardware.\nNow, it just so happens that hardware, being necessarily bound to the constraints of the physical world?, has this inconvenient characteristic called \u0026quot;Mean Time Before Failure\u0026quot; (MTBF) which is of a somewhat statistical nature: the more pieces of hardware you have, the higher the chances are that any one of them will fail at any given moment. Furthermore, as these pieces age, their individual chances of failure at any given moment increase. The result of all this is that hardware failure in server farms becomes not just something that there is a high risk of; not even just something that is inevitable; it actually becomes a regularly occurring phenomenon. As such, hardware failure cannot be addressed on an as-needed basis via crisis management responses; it must be addressed systematically, as a normal part of the operation of the system. This means that the software that runs on that hardware must be capable of continuing to function as if nothing happened despite pieces of the hardware randomly failing and being replaced all the time.\nA software system that manages to continue functioning despite parts of it ceasing to work is called a resilient software system. If we want to add the resilience concern into our definition, then this is what we are left with:\nFrom a purely technical standpoint, a microservice is a scalable and resilient module.\nFurther reading: On Stateless Microservices\n(1) According to Neal Ford, this started with Pets.com; see Neal Ford: \u0026quot;Stories Every Developer Should Know\u0026quot; at YOW! 2018, starting at 35:56\n","date":"2021-10-14T17:39:23.324Z","permalink":"https://blog.michael.gr/post/2021-10-so-what-is-microservice-anyway/","title":"So, what is a Microservice, anyway?"},{"content":"I did a quick search for the term and did not find anything concrete, so I thought I might as well publicly document my thoughts.\n(Useful pre-reading: About these papers)\nAlmost everyone doing microservices today will tell you that microservices need to be stateless. In another post of mine I explain that statelessness is not an end in and of itself; instead, it is just a means to an end. The desired technical ends are scalability and resilience, and statelessness is just one way of achieving them. Furthermore, I explain that statelessness in particular is a very cowardly solution from an engineering standpoint, and it performs very badly. For details, please see On Stateless Microservices.\nWhat remains to be shown is whether there exists an alternative.\nObviously, an alternative to the stateless microservice would be the stateful microservice, so what we are about to examine here is what a stateful microservice could possibly be, and how it would compare to a stateless microservice.\nWhat is a stateful microservice A stateful microservice maintains state for the purpose of expediting the processing of incoming requests, reducing overall server load, (trading memory for processing power and data storage traffic,) and achieving certain things that are difficult to achieve otherwise, such as server-initiated client updates.\nThe state kept by a stateful microservice can include:\nState that has been obtained from the main data store and has possibly undergone expensive transformations. The benefit of maintaining such transient state within the microservice is that the data store does not need to be re-queried, and the possibly expensive transformations do not need to be repeated, with each incoming request; the loading and processing of the data only needs to happen once when the microservice starts, and to be repeated only in response to a notification from the system's messaging backbone that the original data in the main data store has changed.\nState that does not exist in the main data store, and does not need to, because it is of a transient nature, for example information that is only needed during user's visit to a web site and can be dismissed afterwards. This can include information necessary for maintaining a session, such as the session token, and view-related information, such as which page (or pages) of the web site the user is currently viewing. View-related information may be useful for the server to have for various reasons, for example for the purpose of sending server-initiated client updates that are specific to the web page(s) that are being viewed.\nState that may eventually be entered into the main data store but has not yet been entered due to various workflow demands or optimization concerns. For example, the user may be sequentially visiting each page of a wizard workflow, and entering information on each page, but this information should not be merged into the main data store unless the user first reaches the last page of the wizard workflow and confirms their actions.\nFrom the above it should be obvious that a stateful microservice is necessarily session-oriented, meaning that it requires a specific client to talk to. Session-agnostic stateful microservices already exist, and we do not think of them as anything special; they are microservices that implement caches, containing information that is pertinent to not just one client, but to all clients. These microservices are already scalable and resilient because a cache can be trivially duplicated to an arbitrary degree and it can also be destroyed and trivially re-created from scratch.\nWe now need to show how a stateful microservice can still be called a microservice.\nIn a previous post of mine I examined what a microservice really is, and I came to the conclusion that from a purely technical standpoint, it is simply a scalable and resilient module. (See So, what is a Microservice, anyway?) Even if you disagree with this definition, and you regard microservices as necessarily more than that, I hope you will at least agree that the purpose of statelessness in microservices is precisely to achieve scalability and resilience, so the definition of a microservice as a scalable and resilient module can serve as a working definition for the purposes of this discussion.\nSo, we need to show how stateful microservices can be scalable and resilient, just as their stateless counterparts are.\nScalability in stateful microservices can be achieved by means of a header-inspecting, session-aware, load balancing gateway which routes new session requests to the least busy server, and from that moment on keeps routing requests of that same session to the same server. Under such a scenario, rebalancing of the server farm can be achieved simply by killing microservices on overloaded servers and letting the resilience mechanism described next make things right.\nResilience can be achieved by having each instance of a stateful microservice continuously persisting its transient state in an efficient manner into a high-performance backup store which is accessible by all servers in the farm. Thus, if a microservice unexpectedly ceases to exist, it can be reconstructed from that backup on any other server. The trick, as we shall see, is that the backup is taken very efficiently, and in the event that the microservice needs to be reconstructed, the restoration from the backup is also done very efficiently.\nIn more detail, it works as follows:\nWhen a client initially connects to the server farm, no session has been established yet, so the first request that it sends is sessionless. The sessionless request arrives at the load-balancing gateway, which routes it to the least busy server in the farm. This mostly takes care of scalability, since we can always add more servers, which will initially be idle, but as requests for new sessions keep arriving, they are routed to the idle servers instead of the busy ones, so over time, the load distribution evens out. The server that receives the sessionless request creates a new instance of a stateful microservice to handle that request, and the session is established between that microservice and the client. From that moment on, any further incoming requests for that same session are routed by the gateway to the same server, and the server delegates them to the same instance of the stateful microservice. (Alternatively, the microservice and the client may negotiate a direct persistent connection between the two, thus bypassing any middlemen from that moment on.) The newly spawned stateful microservice registers with the messaging backbone of the system to receive notifications about system-wide events, so as to be able to keep its state always up to date. The newly spawned stateful microservice loads whatever state it is going to need, and keeps that state in memory. The microservice processes the request and sends back a response. Possibly also changing its own transient state. Possibly also updating the main data store with information that must always be globally available and up to date, and issuing system-wide notifications about these changes. The microservice takes a backup of itself. The microservice serializes the entirety of its state into a binary blob The blob is written into a persistent key-value store, using the session id as the key. This persistent key-value store is used as a backup, meaning that it is written often, but it is never read unless something bad happens. Continuous persistence of stateful microservices is not expected to pose a performance problem, because:\nSerialization to and from a binary format performs much better than general-purpose serialization into a textual markup like JSON or XML. The size of the blob is expected to be relatively small. (Of the order of kilobytes.) Key-value stores tend to have very high performance characteristics. The backup store can be physically separate from the main data store, (even on a different network,) thus avoiding contention. The act of serializing an in-memory data structure into a single in-memory blob and then sending that blob as one piece into persistent storage is bound to perform far better than a series of operations to update a structured data store. (For one thing, there are no index updates.) Persisting the blob can be done asynchronously and in parallel to the sending of the response to the client, so it does not contribute to user-perceived latency. For as long as the session does not expire, the stateful microservice can remain alive, continuing to serve requests efficiently, taking advantage of the transient state that it contains and keeps up-to-date. Contrast this with the stateless microservice approach, which requires that any request can be handled by any server, therefore each microservice must contain no state at all:\nThe processing of each request begins with zero knowledge of the state of the system, so persistent storage must always be queried to obtain state. These queries represent overhead, and this overhead must be suffered in full before the request can be serviced, thus manifesting as latency to the user. The results of these queries may not be cached, because they may at any moment be rendered out-of-date by any other microservice in the system. An instance of a stateful microservice may prematurely cease to exist due to a number of reasons:\nThe microservice may be terminated on demand in order to rebalance the server farm. The server hosting the microservice may become unavailable due to hardware failure. The microservice may fall victim to the whim of the chaos monkey. If for whatever reason a microservice ceases to exist, the gateway discovers this either on its own, or when the next request arrives from the client.\nThe gateway routes the request to the least busy server in the farm. The server that receives the request sees that there is no microservice to handle requests for that session, so it creates a new one. The newly instantiated microservice checks, whether the key-value store contains a backup for the current session, and discovers that it does. The microservice restores its state from the backup. Operation continues from that moment on as if nothing happened. Between the moment in time that a certain microservice instance prematurely ceases to exist, and the moment in time that a new incarnation of that microservice is ready for showtime on a freshly assigned server, some events from the messaging backbone may be lost. To avoid inconsistencies in the state of the microservice, we must utilize a messaging backbone which is capable of replaying events. For example, if we use Kafka, then the stateful microservice can make sure to include among its persistent state what is known in Kafka terminology as the \u0026quot;consumer offset\u0026quot;. Thus, when the microservice gets reconstructed, it asks Kafka for events starting at that offset, so Kafka replays any missed events before it starts sending new ones. Thus, we ensure that the state of the microservice is always up to date, even in the case of termination and reconstruction. Thus, stateful microservices can achieve not only scalability but also resilience.\nCover image: Photo of two elephants friendly interacting with each other, from The Scientific American: Fact or Fiction?: Elephants Never Forget\n","date":"2021-10-14T16:58:10.352Z","permalink":"https://blog.michael.gr/post/2021-10-14-the-stateful-microservice/","title":"The Stateful Microservice"},{"content":"This post discusses the stateless microservice design pattern; it is meant as support material for other posts of mine that discuss microservices, mainly The Stateful Microservice.\nIs statelessness a requirement for a microservice? In another post (see So, what is a Microservice, anyway?) I examine what a microservice really is, and I come to the conclusion that from a purely technical standpoint, a working definition could be as simple as this:\nA microservice is a scalable and resilient module.\nEven if you disagree with the terseness of this definition, and you regard microservices as necessarily more than that, I hope you will at least agree that it is precisely scalability and resilience that statelessness in microservices aims to address, so this definition serves its purpose at least in the context of this series of posts.\nThere are many who will try to convince you that in order to build a scalable and resilient system, you need statelessness; so much so, that microservices have almost come to be regarded as synonymous with statelessness. This post examines whether this is that in fact so, and what is the cost of doing things this way.\n(Useful pre-reading: About these papers)\nIf we take a step back for a moment and examine the issue from a somewhat distanced point of view, we notice that there is no such thing as a stateless software system. If there was such a thing, it would not be capable of performing any function worth speaking of, and it would necessarily be less useful than a brick, because a brick has physical existence, so you can, at the very least, throw it at someone.\nIf there is one thing that a software system necessarily has, it is state, so there is no word that is more unsuitable to go with the word \u0026quot;software\u0026quot; than the word \u0026quot;stateless\u0026quot;. (By the way, that is also a little something that functional programming aficionados should perhaps take a moment to philosophically ponder about.)\nWhat this all means is that even if you build a system using so-called stateless microservices, that system will still have state; for example, if it is a web shop, it will very conveniently remember me when I come to visit again, and if I order any goods during my visit, it will very inconveniently not forget to send me an invoice. That is all happening due to state, which is stored in the database system of the web shop. So, when people speak of microservices with no state what they actually mean is microservices with no transient state. The state is definitely there, the system just does not rely on any microservice remembering any of it. Each microservice refrains from keeping any state in memory for any longer than it absolutely has to; it always begins the processing of every single transaction by querying the database for all necessary state, and it makes sure to persist any changed state into the database before proclaiming the transaction complete.\nStateless microservices were invented because statelessness is an easy way of achieving scalability and resilience: if a module does not keep any state, then an indefinite number of copies of that module can be created to process requests in parallel; any request arriving at the server farm can be serviced by any instance of that module, and any subset of copies of the module can be destroyed at any moment, without depriving the system from its ability to function.\nThat's great, but statelessness is not an end in and of itself; it is a means to an end; it is just one way of achieving scalability and resilience. This is proven by the fact that the database systems upon which stateless microservice architectures are built are most certainly not stateless at all, and yet they do somehow manage to be scalable and resilient. Obviously, they are doing something differently.\nWhat is wrong with statelessness? When building a system which needs to be scalable and resilient, and also needs to be very stateful as a whole, one has to begin with a scalable and resilient data layer as a foundation. Luckily, there exist various commercially available products that accomplish this. On top of that foundation, one has to build their application-specific logic in a way that is also scalable and resilient. Stateless microservices will achieve this, but they are one of the worst performing, and from an engineering standpoint most cowardly ways of achieving scalability and resilience. Choosing the stateless microservices approach is like saying the following:\nState is hard; we do not have the slightest clue as to how we can maintain state and at the same time remain scalable and resilient; but look, the creators of our database system are very smart folks, they seem to have figured it all out! So, here is what we will do: we will delegate the entire task of maintaining state to them!\nThat is how we arrived at the stateless microservice model, which I like to call the \u0026quot;Dory\u0026quot; model, after the fish that suffered from amnesia in the Finding Nemo movie.\nIn the Dory model, every single incoming transaction gets processed by a microservice that is drawing a complete blank. Upon receiving the request, the microservice starts with very basic questions:\nWho am I, and what is this strange place I am running in?\nWho are these folks sending me requests, and why?\nShould I respond to them, or should I four-oh-three them away?\nLet's start by authenticating them...\n...and it goes on like that. For every single request, there are multiple round-trips to the database while the microservice is discovering more and more about what it is being requested to do and whether it should in fact do it, and even more round-trips to retrieve the information that will go into the response, including very basic information that hardly ever changes, such as the name of the visitor on whose behalf the request was sent, and in multi-tenancy scenarios even the name of the tenant on whose behalf the website is being served.\nWhen the transaction is nearing completion, the stateless microservice will meticulously store every single little piece of changed state in its exact right place in the database, as if it is making notes to itself, lest it forgets.\nFinally, once the transaction is completed, the microservice will proceed to deliberately forget absolutely everything that it learned during the processing of the transaction, before it starts to wait for the next transaction.\nI am not going to say that this is preposterously inefficient, but it is preposterously inefficient.\nIncidentally, the magnificent inefficiency of stateless microservices makes them to a certain extent a self-serving paradigm: in order to scale up you might think you need them, but once you have them, they will perform so badly, that boy oh boy, are you going to need to scale up!\nAnother problem with stateless microservices is that they cannot take any initiative of their own, they are restricted to only responding to incoming requests. This poses a problem with server-initiated client updates, which in certain circles are known as \u0026quot;push notifications\u0026quot;. A server-initiated client update happens when the server decides to send some data to the client at an arbitrary moment in time, as a result of some event occurring on the server, without the client first having to request that data.\nActually, the very term \u0026quot;push notification\u0026quot; seems to have originated from system designs in which such sending of data is a difficult task, as if the developers have to put their shoulders against the notification and push it all together to make it straddle the great divide between the server and the client. In other designs, where asynchronous bi-directional communication is the default mode of operation, there is no need for such laboriousness; server-initiated client updates are just part of the normal way things work. Alas, you cannot have that with stateless microservices, because bi-directional communication requires the notion of a session, which in turn implies a notion of state, which is a no-no.\nConsequently, software systems that utilize the stateless microservice design pattern address the problem of server-initiated client updates in various wacky hacky ways:\nSome opt to not have any; if the user wants to see what has changed, let them refresh the page. This can cause serious problems in systems where multiple clients may edit the same data, since the system has no way of alerting a client that the data they are editing is also being edited by another client at the same time.\nSome use polling, meaning that each client keeps sending requests to the server at regular intervals asking whether there are any updates. This is wasteful, because each of these requests represents work that needs to be done, but very few of them will result in anything useful happening. At the same time, in order to reduce server load, the polling cannot be too frequent, which in turn means that there will always be a time lapse between the moment that an event occurs on the server and the moment that the clients take notice.\nSome opt to have special stateful modules working side by side with the stateless microservices to handle the push notifications in a completely separate way, under the assumption that notifications are a kind of optional, \u0026quot;nice to have\u0026quot; thing anyway, so if performance suffers due to lack of scaling, or if service is interrupted due to lack of resilience, it will not hurt too much. On top of being clunky, this approach is also short-sighted because from the entire broad topic of server-initiated client updates it only considers the narrow case of updates being used for the sole purpose of on-screen notifications.\nFurther reading: The Stateful Microservice\nCover image: Dory, the yellow-blue fish (a Royal Blue Tang) that suffered from amnesia in the 2003 movie Finding Nemo by Pixar.\n","date":"2021-10-14T10:53:38.318Z","permalink":"https://blog.michael.gr/post/2021-10-on-stateless-microservices/","title":"On Stateless Microservices"},{"content":"\rThis is part of a series of posts in which I am documenting what is wrong with certain popular programming languages that I am (more or less) familiar with. The aim of these posts is to support a future post in which I will be describing what the ideal programming language would look like for me.\nI will be amending and revising these texts over time.\n(Useful pre-reading: About these papers)\nWhat is wrong with Scala: The garbage collector. Curly braces. Functional Nazism It is true that Scala does not suffer from functional Nazism as much as other functional languages, but it still has some entirely unwarranted requirements and limitations that seem to have no purpose other than to hinder imperative programming: Cannot reassign method parameters. No for-loop the way we know it. Functional weirdness Some language idioms may seem entirely natural to functional programming aficionados, but disciples of the imperative school tend to find them rather upsetting: The ability to return the last evaluated value without an explicit return statement sometimes makes it hard to tell what will actually be returned by a function. Incredibly complex syntax Results in torturously slow compilation. \u0026quot;look ma, no semicolons\u0026quot; adds nothing of tangible value, but it does make things very difficult for the compiler. A compilation unit often becomes entirely unparseable from top to bottom just because of a single-character syntax error at an unknown place somewhere in it. Arcane syntax Generics use square brackets instead of angle brackets. Ultra-compact generic type bound specifications are probably very convenient for the seasoned Scala programmer, but a severe deterrent for the newcomer. Companion objects feel entirely superfluous and clunky. Inelegant language keywords like def, var, val. Inelegant built-in collection class names like Seq. Note: the above list of disadvantages is kind of short, because I am not intimately familiar with the language.\nFeedback is more than welcome: you'd be doing me a favor. However, be aware that blogger sometimes eats comments, so be sure to save your text before submitting it. If blogger eats your comment, please e-mail it to me.\n","date":"2021-10-04T15:39:46.959Z","permalink":"https://blog.michael.gr/post/2021-10-what-is-wrong-with-scala/","title":"What is wrong with Scala"},{"content":"\rThis is part of a series of posts in which I am documenting what is wrong with certain popular programming languages that I am (more or less) familiar with. The aim of these posts is to support a future post in which I will be describing what the ideal programming language would look like for me.\nI will be amending and revising these texts over time.\n(Useful pre-reading: About these papers)\nWhat is wrong with C#: The garbage collector. Curly braces. Member initializers execute in a static context. This is far worse than Java's deficiency of member initializers not having access to constructor parameters; this renders the language almost unusable. Delegates are superfluous and problematic. They could have been implemented as single-method interfaces, as in Java, thus keeping the language simpler. The ability to have an anonymous method implementing a delegate is far less useful than the ability to have an anonymous class implementing an interface. The built-in collection model is lame: Arrays implement the IList interface, which might initially seem like a great idea, until you realize that IList has Add(), Insert() and Remove() methods, which of course cannot be honored by an array. The IReadOnlyList interface was added as an afterthought, and it is not a super-interface of IList. The IReadOnlyList interface does not have an IndexOf() method. This poses problems that cannot be solved by adding an extension method, because the object implementing IReadOnlyList may have its own IEqualityComparator, which the extension method will be blissfully unaware of. Enumerators still have to implement the legacy, non-generic GetEnumerator() method. Fluent style (Linq) is limited to working almost exclusively with IEnumerable\u0026amp;lt;\u0026amp;gt;. Built-in events are problematic. They are unsuitable as a language feature and should have been left for runtime libraries to implement. If one event handler throws, the rest will not be invoked. If one event handler causes another event handler to be removed, the removed event handler will still be invoked. An event is a special kind of thing which cannot be passed to a function, not even by reference. (As all properties are.) As a result, you have to always hand-code the addition of a handler to an event, and to also separately hand-code the corresponding removal. It is absolutely impossible to pass an event to a function, along with a handler and a boolean flag specifying whether to add or remove the handler. This makes it impossible to gather all symmetric initialization and cleanup operations in one place, which in turn leads to buggy software. Extension methods They are a hack. They are one of the most calamitously misused features of the language. Java has shown how to do this right with default interface methods. Allegedly, default interface methods will also come to C#, but it is not like extension methods will ever be removed from the language to undo the harm they have caused. Parameters declared with ref or out Prior to the introduction of tuples, ref and out parameters could sometimes come in handy, but only in very rare cases. At the same time, they are one of the most misused features of the language. Now that tuples are part of the language syntax, parameters by reference are nothing but a liability. Names of variables poison enclosing scopes. If you declare a local variable inside the curly braces of a for loop, you are not allowed to declare a variable with the same name after the closing brace of the for loop. Duh? No function-local classes. You can have function-local functions, which is awesome, but you cannot have function-local classes. Duh? The language runtime is shared with other languages like Visual Basic, and some decisions have been made there in favor of Visual Basic. For example, when you dereference a null pointer you do not get a \u0026quot;Null Pointer Exception\u0026quot;, you get an \u0026quot;Object Reference Is Not Set To An Instance Of An Object\u0026quot; exception. (Presumably because the words \u0026quot;Null\u0026quot; and \u0026quot;Pointer\u0026quot; would cause epileptic seizures to Visual Basic programmers.) No compiler-enforced function purity. You cannot somehow declare that a method must be pure and have the compiler enforce that the method, and any overrides of that method, are pure. No compiler-enforced immutability. You cannot somehow declare and advertise that a class is immutable and have the compiler enforce that the class, and its descendants, are immutable. Certain things about the runtime are completely bonkers. For example: The notion of a \u0026quot;current directory\u0026quot;, which is a piece of mutable global state that is shared across all threads, and even across all AppDomains within a process. (So much for AppDomain isolation!) The behavior of the ThreadAbortException. What happens to the process exit code if the dotNet process exits due to an unhandled exception. (See https://stackoverflow.com/q/60729865/773113) Collecting a stack trace is a ridiculously slow operation. Throwing an exception is a stupendously slow operation. In order to send a file to the recycle bin, everyone suggests that you should include a module called \u0026quot;Microsoft.VisualBasic\u0026quot;. Duh? Adding injury to insult, if you try this, you will discover that it is broken, it just does not work. No namespace-private visibility C# has always provided assembly-private visibility, which is much more useful than Java's package-private visibility, however at some point Java somewhat fixed that by introducing modules, and the ability to specify which packages are exposed by a module. Now C# lags behind Java in that it does not support namespace-private visibility, which means that everything is visible to everything within an assembly, which can easily lead to chaos if the assembly is large. There is a way to somewhat mitigate this by using partial classes as if they were namespaces, but it is hacky. Interface method implementations are not tied to the interfaces they implement. When you declare a class to implement a certain interface, and then you add a method to that class which implements a certain method of that interface, there is absolutely nothing in the declaration of that method to indicate or even hint that it is implementing a method of that interface. Consequently, if you remove the interface from the list of interfaces implemented by the class, the compiler cannot warn you that methods within this class that were implementing methods of that interface are not meaningful anymore. You can try to overcome this problem with explicit interface method implementation, but it is optional and therefore its use cannot be enforced, plus if you use it then you are stuck with other problems, see below. Explicit interface method implementations are not directly accessible. If you declare a method within a class that explicitly implements a method of an interface implemented by the class, then you cannot invoke that method from within that class, unless you first cast this to that interface. What the actual fuck? The switch statement requires a break at the end. If you omit the break at the end, the compiler complains that Control cannot fall out of switch from final case label. Huh? ","date":"2021-10-04T15:25:56.729Z","permalink":"https://blog.michael.gr/post/2021-10-what-is-wrong-with-csharp/","title":"What is wrong with C#"},{"content":"\rThis is part of a series of posts in which I am documenting what is wrong with certain popular programming languages that I am (more or less) familiar with. The aim of these posts is to support a future post in which I will be describing what the ideal programming language would look like for me.\nI will be amending and revising these texts over time.\n(Useful pre-reading: About these papers)\nWhat is wrong with Java: The garbage collector. Curly braces. Primitive types are cumbersome. Each one of the primitive types boolean, byte, char, short, int, float, long, double is a snowflake which must always be handled differently from the others. They cannot all be treated uniformly as value types. To allow for at least some uniform treatment, one must keep converting back and forth between them and their corresponding wrapper classes (Boolean, Byte, Character, Short, Integer, Float, Long and Double) which is clunky and inelegant. Still no user-defined value types in 2022. Since version 14, Java supports records, but they are still allocated on the heap and passed by reference. So, an array of 1000 records which would be a single memory block in C# is 1001 memory blocks in Java. Allegedly, a future version of Java will support value types, but knowing how bytecode is structured and how the JVM works: This is going to be extremely difficult to achieve Will probably be a cumbersome addition to the language The existing awkward primitive types will of course stay with us forever in the name of backwards compatibility. Still no value tuples in 2022. C# has been doing a pretty good job at that. No conditional compilation. Cannot even declare a constant whose value is externally supplied. Generics are decent, but still lacking. Type erasure allows unsafe constructs which may result in \u0026quot;heap pollution\u0026quot;. Type erasure makes it impossible to disambiguate entities based on their generic parameters, thus making it impossible to overload based on generics. This forces us to give artificially different names to entities that would ideally share the same name. Working with generics inevitably requires either littering the code with @SuppressWarnings( \u0026quot;unchecked\u0026quot; ), or entirely disabling the \u0026quot;unchecked\u0026quot; warning, which opens up another can of worms. No C#-style properties. No operator overloading. In general, the language design philosophy of Java seems to be overly protectionist towards the idiot programmer, at the expense of the expert programmer who just can't have a feature that they want because it would be potentially dangerous for the idiot. This is roughly the same narrow-minded protectionist design philosophy that has been employed by Apple and has given rise to what is known as \u0026quot;the mac user\u0026quot;, which is a code-word for \u0026quot;idiot\u0026quot;. No namespaces. Packages are ill-conceived and lame: Each source file must be associated with one and only one package. So, if the source file is to contain multiple classes, all these classes must belong to the same package. Each source file may contain no more than one public class. Any additional classes must be package-private. There is no equivalent to the namespace aliases of C#. Packages are unrelated to packaging. (See lack of assemblies.) Packages (and the lack of assemblies) force programmers to cram an impossibly large number of classes within the same package so as to be able to keep some of them package-private, because the moment you try moving a class into a separate package to reduce the clutter, this class must now become public, so as to remain accessible by classes from the original package. Despite the fact that package names look hierarchical, packages are not at all hierarchical: Each package is completely separate from all other packages. There exists no special relationship between two packages by virtue of their names being one nested within the other. (In C#, a namespace inherits from all namespaces in its ancestry line.) It is impossible to address a class in a sub-package with a partial (relative) sub-package name. This, in combination with the fact that there is no equivalent to namespace aliases, means that two classes with identical names in different packages can only be handled using fully qualified class names. Since fully qualified class names are cumbersome to work with, most people resort to assigning globally unique names to their classes. This is very clunky, and it looks retarded, because it essentially results in class names that contain the name of their package. This is an uphill struggle and never quite successful, because you might give unique names to all your classes, but you might use some library with class names that conflict with yours, so there will always be some fully qualified class names around. No C#-style assemblies. Individual class files scattered all over the place are cumbersome to work with. The filesystem/jar-file duality is very cumbersome to work with. Jar files only deal with packaging; they offer no support for specifying what is exported and what is kept private. Modules were added as an after-thought, and they give some control over what to publish and what to keep private, but the unit of publication is still the package, not the class, which means that package-private classes are still necessary, which in turn means that huge packages are still necessary. Class loaders are lame. They, as well as many other language features, are a relic from the java web applet era. They are very cumbersome to work with. They unnecessarily impose a significant performance penalty by doing a lot of work on a per-class basis instead of a per-module basis. Lame access rules. Everything that is package-private is also protected. (Duh?) Inner classes have access to private members of the enclosing class; this is probably okay; however, the enclosing class also has access to private members of inner classes, which is retarded. Member initializers have no access to constructor parameters. Member initializers execute between the invocation of the super constructor and the statement that immediately follows it, which technically makes sense, but these jumps in the flow of execution are completely counter-intuitive to the novice programmer, who is precisely the type of programmer that the language caters to. Scala has shown how to do this right. The syntax for invoking the super constructor suggests that one might be able to insert statements before the call to super, but this is not the case. (The deviation from the C++ syntax would be justifiable if the new syntax had something to offer, but it does not.) The language falls short of doing the one sensible thing that this syntax would allow, which would be to be able to put code before the call to super, as long as this code does not try to access this, for example assertions on the constructor parameters before passing them to super; but no, you cannot do that. No named / optional parameters to functions. (No default parameter values.) Default interface methods cannot be final. Any class implementing an interface may inadvertently re-implement functionality which has already been provided by a default method. An interface cannot guarantee that a certain method will have a specific behavior because any class implementing that interface may override that behavior. Interface methods cannot be protected. It is sometimes useful to have a certain interface method that is only visible by implementing classes, but no, we cannot have that, all methods must be public. Interface methods cannot be private. It is sometimes useful to have a certain interface method that is only visible by default interface methods within that same interface, but no, we cannot have that, all methods must be public. Lambda argument names are not allowed to mask the names of variables of the enclosing scope. This is very lame because: It forces the programmer to invent new, unnatural names for lambda arguments. Variables of the enclosing scope cannot be masked, so they remain accessible within the lambda, and can thus be accessed by mistake, leading to bugs that are very hard to detect. No member literals and not even a 'nameof' operator. No nullable/non-nullable semantics for reference types. (C# 8 does a fairly decent job at that.) No variable declarations inside assignment expressions. (while( (var line = next()) != null )) No nested methods. You can have a function-local class, but you cannot have a function-local function. The workaround is to declare and instantiate a function-local anonymous class containing the nested method, but this is cumbersome, unnecessarily verbose, and incurs a performance penalty. No redefining of names (as with the new keyword of C#) The long history of the language inevitably means that there are some bad choices of yore which interfere with newly introduced features. For example: The ability to use the same name for a field and a function never really offered anything of value, but it did necessitate the introduction of the cumbersome double-colon operator when function references were added to the language. Checked exceptions. They were a good idea in principle, but turned out to be too cumbersome in practice. With the advent of lambdas, they represent nothing but hindrance. Collecting a stack trace (and therefore also throwing an exception) might not be as excruciatingly slow as it is in C#, but it is still unnecessarily slow, and prohibitively slow for some purposes. No feature like the __FILE__ and __LINE__ intrinsic macros of C++. There is no way to obtain this information without walking the stack, and is especially problematic since walking the stack is unreasonably slow. The built-in collection model is very outdated and lame. Arrays do not implement any of the collection interfaces so they always need special handling. The Iterator interface is lame. The hasNext() and next() methods are unusable in a for-loop. (A for-each loop can be used with an Iterable, but then you have no access to the Iterator.) A filtering iterator cannot be implemented without cumbersome look-ahead logic and then it is impossible to use it for removing items from the collection because looking ahead means that you are always past the item you want to delete. Lack of unmodifiable collection interfaces means no compile-time readonlyness. Every single collection instance looks mutable, since it is implementing an interface that has mutation methods, but quite often is secretly immutable, meaning that if you make the mistake of invoking any of the mutation methods, you will be slapped with a runtime exception. Fluent collections (collection streams) are lame. They are unnecessarily verbose They require every single call chain to begin with a quite superfluous-looking stream() operation They almost always have to be ended with an equally superfluous-looking collect() operation. They are not particularly extensible because they are entirely based on a single interface (Stream). Their only point of extensibility is at the very end of each call chain, by means of custom-written collectors. Collectors are convoluted, so writing one is not trivial. Collection streams work by means of incredibly complex logic behind the scenes, so: They are very difficult to debug. They are noticeably slower than C#-style fluent collection operations even before we consider the collection step at the end. The collection step is tantamount to making an unnecessary safety copy of the information produced by the collection stream chain. Collection streams are unnecessarily convoluted due to the ill-conceived notion that the mechanism used for fluent collection operations should also be usable for parallel collection operations. Various standard classes are implemented in lame ways. For example: All input-output stream classes suffer a performance handicap due to unnecessarily and ill-conceivedly trying to be thread-safe. Input-output functionality is often achievable not via interfaces, but instead via abstract classes with an unnecessarily verbose set of methods, which makes extending them a tedious and error prone endeavor. (E.g. java.io.Writer, java.io.StreamWriter.) There is no way to attempt parsing a number and obtain an indication as to whether the parsing succeeded or not, without: Suffering the performance penalty of an exception being thrown Having to write code that catches the exception to take notice that parsing failed. (And funnily enough, even though the Java runtime makes liberal use of checked exceptions everywhere, the parse-failed exception is unchecked.)\nThe for-each loop does not do anything about closeable iterators. (The for-each loop of C# properly disposes disposable enumerators.) The try-with-resources statement requires a variable to be defined to hold the closeable object. (The equivalent 'using' statement of C# has no such requirement.) The language runtime if full of always-on error checks instead of using assertions. The inner workings of the language runtime are convoluted, and its performance is hindered, by the operation of various unrequired and arguably ill-conceived mechanisms such as \u0026quot;access checking\u0026quot;, \u0026quot;bytecode verification\u0026quot;, \u0026quot;protection domains\u0026quot;, and even some optional \u0026quot;security manager\u0026quot;. (The security manager is finally being deprecated as of Java 17.) No compiler-enforced method purity. It is not possible to declare a method as pure and have the compiler enforce that it, and any overrides of it, are in fact pure, No compiler-enforced immutability. It is not possible to declare a class as immutable and have the compiler enforce that it, and any derived classes, are in fact immutable. Still no string interpolation in 2021. Inconsistent rules for curly braces. In most cases, curly braces are unnecessary unless the scope they enclose consists of more than one statement. However, the curly braces are mandatory in some arbitrary cases, e.g. for method bodies and for try-catch-finally statements. Lame style conventions, for example: Underscores are inadvisable, which is retared. Methods, fields, variables, and parameters are to be named in camelCase, which is retarded. Package names are to be named in all lowercase, which is retarded. The curly brace style is to be egyptian, which is retarded. No means of programmatically breaking into the debugger as per the System.Diagnostics.Debugger.Break() method of C#. Class\u0026lt;T\u0026gt; is a misnomer. It is actually a type, because it may stand for either a class or an interface. C# does better here, too. Note: the above list of disadvantages is kind of long, because I am intimately familiar with the language.\nFeedback is more than welcome: you'd be doing me a favor. However, be aware that blogger sometimes eats comments, so be sure to save your text before submitting it. If blogger eats your comment, please e-mail it to me.\n","date":"2021-10-04T15:22:38.691Z","permalink":"https://blog.michael.gr/post/2021-10-what-is-wrong-with-java/","title":"What is wrong with Java"},{"content":"\rThis is part of a series of posts in which I am documenting what is wrong with certain popular programming languages that I am (more or less) familiar with. The aim of these posts is to support a future post in which I will be describing what the ideal programming language would look like for me.\nI will be amending and revising these texts over time.\n(Useful pre-reading: About these papers)\nWhat is wrong with C++: Multiple inheritance. Incredibly complex syntax. (Would you like some const with your const?) Cumbersome syntax. (Member variables cannot be initialized at the point of declaration.) Requires splitting the code into header files and implementation files. The ability to use incredibly complex constructs (pointer to pointer to pointer) leads to code that is understood only by the original author, only within a short time after writing it. After that short period of time has elapsed, there exists nobody in the entire universe who understands that code. The ability to freely manipulate pointers often leads to illegal memory accesses. Manual memory management often results in dangling pointers or memory leaks. Lack of generic types. Templates are much more cumbersome to write than generic types. Templates do not promote abstraction. Templates result in a larger executable, which at runtime translates to a larger working set, which translates to reduced performance. Lack of C#-style properties. (Though perhaps some of their functionality can be achieved by operator overloading?) Intentionally undefined behavior. Is essentially a form of error-hiding, and often results in code that works by coincidence but will horribly break under slightly different circumstances. No reflection. RTTI is a very poor excuse for a substitute to reflection. The built-in (stl) collection model is lame: Arcane nomenclature (whoever thought you can push into a vector!) No collection class hierarchy: each collection class is a snowflake. They do not, for example, all inherit from some Iterable common base class. No abstraction: each collection class has its very own snowflake iterator class. The preprocessor hinders static analysis and removes any guarantees about the semantics of the source code. Gives the programmer a false sense of control over code generation. Thinking both at the problem-solving level and at the code generation level results in severe cognitive overhead. At the end of the day, this is all an illusion, as the optimizer will generally do things quite differently from what the programmer imagined. The \u0026quot;you don't pay for what you did not order\u0026quot; dogma prevents a multitude of extremely useful error checks (such as array index out of range) and safeguards (such as guaranteed zero values for non-explicitly initialized members and array elements.) Curly braces. Note: the above list of disadvantages is kind of short, because my C++ is a bit rusty.\nFeedback is more than welcome: you'd be doing me a favor. However, be aware that blogger sometimes eats comments, so be sure to save your text before submitting it. If blogger eats your comment, please e-mail it to me.\nOld comments\nAnonymous 2022-11-03 16:35:12 UTC\nMany of these things are out of date.\n\u0026quot;Member variables cannot be initialized at the point of declaration.\u0026quot; \u0026lt;- They can since C++11 \u0026quot;Requires splitting the code into header files and implementation files.\u0026quot; \u0026lt;- Not strictly true, pretty much everything can be in a header file since C++17 \u0026quot;No collection class hierarchy\u0026quot; \u0026quot;No abstraction\u0026quot; \u0026lt;- Since C++11 for ( type elem : range ) iteration is applicable to any range \u0026quot;The preprocessor hinders static analysis and removes any guarantees about the semantics of the source code.\u0026quot; \u0026lt;- Static analysers can preprocess too \u0026quot;a multitude of extremely useful error checks (such as array index out of range) and safeguards (such as guaranteed trap values for non-explicitly initialized members and array elements.)\u0026quot; \u0026lt;- undefined behaviour means debug builds are allowed to have these things, and checked equivalents are also available\nOthers are fixable with a style guide\n\u0026quot;Manual memory management often results in dangling pointers or memory leaks.\u0026quot; \u0026lt;- Memory management is done by library code. \u0026quot;Templates are much more cumbersome to write than generic types.\u0026quot; \u0026lt;- Only when you declare a class template's member functions out-of-line.\nAnd in some cases I'd say you are flat wrong\n\u0026quot;Templates do not promote abstraction\u0026quot; \u0026lt;- You can't add an interface to a class post-hoc, whereas you can have a class that models a type constraint post-hoc\n","date":"2021-10-04T14:23:59.622Z","permalink":"https://blog.michael.gr/post/2021-10-what-is-wrong-with-cplusplus/","title":"What is wrong with C++"},{"content":"\rI have something blasphemous to tell you.\nUnit Testing is wrong.\nThere, I said it.\nI know I just insulted most people's sacred cow.\nSorry, not sorry.\nI will explain, bear with me.\n(Useful pre-reading: About these papers)\nSo, what is Unit Testing anyway? Unit Testing, according to its definition, aims to examine a module in isolation, to make sure that it behaves as expected without uncertainties introduced by the behavior of other modules that it interacts with. These other modules are known as dependencies. To achieve this, the test refrains from connecting the module with its dependencies, and instead emulates the behavior of the dependencies. That is what makes it a Unit Test, as opposed to an Integration Test.\nThe emulation of the dependencies is meant to be done in a very straightforward and inexpensive way, because if it was complicated, then it would introduce uncertainties of its own. So, if we were to imagine for a moment that the math library is a dependency of the module under test, (just for the sake of the example,) when the module under test asks for the cosine of an angle, the Unit Test does not want to invoke the actual math library to perform the cosine computation; instead, the Unit Test makes sure beforehand to supply the module under test with such inputs that will cause the module to work with a known angle of say 60 degrees, so the Unit Test can anticipate that the module will ask for the cosine of a 60 degree angle, at which point the Unit Test will supply the module under test with a hard-coded value of 0.5, which is known to be the cosine of 60 degrees. The Unit Test then proceeds to make sure that the module does the right thing with that 0.5 and produces the right results.\nIn doing so, the Unit Test expects the module under test to interact with each of its dependencies in a strictly predetermined way: a specific set of calls is expected to be made, in a specific order, with specific arguments. Thus, the unit test has knowledge of exactly how the module is implemented: not only the outputs of the module must be according to spec, but also every single little detail about the inner workings of the module must go exactly as expected. Therefore, Unit Testing is white-box testing by nature.\nWhat is wrong with White-Box Testing White-box testing is not agnostic enough.\nJust as users tend to test software in ways that the developer never thought of, (the well known \u0026quot;works for me but always breaks in the hands of the user\u0026quot; paradox,) software tests written by developers who maintain an agnostic stance about the inner workings of the production code are likely to test for things that were never considered by those who wrote the production code. White-box testing is a laborious endeavor.\nThe amount of test code that has to be written and maintained often far exceeds the amount of production code that is being tested. Each modification to the inner workings of production code requires corresponding modifications to the testing code, even if the interface and behavior of the production code remains unchanged. With respect to procedural logic within the module under test, the Unit Test has to make sure that every step of each workflow is followed, so the test essentially has to anticipate every single decision that the module will make. This means that the test duplicates all of the knowledge embodied within the module, and essentially constitutes a repetition of all of the procedural logic of the module, expressed in a different way. This problem has also been identified by others, and it is sometimes called the \u0026quot;over-specified tests problem\u0026quot;. White-box testing suffers from The Fragile Test Problem.\nA bug fix in the production code more often than not causes tests to break, which then have to be fixed. Note that this often happens even if we first write a test for the bug, which is expected to initially fail, and to start passing once the bug fix is applied: other previously existing tests will break. Unfortunately, it is often unclear to what extent the tests are wrong, and to what extent the tests are right but the production code suffers from other, dormant bugs, that keep causing the tests to fail. When fixing tests as a result of bug fixes in production, the general assumption is that the production is now correct, therefore the test must be wrong, so the test is often hastily modified to make it pass the existing production code. This often results in tests that \u0026quot;test around\u0026quot; pre-existing bugs, meaning that the tests only pass if the bugs are there. White-box tests are not re-usable.\nIt should be possible to completely rewrite a piece of production code and then reuse the old tests to make sure that the new code works exactly as the old one did. This is impossible with white-box testing. It should be possible to write a test once and use it to test multiple different implementations of a certain module, created by independently working development teams taking different approaches to solving the same problem. This is also impossible with white-box testing. White-box testing hinders refactoring.\nQuite often, refactorings which would affect the entire code base are unattainable because they would necessitate rewriting all unit tests, even if the refactorings themselves would have no effect on the behavior of the module, and would only require limited and harmless modifications to the production code, such as the case is when replacing one third-party library with another. White-box testing is highly presumptuous.\nWhite-box testing claims to have knowledge of exactly how the dependencies behave, which may not be accurate. As an extreme example, the cosine of 60 is 0.5 only if that 60 is in degrees; if the cosine function of the actual math library used in production works with radians instead of degrees, then the result will be something completely different, and the Unit Test will be achieving nothing but ensuring that the module will only pass the test if it severely malfunctions. In real-world scenarios the wrongful assumptions are much more subtle than a degrees vs radians discrepancy, making them a lot harder to detect and troubleshoot. In the preface of the book The Art of Unit Testing (Manning, 2009) by Roy Osherove, the author admits to having participated in a project which failed to a large part due to the tremendous development burden imposed by badly designed unit tests which had to be maintained throughout the duration of the development effort. The author does not go into details about the design of those unit tests and what was so bad about it, but I would dare to postulate that it was simply the fact that they were... Unit Tests.\nIs white-box testing good for anything? If you are sending humans to space, or developing any other high-criticality system, then fine, go ahead and do white-box testing, as well as inside-out testing, and upside-down testing, and anything else that you can think of, because in high-criticality software, there is no amount of testing that constitutes too much testing. However, the vast majority of software written in the world today is not high criticality software, it is just plain normal, garden variety, commercial software. Applying space-grade practices in the development of commercial software does not make business sense, because space-grade practices tend to be much more expensive than commercial practices.\nIn high criticality, it is all about safety; in commercial, it is all about cost effectiveness.\nIn high criticality, it is all about leaving nothing to chance; in commercial, it is all about meeting the requirements.\nWhat about leaving nothing to chance? It is true that if you do black-box testing you cannot be absolutely sure that absolutely everything goes absolutely as intended. For example, you may be testing a module to ensure that given a certain input, a certain record is written to the database.\nIf you do white-box testing, you can ensure not only that the record has the correct content, but also that the record is written once and only once.\nIf you do black-box testing, all you care is that at the end of the day, a record with the correct content can be found in the database; there may be a bug which inadvertently causes the record to be written twice, and you would not know.\nSo, at this point some might argue that in promoting black-box testing I am actually advocating imperfect software. Well, guess what: in the commercial sector, there is no such thing as perfect software; there is only software that meets its requirements, and software that does not. If the requirements are met, then some record being written twice is just a performance concern. Furthermore, it is a performance concern not only in the sense of the performance of the running software system, but also in the sense of the performance of your development process: By established practice, it is perfectly fine to knowingly allow a record to be written twice if eliminating this duplication would require too much development work to be worth it, so how is this any different from following an efficient development methodology which might allow that record to be written twice?\nThis is in line with the observation that nobody aims to write software that is free from imperfections. Virtually every single method that returns a collection in all of Java code written since the dawn of time makes a safety copy of that collection; these safety copies are almost always unnecessary, and yet people keep making them, because they do not want to be concerned with what is safe and what is not safe on a case by case basis; case-by-case addressing of safety concerns is the stuff that bugs are made of. Software that is free of bugs is software that meets the requirements, and that's all that counts.\n(Note: personally, I never make safety copies of collections; I use special unmodifiable collection interfaces instead; but that's a different story.)\nConclusion In the book Design Patterns: Elements of Reusable Object-Oriented Software (Addison-Wesley, 1994) by The Gang of Four (Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides) one of the principles listed is:\nProgram against the interface, not against the implementation.\nVirtually all software engineers agree with this self-evident maxim, and nobody in their right mind would take issue with it. To program against the implementation rather than the interface is universally considered a misguided practice.\nIn the context of testing, the corollary to this maxim is:\nTest against the interface, not against the implementation.\nIn other words, do black-box testing, not white-box testing.\nThis is not a unique idea of my own, others have had the same idea before, and have similar things to say. Ian Cooper in his \u0026quot;TDD, where did it all go wrong\u0026quot; talk states that in TDD a Unit Test is defined as a test that runs in isolation from other tests, not a test that isolates the unit under test from other units. In other words, the unit of isolation is the test, not the unit under test. Some excerpts from the talk are here: Build Stuff '13: Ian Cooper - TDD, where did it all go wrong and the full talk is here: TDD, Where Did It All Go Wrong (Ian Cooper, 2017)\nOther references:\nObject-Oriented Reengineering Patterns (Demeyer, Ducasse, and Nierstrasz) - Tests - Your Life Insurance - 6.05 - Test the Interface Not the Implementation Richard Lord - Test the interface, not the implementation (via archive.org) If not Unit Testing, then what? So, one might ask: if Unit Testing is wrong, then what should we be doing instead? The original impetus behind the invention of Unit Testing still remains: when we test a module we want to make sure that the observed behavior is not affected by potential malfunction in its dependencies. How can we avoid that?\nThe way I have been handling this in recent years is by means of a method that I call Incremental Integration Testing. You can read about it here: Incremental Integration Testing.\n","date":"2021-09-22T13:47:17.796Z","permalink":"https://blog.michael.gr/post/2021-12-white-box-vs-black-box-testing/","title":"White-Box vs. Black-Box Testing"},{"content":"Scenario: You are using a laptop with one or more external monitors at home and (a different set of external monitors) at the office. Problem: Every once in a while, when you switch between home and office, some buggy application will not handle its screen positioning correctly, so it will open up off-screen. In the taskbar you can see that the application has launched, but the application window is invisible. How to fix: Switch to the application by clicking its taskbar icon and making sure it is highlighted. Nothing seems to happen, but the application does receive keyboard focus. Press [Alt]+[Space]. The system menu of the application should now appear at the edge of one of your monitors; if not, continue following the instructions anyway. In the system menu, select \u0026quot;Move\u0026quot;. If you cannot see the menu, just press the [Down Arrow] key once, and then the [Enter] key. Press one of the arrow keys, e.g. the [Left Arrow]. This should start the window moving. Move the mouse. This should bring the window into one of the monitors. Position the window where you want it, and click the mouse once to drop it there. Alternatives Various people on the interwebz suggest the following alternative means of fixing the problem. They seem a lot simpler, but none of them works for me.\nAlternative #1: Right-click on an empty area of the taskbar to open up the taskbar context menu. Select \u0026quot;Cascade windows\u0026quot;. (Or any other window arrangement option.) Alternative #2: Switch to the application by clicking its taskbar icon and making sure it is highlighted. Nothing seems to happen, but the application does receive keyboard focus. Press [Win]+[Left Arrow]. (Or [Win]+[Right Arrow], or any other window arrangement hotkey.) Alternative #3: Switch to the application by pressing [Alt]+[Tab] as many times as necessary to select it. Nothing seems to happen, but the application does receive keyboard focus. Press [Win]+[Left Arrow]. (Or [Win]+[Right Arrow], or any other window arrangement hotkey.) ","date":"2021-09-12T03:23:36.209Z","permalink":"https://blog.michael.gr/post/2021-09-in-windows-how-to-recover-invisible/","title":"[SOLVED] In Windows, how to recover an invisible application window"},{"content":"\rAbstract My thoughts and notes on how I would like a new programming language to look like.\nThe unique selling point of the language is:\nAutomatic memory reclamation without garbage collection.\nOther selling points of the language are:\nSimple and elegant. (So that it is suitable for the academia.) Expressive. (So that it is suitable for experienced programmers.) Consistent. (So that it is attractive to developer teams.) Guiding. (So that it promotes best practices.) Fast. (So that it is suitable for high performance computing.) Lean. (So that it is suitable for resource-constrained computing.) This is work-in-progress; It is bound to be heavily amended as time passes, especially if I try some new language, like Kotlin or Rust.\nSummary of language characteristics (Useful pre-reading: About these papers)\nThe main goals of the language are achieved via the following characteristics:\nFor simplicity and elegance:\nScoping by indentation instead of curly braces. (Similar to python.) Keyword-rich syntax which avoids cryptic abbreviations and symbols. Clear distinction between what is a statement and what is an expression. Automatic memory management. For expressiveness:\nLightweight properties, user-defined operators, generics, etc. Type inference whenever possible. Explicit nullability of reference types. Full support for functional programming. Full support for imperative programming without functional Nazism. Async-transparency. For consistency:\nExtensive, mandatory, and in many cases non-suppressible, code inspections. Whenever possible, only one way of expressing any given thing. Extensive and strict formatting rules ensure all code looks the same. Reformatability spares developers from having to type code in a particular way. For performance:\nStrongly typed. Primitive value types correspond to machine words. Intermediate-code-based, Just-In-Time compiled. Fibers. (By means of async-transparency.) For leanness:\nReference counting instead of garbage collection. Minimalistic mandatory runtime library. Separate and optional standard library. For a list of shortcomings of other languages, which this language intends to fix, see:\nWhat is wrong with C++ What is wrong with Java What is wrong with C# What is wrong with Scala Language characteristics in detail Supports reference types and value types, as C# does.\nThe null value is valid only with explicitly nullable reference types.\nAs in C# 8.0 with #nullable enable: A non-nullable reference can be used when a nullable reference is expected. A nullable reference cannot be used when a non-nullable reference is expected, unless: the compiler knows, via data-flow analysis, that the value is not null. For example, by means of an if-statement which precludes null. the value is explicitly cast to non-null. (As with the \u0026quot;null-forgiving\u0026quot; or \u0026quot;damnit\u0026quot; operator in C#.) However, unlike C#: The non-null cast is also an assertion against null, so it does not just circumvent the nullability checks of the compiler, it acts as an if-statement which precludes null. Thus, a non-nullable reference can never accidentally hold null. It is illegal to apply the non-null cast on a reference that is already non-nullable. It is illegal to assign the result of the non-null-cast to a nullable reference. Compiles into an intermediate code format. There are two possibilities:\nLLVM. A new intermediate code format called ObjectCode, which is either interpreted or further compiled into machine code by a Just-In-Time (JIT) compiler. Functionally, ObjectCode is a stack machine language, just as JVM ByteCode is. ObjectCode is expressed as a hierarchical data structure. A binary ObjectCode file is the result of the serializing that data structure into a binary stream. Serialization into a text stream should also be possible. ObjectCode is not trying hard to look like machine language, the way JVM ByteCode does. For example: Instructions have no alternative short-form versions that accomplish the same thing but with fewer bytes. There are no instructions for operations between Integer, Real, Boolean, etc; instead, these operations are available as methods exposed by those value types. Very few instructions have knowledge of any particular data type: Boolean operations have knowledge of the boolean type. (So that the compiler can apply short-circuit evaluation and branching.) The throw instruction has knowledge of the Exception type. The switch instruction has knowledge of the integer type. No unnecessary JVM gimmicks like bytecode verification, stack verification, etc. Executable code is packaged into modules which correspond to C# assemblies.\nSo, no myriads of class files floating around. Each class in a module has its own timestamp. When a module is being made, unchanged classes are copied verbatim from the old module instead of being recompiled, thus retaining their timestamps. For the benefit of benchmarking, the runtime environment can be programmatically instructed to JIT everything at once so that nothing gets interpreted from that moment on.\nAsync-transparency and fibers.\nLooks and feels synchronous, but works asynchronously under the hood. A function can be declared as async; this signifies that the function works asynchronously, but nothing else changes: When invoking: you call it and obtain its result just as with any other function., When implementing: you just return a result, just like any other function. When an async function is invoked, the compiler does not emit a direct invocation to the function; instead, it invokes a special InvokeAsync function of the runtime, which accepts the function to be invoked as a parameter, and returns the result returned by the function. So, it looks as if the runtime will invoke the target function, block-waiting for it to complete, and return the result. However, the runtime does the following instead: starts the asynchronous operation, obtains a promise under the hood, sets aside the promise and the current stack, proceeds to do other stuff. When the promise is satisfied, the runtime: gets the return value from the promise, switches back to that stack, continues execution from there. Note: something called \u0026quot;_hyperscript\u0026quot; already purports to support async-transparency; I do not know whether they switch stacks or pass promises/futures under the hood all over the place. See https://hyperscript.org/docs/#async Note: this is related to OpenJDK JEP 425: Virtual Threads. See https://openjdk.org/jeps/425 An abstraction of an EventDriver is provided, which encapsulates an event driven system. A ConcreteEventDriver is provided, which is a default (\u0026quot;reference\u0026quot;) implementation of an event-driven system. The EventDriver does not contain a post method; instead, it exposes an Injector interface, which does. So, code that only needs to post only needs to have access to an Injector, not to the whole EventDriver. Threads and thread-pools exist for interfacing with legacy systems; the preferred way of working is with fibers and fiber-pools. Each fiber-pool has its own event-driver. TODO: describe exactly what a fiber is. TODO: describe how a fiber exposes a proxy for invocation from other fibers and how the proxy asserts that everything passed back and forth is either thread-safe or immutable. Note that in multi-threaded execution models purity is of very limited usefulness because it does not prevent reading mutable state, so it does not avoid race conditions. However, this language makes use of fibers instead of threads, so there can be no race conditions, so purity becomes useful. Support for functional programming.\nFor example: Lambdas. Tuples. Everything is read-only by default. The keyword mutable must be used to denote something which may vary. (Scala's var and val are too cryptic and too similar; mutability must stand out like a sore thumb.) So, the syntax for declaring a mutable local integer is: mutable local x: integer It is an error to declare something as mutable and forget to ever mutate it. All interfaces are pure by default. A special keyword impure must be used to denote an interface which is allowed to contain impure methods. It is an error to declare an interface as impure and forget to include any impure methods in it. Methods can be either pure or impure, and this has severe implications on what they may and may not do. Most language constructs like if, for, while, switch etc. have both functional and imperative forms. The functional forms must be pure; the imperative forms can be impure. The functional forms may not use flow-control keywords that would affect enclosing scopes; in other words, A functional construct may not use the return keyword to exit the current function A functional construct may not use the break or continue keywords to exit or repeat an enclosing loop. The functional forms make use of the yield keyword to produce values. So, the functional if statement is if( x ) yield 5; else yield 6; Functional loops evaluate to Enumerable and each execution of yield produces a new element. The standard library offers various monads like Optional, Try, and other common functional goodies. The standard collections support fluent constructs. The functional constructs are like Scala's collections, which means that they are somewhat like C#'s linq and not like Java's collection streams. There is no support in the standard collections for parallelization. No such thing as the ref or out parameters of C#. However: No functional Nazism. No obstacles to having mutable state, other than having to use an extra keyword here and there. A proper for loop. Even the functional version of the for loop is a first-class language construct, not yet another higher order function. Thus, when single-stepping through code, you do not have to remember to use step-into instead of step-over in order to skip the header of the loop and reach the body of the loop. Proper break and continue keywords. Freedom to re-assign parameters. Thus making the original value inaccessible. To allow this, the mutable keyword must be added to the parameter. The mutable keyword on a parameter has no meaning for the caller of the method, and therefore does not become part of the method prototype. Everything that can be accomplished functionally can also be accomplished imperatively. No functional gimmicks. The expression evaluated last within a function does not magically become the return value of the function without a return statement; return statements cannot simply be omitted. Same for yield statements. No copy-on-mutation collections. No such thing as Scala's Unit. Two approaches are possible: We maintain a clear distinction between functions and procedures, in which case Unit is unnecessary just as void is unnecessary. Everything is a function, but instead of Unit we stick to good old familiar void, which now becomes an actual data type of which there exists only one instance. Normally, the instance of void should never need to be accessed, (and therefore might not even be accessible,) because it is implied when necessary. For example, the statement return is equivalent to return void.instance. The compiler makes a very clear distinction between statements and expressions. A block scope consists of statements. Statements and expressions are not interchangeable: A statement may contain expressions, but an expression may not contain statements. An expression cannot appear in place of a statement. A statement cannot appear in place of an expression. (With the possible exception of throwing an exception.) Most languages allow invoking a function and ignoring its return value; we put an end to that abhorrent malpractice. When a statement is expected, and we use something which yields a value, that value must be dealt with, in order to be left with a statement and not an expression. The language might provide a mechanism for ignoring a value, (perhaps a cast to void?) but this can also be accomplished by invoking a void-returning method which accepts one parameter and just ignores it. Assignment is a statement, and it requires the use of the let keyword, as in let a = 5; unless a field or local is being declared and initialized at once, in which case the let keyword is omitted, as in local a = 5; This has some drawbacks and some benefits: Drawback: We cannot initialize multiple variables in one go, as in let a = b = c = 5; because everything after the first = must be an expression. That's inconsequential, perhaps even arguably a benefit. Drawback: We cannot assign and compare in one go, as in if( ( let a = f() ) \u0026gt; 5 )... because assignment is a statement, so it cannot be used inside an expression. That's inconsequential, perhaps even arguably a benefit. Benefit: since the compiler can always tell whether it is compiling a statement or an expression, it can treat certain things differently depending on whether they appear in a statement or an expression. Namely, the equals sign can now be used either in a statement, as the assignment operator, or in an expression, as the equality check operator. Thus, after so many decades, we can finally say good-bye to the inelegant double-equals (==) legacy of C, and start using the single equals sign for equality comparison, as it was always meant to be. The inequality operator can either stay as != or become \u0026lt;\u0026gt;. The prefix and postfix increment operators are problematic because they are expressions with side-effects, (they both mutate an existing value and yield a new value,) so we might disallow them, and require the use of the long form instead: let x = x + 1; If we keep them, then they will certainly only be allowed in expressions. (You could make it a statement with (void) x++; but why would you?) We keep static as in Java and avoid Scala's inelegant companion objects. There is no support in the standard collections for parallelization. When declaring a lambda, the keyword function must be used. When declaring a tuple, the keyword tuple must be used. Everything is private by default, unless explicitly given a higher visibility.\nTherefore, the language does not have a keyword to indicate that something is private. Note that this also applies to interface methods: if you want an interface method to be public, you have to declare it as public, otherwise it stays private and may only be invoked from other methods of the same interface. Everything is non-inheritable by default, unless explicitly declared as inheritable.\n(Except for interfaces, which are by definition inheritable.) Therefore, the language does not have a keyword to indicate that something is non-inheritable (sealed in C#, final in Java.) Note that this also applies to interface methods: if you want an interface method to be overridable, you have to declare it as overridable. This makes certain other rules unnecessary, for example we do not have to stipulate that it is an error to explicitly declare a method as non-overridable in a class which has already been declared as non-inheritable. It is an error to declare something as inheritable and fail to ever inherit from it. This is enforceable because inheritance is confined within a module, so all members of an inheritance hierarchy are known during the compilation of the module. Emphasis on purity.\nThere are two ways we can go about this, and which way we will go is yet to be decided. Procedures and functions A method can be either a procedure or a function. A procedure: Does not return anything. Is impure. (Must have at least one side-effect.) Can indicate failure only by means of throwing an exception. A function: Returns something. Can indicate failure either by throwing an exception or by returning a Try monad. Is pure. (Must have no side-effects.) Experimental idea: the keyword method can be used to denote a higher order method which is either a procedure or a function depending on whether its parameter is a procedure or a function. It must have a parameter declared as method instead of the more specific procedure or function. It may have additional parameters that are explicitly function or procedure. It must treat its parameter method as a function, meaning that when it invokes that method, it must obtain a return value from it. It can be coded as a function, meaning that it can return that value. From the point of view of the caller, it behaves either as a procedure or as a function depending on whether the caller passes a procedure or a function to its method parameter. The caller may actually pass yet another a method to it, in which case the caller is in turn a method instead of a procedure or function. Such a construct would eliminate the need to declare both a function and a procedure for each higher order operation, and at the same time avoid the inelegance of Unit. Pure and impure methods All methods are functions. Methods that have nothing to return must be declared to return void, which is equivalent to Scala's Unit in the sense that it is an actual data type of which there exists only one instance. Thus, void-returning and non-void-returning functions can be treated in exactly the same way in all situations. For example: From within a void function we can use the return keyword to return the result of invoking another void function. This in turn means that a single higher order function can operate both on void-returning and non-void-returning functions. Impure methods must be explicitly marked with the impure keyword. An impure method may return either void or non-void. A pure method must return non-void. (It would not make sense to return void, because it cannot perform any side-effects, so its sole reason of existence is to return something.) In all cases: A pure method / function: May not assign to any field of this. May not invoke any impure methods / procedures on any of its parameters, including this. May not escape an impure interface of any of its parameters, including this. It is okay to escape pure interfaces, since there will be no side-effects. May still declare and manipulate mutable locals, including the ability to escape mutable locals or impure interfaces thereof. It would be nice to be able to say that a pure method / function can never throw an exception; however, we cannot do that, because even a pure method / function can, for example, accidentally divide by zero. Mechanisms are provided whereby purity checks can be suppressed when necessary, in order to allow for functions which, although formally pure, may under the hood modify caches, update statistics, perform diagnostic I/O, etc. Emphasis on readability, at the expense of terseness when necessary.\nTyping is not one of the major problems faced by our profession; unreadable code is. The language should be suitable for universities to teach, so unlike Scala, it needs to have a low entry barrier. All language keywords are fully spelled out and avoid unnecessary technicalities . No inelegant abbreviations like fun, def, mut, etc. A function is denoted by function. (Duh!) A field is denoted by field. (Duh!) A mutable field is denoted by mutable field. (Duh!) A local is denoted by local. (Duh!) A mutable local is denoted by mutable local. (Duh!) The Boolean type is boolean, not bool. The Integer type is integer, not int. Nobody will ever have to type i, n, t, e, g, e, r, because any halfway decent code editor will give you integer if you just type i, hit Ctrl+Space to open up auto-completion, and then Enter to pick the first suggestion. The Long Integer type is long integer, not long. The 64-bit IEEE floating point type is real, not double. The 32-bit IEEE floating point type is short real, not float. In general, the language aims to reduce the amount of parentheses. Expressions may not be parenthesized, only sub-expressions may. So, the popular construct return (result) is not just redundant; it is actually a compiler error. In general, the language favors words over punctuation, so: Inheritance by means of extends and implements keywords as in Java instead of the : character of C#. Fully spelled out for each a in b do like C# instead of the for( a : b ) of Java. Boolean operators are words, like Pascal and Python and unlike the C family. i.e. the operators are and, or, and not instead of \u0026amp;\u0026amp;, ||, and !. The compiler handles boolean operators, applying operator precedence and short-circuit evaluation. The compiler maps all other operators to method calls, (observing operator precedence rules,) as follows: a + b maps to a.Plus( b ). a - b maps to a.Minus( b ). a * b maps to a.Times( b ). a / b maps to a.Per( b ). a % b maps to a.Modulo( b ). a ^ b maps to a.Power( b ). a = b maps to a.Equals( b ). a \u0026amp;lt; b maps to a.Below( b ). a \u0026amp;gt; b maps to a.Above( b ). a != b maps to not a.Equals( b ). (*) a \u0026amp;lt;= b maps to not a.Above( b ). (*) a =\u0026amp;gt; b maps to not a.Below( b ). (*) -a maps to a.Negative. ~a maps to a.TwosComplement. ++a maps to a.PreIncrement(). a++ maps to a.PostIncrement(). So, when we code a + b, this will only compile if the type of a has a function called Plus with a parameter of the type of b. (*) These negations are meant to save us from having to have negative forms of the functions; I think they are okay; it remains to be seen if there are situations where this will not work. NaN comes to mind as a possible pitfall, but then again a comparison against NaN should perhaps throw an exception. Preference towards having only one way for any given thing.\nWhen multiple ways of accomplishing the same thing are conceivable, the language design tries, when possible, and when it makes sense, to make a specific choice and prohibit all other ways. For example: When it is unnecessary to qualify an instance member with this, it is an error to qualify it. When it is unnecessary to qualify a static member with the class name, it is an error to qualify it. When the body of the \u0026quot;then\u0026quot; part of an if statement never falls through (because it ends with either a return or a throw statement) it is an error to use the else keyword. Encapsulation:\nA nested scope has access to private members of the enclosing scope. The enclosing scope never has access to private members of nested scopes. Note that this corrects the insanity of Java which allows an enclosing class to have access to private members of nested classes. (Duh!?) When a source file declares a namespace as public, only the classes in that source file are exported. This stipulation is necessary since multiple source files may declare a namespace, but only some of those source files might declare the namespace as public. A module may expose interfaces, enums, records (value types), and classes. However, when a module exposes a class, what actually gets exposed is only the interface of that class, not a class itself. In other words, the language will never expose across modules the constructor of a class, nor its protected methods. This has some very interesting implications: All classes participating in an inheritance hierarchy must be defined within a single module: One cannot extend a class defined in another module. All classes participating in an inheritance hierarchy are known during the compilation of the module that contains the hierarchy. This allows for certain useful optimizations. The creation of a new instance of a class defined in another module cannot be accomplished by invoking a constructor; it can only be accomplished via a factory method. Memory management: Reference counting instead of garbage-collection.\nThe memory model looks a lot like the memory model of Java and C#: The heap consists of big chunks of memory that are allocated from the operating system at once. The runtime does its own memory management within these chunks, for efficiency. Objects are actually pointers to objects that live on the heap. Pointers cannot be manipulated as they can in C++. Value types live either in local storage or as members of other types. When necessary, value types can be treated as reference types by means of boxing. Pointers are implemented as smart (shared) pointers, so that: There is no need for garbage collection. There is no need for each object to have its own lock. There is no need for finalization. There are no preposterous situations like object resurrection. There are fewer sources of randomness and non-determinism in the memory layout and in the responsiveness of the code. Destruction is assured and immediate the moment an object ceases to be referenced. Destruction involves real destructors as in C++. While a destructor executes, all objects referenced by the object being destructed are guaranteed to still be present and alive. (Unlike garbage-collected languages, where finalizers have to cope with the fact that some of the referenced objects may have already been collected.) The reference count is accommodated in the object itself, so smart pointers can be appreciably more lightweight than in C++. The runtime may choose to implement smart pointers using double indirection, so as to be able to perform memory defragmentation. Addressing the pitfalls of reference counting: Reference counting suffers from two pitfalls: Long reference chains: May result in stack overflow when disposed. Circular references: Result in memory leaks. We address these pitfalls as follows: Long reference chains: We solve this by making destructors deliberately fail if they are ever re-entered, so that we can detect the deallocation of even the smallest chain that consists of only two nodes. The programmer can then modify their code to do one of the following: Manually perform the destruction of the chain in a way that avoids recursion. Refactor things so that the objects are kept in a collection instead of forming an ad-hoc chain. Explicitly unlink and destroy the chain using the delete chain keyword, which works in a non-recursive way. Circular references: A debug-time-only mark-sweep checker that runs on its own thread detects leaked cyclic object graphs and warns the programmer about them. (It does not attempt to fix anything.) The programmer can then modify their code to do one of the following: Break any cycles in the graph before unlinking it. Explicitly unlink and destroy the cyclic graph using the delete cyclic keyword, which gracefully handles cyclic object graphs. These means of addressing the pitfalls of reference counting are not perfect, so some extra maintenance will sometimes be required. For example, we might think that we are properly handling all cyclic object graphs, but as a result of a change somewhere, we may now discover that we have a new cyclic object graph, which we must deal with; Still, the extra trouble is expected to be rare, and it is expected to be very well worth all the trouble we save by not having to have a garbage collector. Of interest: https://verdagon.dev/blog/hybrid-generational-memory Constructor syntax like Scala.\nConstructor parameters in the class header. Constructor code in the class body. (With the additional restriction that it must all appear up-front.) Additional constructors by means of static factory methods. Any constructor parameters that are referenced by methods automatically become fields so that we do not have to declare extra fields and initialize them from the parameters. Strong distinction between release runs and debug runs.\n(But not necessarily different builds; Optimization is a JIT concern.) Externally supplied constant values.\nA special type of constant can be defined, whose value is not specified in the source code, and must instead be supplied later: During compilation, by means of a special parameter to the compiler, or At runtime, by means of a special parameter to the launcher. These constants are better than C-style \u0026quot;manifest constants\u0026quot; and C#-style \u0026quot;defined symbols\u0026quot; because they are well defined, strongly typed, mandatory, and obey normal static immutable field rules. This means that: It is possible to know the set of all external constants that must be defined in order to compile and run something. An attempt to compile or run something without supplying all external constant values will always result in an error. An attempt to supply an external constant value for a non-existent external constant will always result in an error. Each externally supplied constant value must be of the correct type expected by the constant declared in the code. When using external constants for conditional compilation, the code paths that are not selected will result in no code being generated, but must still pass compilation, so there is no danger of code rot. With some help from the loader we can write tests that exercise code under different values for runtime-supplied external constants. Integer types:\nFixed Integer types with explicitly defined sizes, as per C#. Flex integer types whose size is determined by the runtime according to what is most efficient for the underlying hardware architecture. Each flex integer has a \u0026quot;Guaranteed Width\u0026quot;, which is the minimum width that this integer is guaranteed to have on any hardware architecture. These widths are: 8 bits for tiny integer 16 bits for short integer 32 bits for integer 64 bits for long integer On debug runs, the runtime checks all operations on flex integers, and if there is an overflow past the guaranteed width, a runtime exception is thrown. Thus, we ensure consistent flex integer behavior on any architecture. This corrects the narrow-mindedness of C# where int has been defined to be exactly 32 bits long, even on architectures with a larger machine word size. (Which is pretty much all major architectures today that 64-bit is the norm.) Full set of signed and unsigned integers as per C#, both for the fixed and flex flavors.\nExceptions\nLightweight exceptions that are inexpensive to throw and to catch. No such thing as the \u0026quot;checked\u0026quot; exceptions of Java. No extra baggage: The base Exception class does not even have a \u0026quot;message\u0026quot;, let alone a \u0026quot;localized message\u0026quot;. The ToString() method of the base Exception class: Is not overridable. Yields a string consisting of the class name of the exception followed by the name and the string representation of the value of each one of its fields, obtained using reflection. If you want an exception to result in a human-readable error message that you can actually show to an end user, you have to accomplish this entirely by yourself. (Please make sure to do this in the end-user's native language, which, statistically speaking, is unlikely to be English.) Standard Collections Model\nThe standard language runtime provides the following: An assortment of unmodifiable collection interfaces: Enumerable, Collection, List, Map, etc. Enumerable exposes a property for accessing the current element, and separate methods for checking whether there exist more elements and for advancing to the next element, as in C#. A Collection is an Enumerable with a length and the ability to check whether it contains a certain element, as in Java. Map is also a collection of Map.Entry. This is as in C#, where a Dictionary is a collection of KeyValuePair. This is unlike Java, where Map is not a collection, and in order to obtain the collection of entries you must invoke Map.entrySet(). Factory methods create immutable collection classes implementing the unmodifiable collection interfaces. An assortment of \u0026quot;rigid\u0026quot; (i.e. mutable, but structurally immutable) interfaces which extend the unmodifiable interfaces adding methods to replace existing items but no methods to add or remove items: RigidEnumerable, RigidCollection, RigidList, RigidMap, etc. An assortment of mutable collection interfaces which extend the rigid interfaces adding add/remove/clear methods: MutableEnumerable, MutableCollection, MutableList, MutableMap, Queue, Stack, etc. A MutableCollections factory exposing methods that create mutable collection classes implementing the mutable collection interfaces. The Values collection of a mutable map returns a RigidCollection of map values, so that: You can replace an element in this collection, which will have the side-effect of associating an existing key with a new value. You cannot add an element to this collection, which makes sense because you have no means of specifying the key that should map to the newly inserted value. The method for adding an item to a collection is called 'Add', not 'Push'. For consistency, even the Stack collection exposes an Add method, not a Push method. Collaboration between the language runtime and collections: The for-each loop operates on Enumerable. The loop variable can be reassigned, causing the current element of the Enumerable to be replaced with a new value. In this case, the for-each loop requires a RigidEnumerable. A special keyword allows removing the current item, in which case the for-each loop requires a MutableEnumerable. Since we have proper destructors, there is no need for special handling of disposable enumerators. (Something which C# provides, but Java lacks.) An array literal evaluates to an instance of RigidList, so the language is free from arrays, like Scala. Heavy promotion of assertions and plenty of built-in extra error-checking on debug runs, such as:\nArithmetic checking An exception is thrown when any of the following occurs: Division by zero. Fixed integral type overflow. (This can be selectively suppressed on an individual expression basis as with the \u0026quot;unchecked\u0026quot; keyword of C#.) Flex integer guaranteed width overflow. (Possibly) Operations on NaNs. Throwing Switches If the switch data type is exhaustively switchable (e.g. boolean): It is an error if not all cases are covered and no default case is provided. It is an error if all cases are covered and a default case is provided. If the switch data type is not exhaustively switchable (e.g. integer): If no default case is provided, an implicit default case is supplied by the compiler which throws an exception. This plays nicely with code coverage: no more uncoverable assertions in unreachable default clauses. If you want a switch statement with default case fall-through on a non-exhaustively switchable type, add an empty default case. (Duh!) Big on warnings and errors.\nMost things traditionally thought of as warnings are errors. Most checks of the kind that IntelliJ IDEA calls \u0026quot;inspections\u0026quot; are built-into the language as warnings, many of them even as errors. Selective warning suppression only; no bulk suppression. Warning suppression is possible only on the individual statement where the problem occurs, never on a larger scope. Warnings always cause compilation to fail. It is as if a \u0026quot;treat warnings as errors\u0026quot; option is always on and cannot be turned off. The difference between warnings and errors is not that you can ignore warnings and proceed to run; the difference is that a warning can be suppressed, whereas an error cannot. Furthermore, the language designates a message as a warning or an error based not on its severity, but instead on whether the programmer can reasonably be required to fix it or not. If it is reasonable to require the programmer to fix it, then the programmer better fix it, so there is no need to be able to suppress it, so it is an error. If it is unreasonable to require the programmer to fix it, then the programmer should be able to suppress it, so it is a warning. For example: If you have an unused import statement, you can very easily remove that import statement, so it is reasonable to require you to fix it. Therefore, the \u0026quot;unused import\u0026quot; message is an error. If you have marked something as deprecated, and yet you must still make use of it in a couple of places until the day that it gets completely removed, then you have no way of fixing this problem, therefore you must be allowed to suppress it, therefore the \u0026quot;use of deprecated symbol\u0026quot; message is a warning. You will, however, have to explicitly suppress that warning on each and every usage of that symbol. A warning suppression on a statement that does not actually produce a warning is an error. Syntax:\nLine-oriented, with scoping dictated by indentation (roughly as in Python) instead of curly braces. Since it is very difficult (if not impossible) to express indentation rules in a formal grammar, this is handled by the tokenizer: When the indentation increases, the tokenizer emits a hidden scope-start token. When the indentation decreases, the tokenizer emits a hidden scope-end token. The tokenizer also handles line breaking and line joining, so that the parser ends up parsing a C-style language. There are two types of statements: simple and compound. A simple statement occupies a single line; it may contain expressions, but it may not contain any nested scopes. A compound statement begins with a simple statement as a header, and is followed by a dependent scope. A scope contains statements, which may in turn be either simple or compound. Some constructs that normally correspond to compound statements (e.g. the if statement) also come in \u0026quot;expression form\u0026quot;. The for loop does not have an expression form, due to the extra complexity of the multiple statements that it contains; however, the for-each loop does come in expression form. Normally, each simple statement must be on a separate line. To allow joining multiple simple statements in one line, a special line-joining punctuation is used, which is the semicolon. Therefore, the semicolon is illegal at the end of a line. Normally, an entire simple statement must be contained within a single line; in other words, a simple statement may not span multiple lines. To allow splitting a simple statement into multiple lines, a special line-splitting construct is used. This construct is to be determined: It may be a backslash at the end of the line that is being split into the next It may be double the amount of indentation on the next line, signifying that it belongs to the previous one. It may be both of the above. Formatting:\nThe code formatting style of the language is thoroughly and unambiguously defined by an extensive set of rules. Some degree of freedom is allowed, but even that is unambiguously controlled by special punctuation that exists specifically for that purpose. This means that the formatting of a source file is thoroughly, accurately, and deterministically predictable from the language formatting rules and the punctuation present within the file. This in turn allows code editors that can: at any moment reflow an entire source file to its proper format, or even: continuously reflow code, as it is being typed, to its proper format. This in turn allows a compiler which imposes strict enforcement of the formatting rules, so that the slightest deviation, even by a single space, is a compiler error. This brings us to the following paradox: Even though the formatting rules are extremely detailed, And even though the enforcement of the formatting rules is draconian, The programmer never has to worry about code formatting, because it is being taken care of automatically. The benefit of all this is that all code by all programmers will always have the exact same formatting, and yet no programmer will ever have to be bothered with having to type code in a specific way. (It will also make the language parser slightly faster.) Some indicative highlights of the formatting rules: Tabs for indentation The tab character denotes indentation, and may only appear at the beginning of a line; it is prohibited anywhere else. Only the tab character may be used to denote indentation; the use of anything else to denote indentation, including the space character, is an error. It is an error to have indentation in a line which is otherwise blank. The language defines where a space may and may not appear. When a space is expected, exactly one space must be given. (For example, right after a comma.) When zero spaces are expected, exactly zero spaces must be given. (For example, right before a comma.) Note that this prevents tabular code formatting, which is the practice of inserting spaces to column-align similar parts of consecutive statements. That is okay, because tabular code formatting is a bad idea anyway, since it is a source of needless git merge conflicts. In any case, if some folks really need tabular code formatting, they can achieve it via spacing comments ( /* */ ). The language strictly defines when and how blank lines may be used. For example: There must never be two consecutive blank lines anywhere, at all, under any circumstances, for any reason, ever. There must always be exactly one blank line before a block comment. (Even a single-line block comment.) If you want a comment without a blank line, then use a line comment instead of a block comment. There must never be a blank line anywhere else, including: Between method definitions. This allows us to define whole groups of single-line methods without wasting a lot of screen real estate. If you want blank lines between method definitions, add a block comment before each method definition; thus, a blank line will be mandatory before the block comment. Between lines of code. Most programmers have the habit of using blank lines within method bodies, to separate logical groups of lines of code. This is bad practice, because only the programmer who wrote the code knows why those lines form a separate group and why that group should stand out from the rest. If you have multiple conceptually distinct groups of lines of code within a single method, then either: Add block comments explaining what each group does, (in which case a blank line before the block comment is mandatory,) or Move each group into a separate function, and give the function a descriptive name. The language supports functions nested within functions, so you can do this without polluting the namespace of the class. The language uses no curly braces, so you will not be wasting a lot of screen real estate in doing so. Between class definitions. This allows us to define whole groups of single-line classes without wasting a lot of screen real estate. Admittedly, single-line classes are rare, so let's just say that this rule exists just for consistency. Special formatting punctuation allows overriding language default formatting rules on a case per case basis. For example: A \u0026quot;line splitter\u0026quot; is a special punctuation character which allows splitting a construct into multiple lines when the language formatting rules would have normally required that construct to be all in one line. For example: the language formatting rule for expressions is that an expression must fit in one line; so, if an expression needs to span multiple lines, a line splitter must be used to indicate precisely at which point the expression is to break into the next line. The use of a line splitter in a place where it is not required is an error. The \u0026quot;line joiner\u0026quot; is a special punctuation character which allows a construct to appear all in one line when the language formatting rules would have normally required that construct to be split into multiple lines. For example: the language formatting rule for methods is that the body of the method must be on a separate line from the prototype. So, if a very short method needs to fit entirely in one line, a line joiner can be used to allow this. The use of a line joiner in a place where it is not required is an error. Capitalization The language is case sensitive, and capitalization matters a lot more than in other languages. Identifier casing must be one of the following: lowercase SentenceCase kebab-case SentenceKebab-case Note that kebab-case is possible because the language mandates spacing around operators, so there is no possibility to confuse an identifier containing a dash with the dash operator between two identifiers. The following are expressly disallowed: The dash as first or last character of an identifier. camelCase. UPPERCASE and SCREAMING-KEBAB-CASE. Two or more consecutive capital letters. For an explanation why, see the following section about spell-checking. Separate capital letters with dashes; for example, XSpacing is not allowed, but X-Spacing is fine. Do not use acronyms; use either: fully spelled out words, i.e.. \u0026quot;GraphicalUserInterfaceStyle\u0026quot;, or words that replace acronyms, i.e. \u0026quot;GuiStyle\u0026quot;. Underscores and all forms of snake_case. (Though an underscore alone might act as a special identifier, or special punctuation, to be determined.) This is because we support kebab-case, and snake_case does not look sufficiently different from kebab-case. Kebab-case is preferable to snake_case because on most keyboards the dash is slightly easier to produce than the underscore, since it does not require Shift. Some capitalization rules apply to language constructs and are enforced by the compiler: All names of types and namespaces must start with an uppercase letter. All public and protected member names must start with an uppercase letter. All private members must start with a lowercase letter. All local and parameter names must start with a lowercase letter. Spell Checking The language comes together with a spell-checking dictionary, the contents of which are part of the language specification. A module can have a supplemental user-defined spell-checking dictionary file which: Is meant to be committed to source control Is meant to undergo code review just as any other source file. The compiler spell-checks source code and issues a warning if it encounters any unrecognized words. Specifically, the compiler will issue a warning when any of the following fails to pass spell-check: Any part of an identifier. A word inside a string literal. A word in a comment, unless it is markup referring to an identifier. For the purpose of spell-checking, identifiers are broken into parts based on SentenceCase and kebab-case boundaries, as well as boundaries between letters and digits. This means that: \u0026quot;CryptoGraphy\u0026quot; will not pass spell-check unless \u0026quot;graphy\u0026quot; has been added to the spell-checker. (It shouldn't; it is not an English word; use \u0026quot;Cryptography\u0026quot; instead.) \u0026quot;Mousepointer\u0026quot; will not pass spell-check unless mousepointer has been added to the spell-checker. (It shouldn't; it is not an English word; use \u0026quot;MousePointer\u0026quot; instead.) A warning for a misspelled identifier is issued only at the point of definition and not on each occurrence of the identifier, so that: You only see the warning once, not five hundred times. There is no warning at all for identifiers that you have no control over, due to them being defined in external modules. In other words, a module does not have to duplicate the spelling dictionaries of external modules, nor does a module have to ship with its spelling dictionary. Two or more consecutive capital letters are disallowed because: Each individual capital letter acts as a word delimiter, so it constitutes a word by itself. To allow for single-letter variables, every individual letter passse spell-check. So, a word made of capital letters circumvents the spell-checker. The language does not allow circumventing the spell-checker. (One day someone will inevitably submit a feature request for some means of disabling the spell checker; the answer they will receive is that if they do not have to use this language; there are so many other languages to choose from.) Comments\nSpecial formatting within comments is achievable with the use of Markdown as opposed to HTML or any ad-hoc syntax. This special formatting is available in all comments, not just doc-comments. Some extensions to markdown are necessary in order to specify relationships between code. For example, when defining a link, one can omit the part within the parentheses, in which case the part within the square brackets is expected to be a resolvable symbol, and the resulting link points to that symbol. The syntax for specifying the symbol requires no gimmicks like the hash-sign which is needed in Java's doc-comments to separate the type name from the member name. If the symbol is not fully qualified then there must be an import statement for that symbol somewhere within the source file. The use of a symbol in a comment is enough to prevent the corresponding import statement from being flagged by the compiler as unused. Possibly: allow the comment that describes a parameter to be placed with the parameter itself. Inheritance\nA class may extend only one other class but implement any number of interfaces. The only difference between a class and an interface is that an interface cannot have fields, a constructor, or a destructor; in all other respects, classes and interfaces are equivalent, meaning that an interface can have static, public, protected, and private methods. By default, a class cannot be extended unless it is marked as extensible. By default, a method of a class cannot be overridden/extended unless it is marked as follows: If it is abstract, it must be marked as abstract. If it is overridable, must be marked as overridable. (Duh!) This corrects Java's exuberance of allowing any method to be overridden unless declared \u0026quot;final\u0026quot;, and C#'s unwarranted technicalism of calling such methods \u0026quot;virtual\u0026quot;. If it is overridable with the provision that overriding methods must invoke the base method, it must be marked as extensible. Methods that override other methods must be marked as follows: A class method which implements an abstract method must be marked with implements base. A class method which overrides an overridable method must be marked with overrides base. A class method which extends an extensible method must be marked with extends base. Within the extending method: The base method must be invoked exactly once. (This can become a bit complicated with alternative execution paths, so we might want to mandate that there must be only one possible execution path at the point where the base method is invoked.) The invocation of the base method can be simplified: The name of the method can be replaced with base. The parameters can be omitted. In this case the base method is invoked with the values that the parameters have at the moment of the invocation, allowing the extending method to alter the values of the parameters before invoking base. A class method which implements a method of an interface must be marked with implements X. X is the interface-qualified-method-name of the method being implemented. The implementing method name may differ from the implemented method name (as long as the parameter list matches) and it will be accessible via both names. X can also be a comma-separated list of interface-qualified-function-names, if the method implements multiple interface methods of different interfaces. In this case, the method will be accessible via any of the names. Note that this corrects the stupidity of C# where no special marking is necessary for a class method that implements an interface method. Note that C# provides a syntax for optionally specifying that a class method implements a method of a particular interface, but makes the implementing methods inaccessible, which renders the feature unusable. Note that a class method may both override a superclass method and implement interface methods by adding both overrides and implements. A class method does not automatically become overridable or extensible by virtue of implementing, overriding, or extending another method; it must in turn be marked as overridable or extensible if that is the intention. Built-in Intertwine.\nBuilt-in Domain-Oriented Programming features.\nAlternatively, look into Scala's implicit parameter lists. Built-in support for testing.\nBundled Testana, see Testana: A better way of running tests. Somewhat different testing semantics than JUnit: The test class does not get re-instantiated prior to invoking each test method. No 'before' method: use the test class constructor for this. No 'after' method: use the test class destructor for this. Use of the exact same assertion facility for test code as for production code. No other test facility gimmicks like \u0026quot;expect\u0026quot;, \u0026quot;assume\u0026quot;, etc.: write the darn thing in code. Test methods are always executed in the order in which they appear in the source file. When a test class is derived from another test class, the test methods of the base class are always executed before the test methods of the derived class. To enable separate testing of debug runs and release runs, assertions are always enabled for the testing code, but for the code-under-test they can be either enabled or disabled. Even though all source files that constitute a module are compiled into a single binary file, (as per C# assemblies,) each class within that binary file comes with its own timestamp, to accommodate tools like Testana. (Possibly) Explicit distinction between logic classes and data classes.\n(Possibly) Built-in versioned externalization of data classes. (Possibly) Built-in data-modelling framework for the data classes. Built-in internationalization features (i.e. Unicode strings and culture-aware operations) but also full support for ANSI strings and culture-neutral operations.\nLightweight properties, exactly like in C#, with additional compiler support for obtaining a property as a separate entity and manipulating it independently of the object that it belongs to. (Probably a value type containing a reference to the object that owns the property and a reference to the reflection object that represents the property.)\nNO compiler support for events.\nTime Coordinate Data Type\nInternally represented as a 64-bit IEEE floating point number of days since some epoch, allowing for: low-precision coordinates billions of years away from the epoch femtosecond precision coordinates near the epoch. A special static \u0026quot;this\u0026quot; keyword (this class?) that you can use to refer to the current type in a static context without having to code the name of the type as you have to do in Java and in C#.\nProper method literals and field literals\nNo compromise like the nameof() of C#. For example: Method m = method someMethod; assigns to m the reflection method object of someMethod but causes a compiler error if someMethod has overloads. Method m = method someMethod(int); assigns to m the reflection method object of a specific overload of someMethod. Method m = this method; assigns to m the reflection method object of the method that is currently being compiled. Field f = field someField; assigns to f the reflection field object of someField. Source intrinsics\nA source-line intrinsic similar to the __LINE__ macro of C and C++ or the [CallerLineNumber] attribute of C#. A source-file intrinsic similar to the __FILE__ macro of C and C++ or the [CallerFileName] attribute of C#. Note that the source filename yielded by this intrinsic is relative to the root of the source tree, not absolute. A source-root intrinsic which yields the absolute path to the root of the source tree. Namespaces, mostly as seen in C#, but with some differences.\nYou cannot just import a namespace and make everything in it accessible; instead, you have to do one of the following: Import a specific name from a namespace; (like a java import statement without a wildcard;) then, you can use that one name without qualification. Import an entire namespace, but with a namespace alias, like in XML; then, you can use any name from that namespace, but each name will have to be qualified with the chosen alias. Compiler-enforced conformity between directory names and namespace names, as in java, and unlike C#. (Or, as in C# with ReSharper.) Compiler-enforced conformity between source file names and class names, as in Java and unlike C#, with a couple of differences: The file name of a source file may match a namespace defined in that file. The file name of a source file may match the base-most class defined in the file, but the file may also contain additional classes derived from it. System functionality injection\nSystem functionality is available strictly via interfaces. The main entry point function of a program declares in its argument list each system interface that it intends to use. Yes, this can become unwieldly; and yet that's how it is going to be. When about to run an application, the runtime uses reflection to discover which interfaces are needed by the entry point, and passes each one of them to it. From that moment on, user code makes sure to propagate system interfaces to all application code that needs them. This means that no system functionality is provided statically. For example: The data type for expressing time coordinates does not include a static method for obtaining the current time, as in most other languages. Instead, there is a SystemClock interface which provides this functionality, and this interface must be obtained via main() and propagated to all places that need to use it. Similarly, if you want to open a file, you cannot just instantiate a file class; you have to obtain the FileSystem interface, and ask it to open a File for you. Runtime environment:\nDirectoryPath and FilePath interfaces that encapsulate file-system pathnames and filenames, so that one rarely needs to engage in string manipulation with paths. No such thing as a \u0026quot;current directory\u0026quot;; All paths are absolute. When a path is constructed from a string, the absolute path is immediately computed, and the computation may take into account whatever the host system considers to be the \u0026quot;current directory\u0026quot; of the process. (So, by obtaining the DirectoryPath of \u0026quot;.\u0026quot; one can discover the current directory, but one has no way of changing it, and the runtime environment will never change it.) Cover image: \u0026quot;Coding Software Running On A Computer Monitor\u0026quot; by Scopio from NounProject.com\n","date":"2021-09-01T04:47:26.198Z","permalink":"https://blog.michael.gr/post/2022-07-a-programming-language/","title":"A Programming Language"},{"content":" Actor Wayne Knight in the original Jurassic Park movie\nplaying the role of the unscrupulous programmer Dennis Nedry, (anagram of \u0026quot;Nerdy\u0026quot;,) the main villain. **** **Malicious Inaction** (noun) any situation where a piece of software encounters an unexpected condition and responds by deliberately doing nothing, including *not* throwing an exception. Synonyms: Silent Failure; Deliberate Malfunction; Unscrupulous Programming; Undermining; Sabotage; Treachery; Subversion; Vandalism. I think that the term \u0026quot;Silent Failure\u0026quot; fails to express the amount of harm done. Sure, the word \u0026quot;failure\u0026quot; indicates that something went wrong, but the word \u0026quot;silent\u0026quot; somewhat lessens the severity of the term, and it makes sound as if no feathers were ruffled, so it may have been alright.\nWell, no. It was not alright. It never is. We need a stronger term to better capture the harm caused by the sinister practice of hiding error. We need a term that clearly conveys wrongdoing, a term that assigns blame and shame.\nHence, I present to the world my new and improved term: Malicious Inaction.\n","date":"2021-07-27T11:27:10.824Z","permalink":"https://blog.michael.gr/post/2021-07-malicious-inaction/","title":"Malicious Inaction"},{"content":"For various projects of mine I need to be able to synthesize sound, so I decided to take a quick dabble in the realm of Digital Signal Processing. I mean, how hard could it be, right?\nAfter some fooling around with the Sampled Audio Subsystem of the Java Virtual Machine I was able to hear sinusoidal waveforms of various frequencies from my speakers, and I was starting to think that I am probably almost done, until I tried to play square waveforms. That's when I realized that I had barely scratched the surface. The square waveforms sounded pretty awful at any frequency, and especially at high octaves they sounded like random notes. Triangular waveforms, sawtooth waveforms, really anything except sinusoidal waveforms all suffered from the same problem.\nA bit of googling around soon revealed the name of the source of my troubles: aliasing.\nA naïvely sampled square wave,\nexhibiting a bad case of aliasing. Note how some of the peaks and valleys consist of 3 samples, while some consist of only 2 samples. (Useful pre-reading: About these papers)\nThe Wikipedia article about aliasing (https://en.wikipedia.org/wiki/Aliasing) has a lot to say, but in my own words, aliasing is a phenomenon that occurs when we try to digitally approximate an analog signal by naïvely generating samples that try to match as closely as possible the approximated signal. It occurs with graphics, when we try to approximate an ideal line using pixels of a certain discrete size, and it also occurs with sound, when we try to approximate an ideal waveform using samples at certain discrete intervals. Aliasing essentially means that our digital approximation will not only deviate from the ideal signal, but it will actually deviate by varying degrees along the signal, and these varying degrees of deviation will exhibit cycles of repetition. These repetitive variations in the deviation are, more often than not, perceptible by our senses, and rather unpleasant.\nWhen looking for a solution to the problem of aliasing in audio, the first port of call is of course the various Sound Engineering and Digital Signal Processing forums, like dsp.stackexchange.com or kvraudio.com/forum. Unfortunately, folks involved with audio tend to speak an exotic language which consists of the articles and prepositions of English, with nothing but utterly incomprehensible jargon in-between. They like to talk about harmonics, and band-limited waves, and band-pass filters, and Nyquist frequencies, and Lagrange interpolations, and DC compensations, and all sorts of mumbo jumbo that seems, at least to me, to bear absolutely no relation to buffers containing floating point sample values, which is all that my little world consists of. This makes it very hard for someone who is not a sound engineer to obtain useful information and apply it to solve problems.\nFurthermore, some of the jargon that DSP folks use refers to concepts that are computationally expensive to the point of being inapplicable to any practical problem. I found a certain solution here: https://www.nayuki.io/page/band-limited-square-waves which even includes source code in Java, and it does indeed sound awesome: all undesirable effects are gone, and each note sounds pure and at the right frequency. Unfortunately, this solution is an application of the well known and hopelessly theoretical approach to square waveform generation by means of adding up an infinite number of sine waves, so it is extremely computationally expensive and therefore completely useless for any real-world purpose.\nThe fact that a square waveform can be generated using the infinite sine wave superposition method may, in some utterly roundabout way, have some application in something I guess, but as far as the real world is concerned, it is nothing more than a curiosity; an \u0026quot;interesting to know\u0026quot; factoid, about as useful as the factoid which says that the tomato is technically a fruit rather than a vegetable.\nThen I managed to find floating on the interwebz a very promising solution that goes by the bizarre name of \u0026quot;PolyBLEP\u0026quot;. It came with C++ code which I ported to Java. The C++ code can be found here: https://github.com/martinfinke/PolyBLEP and discussion by its author, Martin Finke, is here: http://www.martin-finke.de/blog/articles/audio-plugins-018-polyblep-oscillator Later I also came across a library called \u0026quot;DaisySP\u0026quot;, which contains more or less the same code.\nEven though a square waveform generated using Polyblep looks anything but square, it sounds exactly right, and it does not suffer from aliasing. Also, Polyblep is not too expensive computationally.\nA square waveform generated using PolyBLEP. Unfortunately, I have a few problems with Polyblep:\nThe math is just voodoo magic to me; I do not have the slightest clue as to how it works. The code is specifically written to generate a number of predefined waveforms; it will not work with just any waveform. My inability to understand the math means that I cannot adapt the algorithm to work with any waveform. So, I decided to ask for help.\nI created a post on stackoverflow: \u0026quot;Efficient generation of sampled waveforms without aliasing artifacts\u0026quot; https://stackoverflow.com/q/66943566/773113 and a post on the KVR forum: \u0026quot;Call for help regarding anti-aliasing\u0026quot; https://www.kvraudio.com/forum/viewtopic.php?p=8370848\nA number of kind people posted answers, and I did learn some useful things from these answers; unfortunately, it was not enough to help me solve my problem, and so far, nobody has volunteered to open up a conversation with me.\nAlso, some of the people who tried to enlighten me, although undoubtedly well intended, and although admittedly quite lucid in their explanations, seem to suffer from delusions, or misconceptions at best. For example, I heard it being said that a good digitization of a square waveform that is free from aliasing is supposed to have wiggles right before and right after each cliff. These are the wiggles that are characteristically produced by their favorite preposterous method for square waveform generation, the infinite sine superposition method, which as I have already explained, is useless. I came across people who are under the impression that this is what Polyblep does. Well, sorry, but it does not.\nI may not fully understand how Polyblep works, but I have seen its results on a wave editor, and there most definitely exist no such wiggles. And there could be no such wiggles, because Polyblep only operates on one, maximum two samples before and after each cliff, regardless of frequency and sampling rate, and you need several samples in order to produce wiggles. To put it in other words, generating such wiggles would necessarily be computationally expensive, and Polyblep does not do anything that is anywhere near expensive enough. And yet, Polyblep sounds quite good without any such wiggles, which proves that the wiggles are unnecessary.\nIt does not take a genius to figure that out, because these wiggles are tiny compared to one entire cycle of a wave, which means that they represent frequencies that are far beyond the audible range, which in turn means that they could not possibly have any effect on anything whatsoever. These wiggles are just a relic from the analog times, and yet people keep superstitiously referring to them at this digital day and age.\nSo, I decided to try and tackle the problem all by myself, reasoning that it is just buffers containing floating point sample values, so the solution should come from the realm of algebra and geometry, without the need for lofty DSP concepts, nor deep math.\nFirst of all, let us state the goal:\nI want to be able to describe any waveform using what I like to call the \u0026quot;Unit Wave\u0026quot;. This is a series of connected straight line segments in two dimensions, where Y corresponds to amplitude and X corresponds to phase. Y varies between -1.0 and +1.0, while X varies from 0.0 to 1.0, and is of course non-decreasing. (In the future I might upgrade the concept from straight line segments to polynomial curve segments, but for now, straight line segments will do.)\nA Unit Wave describes a single period of a certain waveform, irrespective of frequency or sampling rate. The idea is that for any waveform imaginable, one can construct a Unit Wave describing it or approximating it, and by repetitively sampling that Unit Wave, one should be able to generate a continuous stream of that waveform at any desired frequency and at any desired sampling rate. So, the Unit Wave is supposed to be the ultimate solution to the problem of synthetic waveform generation.\nThe trick is that the generated waveform must somehow come out anti-aliased.\nFirst I had an idea which I decided to call \u0026quot;Slicing\u0026quot;. (See Appendix A: The \u0026quot;Slicing\u0026quot; algorithm.) I had high hopes that slicing would provide a solution to the problem, but it did not. The resulting waveform sounded much better than the naïvely generated waveform, but it was not nearly as good as Polyblep: it still suffered from distinctly audible aliasing artifacts.\nThen, I had another idea, which I called \u0026quot;Smoothing\u0026quot;. (See Appendix B: The \u0026quot;Smoothing\u0026quot; algorithm.) The algorithm is not very well defined yet, and there are various features that can be found in waveforms that I am not sure how to handle, but it is pretty straightforward in the case of the square waveform, so I decided that for the time being I would experiment with only square waves. Again I had high hopes that Smoothing would provide a solution, and again I was let down, in exactly the same way: better than naïve, not nearly as good as Polyblep.\nWhen I loaded all four waveforms (Naïve, Polyblep, Slicing, Smoothing) into a multitrack audio editor and saw them one below the other, I noticed that Slicing looked identical to Polyblep about half of the time, while the other half of the time it looked more like the naïve waveform, while Smoothing exhibited the same pattern: half the time right, half the time wrong. Then I made a startling discovery: when Slicing was right, Smoothing was wrong; and when Slicing was wrong, Smoothing was right.\nOf course, this immediately prompted me to try both Slicing and Smoothing cumulatively applied, and voila, the result was an excellent waveform, virtually indistinguishable from Polyblep both to the eye, and to the ear, and to the spectrum analyzer.\nPolyBLEP Slicing + Smoothing This alone should be causing some ruffling of feathers in the DSP community: in some corner of the world, some programmer with practically zero knowledge of DSP and calculus-level mathematics managed to produce results virtually identical to Polyblep using algorithms that he just empirically came up with, utilizing nothing but algebra and geometry, bypassing all their lofty theory and their indecipherable jargon.\nAlso, note that Polyblep is criticized as not being 100% accurate, so the difference between my results and Polyblep that can (barely) be observed using a spectrum analyzer might not necessarily be due to some inaccuracy of my algorithms; it may be due to some inaccuracy of Polyblep.\nThe naïvely sampled square waveform The same square waveform generated using PolyBLEP The same square waveform generated using Slicing + Smoothing. There is practically no visible difference between Polyblep and Slicing + Smoothing; the two plots look identical to such a degree that it is as if someone made a mistake and pasted the same image twice.\nThe (very subtle) differences become visible with the help of an animated gif:\nAnimated gif showing the difference\nbetween PolyBLEP and Slicing+Smoothing Unfortunately, I am still faced with several problems:\nI have some idea as to why slicing produces good results only half of the time, but I am not sure yet how to go about fixing this. Smoothing is still not well defined; specifically, the only kind of waveform on which it will currently produce the intended results is the square waveform. I do not know why, even on the square waveform, smoothing produces good results only half of the time. I do not know why the half of the time that slicing produces good results is the exact opposite of the half of the time that smoothing produces good results. (It is just a fortuitous happenstance that this is so, because it allows me to obtain very good overall results by cumulatively applying both methods.) As a result of the above, I am not sure whether I should put further work into one of the two approaches, to make it work by itself, or whether I should continue in the direction of cumulatively applying both approaches. (Which still necessarily involves improving the as of yet incomplete smoothing method.) Adding to my dilemma, I am not sure whether I should continue at all in any of the above directions or whether I should instead try to construct a general-purpose implementation of Polyblep which works with any waveform. These are my notes so far.\nThis document will be updated with developments.\nAppendix A: The \u0026quot;Slicing\u0026quot; algorithm\nGenerating the samples that describe a waveform is a lot like sampling an analog signal using an Analog-To-Digital converter. The input to the converter is line segments that describe a wave, and the output is sample values. Of course we are talking about software here, so strictly speaking, there is nothing analog per se, but the input can be thought of as being analog in the sense that: a) we are using floating point values, and b) a line segment consists of an infinite number of points along a slope.\nThe naïve approach to digitizing an analog signal is to generate samples by reading the signal at discrete, infinitesimally narrow points in time. However, if you do this, you are guaranteed to experience aliasing. So instead, let us try something different: Instead of considering infinitesimally narrow points along the wave, let us consider entire slices of the wave, each slice as broad as one sampling period.\nIn other words, for each sample, let us calculate the surface area of a shape defined as follows:\nAlong the x axis, it extends from the current sample time (x) until the next sample time (x+dx). Along the y axis, it extends from y=0 to the line given by the wave function f(x). Let us then convert that measurement of area to a linear sample value by applying some kind of normalization.\nSince the range of our sample values is from -1 to +1, this will involve an exotic notion of negative area, but that should not matter, it should all work out in the end.\nIt stands to reason that the normalization step should consist of dividing the computed wave slice surface area by the maximum slice surface area, which is the surface area of the rectangle with a height of 1 and width equal to dx. So, normalization is essentially just dividing by dx, or multiplying by 1/dx for those of us who are extremely performance-oriented.\nLet us first try this approach with a sine wave, as a proof of concept. It has been a very long time since I was at the University, but luckily I still remember some of the basic principles of Calculus, so I know that the surface area between the curve described by the function y=f(x) and the line at y=0 is the integral of that function. The Calculus book says that the integral of sin(x) from x=x1 to x=x2 is cos(x1) - cos(x2). Therefore, to produce a sine wave using the wave slice approach, we need the following computation:\ny = cos( 2π x1 ) - cos( 2π x2 ) / ((1/samplesPerWave) * 2π)\nSo I tried this, and the resulting waveform is exactly the same as the one produced by y=sin(x), so we are on a good path.\nWhen dealing with line segments, things are even more simple: at each step, the surface whose area we want to compute is the trapezoid defined by one straight horizontal line at y=0, two parallel straight vertical lines at x and x+dx, and one straight slanted line from f(x) to f(x+dx). So, we do not even have to compute the area and then divide by dx, we can just take the midpoint of the slanted segment, which is ( f(x) + f(x+dx) ) / 2.\nAs I have already explained, the slicing approach is better than the naïve approach but not as good as Polyblep. The reason seems to be that it tends to reproduce many of the points of the original analog signal a bit too faithfully, and as I have already explained, faithful reproduction of the original signal is naïve. Specifically, when sampling a square wave at a rate of dx, near a cliff going up, it may happen that a slice will be considered right before and just barely including the cliff, and the next slice will be right after the cliff. Since the cliff will be at exactly the end of the first slice, it will have an infinitesimally small contribution to the computed surface area, so the first slice will measure nothing but flat valley before the cliff at -1.0, while the next slice will measure nothing but flat top after the cliff at +1.0, thereby reproducing the absolute hardness of the cliff, which is something that never happens with Polyblep. I am not sure yet how to go about mitigating this.\nAppendix B: The \u0026quot;Smoothing\u0026quot; algorithm\nIt appears to me that aliasing occurs because the process of sampling at discrete time intervals is bound to sometimes catch, and sometimes miss, various features of a waveform. Take, for example, the peak of a triangular waveform. If a sample happens to be taken exactly at the peak, then no information will be lost. However, far more often, a sample will be taken before the peak, and the next sample will be taken after the peak. Furthermore, the two samples will almost never be equidistant from the peak; one of the samples will always be closer to the peak than the other, their relative distances from the peak will vary, and the variation will exhibit some periodicity, dependent upon the sampling rate and the frequency. This is bound to cause aliasing.\nSo, I am putting forth the hypothesis that if every feature of the waveform was at least as broad as one sampling period, then every feature of the waveform would be sampled at least once during each cycle of the waveform, and so there might be no aliasing.\nTherefore, the question is: can we somehow pre-process a Unit Wave for a given sampling rate to ensure that each of its features is at least as broad as the sampling period?\nWaveforms consisting of straight line segments can have the following kinds of features:\nA non-vertical line segment followed by another non-vertical line segment of a different slope but same slope sign. (For example, two successive segments on the same upwards or downwards slope of a sine wave that is being approximated using line segments.) A non-vertical line segment followed by another non-vertical line segment with a slope of the opposite sign. (For example, the segment right before the peak of a triangular wave, and the segment right after the peak.) A non-vertical line segment followed by a vertical line segment. (For example, a slope followed by a cliff on a sawtooth wave.) A vertical line segment followed by a non-vertical line segment. (For example, a cliff followed by a valley on a square wave.) It is yet unclear to me exactly how to go about pre-processing these cases, but at least I know what to do in the case of a horizontal line segment followed by a vertical line segment followed by another horizontal line segment, which is the one and only feature found on the square waveform.\nThe idea is to displace the top of the vertical line segment horizontally in one direction by dx/2, and the bottom of the vertical line in the other direction by another dx/2.\nThis will turn the vertical line segment into a sloped line segment whose projection on the x axis has a length of dx, and will ensure that some point along this segment will always be sampled on each cycle of the waveform.\nUnfortunately, this approach suffers from various problems. Even though some strategy could be devised as to how to handle each type of feature that can be found along a waveform, it is entirely unclear to me what should happen in cases where shifting a point to the left or to the right makes it bump a previous or next point belonging to the preceding or succeeding feature of the waveform.\n","date":"2021-04-21T04:33:08.919Z","permalink":"https://blog.michael.gr/post/2022-03-digital-audio-waveform-generation/","title":"Digital Audio Waveform Generation"},{"content":"My notes on how to use SVG graphics in a WPF application\nThe Goal The goal is to be able do do things like this:\n\u0026lt;Button Content=\u0026#34;{StaticResource mySvgImage}\u0026#34;\u0026gt; ... where mySvgImage somehow stands for a vector image that has somehow been obtained from an SVG file.\nThe solution must not involve any proprietary, closed-source libraries.\nNaturally, we want one of the following:\nEither directly include SVG files into our application as resources, or, if that is not possible, then: Have an \u0026quot;asset pipeline\u0026quot; approach where our SVG files are automatically converted during build into some format which is suitable for inclusion as a resource. The Problem WPF likes to do everything in its own idiomatic way, so vector graphics in WPF are specified using the following construct:\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;Canvas\u0026gt; \u0026lt;Path Fill=\u0026#34;#FFE68619\u0026#34;\u0026gt; \u0026lt;Path.Data\u0026gt; \u0026lt;PathGeometry Figures=\u0026#34;M8.564 1.289.2 16.256A.5.5 0 0 0 .636 17H17.364a.5.5 0 0 0 .5-.5.494.494 0 0 0-.064-.244L9.436 1.289a.5.5 0 0 0-.872 0ZM10 14.75a.25.25 0 0 1-.25.25H8.25A.25.25 0 0 1 8 14.75v-1.5A.25.25 0 0 1 8.25 13h1.5a.25.25 0 0 1 .25.25Zm0-3a.25.25 0 0 1-.25.25H8.25A.25.25 0 0 1 8 11.75v-6a.25.25 0 0 1 .25-.25h1.5a.25.25 0 0 1 .25.25Z\u0026#34; FillRule=\u0026#34;NonZero\u0026#34;/\u0026gt; \u0026lt;/Path.Data\u0026gt; \u0026lt;/Path\u0026gt; \u0026lt;/Canvas\u0026gt; Even though this notation is very similar to SVG path notation, WPF has no built-in support for SVG. They do not list SVG in their list of supported image file formats. This is preposterous, but that's what it is.\nMicrosoft has added SVG support to UWP via the SvgImageSource class, but they have not bothered doing the same for WPF, probably as part of their greater strategy to let WPF go gently into that good night, since UWP is, supposedly, their new big thing that everyone must now start using whether they like it or not.\nAllegedly, there exists something called \u0026quot;XAML Islands\u0026quot; which offers means of including UWP controls within WPF applications, (mentioned on Microsoft blogs and on Stack Overflow) but it looks very convoluted, and the result will probably be held together by shoestrings, so I am not going to research that approach.\nThe Inkscape approach Inkscape offers the ability to export SVG to something they call \u0026quot;xaml format\u0026quot;, and it can even do that from the command line, making it suitable for using in an asset pipeline, but this feature is extremely limited. As far as I can tell, Inkscape offers absolutely no options for controlling how the exporting will be done; it just creates files that look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!--This file is NOT compatible with Silverlight--\u0026gt; \u0026lt;Viewbox xmlns=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml/presentation\u0026#34; Stretch=\u0026#34;Uniform\u0026#34;\u0026gt; \u0026lt;Canvas Name=\u0026#34;svg10\u0026#34; Width=\u0026#34;18\u0026#34; Height=\u0026#34;18\u0026#34;\u0026gt; \u0026lt;Canvas.RenderTransform\u0026gt; \u0026lt;TranslateTransform X=\u0026#34;0\u0026#34; Y=\u0026#34;0\u0026#34;/\u0026gt; \u0026lt;/Canvas.RenderTransform\u0026gt; \u0026lt;Canvas.Resources/\u0026gt; \u0026lt;!--Unknown tag: metadata--\u0026gt; \u0026lt;!--Unknown tag: sodipodi:namedview--\u0026gt; \u0026lt;Canvas Name=\u0026#34;AlertMedium\u0026#34;\u0026gt; \u0026lt;Path xmlns:x=\u0026#34;http://schemas.microsoft.com/winfx/2006/xaml\u0026#34; Name=\u0026#34;Shape\u0026#34; Fill=\u0026#34;#FFE68619\u0026#34;\u0026gt; \u0026lt;Path.Data\u0026gt; \u0026lt;PathGeometry Figures=\u0026#34;M8.564 1.289.2 16.256A.5.5 0 0 0 .636 17H17.364a.5.5 0 0 0 .5-.5.494.494 0 0 0-.064-.244L9.436 1.289a.5.5 0 0 0-.872 0ZM10 14.75a.25.25 0 0 1-.25.25H8.25A.25.25 0 0 1 8 14.75v-1.5A.25.25 0 0 1 8.25 13h1.5a.25.25 0 0 1 .25.25Zm0-3a.25.25 0 0 1-.25.25H8.25A.25.25 0 0 1 8 11.75v-6a.25.25 0 0 1 .25-.25h1.5a.25.25 0 0 1 .25.25Z\u0026#34; FillRule=\u0026#34;NonZero\u0026#34;/\u0026gt; \u0026lt;/Path.Data\u0026gt; \u0026lt;/Path\u0026gt; \u0026lt;/Canvas\u0026gt; \u0026lt;/Canvas\u0026gt; \u0026lt;/Viewbox\u0026gt; Besides the obvious problem of useless elements that should not be there, the main problem with these files is that they are just plain unusable, because XAML offers no means of including one XAML file from within another XAML file. The only file inclusion mechanism supported by XAML is resource dictionaries, but this file is not describing a resource dictionary, so it cannot be included.\nIf you go looking for answers on Stack Overflow you will find that people copy-paste the PathGeometry elements from Inkscape-generated XAML files into their own XAML files, and they find nothing retarded in what they are doing. Others suggest to take the XAML files generated by Inkscape and copy them to Blend, and then convert them to Path, because the more clicks the better, I suppose.\nIt may be possible to programmatically load Inkscape-generated XAML files using the following construct:\nvar uri = new Uri(\u0026#34;pack://application:,,,/Resources/MySvgExportedAsXaml.xaml\u0026#34;); using( Stream stream = App.GetResourceStream( uri ).Stream ) { object o = XamlReader.Load(stream); } ... but then it is unclear how to make objects created this way usable from within XAML.\npunker76 (the main person behind MahApps) offers a way of programmatically creating an icon pack in his blog post titled \u0026quot;How to create a new IconPack with custom SVG Paths\u0026quot;. Such an icon pack can then be conveniently used from within XAML. However, his solution is tied to MahApps, while I would prefer an independent solution, and it is still not clear how to make that work with an object obtained by invoking the XamlReader.Load() method.\nIn any case, the process already looks quite a bit convoluted, so that's where I stop researching the Inkscape approach.\nReferences:\nStackoverflow: \u0026quot;Import separate XAML as ResourceDictionary and assign x:Key\u0026quot; Stackoverflow: \u0026quot;Inkscape (vector graphic)\u0026quot; Libraries If you go searching for library suggestions on Stackoverflow you will find the following:\nDevExpress It looks very promising, (if we are to judge by how cool their web site looks,) but it is proprietary, so it is off-topic in this discussion. Svg2Xaml This project is hosted in codeplex, which is closing down in July of 2021, and the author does not seem to have moved his project elsewhere. Some guy called Stefan Mischke has forked it on GitHub and has made a nuget package out of it, but the lack of support and documentation makes this very unpromising. BerndK/SvgToXaml This just converts svg to xaml. It might be possible to use it at runtime to load svg resources and convert them to xaml paths, and then use some other mechanism to make these paths usable from within xaml, but the process seems convoluted. SVG rendering engine This is described as a WinForms solution, so whatever it does, it will not be directly usable from within XAML. I am listing it here because it might turn out to be useful, but I will not research it any further. ReaderSVG from AB4D (link is defunct) This may have been an open-source project at some point a long time ago, but it looks like it has now become a commercial product, and archive.org does not index the old home of the project anymore. mntone/SvgForXaml The README.md file shows a XAML snippet that looks promising, but it also says \u0026quot;Requirement: Win2D\u0026quot; and contains a warning about Windows 10 Aniversary Update, and I have no clue what are the implications of these statements. Overall, the project looks perhaps a bit too simplistic. Only try if other avenues fail. CodeProject: SVGImage Control The author says \u0026quot;The SVG parser and render is still not complete, but it is in a state where it can read most 'simple' SVG files I have found online so far.\u0026quot; So, no time should be invested on this. See next entry, \u0026quot;dotnetprojects/SVGImage\u0026quot;. dotnetprojects/SVGImage Initially forked from CodeProject: \u0026quot;SVGImage Control\u0026quot;; appears to be in active development, (last update 5 months ago.) Its README.md looks tacky, but it may work, and it even has a nuget package. SharpVectorGraphics (aka SVG#) A very old, abandoned project. Has no documentation. Uses org.w3c.dom.svg to convert to GDI GraphicsPath. Contains some incomplete code. Not worth looking at. This is what SharpVectors (see below) started out from. SharpVectors Apparently, this is in active development, (last update 3 months ago,) and there is a nuget package, so this looks promising! Also see: Stackoverflow: \u0026quot;SvgViewbox doesn't show tooltip on mouse over\u0026quot; References:\nStackoverflow: \u0026quot;Library for using SVG in Windows Forms / WPF?\u0026quot; What if I already have some XAML graphics? If you are already using some graphics that exist in XAML-only form, (for example, the MahApps icon packs,) and you would prefer to have all vector graphics expressed in a single format throughout your application, then you might want to perform a one-time conversion of your existing XAML graphics to SVG. This will also allow you to get rid of the MahApps icon packs, which are many megabytes large, while your application is presumably using only a tiny fraction of them.\nFor simple graphics, I have found that XAML notation is identical to SVG notation, so simply pasting the XAML string (\u0026lt;PathGeometry Figures=\u0026quot;...\u0026quot;) into the corresponding SVG string (\u0026lt;path d=\u0026quot;...\u0026quot;) yields a valid SVG file that you can work with. Since this is a one-time-only operation, it is not too bad if it has to be done manually.\nIf there is a need for more advanced conversions, it appears that there are no tools out there for converting XAML to SVG, but this might be useful: Stack Overflow: \u0026quot;Wpf InkCanvas save stokes as svg\u0026quot;\nSVG Validation Of course, if you start generating SVG from other sources, you are bound to end up with invalid SVG files. Unfortunately, as far as I can tell, Inkscape does not give the slightest indication that there is something wrong with an SVG file. If you give Inkscape an invalid file to load, it will silently fail and render nothing.\nTo troubleshoot SVG issues, a somewhat decent SVG validator is validator.nu. (At least it helped me find problems in one SVG file, and after I fixed it, Inkscape successfully loaded it.)\nAnother validator, which though is a bit too verbose and a bit too strict, is the W3C validator.\nThere also exists something called \u0026quot;SVG Sanitizer\u0026quot; at svg.enshrined.co.uk, written by some guy called Daryll Doyle, it is completely useless, do not waste any time with it. (You can take hint from the fact that it is written in PHP.)\nOld comments\nLDP2Go 2024-12-05 10:04:26 UTC\nСпасибо!\n","date":"2021-02-12T08:02:35.98Z","permalink":"https://blog.michael.gr/post/2021-02-svg-in-wpf/","title":"SVG in WPF"},{"content":"Please note that this is work in progress. I am still working on it and refining it, as my understanding of it improves.\nI have a set of public repositories on GitHub showcasing my work on java with maven. These projects are interdependent, so when you check out one of them, in order to compile and run it you need the binaries of some of the others. You could manually check out all of them and put them in an IDE project, but that's too much work. Solving this problem requires having Continuous Integration \u0026amp; Continuous Deployment (CI/CD) in place, so I decided to try my luck in setting one up using free services only.\nThe process involves three entities:\nA Source Repository. (Where our source code is hosted.)\nI use GitHub for this. Possible alternatives: GitLab BitBucket A CI/CD provider. (Where the actual CI/CD takes place.)\nI decided to use CircleCI for this, but in retrospect it was a bad idea, because it does not support GitLab. Possible alternatives: GitLab - I want to use it as a source code repository, and I don't want to put all my eggs in one basket, so I don't want to use it for anything else. GitHub - I want to use it as a source code repository, and I don't want to put all my eggs in one basket, so I don't want to use it for anything else. BitBucket - it is by Atlassian. Need I say more. Appveyor - gives various errors like \u0026quot;There was an error while trying to complete the current operation. Please contact AppVeyor support.\u0026quot; -- Lots of open source projects are using it though, so it might be worth a second try. Travis CI - only works with github. JFrog - overwhelmingly fancy front page followed by a not particularly fancy user experience once you get past the front page. Once I have registered, there is no way for me to log back in. semaphoreci.com - only works with github. buddy.works - after you have given them your e-mail address, they tell you that it is free but they require a valid payment method. atlassian.com/software/bamboo - it is by Atlassian, need I say more. drone.io - not only it works with nothing other than github, they assume that I am using github, which is very annoying. octopus.com - registration fails with \u0026quot;Please use your work email address.\u0026quot; buildkite.com - might work; not particularly user friendly. codefresh.io - might work; they unnecessarily complicate things with mandatory docker images. An Artifact Repository. (Where the binaries are stored.)\nI found a place called Repsy for this; Repsy is minimalistic, unrefined, and they even have bad English on their web site, but it will do for now. Possible alternatives: GitHub Packages GitLab Package Registry JFrog Artifactory We begin with a situation where we already have the Source Repository (GitHub) and we want to set-up the CI/CD Provider (CircleCI) and the Artifact Repository (Repsy).\nSetting up the Artifact Repository Go to Repsy and create a free account. The username in this case will be mikenakis. Create a public maven repository in Repsy. The repository name in this case is mikenakis-public. Testing the Artifact Repository Next, we need to make certain changes to our local setup so as to be able to deploy artifacts from within our local Development Environment directly to Repsy, to make sure this works before we even try CI/CD.\nIn the maven folder (which is under ~/.m2 in Linux, %USERPROFILE%\\.m2 in Windows) create a settings.xml with the following content (or if one already exists, make sure it includes the following content) \u0026lt;settings\u0026gt; \u0026lt;servers\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;repsy-mikenakis-public\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;mikenakis\u0026lt;/username\u0026gt; \u0026lt;!-- Repsy username --\u0026gt; \u0026lt;password\u0026gt;\\*\\*\\*\\*\\*\\*\\*\\*\u0026lt;/password\u0026gt; \u0026lt;!-- Repsy password --\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;/servers\u0026gt; \u0026lt;settings\u0026gt; Note: The username and password for deploying artifacts into Repsy repositories is the same as the username and password of the Repsy account. The password does not have to be included here as plaintext; it can be encrypted. For instructions on how to do that, see Apache/ Maven/ Password Encryption. Add the following to each and every pom.xml file that is going to take part in CI/CD: \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;repsy-mikenakis-public\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://repo.repsy.io/mvn/mikenakis/mikenakis-public\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; \u0026lt;distributionManagement\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;repsy-mikenakis-public\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://repo.repsy.io/mvn/mikenakis/mikenakis-public\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/distributionManagement\u0026gt; That's it, we should now be able to execute mvn deploy in our local environment and deploy to Repsy.\nNote: in reality I had many problems getting mvn deploy to work, because I have not really been using maven, I have only been using pom.xml files to describe my projects and then letting IntelliJ IDEA handle everything else, and it turns out that IntelliJ IDEA is a lot smarter and a lot more forgiving than maven is. My pom.xml files needed a lot of re-working to get them to actually work with maven. But that's a different story.\nSetting up the CI/CD Provider Go to CircleCI and create a free account. Go to \u0026quot;Organization Settings\u0026quot; -\u0026gt; \u0026quot;Contexts\u0026quot; and create a new context; let's call it my-context. Add an environment variable to the context with name REPSY_PASS and the value being the password of the Repsy repository. This way, our scripts will later have access to our Repsy password without us having to include it in any publicly visible source code. CircleCI promises to take extra measures to make sure that environment variables specified via contexts are never exposed to prying eyes. Create a CircleCI project which will correspond to one of our source repositories. The source repository in this case will be bytecode-dump, so the CircleCI project will also be named bytecode-dump. As soon as we have created our project, CircleCI slaps us with their configuration editor, which they must think is very cool, and prevents us from going any further. We are not going to bother with it, because it is quite unhelpful, so: Click the \u0026quot;Use Existing Config\u0026quot; button. A dialog will pop up saying \u0026quot;Have you added a config.yml file?\u0026quot; and offering a couple of vague options that both seem to be dead ends. You might think you are stuck, but fear not and proceed to the next step. Select the \u0026quot;Start building\u0026quot; option. CircleCI will complain that there is no configuration, but that's okay, we will take care of it later. At least we can now access the project settings. Go to \u0026quot;Project Settings\u0026quot; -\u0026gt; \u0026quot;SSH Keys\u0026quot; -\u0026gt; \u0026quot;Checkout SSH Keys\u0026quot;. This is not about checking out any SSH keys, this is about specifying keys for CircleCI to be able to checkout. Click \u0026quot;Add Key\u0026quot;. You might think that you then need to do something with this key, but actually you don't. If you allow a few seconds to pass for the systems to do their magic, and then go to your project on GitHub -\u0026gt; \u0026quot;Security\u0026quot; -\u0026gt; \u0026quot;Settings\u0026quot; -\u0026gt; \u0026quot;Deploy Keys\u0026quot;, you will see that a key will be there, (which though looks entirely different from what CircleCI shows,) and GitHub will say that this key has been added by CircleCI with authorization from you. Whatever. Magic. Completing the configuration Back in our local development environment, in the root directory of our source repository (same level as the pom.xml or parent pom.xml) create a folder called .circleci. In the .circleci folder add a file called mvn-settings.xml with the following content: \u0026lt;settings\u0026gt; \u0026lt;servers\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;repsy-mikenakis-public\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;mikenakis\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;${REPSY\\_PASS}\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;/servers\u0026gt; \u0026lt;/settings\u0026gt; In the .circleci folder also add a file called config.yml with the following content: version: 2.1 workflows: my-workflow: jobs: - build: context: my-context jobs: build: docker: - image: cimg/openjdk:15.0.1 steps: - checkout - run: mvn -s .circleci/mvn-settings.xml clean - run: mvn -s .circleci/mvn-settings.xml install - run: mvn -s .circleci/mvn-settings.xml deploy Note that we are supposed to add some additional notation to achieve caching of our maven dependencies, so that they are not all fetched from scratch each time the CI/CD build pipeline runs, but: I have not found a definitive description of how to do this; A magical incantation that I found somewhere initially seemed to work, but then later it caused problems, so I ditched it, and CircleCI should really be automating this in a way which is totally transparent to us. So, for now caching will need to wait. If OpenJDK 15 does not suit you, then you will need to find another docker image, CircleCI has many available. I later switched to JDK 16 with no problems. There are 3 separate maven goals so that the clean goal can be easily commented out, and so that if the install goal fails then the deploy goal will not be executed at all. (Otherwise, rumor has it that you might get partial deployments.) Firing it up Commit and push to GitHub. Once these files have been committed to GitHub, CircleCI will take notice, and it will run our CI/CD pipeline. Do not expect it to run successfully on the first try; there will be errors, and there will be trouble. But once you get it to work successfully, it will be worth the trouble.\n","date":"2021-02-10T10:34:45.353Z","permalink":"https://blog.michael.gr/post/2021-02-java-with-maven-giving-cicd-try/","title":"Java with Maven: Giving CI/CD a try"},{"content":"\rThis is a brief technical explanation of MVVM, with enough detail (borrowed from its WPF implementation) and examples to allow the reader to grasp how it actually works.\nMVVM is an architectural design pattern for building interactive applications. Its aim is to achieve complete decoupling of application logic from presentation logic. MVVM is not something new, and it was not even new at the time that it became popular. It was named by John Gossman in 2005, who states that it is the same as the Presentation Model pattern named by Martin Fowler in 2004, who in turn states that it was previously known as the Application Model pattern in certain Smalltalk circles as early as in the 1980s, and that's where we lose track of it: for all we know, it may have originated in Ancient Egypt. The acronym stands for Model, View, Viewmodel. Model refers to the main estate data model of our application; it is optional and largely irrelevant, so it will only be mentioned a couple of times here. View refers to the user interface. Viewmodel is the secret sauce, but in essence it refers to the application logic. Application logic is placed in objects known as viewmodels. A viewmodel does the following: Publicly exposes state, part of which is publicly mutable. Issues notifications about any mutation of its state. Responds to state mutations with behavior, such as querying and updating data stores, issuing events in some messaging backbone, etc. Manifests its behavior by means of further modifying its own state, which in turn generates more state mutation notifications. This allows the following very simple workflow: When the presentation layer modifies a property of a viewmodel, the viewmodel takes notice and exhibits its behavior. When a viewmodel modifies one of its own properties, the presentation layer takes notice and updates the screen. Thus, a viewmodel essentially implements a fully interactive and yet completely abstract (i.e. not graphical) user interface, with mutable properties instead of editable controls. The viewmodel is free from presentation concerns such as where on the screen the properties may be shown, what user interface controls may be used to show them, etc. Note: GUI pushbuttons, which have no state, are implemented as special \u0026quot;Command\u0026quot; objects that are exposed by a viewmodel besides its properties, but they could also be implemented as Boolean properties, where a transition from say, true to false triggers behavior. Command objects make viewmodels more self-documenting and more usable, but they are nothing but a nice-to-have feature: in principle, everything could work with just properties. If we wanted to allow the user to edit a Customer entity in a modal dialog box: There will be a viewmodel for this dialog box, which exposes: One property for each editable field of Customer. One command for the 'OK' button One boolean property which stands for the 'enabled' state of the 'OK' button. One command for the 'Cancel' button, presumed to be always enabled. The viewmodel may set the enabled state of the 'OK' command to true only once the user has made changes to the fields. When the 'OK' command is triggered, the viewmodel performs validation. If validation fails, the viewmodel instantiates another viewmodel which stands for an error message, and the view chooses how to show it, e.g. with a modal dialog box or with a temporary \u0026quot;toast\u0026quot;. If validation succeeds, the viewmodel persists the entered information in the data store. If the user opts to cancel, then the viewmodel discards the edited information. Viewmodels are so agnostic of presentation concerns that they can in fact be instantiated without any user interface at all. This allows us to test the entirety of the behavior of our application logic without a user interface and without having to examine any of its private implementation details such as its data store. Our application logic tests simply modify the public state of viewmodels and examine how the viewmodels further modify their public state in response. The presentation layer consists of views. In a desktop application, views are user-defined controls, panels, windows, dialogs, etc. In a web application, views would be HTML fragments. Each view type is specific to a particular viewmodel type, and contains bindings, which describe how each property of the viewmodel is bound to each property of a control within the view. So, a CustomerForm view which is meant to display a Customer viewmodel has a binding which specifies that the Name property of the customer should be bound to the Text property of a certain TextBox control within the view. Note that these associations are purely declarative, and they reference nothing but statically available information, (data types and their members,) which means that they can be described using a markup language, i.e. without the need to write any application-specific code to build up the user interface. A viewmodel may contain a property which is in turn another viewmodel. Let us call that a child viewmodel. In this case, the view can do one of two things: Specify a particular child view type to display that particular child viewmodel. Specify a mapping table which defines what type of child view to use for displaying any different possible type of child viewmodel. Views are resolved at runtime, based on the actual type of the child viewmodel, which can be more derived than the advertised type of the child viewmodel property; so, if a viewmodel exposes a Customer child viewmodel property which can be either of type Customer or of a more derived type WholesaleCustomer, the mapping table can specify a different child view type for each of these child viewmodel types, and the right child view will be instantiated at runtime depending on the actual type of the child viewmodel. Any child view can in turn contain its own mapping table which defines more associations, or redefines existing associations, so that for example: In the scope of an AllCustomers view, the Customer viewmodel can be associated with a CustomerRow view, so as to present the customer as a row in a tabular control. In the scope of a CustomerDetails view, the same Customer viewmodel can be associated with a CustomerForm view, to present the customer using individual fields laid out on a surface. Note that again, this mapping table consists of nothing but statically available information, (view types and viewmodel types,) so everything is still achievable in markup. A child viewmodel property can be optional or nullable, thus allowing the application logic to control whether an entire section of the user interface is available or not at any given moment. A child viewmodel property can be a collection of viewmodels, allowing for a corresponding child view which is a list control or a tab control. Viewmodel type mapping still applies, so if the collection contains viewmodels of different types at runtime, the resulting list control will consist of different kinds of rows, or the resulting tab control will consist of different kinds of tabs. ","date":"2021-01-16T14:47:04.693Z","permalink":"https://blog.michael.gr/post/2021-01-the-mvvm-architectural-design-pattern/","title":"The MVVM architectural design pattern"},{"content":"\rAbstract Garbage collectors have given us a false sense of security with respect to what happens to an object once we stop thinking about it. The assumption is that it will be magically taken care of, but this does not always go as hoped, resulting in memory leaks and bugs due to failure to perform necessary cleanup. Tools for troubleshooting such problems are scarce, and not particularly helpful, so finding and fixing such problems is notoriously difficult.\nA methodology is presented, which differs from current widespread practices, for maintaining awareness of, and exercising full deterministic control over, the lifetime of certain objects in a garbage-collected environment. We issue hard errors in the event of misuse, and accurate diagnostic messages in the event of omissions, thus improving the robustness of software and lessening the troubleshooting burden.\n(Useful pre-reading: About these papers)\nDefinition An object can be said to have a concept of lifetime if at some point it must perform some cleanup actions, after which it must never be accessed again.\nA first look at the Problem One of the original promises of garbage collectors was that we should not have to worry about the lifetime of objects; however, there exist various known situations where objects do, by their very nature, have an inherent notion of lifetime, so we do have to worry about it; for example:\nObjects that model real-world processes with an inherent concept of lifetime, such as: A user's visit to a web site, represented by a web session which at some moment expires. The printing of a document, represented by a print job which at some moment completes. Objects implementing application behaviors with a clearly defined end, such as: A dialog window which at some moment gets dismissed and ceases to exist. Additionally, there exist certain programmatic constructs which require a notion of lifetime; for example:\nAn event observer which must at some point unregister from the event source that it had previously registered with. A database transaction which must at some point end, either by committing it or rolling it back. Generally, any situation where: We must remember to undo something which was previously done. Some initialization must be balanced by some corresponding cleanup. Furthermore, any object which contains an object that has a notion of lifetime needs to have a notion of its own lifetime, so as to be in a position of controlling the lifetime of the contained object. Thus, there tends to be a need for objects with a notion of lifetime to form a containment hierarchy whose root is the main application object.\nUnfortunately, in garbage collected environments, object lifetime is not given as much attention as it deserves. Software architectures tend to underestimate its importance, give it only a partial treatment, and invariably do it in ad-hoc ways, without any clearly defined methodology or aiding infrastructure. All to often, an object with an inherent notion of lifetime is built without explicit knowledge of it; instead, its lifetime is treated only implicitly. Thus, the software design has no knowledge of, and no control over, the lifetime of that object, and relies on the garbage collector to magically take care of it.\nOnce we leave an object up to the garbage collector to take care of, we completely relinquish control over what happens next: there are no guarantees as to when the object will be collected, or even as to whether it will in fact be collected; there will be nothing to inform us of the outcome, and we have no way of influencing the outcome. Thus, when object-lifetime related trouble happens, it is by its nature very difficult to troubleshoot, diagnose, and fix; nonetheless, most programmers try to avoid dealing with object lifetime if they can, and each time problems pop up, they try to fix them on an as needed basis.\nThe following kinds of trouble are common:\nDirect failure to perform necessary cleanup: the false sense of security offered by the garbage collector sometimes makes programmers forget that it only reclaims unused memory, it does not do any other cleanup for us, such as unregistering observers from event sources. This usually needs to be done manually, and it requires that the observer must have a notion of lifetime. An event source could in theory be asserting that every single observer did eventually remember to deregister; however, such a technique would require not only observers to have a notion of lifetime, but also the event source itself. Thus, there is no widespread use of such a technique, because there is no widespread use of object lifetime awareness in the first place. Memory leaks: in an ideal world, the magic of the garbage collector would always be strong and true, but in practice it is not, due to subtle human error such as inadvertently keeping around a reference to an object, thus preventing it from being garbage collected. Lack of object lifetime awareness only exacerbates this problem. Troubleshooting in the dark: an object with a notion of lifetime can either be alive or dead. If the lifetime of the object is explicit, we can always inspect that state with the debugger. If not, then we never know whether the object that we are looking at is meant to be alive or not. Inability to detect misuse: a very common mistake is continuing to access an object even after its lifetime is over. When the object has no explicit knowledge of its own lifetime, it cannot assert against such mistakes. Existing mechanisms Garbage collectors and their associated execution environments do provide some machinery which is related to the topic of object lifetime management, namely finalization, disposal, \u0026quot;automatic\u0026quot; disposal, and weak references, but as we shall see this machinery alone is woefully inadequate.\nFinalization In Java we have the Object.finalize() method, and in C# we have \u0026quot;destructors\u0026quot;, which are actually not destructors at all, they are finalizers, too. 1 The garbage collector will invoke the finalizer of an object right before reclaiming the memory occupied by it, so that the object can in theory perform some cleanup at that moment. Unfortunately, this mechanism is notoriously unreliable:\nAn object will not be finalized unless it first becomes eligible for collection, and in order for that to happen, it must first become unreachable. However, the object may remain reachable due to subtle mistakes such as unknowingly keeping a reference to it in some list, thus resulting in objects which never get finalized. When an object does become eligible for collection, the moment that it will actually be collected largely depends upon the whim of the garbage-collector, which is non-deterministic, both according to the documentation and as observed by experimentation. There are no guarantees as to when an object will be collected, or even as to whether it will ever be collected, despite being eligible. If the garbage collector works in aggressive mode, (common in servers,) the object will be collected sooner rather than later, but how soon depends on variables that we practically have no control over, such as at what rate existing objects are becoming eligible for collection, how many objects are pending to be finalized, etc. So: Even though the object may be finalized within milliseconds, there are no guarantees as to how many milliseconds, and also please note that \u0026quot;milliseconds\u0026quot; is still a far cry from \u0026quot;now\u0026quot;. Even if the object gets finalized as quickly as possible, this is still going to happen in a separate thread, so finalization will always be desynchronized from the set of instructions which rendered the object eligible for finalization. If these instructions are followed by another set of instructions that in any way rely on finalization having already taken place, the second set of instructions will almost always fail. If the garbage collector works in non-aggressive mode, (common in desktop and console applications,) then: The object might not be collected unless the virtual machine starts running out of memory, which is at an entirely unknown and usually distant time in the future. The object may still not be collected at all if our software completes before exhausting its available memory. If we are only dealing with unmanaged resources that belong to the current process, they will be automatically reclaimed upon process termination, so all will be good, but if we are dealing with resources that are external to our process, (e.g. controlling a peripheral,) these resources will not be released. The garbage collector orchestrates collection and finalization according to memory availability concerns, but not according to other concerns which it has no knowledge of; consequently, if we are acquiring instances of a certain scarce resource at a high rate, the garbage collector will not hasten collection and finalization in response to the scarcity of that resource, because it has no knowledge of that scarcity. However, if that resource only gets recycled when collection and finalization occurs, and if we do not happen to be allocating and freeing memory fast enough to trigger frequent enough collection and finalization, then we will be consuming the resource faster than it is being recycled, so we will run out of it, despite everything seemingly being done right. Finalization is unordered and does not respect containment hierarchy, which means that when the finalizer is invoked on an object, a random subset of the objects contained in this object may have already been finalized. This is a completely chaotic situation which makes it impossible to get anything non-trivial done within a finalizer. The chaotic and non-deterministic conditions under which the finalizer executes make it virtually impossible to test any code that you put in a finalizer, so virtually all finalizers are written on a best-effort basis: if it seems to work as written, it will hopefully keep working, fingers crossed. Finalization is documented as having a high performance cost, so the standing advice is that it is best to minimize its use, or avoid it altogether if possible. Therefore, relying on finalization does not give us control over anything, on the contrary it takes control away from us. The official literature of both Microsoft DotNet and the Java Virtual Machine recommends using finalization for the purpose of releasing unmanaged resources, which is not just unhelpful but actually wrongful advice to give. The entire software industry has been blindly following this advice without first questioning its correctness, which has resulted in lots of buggy software out there.\nDisposal In Java we have the Closeable interface, and in C# we have the IDisposable interface. The benefit of using these interfaces is that they allow explicit (i.e. deterministic and synchronous) triggering of cleanup, instead of relying on finalization to trigger it.\nA C#-only note: In the C# world there is an understanding that IDisposable may also be used for performing regular cleanup at the end of an object's lifetime; however, its primary reason of existence is still regarded as being the release of unmanaged resources, so people are trying to use it for both purposes. At the same time, the releasing of unmanaged resources is still regarded as something that must always be attempted during finalization, so people are trying to write disposal methods which must work both when explicitly invoked during normal program flow, and when invoked by the finalizer. Needless to say, the complexity of this task is daunting, and the result is incredible amounts of confusion. The Dispose(bool) pattern has been invented to help manage the chaos, but the result is still preposterously complicated, it suffers from boilerplate code on every single object that implements the pattern, it is largely untestable, and what is most disappointing is that absolutely no thinking seems to have gone in the direction of avoiding all this chaos in the first place.\nOverall, the problem with disposal is that it is very easy to accidentally omit, and when it is omitted there is usually nothing to tell us that we did something wrong, other than performance degradation and inexplicable malfunction. As a matter of fact, the designers of both Java and C# anticipated the inevitability of such omissions, so they invented finalization as a fallback mechanism which is hoped to save the day despite the omissions. However, since finalization is notoriously unreliable, it is not a solution either; it is more like implementing an insurance policy by purchasing lottery tickets.\nUnfortunately, the availability of finalization, and its deceitful promise of making everything right by magic, has steered programmers to regard disposal as largely optional, while in reality it is essential. All that disposal needs in order to be actually useful is a mechanism that will warn us when we forget to perform it, instead of a mechanism that will try to magically fix our omissions.\n\u0026quot;Automatic\u0026quot; disposal Both Java and C# provide special \u0026quot;automatic\u0026quot; disposal constructs, namely the try-with-resources statement of java, and the using keyword of C#, both of which implicitly invoke the disposal method even if an exception is thrown. However:\nThe only thing that is automatic about these constructs is that if you remember to use them, then they will save you from having to write some code that disposes an object; unfortunately, You may very easily forget to use them. You may very easily forget that your object requires disposal and therefore be unaware of the fact that you should have used them. These constructs can only be used in the simplistic scenario where the lifetime of an object is fully contained within the scope of a single method; unfortunately, in all but the most trivial situations what we actually have is objects which are contained within other objects and live for a prolonged time, so the method that creates them is different from the method that destroys them. In all these cases, the automatic disposal constructs are of no use whatsoever, and the programmer must remember to do everything right. Weak References A weak reference is an object which receives special treatment by the execution environment, to achieve something which is not normally possible. It contains a reference to a target object, which is disregarded by the garbage collector when determining whether the target object is accessible. Therefore, if there are no other references to the target object, then the target object is allowed to be garbage-collected, at which point the reference inside the weak reference object is replaced with null.\nWeak references do not actually help us manage the lifetime of objects, but they have been suggested as a mechanism that can help us design things so that there is no need to manage the lifetime of objects. The idea is that we can implement the observer pattern using weak references, so that observers do not need to remember to unregister themselves from the event source; instead, they can simply be allowed to become garbage-collected, and the event source will subsequently forget them.\nThis approach suffers from a number of drawbacks:\nWeak references might save us from having to worry about the lifetime of event observers, but they do nothing for a wide range of other situations that require cleanup at the end of an object's lifetime. Weak observers run the danger of being prematurely garbage-collected. When this happens, it is very difficult to troubleshoot, and the fix tends to require tricks and hacks. Weak references are a bit too low level, a bit too esoteric, and a bit like magic, so suggesting their widespread use by the average programmer is a bit of a tough proposition. The use of weak references represents a step backwards from the stated goal of gaining more control over the inner workings of our software. A deeper look at the problem In a language like C++, which has proper destructors, the lifetime of an object is well defined, and the compiler does all the work necessary to guarantee that this lifetime will end at the exact right moment, as long as we are using either local storage or smart pointers. However, in garbage-collected languages we have none of that; the lifetime of objects is not well defined by the language, so there is virtually nothing that the compiler can do for us. (As we have already shown, the try-with-resources statement of Java and the using keyword of C# are of very limited usefulness.)\nIn order to implement necessary cleanup at the end of an object's lifetime in garbage-collected languages, programmers either rely on finalization, or explicitly invoke objects to let them know that their lifetime is over. As we have already shown, finalization is asynchronous and non-deterministic, so it is unsuitable for basing any essential function of our software upon it, which means that explicit object lifetime termination is the only viable option.\nUnfortunately, explicit object lifetime termination suffers from its own range of problems:\nThere is usually nothing in the code to give us a hint that we should place a call to end the lifetime of an object. When we forget to end the lifetime of an object, there is never any immediate error to tell us that we forgot. The problems that subsequently occur tend to be subtle, so we often do not notice them until a considerable time after the fact. For example, forgetting to unregister an observer from an event source turns the observing object into a memory leak, and causes the observing method to keep being needlessly invoked by the event source, to perform actions that in the best case only waste clock cycles without any value or effect, and in the non-best case cause malfunction. When the malfunction does get noticed, it often seems inexplicable and does not tend to point to the source of the problem. Even when we discover that a certain malfunction is due to the lifetime of an object not having been ended, it is usually difficult to tell at which point in the code it should have been ended. Often, in order to know this, we first need to know where in the code the object was allocated, but this information is not normally available. Writing tests to catch omissions in object lifetime control is not only hard and tedious, but it also requires testing against the implementation rather than testing against the interface, which violates recommended best practices. (To test whether object A properly ends the lifetime of object B, we have to mock B and ensure that its lifetime termination method is invoked by A, but if we do this then we are by definition testing against the implementation of A.) The Solution Whereas it is generally true that \u0026quot;if you do everything right there will be no problems\u0026quot;, this is a very bad rule to live by, because it completely disregards another very important rule which says \u0026quot;there will be mistakes\u0026quot;. Reliance on everything being done right tends to result in brittle software designs, because some things will inevitably go wrong. We are definitely not advocating designs that are tolerant of mistakes; however, a software design must at the very least offer means of detecting mistakes and responding to them with hard error, diagnostic messages, or both; a design which relies on mistakes not being made, and at the same time is incapable of detecting the mistakes that will nonetheless be made, is doomed to run into trouble.\nObject lifetime awareness is a design pattern for writing robust software. It begins by acknowledging that in garbage collected languages there tends to be widespread uncertainty with respect to the lifetime of objects, which results in bugs that are very difficult to troubleshoot and fix. While the garbage collector would ideally be taking care of a lot of things, by its nature it cannot take care of everything, and in practice it often does not even take care of things that it is expected to, due to subtle human error.\nThe impetus behind object lifetime awareness is that we have had enough of this uncertainty, so we are taking matters into our own hands by establishing definitive knowledge of the lifetime of our objects and taking full control over it. When an object has an inherent notion of lifetime, this notion must always be made explicit, and handled in a certain structured and recognizable manner, so that when mistakes occur, we receive hard errors and diagnostic messages, allowing us to fix problems without troubleshooting in the dark.\nSpecifically, every lifetime-aware object must do the following:\nEncapsulate an \u0026quot;alive\u0026quot; state which: Starts as \u0026quot;true\u0026quot;. At some moment transitions to \u0026quot;false\u0026quot;. Is not exposed. It can be asserted. Can be inspected with a debugger. On debug runs, the lifetime-aware object must respond with hard error to any attempt to invoke any of its public instance methods once its lifetime is over. On debug runs, the lifetime-aware object must discover any omission to end its lifetime, and generate a diagnostic message if so. (More on how to achieve this later.) Please note that the definition of a \u0026quot;debug run\u0026quot; varies depending on which language you are using:\nIn C# it is a run of the debug build. In Java it is a run with assertions enabled. Please also note that automated test runs are usually debug runs.\nLuckily we do not have to add lifetime awareness to all objects, we only need to add it to objects that belong to one or more of the following categories:\nObjects that by their nature have a concept of lifetime, such as timers, windows, files, network connections, notification suppressors, etc. Objects that once initialized, are known to have some cleanup to do eventually. Objects with which other objects may register in some way. (To ensure that each object that registers does not forget to unregister.) Objects that contain (own) other lifetime-aware objects. In each system the objects that can benefit from lifetime awareness tend to be relatively few, while the majority of objects can continue being blissfully unaware of their lifetime, letting the garbage collector handle it.\nIn certain rare cases, a lifetime-aware object may control its own lifetime; however, far more often, the lifetime of an object is meant to be controlled by other objects. In these cases, the lifetime-aware object should implement the disposal interface of the language, primarily in order to document the fact that it is lifetime-aware, and secondarily so that the \u0026quot;automatic\u0026quot; disposal mechanism of the language can be used when the opportunity arises.\nIn Java, that would be an object implementing the Closeable interface, thus allowing us to sometimes make use of the try-with-resources statement. In C#, that would be the an object implementing the IDisposable interface, thus allowing us to sometimes make use of the using keyword. Please note that the use of these interfaces here has nothing to do with releasing unmanaged resources; The goal is object lifetime awareness, while the releasing of unmanaged resources is at best a side note and largely a red herring in this discussion. It is true that the original intention of these interfaces was to allow releasing unmanaged resources, but there is absolutely nothing, either in the interfaces themselves, or in the language specifications, or in the respective compilers, or in the respective runtime environments, which says that this has to be the only purpose of these interfaces, or the only way they should be used, or the only way they can be used. So, here we are using them for something else. Please completely disregard the issue of unmanaged resources for now, we will address them later.\nBy making objects aware of their own lifetime, we achieve the following:\nAny discrepancy between an object's expected alive state and its actual alive state (i.e. whether we think it should be alive vs. whether it actually is alive) can be asserted against and therefore be swiftly and infallibly detected without any need for white-box testing. The alive state of an object can be explicitly and deterministically controlled without ever having to rely on finalization to do it for us. All necessary cleanup can be done when the alive state transitions to false, thus ensuring that each initialization action is always balanced by its corresponding cleanup action. This includes ending the lifetime of any contained (owned) objects, unregistering the object from whatever it had previously registered with, etc. At the end of the object's lifetime we can take whatever extra measures are within our power to take in order to ensure that the lifetime of other objects is being correctly managed. For example, we can assert that any objects which had previously registered with this object have by now unregistered themselves. More broadly, we construct our software to be in complete control over its inner workings, instead of leaving things to chance. Detecting omissions The main thing which makes object lifetime awareness a viable proposition is the promise of useful diagnostic messages in response to omissions to explicitly end the lifetime of objects. Without such diagnostic messages, object lifetime awareness would not be much different from existing practices.\nInterestingly enough, (or perversely enough, depending on how you would like to see it,) the mechanism that we leverage in order to detect such omissions is the garbage collector itself. The idea is that an object can check during finalization whether it is still alive or not: if it discovers that it is being finalized while still alive, then this means that the programmer forgot to explicitly end the lifetime of the object at an earlier moment.\nIt is very important to note that once we detect that an object is still alive during finalization, we specifically refrain from repeating the widespread mistake of trying to correct the problem: we most certainly do not attempt to end the lifetime of the object at that moment; instead, we only generate a diagnostic message, alerting the programmer that they forgot to end the lifetime of the object at an earlier time. This is important because the checks performed during finalization are meant to be of a strictly diagnostic nature, (a quality assurance mechanism if you wish,) so they are only meant to be performed on debug runs, so our software better be working correctly without them on release runs.\nOne might protest that an object which has accidentally become a memory leak will never be finalized, so it will never discover that its lifetime was not ended. Luckily, this can be taken care of with a bit of infrastructural support and a bit of discipline: During application shutdown we ensure that our system undergoes an orderly and thorough cleanup phase, where all remaining lifetime-aware objects are terminated. Typically, this simply means ending the lifetime of the main application object, and this should cascade throughout the entire containment hierarchy, ending the lifetime of all objects. Once this cleanup phase is complete, and if this is a debug run, we force a full garbage collection, and we wait for it to complete before exiting the application. In doing so, we ensure that all finalizers are invoked, and this includes the finalizers of any objects that were inadvertently memory-leaked. Thus, any omission to end the lifetime of an object is detectable in the worst case during application shutdown. For this to work optimally, some extra discipline is necessary, for example avoiding to directly or indirectly anchor lifetime-aware objects in static storage.\nIn actual practice most omissions to control the lifetime of objects happen without the objects necessarily also becoming memory-leaked, so the objects do get garbage-collected, so the omissions are detected at various moments during runtime when garbage collection occurs. For this reason, it is beneficial on debug runs to introduce regular forced garbage collection, thus detecting omissions as soon as possible after they happen. The right moment to force garbage collection tends to be:\nOn web servers, immediately after servicing each client request. On desktop applications, immediately after each application logic idle event. (An application logic idle event is similar to the graphical user interface idle event, except that it happens less frequently, i.e. not after every single event from the input system such as a mouse move, but only after the application logic has actually had some work to do.)\nOn data processing systems with a main loop, at the end of each iteration of the main loop. It is worth stressing that forced garbage collection only needs to be employed as a diagnostic tool, and only on debug runs. On release runs there is never a need to force garbage collection because all object lifetime control issues are presumed to have already been addressed.\nForced garbage collection can also be used as a diagnostic tool during automated software testing; however, if our tests are fine-grained, (as the case usually is with unit tests,) it is advisable to refrain from forcing garbage collection after each test, because a full run of the garbage collector tends to be expensive, so its frequent use may multiply the total run time of a test suite by a very large factor. The ideal is to perform just one forced garbage collection at the end of all tests, and if any object lifetime control failures are detected, then and only then do another run of all tests with forced garbage collection enabled after each test, to detect precisely in which tests the failures occur.\nIn order to force garbage collection at will during testing, one needs a testing framework which supports this, and I am not aware of any, but if you do not make use of any exotic features of your existing testing framework, it is easy to write your own and take control yourself.\nAddendum: Lifeguards For an object to be aware of its own lifetime and to issue diagnostic messages when its lifetime is not properly controlled, a certain amount of functionality is needed, and we do not want to be coding this functionality by hand in each class that we write, so we will be delegating as much of the work as possible to a separate class. An appropriate name for such a class would be ObjectLifetimeGuard, but this is a mouthful, so we will simply abbreviate it to LifeGuard.\nThe lifeguard exposes only 2 methods:\nA lifetime-assertion method which is invoked to assert that the lifeguard is still alive. An end-of-lifetime method which is invoked to let the lifeguard know that its lifetime is over. With the introduction of the lifeguard, each lifetime-aware class only needs to do the following:\nObtain during construction, and fully encapsulate, an instance of LifeGuard. Perform an assertion at the beginning of each public instance method, (by definition only on debug runs, since it is an assertion,) which simply delegates to the lifetime-assertion method of the lifeguard. Implement the object disposal interface of the language at hand, performing whatever cleanup actions are necessary, and then delegating to the end-of-lifetime method of the lifeguard. The lifeguard does the following:\nOn debug runs, it encapsulates an alive state which starts as true. It implements the is-alive-assertion method as follows: On debug runs, it returns true if the object is alive, and throws an exception if not. On release runs, it always throws an exception, because it is only meant to be invoked from within assertions, and assertions are not meant to execute on release runs. It implements the end-of-lifetime method as follows: On debug runs, it first asserts that the object is currently alive, and then transitions the alive state to false. On release runs, it does nothing. On debug runs it defines a finalizer which checks whether the object is still alive during finalization, and generates a diagnostic message if so. Notes:\nA lifeguard is obtained by invoking a factory method instead of using the new keyword, because this method will return something different depending on whether this is a debug run or a release run. The factory can come in the form of a static method for simplicity, or it can come in some other form if necessary.\nThe interface of the lifeguard has been designed in such a way that its alive state can be asserted without being exposed. This has the effect of:\nPreventing misuse\nAllowing for a high performance implementation for release runs which does not even contain that state. Still allowing the alive state to be inspected with a debugger on debug runs. Lifetime-aware objects that have a need for some similar state which is queryable must implement it separately. The fact that on debug runs this state will be mirroring the alive state of the lifeguard is irrelevant.\nIn certain environments which support asynchronous method invocations it might be impossible to guarantee that no method is ever invoked past end of lifetime; these are exceptions to the rule, which need special handling by means of if statements instead of assertions. Since the lifeguard only allows asserting the alive state without exposing it, such objects will need to implement their own alive state in parallel to the lifeguard. As a rule, triggering hard error is preferable over generating diagnostic messages; however, an omission to end the lifetime of an object can only be detected during finalization, and by that time it is already too late for any fail-fast measures, so what we have here is an exception to the rule: in this particular case, it is okay if we just generate a diagnostic message. If needed, extra measures can be taken to alert the programmer to not forget to look at the diagnostic messages. The diagnostic message generated in the event of an omission to end the lifetime of an object is meant to include a stack trace, complete with source filenames and line numbers, showing precisely where in the source code the object was allocated, to help us easily locate and fix the problem. Unfortunately, this stack trace needs to be collected by the lifeguard during construction, just in case it will need to be displayed during finalization, but in many environments collecting a stack trace is unreasonably expensive, so if each lifeguard instantiation was to involve collecting a stack trace, this would run the danger of slowing down our debugs runs to the point of making them unusable. (Obtaining a stack trace with source filenames and line numbers a few dozen times per second incurs a noticeable penalty on the JVM, while under DotNet the penalty is catastrophically more severe.)\nFor this reason, a special procedure is necessary: by default, stack traces are not collected, so a lifeguard which detects an omission to end the lifetime of an object reports only enough information to help us identify the class of the containing object. Once we know the class, we can go to the source code and flip a flag which enables stack trace collection for lifeguards of that specific class only, so that we can then re-run and obtain a message which includes a stack trace. Once we have solved the problem, we put the flag back to its default value to avoid the performance hit.\nBoth in C# and in Java there is an established tradition which says that methods involved in the closing or disposing of things should be forgiving, in the sense that multiple invocations should be permitted with no penalty. In my opinion this practice is ill-conceived, so instead I prescribe an end-of-lifetime method which asserts that it is never invoked twice. This is in line with the overall theme of object lifetime awareness, which is to gain greater control over the inner workings of our software. I am perfectly aware of the fact that this is parting ways with a tradition cherished by the entire industry; it is perfectly fine to part ways with traditions when you know better, especially since another term for tradition is \u0026quot;capricious progress-stopper\u0026quot;. The lifeguard is designed in such a way that on release runs it contains no state and performs no action; therefore, it need not be instantiated once per lifetime-aware object; instead, it can be a singleton, and all lifetime-aware objects can receive the same reference to its one and only dummy instance. Thus, the performance cost of using the lifeguard on release runs is near zero. An implementation of lifeguard in C# is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 namespace SomeNamespace; using Sys = System; using Collections = System.Collections.Generic; using System.Linq; using SysDiag = System.Diagnostics; using SysComp = System.Runtime.CompilerServices; public abstract class LifeGuard : Sys.IDisposable { public static LifeGuard Create( bool collectStackTrace = false, // [SysComp.CallerFilePath] string? callerFilePath = null, // [SysComp.CallerLineNumber] int callerLineNumber = 0 ) { if( !DebugMode ) return ProductionLifeGuard.Instance; Assert( callerFilePath != null ); if( collectStackTrace ) return new VerboseDebugLifeGuard( 1 ); return new TerseDebugLifeGuard( callerFilePath!, callerLineNumber ); } public abstract void Dispose(); public abstract bool IsAliveAssertion(); private sealed class ProductionLifeGuard : LifeGuard { public static readonly ProductionLifeGuard Instance = new ProductionLifeGuard(); private ProductionLifeGuard() { } //nothing to do public override void Dispose() { } //nothing to do public override bool IsAliveAssertion() =\u0026gt; throw new Sys.Exception(); //never invoke on a release build } private class DebugLifeGuard : LifeGuard { private bool alive = true; private readonly string message; protected DebugLifeGuard( string message ) { this.message = message; } public sealed override void Dispose() { Assert( alive ); alive = false; System.GC.SuppressFinalize( this ); } public sealed override bool IsAliveAssertion() { Assert( alive ); return true; } protected static string GetSourceInfo( string? filename, int lineNumber ) =\u0026gt; $\u0026#34;{filename}({lineNumber})\u0026#34;; ~DebugLifeGuard() { SysDiag.Debug.WriteLine( \u0026#34;Object still alive!\u0026#34; ); SysDiag.Debug.WriteLine( message ); } public override string ToString() =\u0026amp;gt; alive ? \u0026#34;\u0026#34; : \u0026#34;END-OF-LIFE\u0026#34;; } private sealed class TerseDebugLifeGuard : DebugLifeGuard { public TerseDebugLifeGuard( string callerFilePath, int callerLineNumber ) : base( $\u0026#34; {GetSourceInfo( callerFilePath, callerLineNumber )}\u0026#34; ) { } } private sealed class VerboseDebugLifeGuard : DebugLifeGuard { public VerboseDebugLifeGuard( int framesToSkip ) : base( buildMessage( framesToSkip + 1 ) ) { } private static string buildMessage( int framesToSkip ) =\u0026gt; string.Join( \u0026#34;\\r\\n\u0026#34;, getStackFrames( framesToSkip + 1 ) // .Select( getSourceInfoFromStackFrame ) ); private static SysDiag.StackFrame[] getStackFrames( int framesToSkip ) { var stackTrace = new SysDiag.StackTrace( framesToSkip + 1, true ); SysDiag.StackFrame[] frames = stackTrace.GetFrames()!; Sys.Type type = frames[0].GetMethod().DeclaringType; Assert( typeof(Sys.IDisposable).IsAssignableFrom( type ) ); return frames.Where( f =\u0026gt; f.GetFileName() != null ) // .ToArray(); } private static string getSourceInfoFromStackFrame( SysDiag.StackFrame frame ) { string sourceInfo = GetSourceInfo( frame.GetFileName(), frame.GetFileLineNumber() ); return $\u0026#34; {sourceInfo}: {frame.GetMethod().DeclaringType}.{frame.GetMethod().Name}()\u0026#34;; } } } Note that in theory, private readonly string message may have already been finalized by the time the destructor attempts to use it. In reality, I have never encountered this happening. If it becomes a problem, a simple string.Intern() could be used to permanently anchor these strings in memory, and that is okay despite the fact that it essentially introduces a memory leak, because it is only applicable to debug runs.\nDebugMode is defined as follows:\n1 2 3 4 5 6 7 8 9 10 11 public static bool DebugMode { get { #if DEBUG return true; #else return false; #endif } } This allows us to minimize the use of #if DEBUG, which is ugly and cumbersome, and often results in code rot in the #endif part, which is only discoverable when trying to compile the release build.\nAddendum: Ad-hoc alive states Object lifetime awareness comes with a piece of advice:\nAvoid ad-hoc alive states, implement them as separate lifetime-aware objects instead.\nWhat this means is that a class should refrain from exposing a pair of methods for entering and exiting some special state of that class, and instead it should expose only one method which creates a new lifetime-aware object to represent that special state, and to exit the state when its lifetime is ended. Then, if the class has any methods which may only be invoked while in that special state, these methods must be moved into the special state object, so that they are not even available unless the special state has been entered.\nBy following this advice we split the interface of our object into smaller interfaces that are more simple and intuitive, we clearly document what is going on by making use of the lifetime-awareness pattern, and we take advantage of the error-checking and diagnostic facilities of the lifetime-awareness mechanism.\nAn example of an interface which could have benefited from this advice is the JDBC API. This interface exposes a multitude of methods for dealing with a relational database, and among them it exposes a pair of methods for beginning and ending a transaction. A better way of structuring that interface would have been to expose a single method for creating a new transaction object, which in turn ends the transaction when disposed. Then, all the data manipulation methods would be moved into that object, so that it is impossible to manipulate data unless a transaction is active.\nAddendum: Unmanaged Resources As we have shown, by leveraging hard error and diagnostic messages on debug runs and test runs, the object lifetime awareness pattern guarantees cleanup at the end of an object's lifetime.\nConveniently enough, this cleanup can, and should, include the releasing of unmanaged resources.\nThis in turn means that we never need to involve finalization for this task, not even as a fallback mechanism: unmanaged resources can be released infallibly, deterministically, and synchronously, i.e. always right now, as opposed to at some unknown moment later in time, if at all. This also means that on release runs we do not need finalization at all.\nIn essence, the releasing of unmanaged resources loses the special status that it has enjoyed so far, and becomes regular cleanup just as any other kind of cleanup. Our software sees to it that all necessary cleanup is always performed, without leaving anything to chance, and without any distinctions between really important cleanup and not-so-important cleanup.\nC#-only note: This also means that there is no more need for that Dispose(bool) nonsense, either.\nFurther research and recommendations Lifetime aware objects may benefit from a lifetime control service being propagated throughout the containment hierarchy so that they can register and unregister from it, thus:\nEliminating the need for a static factory of lifeguard; Allowing us to at any given moment traverse the entire graph of lifetime-aware objects to see who is still alive; Making it impossible to inadvertently construct a lifetime-aware object without having explicit knowledge of the fact that it is lifetime-aware, since the lifetime control service must be passed to its constructor. Object lifetime awareness has the theoretic potential of completely eliminating all finalization overhead. Unfortunately, as things stand today, this potential cannot be realized, because existing runtime environments still offer essential classes that make unconditional use of finalization; e.g. classes that represent files, sockets, etc. These environments could benefit from new implementations of such essential classes that make use of the object lifetime awareness pattern so as to also avoid finalization. (While at it, please also note that these same classes could really benefit from not being needlessly multithreading-aware; when we have a use for multithreading awareness, we can add it ourselves, thank you.)\nAdditionally, if it could be definitively established that finalization is to be used only for the purpose of generating diagnostic messages, then the entire machinery implementing finalization in runtime environments could be greatly simplified from the monster of complexity that it is today. Consider, for example, that garbage collectors are currently built to handle such preposterous situations as \u0026quot;object resurrection\u0026quot;, which is what may happen if a finalizer decides to anchor an object in memory, thus taking an object which had previously become eligible for collection and making it not eligible anymore. If finalization could be made trivial, then object resurrection could become impossible, or it could result in hard error rather than having to be handled.\nAlso see my previous post Mandatory disposal vs. the 'Dispose-disposing' abomination\nCover image: The Thinker (French: Le Penseur) by Auguste Rodin (From Wikipedia)\nThe language feature that C# calls \u0026quot;destructor\u0026quot; is a misnomer; it is not a destructor, it is a finalizer, and the choice of the tilde syntax to denote finalizers in C# as if they were C++ destructors has caused nothing but confusion. Microsoft has been reluctantly acknowledging this and quietly correcting their terminology in their documentation.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2020-12-27T10:46:17.746Z","permalink":"https://blog.michael.gr/post/2020-12-27-object-lifetime-awareness/","title":"Object Lifetime Awareness"},{"content":"If you have ever done any software development under Microsoft Windows you have probably come across this famous error message:\nSystem.IO.FileNotFoundException : Could not load file or assembly 'Acme.dll' or one of its dependencies. The specified module could not be found.\u0026quot;\nModern software makes heavy use of dynamic link libraries, and the problem with this kind of libraries is that for various reasons they might not be there when you need them, resulting in runtime errors. This is the runtime error you get under Windows when this happens.\nNaturally, when you see this message, the first thing to do is to check whether Acme.dll is there, and what you usually discover is that the file is indeed there. When dealing with computers, most error messages that you come across tend to leave some room for troubleshooting, but when the system is reporting that a certain file does not exist on your very own filesystem, while the file is most certainly there, the situation seems really hopeless. You are stymied.\n(Useful pre-reading: About these papers)\nAt this point you are likely to start shot-gunning the problem by trying various random tricks in the hope that one of them will magically make the problem go away: you try running the application again just in case it was a glitch, you try obtaining a fresh copy of the library in case this one somehow got corrupted, you restart Windows because we always try that, right? -- and none of these attempts yields any results.\nFinally, when you have exhausted all other possibilities, you decide to take a closer look at the error message again. You notice that it says that it could not load Acme.dll \u0026quot;or one of its dependencies\u0026quot;, so you start with a new hypothesis: what cannot be found is not Acme.dll itself, it is one of the libraries that Acme.dll in turn tries to load. But which one? The next sentence says \u0026quot;the specified module could not be found\u0026quot;, but which module is the specified module? There is only one module being named in the error message, and that is Acme.dll, so this must be the specified module, right?\nWhat is happening here is that this error message is a notorious instance of trolling with which Microsoft has been torturing software professionals for decades now by telling them lies instead of reporting the actual problem. The problem is indeed that one of the dependencies of the library in question could not be found, but Windows will not tell you which one. Instead, it will give you this insidiously worded message which will send you looking for the problem in wrong directions.\nThere is probably some programmer who worked at Microsoft some 30 years ago and is probably a pensioner by now, who has a permanent evil grin on his face knowing that he has personally caused millions of work hours wasted all over the planet over the course of several decades simply by creating this particular error message in this particular way. (Or, perhaps, he told his manager that the task he was working on would need to take a little longer because he had to collect all necessary information to produce a useful error message, and his manager told him to not do that because deadlines.)\nTo fix this problem you have to use some special software called a \u0026quot;Dependency Analyzer\u0026quot; to trace all the dependencies of your application and locate the one that fails to load.\nThe msvcr100.dll sub-problem Quite often the dependency analyzer finds that the culprit is msvcr100.dll or something similar, which you might be completely unaware of. This msvcr100.dll is the dynamically linkable runtime library for software written using some old version of Microsoft Visual C++, which is a very popular language for writing all sorts of software under Windows, so if your application is using any third-party libraries, then one or more of them have almost certainly been written in MSVC. For some reason, many developers of libraries choose to make their product depend on an external instance of the MSVC runtime library instead of statically linking the MSVC runtime library into their product, and this creates an extra moving part which must be dealt with by others, like you.\nWhat is especially treacherous about msvcr100.dll in particular is that most large commercial applications contain it and install it in your Windows/System32 folder, so once you have installed a few commonly used apps on your machine you almost certainly have msvcr100.dll. This causes two major problems:\nYou are completely oblivious to the fact that the application that you are developing indirectly depends on msvcr100.dll without including it in its own installer. You only discover this problem when you try to run your application on a relatively fresh installation of Windows. Not only you are oblivious about the msvcr100.dll problem, but also, the developers of libraries that you use might also be oblivious to it. For example, HDF5DotNet.dll is a popular library used by DotNet applications for reading and writing HDF5 files. This library depends on msvcr100.dll but does not include it in its installable package, nor is there any mention in their documentation about the fact that msvcr100.dll must be present in order for their library to successfully load. Note that an msvcr100.dll with a size of 773968 bytes may exist under C:\\Windows\\SysWOW64, but it may not necessarily be the one you need. Your application might instead depend on another msvcr100.dll with a size of 829264 bytes under C:\\Windows\\System32.\nThe approach recommended by Microsoft for solving this kind of problem is to include the \u0026quot;Microsoft Visual C++ Redistributable\u0026quot; installable package, and install it as part of the application's installation process. An easier way is to simply include a copy of msvcr100.dll in the directory from which your application launches.\n","date":"2020-12-16T12:15:11.799Z","permalink":"https://blog.michael.gr/post/2020-12-16-the-famous-could-not-load-file-or/","title":"The famous \"Could not load file or assembly or one of its dependencies\" error message"},{"content":"\rAbstract A Software Design Pattern for concurrent systems is presented, which makes race conditions something that can be asserted against and thus deterministically eliminated rather than stochastically reduced or minimized.\nA description of the problem Every Software Engineer who has dealt with concurrency knows that it is hard. The bane of concurrency is race conditions: when a thread accesses data without taking into account the fact that the data is shared with other concurrently running threads which may alter that data at any unforeseeable moment in time.\n(Useful pre-reading: About these papers)\nThere exist two kinds of race conditions that I can think of, let's call them physical and logical. (I just made up these terms, perhaps they have already been studied and given other names, but I am unaware of that.)\nPhysical Race Conditions happen due to the way the underlying hardware works. One example is trying to read a variable consisting of who machine words, thus requiring two successive read operations which are not atomic, while another thread is simultaneously writing to that variable, resulting in garbage being read. Another example is two threads simultaneously performing increment operations on the same memory location, where a memory increment is implemented by the CPU as a non-atomic sequence of read-increment-write operations, resulting in some of the increments being lost. Logical Race Conditions happen when application logic fails to account for concurrency. For example, checking whether a collection contains a value, and if not, adding the value to the collection: when two threads try to do this, it will sometimes happen that they will both find that the collection does not contain the value in question, and will both add it, resulting in a duplicate. Depending on whether the implementation of the collection allows duplicates or not, this will result either in soft malfunction, (a duplicate where it was not intended,) or in hard failure due to the collection throwing a \u0026quot;duplicate element\u0026quot; exception. Note that logical race conditions can occur even if we have taken all necessary precautions (i.e. locking) to avoid physical race conditions. Incidentally, this is the reason why many of the so-called \u0026quot;concurrent\u0026quot; collections like the \u0026quot;concurrent map\u0026quot; are of very limited use: sure, they guarantee that they will not crash and burn, but they do not guarantee correct results.\nRace conditions exhibit a disastrous combination of unfortunate qualities:\nNon-deterministic: you cannot reproduce them at will, they just appear to happen at random, so you can almost never use the debugger to troubleshoot them. Sensitive to troubleshooting instrumentation: not only they never manifest while single-stepping through code, but if you introduce extra code to detect them, they may seemingly disappear, because they are highly dependent on timing. The moment you remove the instrumentation however, they may start manifesting again. Elusive: their effects are usually observed not at the moment that they occur but after the fact, so it is difficult to tell what happened and why it happened. Confusing: sometimes, the malfunction that they cause seems at first impossible to happen, requiring extensive troubleshooting before the realization sinks in that it must be due to a race condition. Multi-faced: in many cases the effects of a race condition differ on each manifestation, so you are never sure whether you are chasing one issue or several issues at the same time. Untestable: there is no unit test that can catch race conditions or give any assurances for their absence. Treacherous: a race condition which happens on average once every million seconds of usage may take months before it manifests in your development environment, and yet once there are a million customers using your software, there will be one customer encountering it roughly every second. Catastrophic: program state corruption tends to result in complete failure of the software. Solutions that try to avoid the problem Since concurrency with locks is so hard, a number of mechanisms have been invented that try to implement concurrency without locking.\nImmutability (functional programming): if all program state is immutable, then there is no possibility of one thread modifying some state while another thread is trying to read it, because there is no state that can be modified. Therefore, no locking is necessary.\nDisadvantages:\nFunctional programming and immutability are not ubiquitous, and it is yet to be seen whether they will ever become ubiquitous. Many implementations of Functional Programming are not purely functional, they mix mutability with immutability, so the problem remains. Functional programming is only common in high-level systems running on garbage-collecting virtual machines. It is rare in mid-level systems and virtually absent in low-level systems. Many of the data structures that give the illusion of mutability while their inner workings are immutable tend to be computationally more expensive than their straightforward mutable counterparts. Message-passing: threads never share any data, instead they only work on data that they exclusively own, and they exchange data by means of immutable messages passed through message queues. Essentially, in the entire system there is only one little piece of code which employs locking, and that is the concurrent message queue implementation. The idea is that we should be able to get at least that small part right.\nDisadvantages:\nPerformance suffers as the amount of data exchanged among threads increases. Performance also suffers since data can never be manipulated in-place, it must be placed in a message, the message must be posted into a queue, a thread context switch must usually occur for the receiving thread to process the message, and then the reverse path must be followed for the original thread to receive the result. (When manipulating data in-place, a thread context switch will only occur when attempting to obtain a lock while another thread already holds that lock, which may be a rare occurrence.) Nowadays in order to avoid the tedious creation of countless message classes you are more likely to just post a lambda into the message queue, but then you have a lambda which is declared in one thread but executed in another thread, so you still have to be extremely careful with what that lambda is allowed to touch. Other: exotic mechanisms such as the single-writer principle of the Rust programming language.\nDisadvantages:\nThey tend to require compiler support. (So, a mechanism that can be implemented in any language would still be of value.) So, avoiding race conditions when practicing concurrency by means of locking is an existing problem in need of a solution.\nA deeper look at the problem At the heart of the race condition problem lies the \u0026quot;to lock or not to lock\u0026quot; conundrum. The choice of what to do lies in a continuum between two absurd extremes:\nUltra-fine grain locking: Always lock every single little piece of mutable state when accessing it, and only while accessing it. Ultra-coarse grain locking: Place a global lock on the entirety of your mutable state on program start and release it on program exit. Obviously, neither of these extreme approaches would work.\nUltra-fine grain locking would result in an unreasonable amount of bloat in all code that we write, it would suffer performance-wise, and although it would eliminate physical race conditions, it would do nothing for logical race conditions.\nUltra-coarse grain locking would not work either because increasing the lifetime of a lock also increases the chances that other threads will be blocked, with the absurd extreme of the lock lifetime being equal to program runtime resulting in all threads becoming permanently blocked and no actual sharing of any mutable state ever taking place.\nSo, the answer to the \u0026quot;to lock or not to lock\u0026quot; conundrum always lies somewhere in-between:\nAlways lock for as long as necessary, but try not to lock any longer than necessary.\nThis leads to the number one cause of race conditions:\nTrying to lock for as long as necessary but to avoid locking longer than necessary means that there will always be code which is accessing mutable state without first acquiring a fine grain lock, and instead is assuming that a coarser grain lock has already been acquired by some other code higher up the call-tree. (Remember, computer science trees are upside-down.) This assumption leaves open the possibility of human error, as the programmer who wrote the code higher up the call-tree may have forgotten to lock, thus putting all code below it at risk of race conditions.\nThis situation is so widespread that it may be hard to realize its full extent: every single time we invoke a standard runtime library mutable collection class (which is one of the most frequent things we do) we are engaging in this assumption: the collection is not concurrency aware, so it is not placing any locks, but it is manipulating mutable state, so under conditions of concurrency it will fail unless a lock is in place. Essentially, the collection class is doing its job while praying that someone up the call tree has remembered to acquire the necessary lock.\nThe grain of locks affects two things: performance and correctness.\nChoosing the grain of the locks in the most performant way is more of an art than a science, requiring a master of the craft to do it right, and that is okay: experts will always be useful. If no expert is available, performance might end up being suboptimal, but the software will still run, so strictly speaking the expert is not necessary.\nChoosing the grain of the locks in such a way that the program remains correct is also more of an art than a science as things stand today, so it also requires a master of the craft to do it right; however, if we want to be thinking of our profession as a science rather than an art, we cannot have software that tends to crash and burn unless a master of the craft has written it. Therefore, we need a mechanism for detecting and protecting ourselves against the human error which is practically inevitable when an apprentice rather than a master touches the code, or even when the master touches the code while having a bad day.\nRestating the problem A very important first step in solving a problem often is to restate it using terminology that is more conducive to solving it. The term \u0026quot;Race Condition\u0026quot; is somewhat cumbersome because it refers to an unfortunate event which may or may not happen, depending on non-deterministic circumstances. The original 1954 paper by David Huffman, titled \u0026quot;The synthesis of sequential switching circuits\u0026quot;, which contains the first known mention of the term, regards race conditions as something which may exist when a certain instability is detected, so even the original sense referred to events that may potentially occur.\nHowever, if we care about software correctness, then we do not want to be leaving anything to chance, so the fact that the unfortunate event may happen is irrelevant: if circumstances can arise at all which would potentially allow a race condition to occur, then for all practical purposes it must be assumed that the race condition will occur. Therefore, the race conditions themselves should be of no interest to us; what should be of interest is modes of operation that allow race conditions to occur. We will call them Race Modes.\nA Race Mode is an erroneous mode of operation in which a race condition can potentially occur.\nA piece of software either enters race modes, or it does not. If it does enter race modes, then race conditions may occur, and as already established, it should be presumed that they will occur. If the software never enters any race modes, then no race conditions can occur.\nSo, the problem has been restated from \u0026quot;avoiding race conditions\u0026quot; to \u0026quot;avoiding race modes\u0026quot;. The difference may be subtle, but it is important enough to make.\nThe solution In restating the problem as described above we have set ourselves a new goal: how to assert against race modes. If race modes can be asserted against, then the concurrency problem stops being subject to chance and becomes quite deterministic instead: if our software runs and no assertion failures occur, then it never enters a race mode, and therefore no race conditions are possible. (Note that the assertions are not meant to catch race conditions; the assertions are meant to catch race modes.)\nThe mechanism that I have come up with for asserting against race modes is called Coherence and in its simplest form it can be thought of as an abstraction of an Assertable Lock.\nThere are two things you can do with coherence: enter it, and assert it. (By entering coherence we mean executing a piece of code while in coherence, so once that piece of code is done executing, coherence will be exited.) So, coherence gives us the ability to:\nTake measures at certain places in our code to prevent entering a race mode. Ensure in all other places in our code that the necessary measures have been taken to guarantee we are not in a race mode. Thus, coherence saves us from doing ultra-fine grain locking and from making assumptions about locking: by turning locks into something assertable, we do not have to acquire a lock every single time we touch mutable state, but we can assert that a lock has been acquired by code higher up the call tree. Since assertions can compile to nothing on the release build, this is a zero-runtime-cost solution.\nThe name Coherence was chosen as opposed to Assertable Lock because Coherence is meant to be a high level abstraction. The use of an abstraction is necessary for two reasons:\nCoherence is meant to be asserted ubiquitously by any code that accesses mutable state, even by general purpose code such as the standard collection classes. However, general purpose code tends to be (and should remain) agnostic of concurrency, so it should not be burdened with such a low-level and concurrency-specific concept as locking.\nDepending on the concurrency characteristics of the execution environment, there can be different implementations of coherence, some of which do not even involve actual locking, so using the term 'Lock' would be inaccurate and misleading.\nExamples of possible coherence implementations depending on the concurrency characteristics of the execution environment:\nIn a strictly single-threaded environment, a dummy implementation is needed which never places any locks and never fails a coherence check. In a share-nothing environment, a simple implementation will suffice which never places any locks and only fails a coherence check if the currently executing thread is not the thread that owns the mutable state, i.e. the thread in which the mutable state was created. In a thread-pooled, share-nothing environment, a somewhat more elaborate implementation is needed which takes into account the fact that the thread which owns the mutable state may not necessarily be the thread that created the mutable state, since threads are picked from a pool. In a multi-threaded environment with a small amount of shared state, a singular locking implementation will suffice which enters coherence by obtaining a lock on the totality of the shared state and fails the coherence check when that lock has not been obtained. This represents a coarse grain lock, so it might result in sub-optimal performance, but it has the advantage of being simple and avoiding deadlocks. In a multi-threaded environment with a large amount of shared state and high thread contention over it, necessitating finer grain locking for good enough performance, a plural coherence implementation can be used which allows placing independent locks on independent subsets of the shared state, and fails a coherence check when the lock corresponding to a particular subset of state has not been obtained. Care must be exercised to always enter and assert the correct instance of coherence for each subset of state, and to avoid deadlocks in doing so. Regardless of the concurrency characteristics of the execution environment, when the lifetime of a certain piece of mutable state is confined within a single call tree, a simple coherence implementation will again suffice which does not place a lock and simply asserts that the current thread is the thread in which the state was created. (To guard against the mutable state somehow escaping the scope of the call tree in which it was meant to be confined.) Note that Coherence only allows asserting its state and purposefully disallows testing its state. In other words, you cannot have an \u0026quot;if coherence is entered then...\u0026quot; construct. This is done so as to prevent misuse and to allow for high performance coherence implementations that, on the release build, may not have explicit knowledge of whether coherence has been entered or not.\nNote that unlike most existing locking mechanisms, which explicitly allow a thread to obtain a lock multiple times, coherence explicitly disallows re-entrance. I have chosen to do it this way because my approach to Software Engineering is \u0026quot;leave nothing to chance\u0026quot;, so if you are unsure whether you have already obtained a lock on something, and you would like the locking mechanism to be forgiving in case you try to lock twice, then you must be doing something wrong. It is my firm belief that when a piece of framework is in a position of alerting you that you are doing something wrong, it should be alerting you that you are doing something wrong. Of course it may be that I am wrong here, and unbeknownst to me there exist legitimate reasons for having to allow coherence reentrancy; this remains to be seen.\nFurther Research As mentioned earlier, in multi-threaded environments with a large amount of shared state and high thread contention over it, performance concerns often necessitate dividing the state into subsets and having an individual lock for each subset, so that different subsets can be locked independently of each other.\nUnfortunately, when we do this, we run the danger of entering deadlocks, and as it stands, the plural coherence implementation, which is suitable for these scenarios, does not address the issue of deadlocks.\nSome research is necessary to determine whether the plural coherence implementation could do any of the following:\nDetect a deadlock once it happens and provide diagnostic information. Detect a deadlock once it happens and somehow take corrective measure. Detect the possibility of deadlocks and alert the programmer by means of hard error. Be structured in such a way as to make deadlocks impossible. Cover image by reginasphotos from pixabay.com\n","date":"2020-12-12T13:51:31.547Z","permalink":"https://blog.michael.gr/post/2020-12-12-coherence/","title":"Coherence: The Assertable Lock"},{"content":"\rHere is the manual of the ventilation unit in English.\nClick on the pictures below for the documents.\nFerroli HR OptiFor OT-V Ventilation Unit User's Manual in Dutch (PDF)\nFerroli HR OptiFor OT-V Ventilation Unit User's Manual in English (PDF)\nIn a previous post I published the Ferroli BlueSense Boiler User's Manual in English.\n","date":"2020-11-14T15:42:39.584Z","permalink":"https://blog.michael.gr/post/2020-11-14-ferroli-ventilator-manual/","title":"Ferroli HR OptiFor OT-V Ventilation Unit User's Manual in English"},{"content":"This is a little history of the early World Wide Web (WWW) for the benefit of the younger generation which may have not experienced the Internet in its infancy and therefore might not be aware of the horrors that it involved, and why certain things have come to be the way they are today.\nAs you are reading this, and thinking to yourself that it could not possibly have been as bad as I am describing it, remember, the general public was experiencing it using 2400 baud modems.\n(Useful pre-reading: About these papers)\nThe World Wide Web was created in 1989 at CERN to facilitate the sharing of information between universities and scientific institutes around the world. It was still not much more than an academic prototype when two years later, in 1991, it was made generally available outside of the educational and scientific institution. As soon as that happened, both the general public and the commercial sector started very eagerly embracing it, and this was the firing shot for the technology companies to start a big race for market share grab.\nThe World Wide Web was quite unlike anything that humanity had ever seen before: it was not an incremental improvement upon some pre-existing technology that the public was already familiar with, but a totally new thing; it was not another novelty that some might like and some might just not develop a taste for, it was going to be everywhere, used by everyone, and affect everything. It was clear from the beginning that it was going to be big.\nUnfortunately, the people who originally came up with the HTTP protocol, which is what makes the WWW possible, had a very limited idea of how it was going to be used, so their original prototype was woefully inadequate. Thus, in the years following the introduction of the WWW there was a massive effort to improve and extend the protocol. However, there were no standards in place, and no agreement on what should be done and how it should be done. The market demanded functionality much faster than the technologists could create it, and not a single company was in favor of things being done the way a competitor company was suggesting.\nDecisions were made, based not on scientific or technological merit, but on burning market demand and market dominance aspirations. This led to many haphazard solutions being put into place to quickly cover immediate needs without any long-term vision. There was a lot of \u0026quot;I don't care if it is good, I want it yesterday\u0026quot; going on. Features were being implemented in hacky ways because nobody could wait for the protocol to be amended to accommodate them.\nFor example:\nThe web was originally intended only for displaying text, so the prototype did not include any provision for displaying images inside web pages. Support for images was added as an afterthought, and in an entirely ad-hoc way, without first making the necessary amendments to the protocol to accommodate them, so the result greatly suffered in terms of performance, because a new connection had to be established between the browser and the server for every single image on a page. I kid you not, there was a period of time in the early nineties when you would visit a page, and instead of images in it, you would see placeholders. Then, as you patiently waited, one by one the placeholders would be replaced by actual images. The protocol was later amended to address this problem. The web was originally intended to be used only by anonymous users, so the prototype had no provision for visitors being remembered by the server when they come again another day, or even as they navigate from page to page during a single visit. There were some simple forms that could be filled in, but since the server had no idea who was submitting these forms, they could only be used for perfectly anonymous surveys. (And not even perfectly: the server could always take note of the user's IP address.) Support for identifying visitors was hacked into the protocol as a half-baked afterthought as late as in 1994, and only with the narrow-minded goal of enabling a rudimentary shopping cart, after much pressure from the commercial sector, which of course needed to use the web to sell stuff. That half-baked afterthought was the now infamous cookies. The web was originally intended only for navigating from static page to static page, so the prototype had no provision for dynamically changing page content. This was remedied as late as 1995 when Netscape introduced JavaScript on their browser. Despite its name, JavaScript had absolutely nothing to do with the Java Programming Language; the name was a marketing ploy. JavaScript was terribly bad as a language, and even its creator, Brendan Eich, later admitted so. Since Netscape was the predominant browser at that time, the use of JavaScript caught on, and then other browsers started supporting it so as to not appear incompatible with web sites that were already working with Netscape. So, JavaScript became the de facto standard. For a short while during the late nineties Microsoft tried to push their own scripting languages like VBScript and JScript running on their own browser, but luckily nobody cared. Once we had scripts running on the browser, these scripts needed to be able to exchange data with the server, but there was no provision for such a thing in the HTTP protocol. To make matters worse, in the mass security hysteria that followed the realization that in a connected world, everyone was hackable by anyone, all TCP/IP ports across the world had been hastily blocked, except for SMTP and POP3 for e-mail, and HTTP for the web. So, HTTP was practically the only protocol available, despite being unsuitable. For this reason, the monstrosity known as REST was concocted to allow page scripts to communicate over HTTP, and the specification of REST was made to look as if it underlies HTTP, even though it was entirely an afterthought built on top of HTTP. Since then, REST has had a big impact on the way of thinking of web developers, by introducing the notion of a network that spans the globe and consists of resources identifiable by universal locators. Nobody seems to find it the slightest bit suspicious that this world-view was not intentionally designed this way, but came about purely by historical accident. Web sites needed to be able to show new information on a web page not only as a result of user actions, but also as a result of events happening on the server. For example, a chat system would need to show new chat messages to the user as they arrive, without requiring the user to refresh the page. Unfortunately, the HTTP protocol had not been designed for this purpose at all. It was left up to the developers to overcome this limitation by bending and twisting the protocol in ways quite different from how it was designed to be used, namely with this formidable hack known as ajax. It was only in 2009 that the WebSocket standard was introduced, and it was again of course an afterthought, as evidenced by the HTTP \u0026quot;Upgrade Header\u0026quot; hack. It is just that the hack is now built into the protocol, so it is official. Essentially, the protocol was being amended to include ad hoc features that had gained traction because the company introducing them had a big market share. The IETF even stated it as their official philosophy \u0026quot;to keep basing standards upon successful prototypes\u0026quot;, which is another way of saying \u0026quot;we will wait for someone to hack something together, and if it catches on, we will call it part of the standard.\u0026quot;\nThe original technologies that comprised the WWW were so simple that one could barely say that they constituted inventions, but many of the hacks that had to be introduced later in order to accomplish something useful with it, such as cookies and ajax, had to be so ingenious that they could arguably be classified as inventions.\nThere were also many opportunities for improvement that were lost due to lack of consensus, because consensus is very hard to reach among companies at cut-throat competition against each other. So, many things were done incredibly backwards and certainly not in the best interest of the public.\nWe should not be too hard on the original creators of the WWW; after all, experimental prototypes are supposed to be just that: good enough to demonstrate an idea, but woefully inadequate in all other respects. The mistake was the release of the WWW to the public when it was clearly too early, and the initial lack of an arbitrating body, leaving all innovation up to relentlessly competing market forces. Business people are to blame for everything, as always.\nUnfortunately, the rate of adoption of any newly introduced technology appears to depend not so much on its technical merit, but on how successful it is in covering narrow-minded immediate needs at hand. Sometimes, the technology that wins is the one that gets introduced first, despite being vastly inferior to a technology introduced shortly afterwards. Once a certain technology becomes widely adopted, it entrenches itself into the technological landscape, and it stays with us for a very long time, no matter how ill-conceived it was. Any attempt to improve an existing technology must always be backwards compatible, so as to ensure continuity in the transition from the old to the new, otherwise it has no chances of becoming a success, which means that you can never actually change the old technology, you can only add to it. The old problems will always be there, and the new solutions will always suffer to a smaller or larger extent due to those old problems. Thus, the echoes of the unhealthy Wild, Wild Web era of the Internet still linger in the technological landscape even today, some 30 years later.\nReferences:\nw3c.org - Raggett on HTML 4 - chapter 2 - A history of HTML by Dave Raggett\n","date":"2020-10-19T15:05:21.808Z","permalink":"https://blog.michael.gr/post/2020-10-19-the-wild-wild-web/","title":"The Wild, Wild Web"},{"content":"\rA Software Design Pattern which brings the principles of Inheritance, Encapsulation and Polymorphism one level up from the Class level to the Subsystem level, and offers a way of realizing relationships between classes so as to achieve dependency inversion by means of propagation instead of injection.\nPart 1: Dependency Inversion The software that we write often invokes other software to get parts of the job done. These are known as Services or Dependencies. If Class A is making use of some Class B, then Class A depends on Class B, so Class B is a dependency of Class A.\nThe principle of Dependency Inversion says that a class should not contain any direct calls to specific instances of any of its dependencies. Instead, it should receive these instances as parameters during initialization.\nThat's all very nice, but passing dependencies around can become quite a complicated business, and in large systems it can become a nightmare.\n(Useful pre-reading: About these papers)\nVarious mechanisms have been devised for solving this problem. Two that I know of are Service Locators, and Dependency Injection Frameworks. Unfortunately, each of them has some serious disadvantages.\nService Locators\nA service locator is a mandatory global dependency. That's a bad thing to have. At some point you will want to reuse a module in a different system, and that service locator will not be available there, and you will have to start rewriting stuff. Trust me, you will sooner or later regret having it. A service locator may defer compile-time errors to run-time errors. These errors occur when a system is being wired together, but tests are usually wired up differently, so these errors cannot be detected with unit testing or integration testing, you have to do end-to-end testing in order to discover them. Dependency Injection Frameworks\nThey work by magic, and I don't like magic. They tend to embrace silent failure, while I mandate hard failure. They don't have an API that you can call, so you cannot use code completion, you have to know stuff by heart. They tend to make application startup time slow as molasses, while I like application startup to be snappy. They are also a mandatory global dependency. The individual class is a much too fine-grained unit to be applying dependency injection onto. In my 35 years of programming I have encountered the problem of dependency injection a lot, and in the last decade or so I have started solving it with a paradigm that I call Domain Oriented Programming.\nNote that Domain Oriented Programming does not have any direct relation to Domain Driven Design, although it may be a suitable pattern to use when implementing systems designed using the Domain Driven Design paradigm.\nIntroducing Domain Oriented Programming (DOP)\nThe Domain Oriented Programming Design Pattern can be roughly described as follows:\nClasses do not exist in a vacuum; instead, every class has a special relationship with another class by which it is instantiated and from which it obtains its dependencies. The class doing the instantiation and providing dependencies is called Domain, the instantiated class is called Subject. Sometimes a class can be Subject to multiple Domains, more on that later. Every Subject has specific knowledge of its Domain. In some cases Domains also have specific knowledge of their Subjects, and in some cases they do not, more on that later. A Domain contains references to all services that are used by itself and by all of its Subjects; so, when a Subject needs to use some service, it obtains the service from its Domain. Therefore, dependencies do not need to be injected into Subjects. The Domain-Subject relation is hierarchical, so a Subject of one Domain may in turn be Domain to other Subjects. This way, dependencies are propagated from the root of a system all the way down to the leaf nodes without the need to use any special framework to achieve this. The domain-subject relation can exist in two forms: Closed (a.k.a. Realm) and Open (a.k.a. Free).\nClosed (a.k.a. Realm) Domains The Domain is the one and only Domain for its Subjects. It is passed to each Subject as its first constructor parameter. The Domain has complete control over the lifetime of its Subjects. This means that the Domain is the exclusive factory of its Subjects, and can also decide when and if a Subject is destroyed. The Domain and its Subjects are Closely Coupled. This means that not only the Subjects have specific knowledge of their Domain, but also the Domain has specific knowledge of its Subjects. (Close coupling is perfectly okay as long as the Domain limits itself to acting as a factory and the Realm is kept small.) Subjects are usually exposed to the outside world as interfaces rather than as objects. The Realm forms a coherent, closed group which cannot be extended without modifying the Domain class. Open (a.k.a. Free) Domains The Domain does not have specific knowledge of any Subjects, it only exists for the purpose of making dependencies available to other Domains. Open Domains are usually provided as interfaces rather than as actual objects. A Subject of Open Domains can be freely instantiated as long as all the domains necessary for its instantiation are available. It can also be freely disposed. There are a few interesting things to notice here:\nThe Domain is to a Subject what the Object is to a Method. Hopefully a DOP oriented language will be introduced one day which realizes the DOP construct in its grammar, making the Domain reference implicit, just as in Object Oriented Programming the Object reference is always the implicit first parameter to every Method.\n(Incidentally, Java and other languages are already doing something along these lines with non-static inner classes, but we do not want to have to nest the source code of each Subject within the source code of its Domain, especially since a Domain may in turn be Subject of another Domain.)\nDomain Oriented Programming does not require any platform or library: it is just a way of structuring code. So, with DOP, no omnipresent framework is needed for injecting dependencies, and no magic is involved in their propagation; nobody needs to query any service locators for services, (the availability of services is practically guaranteed by the compiler,) and no huge lists of dependencies are passed to constructors, either. Still, at various places where domains are constructed and wired together, all necessary services are supplied, so any one of them can be replaced with a Test Double.\nFurthermore, Domain Oriented Programming is interoperable with non-DOP systems: A group of classes making use of DOP among themselves can be introduced into a system which is already using some other mechanism of Dependency Inversion.\nAt first glance, Domain Oriented Programming can be thought of as employing something like Half-Way Dependency Injection, or Subsystem-level Dependency Injection as opposed to Class-level Dependency Injection. Dependencies are injected into the Domain, and from that moment on Subjects of the Domain can go ahead and fetch their dependencies from the Domain as needed, instead of having their dependencies injected into them.\nThings become even more interesting when we consider Domains that are in turn Subjects of other Domains, forming a hierarchy of Domains, where at each level we have SuperDomains and SubDomains. In this scenario, we do not exactly have Dependency Injection going on anymore, because at each level dependencies are obtained from the level above; however, we still have Dependency Inversion, because dependencies are still not hard-coded in any way, and each Domain has control over each service that it makes available to its subjects, and may, if needed, decide which particular implementation will offer it.\nThe lesson to learn from this is that Dependency Injection was never a goal in and of itself; the goal has been Dependency Inversion, (avoiding hard-coded dependencies, Dependency Independence if you will permit the pun,) and Dependency Injection has been a mechanism for achieving it, but the same goal can be achieved by other means, such as Dependency Propagation, which is what Domain Oriented Programming offers.\nPart 2: Object Orientation at the Subsystem Level Domain Oriented Programming is not only about Dependency Propagation. It reflects the realization that Software being created today is immensely more complex than what it used to be back when Object Oriented Programming was invented and the first Object Oriented languages were laid down, about half a century ago.\nIt used to be that all we needed was a means of coupling groups of functions with the data that they operate on, and that Inheritance, Encapsulation and Polymorphism were only necessary at the class-and-method level; however, as we build more elaborate software, we find ourselves more and more thinking not so much in terms of classes and methods, but in terms of subsystems and classes, or systems and subsystems. Therefore, there appears to be a need for terminology which brings Inheritance, Encapsulation and Polymorphism one level up, to the subsystem level, and by recursive application, to the entire system.\nDomain Oriented Programming offers the Domain as the unit upon which to apply the principles of Object Oriented Programming.\nIn DOP the Domain is the principal polymorphic unit, providing an implementation for a complex interface, and instantiating subjects to polymorphically implement smaller scope interfaces. In DOP the Domain encapsulates its subjects, hiding their nature and lifetime from the outside world. In DOP inheritance is only utilized among Subjects, while the Domain hides from the outside world the fact that it is being utilized. My first public mention of this concept was in this answer of mine on Software Engineering Stack Exchange.\n","date":"2020-06-26T18:46:47.557Z","permalink":"https://blog.michael.gr/post/2020-06-26-domain-oriented-programming/","title":"Domain Oriented Programming"},{"content":"Let me start with a couple of pedantic definitions; stay with me, the beef follows right afterwards.\nConventional wisdom says that validation is different from error checking.\nValidation is performed at the boundaries of a system, to check the validity of incoming data, which is at all times presumed to be potentially invalid. When invalid data is detected, validation is supposed to reject it. Validation is supposed to be always on, you cannot switch it off on release builds and only have it enabled on debug builds. Error checking, on the other hand, is performed inside a system, checking against conditions that should never occur, to keep making sure that everything is working as intended. In the event that an error is encountered, the intent is to signal a catastrophic failure. Essentially, the term Error Checking is shorthand for Internal Error Checking. It can be implemented using assertions, thus being active on the debug build only, and having a net cost of zero on the release build. So far so good, right?\n(Useful pre-reading: About these papers)\nWell, the problem with this conventional view of validation vs error checking is that it heavily relies on the notion of \u0026quot;system boundaries\u0026quot;, which is not a well-defined notion. Unless you are an application programmer, whatever you are building will in all likelihood be a subsystem of a larger system, and that system will in turn be a subsystem of an even larger system, and so on. Therefore, what you think of as the boundaries of your system will never be the actual boundaries of the actual system. You cannot have any claim to knowledge of the boundaries of any system that might incorporate your little creation as a component of it.\nAs a reaction to this uncertainty, most programmers maintain a self-centered view of the component they are developing as the system, and a short-sighted view of the boundaries of their component as the system boundaries. So, on those boundaries they keep doing validation.\nHere is what's wrong with that:\nYour subsystem will be embedded in a larger system, which will be doing its best to always supply your subsystem with valid data. This larger system will be tested by its creators, and will be known to work correctly. This means that it will always supply your system with valid data, so your validation will be useless, and it will just be wasting time on the release build. The validation results returned by your boundary methods will have to somehow be dealt with by the containing system, since ignoring results is considered a terrible practice, even when nothing is expected to go wrong. So, you are forcing the caller to litter their code with checks for your validation results. However, since the caller does not expect anything to go wrong, they will not be able to do anything other than throw an exception in the event that you return a validation failure result. So, not only you are forcing the caller to litter their code with checks for validation results, but these checks in turn will never be triggered. Think about it: this is code that will never be covered by any coverage run. Even in the extremely unlikely event that the containing system will in fact supply your component with invalid input, triggering the scenario where your component returns a validation failure result, and the containing system throws, this is virtually indistinguishable from the scenario where you simply just throw in the first place, as part of your error checking, not validation. So, there is no need for you to return some validation result, no need for the caller to check it, no need for the caller to throw. To put it in simple words, a subsystem's validation failure is a supersystem's internal error. So, what the above means is that the entire industry is doing it wrong. Nothing but the outermost layer of a system should be performing validation, and that's usually some application-specific integration layer. Subsystems should, at most, and as a convenience, offer free-standing validation facilities which may be utilized by enclosing layers as part of their own validation.\nSo, for example, the enclosing system might, in the context of its own validation strategy, ensure that every field in a form has been filled-in, and then it might invoke the date-time subsystem's validation mechanism to verify that the value entered in some date-time field is valid, before feeding that value to that same subsystem, or storing it for feeding it to that subsystem later.\nThat's because only the component which is dealing with the form knows that the information that it receives is coming from the user and therefore needs validation. Once this information has passed validation and accepted into the system, it should never be re-validated. Any inconsistency after that point is an internal error of the system, and therefore a hard error.\n","date":"2020-05-30T17:33:41.438Z","permalink":"https://blog.michael.gr/post/2020-05-30-on-validation-vs-error-checking/","title":"On Validation vs- Error Checking"},{"content":"As the screenshot proves.\n","date":"2020-05-22T10:53:08.048Z","permalink":"https://blog.michael.gr/post/2020-05-22-stackoverflow-is-not-suitable-for/","title":"Stackoverflow is not suitable for poignant questions"},{"content":"\rWhat to reply to a non-programmer who thinks that testing is unnecessary or secondary At some point during his or her career, a programmer might come across the following argument, presented by some colleague, partner, or decision maker:\nSince we can always test our software by hand, we do not need to implement Automated Software Testing.\nApparently, I reached that point in my career, so now I need to debate this argument. I decided to be a good internet citizen and publish my thoughts. So, in this post I am going to be deconstructing that argument, and demolishing it from every angle that it can be examined. I will be doing so using language that is easy to process by people from outside of our discipline.\n(Useful pre-reading: About these papers)\nIn the particular company where that argument was brought forth, there exist mitigating factors which are specific to the product, the customers, and the type of relationship we have with them, all of which make the argument not as unreasonable as it may sound when taken out of context. Even in light of these factors, the argument still deserves to be blown out of the water, but I will not be bothering the reader with the specific situation of this company, so as to ensure that the discussion is applicable to software development in general.\nIn its more complete form, the argument may go like this:\nAutomated Software Testing represents a big investment for the company, where all the programmers in the house are spending copious amounts of time doing nothing but writing software tests, but these tests do not yield any visible benefit to the customers. Instead, the programmers should ensure that the software works by spending only a fraction of that time doing manual testing, and then we can take all the time that we save this way and invest it in developing new functionality and fixing existing issues.\nTo put it more concisely, someone might say something along these lines:\nI do not see the business value in Automated Software Testing.\nThis statement is a bunch of myths rolled up into an admirably terse statement. It is so disarmingly simple, that for a moment you might be at loss of how to respond. Where to begin, really. We need to look at the myths one by one. Here it goes:\nMyth #1: Software testing represents a big investment. No it doesn't. Or maybe it does, but its ROI is so high that you absolutely don't want to miss it.\nIf you do not have software testing in place, then it is an established fact in our industry that you will end up spending an inordinate amount of time researching unexpected application behavior, troubleshooting code to explain the observed behavior, discovering bugs, fixing them, and often repeating this process a few times on each incident because the fix for one bug often creates another bug, or causes pre-existing bugs to manifest, often with the embarrassment of an intervening round-trip to the customer, because the \u0026quot;fixed\u0026quot; software was released before the newly introduced bugs were discovered.\nReally, it works the same way as education. To quote a famous bumper sticker:\nYou think education is expensive? Try ignorance!\nFurthermore, your choice of Manual Software Testing vs. Automated Software Testing has a significant impact on the development effort required after the testing, to fix the issues that the testing discovers. It is a well established fact in the industry that the sooner a bug is discovered, the less it costs to fix it.\nThe earliest time possible for fixing a mistake is when making it. That's why we use strongly typed programming languages, together with Integrated Development Environments that continuously compile our code as we are typing it: this way, any syntax error or type violation is immediately flagged by the IDE with a red underline, so we can see it and fix it before proceeding to type the next line of code. The cost of fixing that bug is near zero. (And one of the main reasons why virtually all scripting languages are absolutely horrible is that in those languages, even a typo can go undetected and become a bug.) If you can't catch a bug at the moment you are introducing it, the next best time to catch it is when running automated tests, which is what you are supposed to do before committing your changes to the source code repository. If that doesn't happen, then the bug will be committed, and this already represents a considerable cost that you will have to pay later for fixing it. The next best time to catch the bug is by running automated tests as part of the Continuous Build System. This will at least tell you that the most recent commit contained a bug. If there is no Continuous Build with Automated Software Testing in place, then you suffer another steep increase in the price that you will have to pay for eventually fixing the bug. By the time a human being gets around to manually testing the software and discovering the bug, many more commits may have been made to the source code repository. This means that by the time the bug is discovered, we will not necessarily know which commits contributed to it, nor which programmers made the relevant commits, and even if we do, they will at that moment be working on something else, which they will have to temporarily drop, and make an often painful mental context switch back to the task that they were working on earlier. Naturally, the more days pass between committing a bug and starting to fix it, the worse it gets. At the extreme, consider trying to fix a bug months after it was introduced, when nobody knows anything about the changes that caused it, and the programmer who made those changes is not even with the company anymore. Someone has to become intimately familiar with that module in order to troubleshoot the problem, consider dozens of different commits that may have contributed to the bug, find it, and fix it. The cost of fixing that bug may amount to more than a programmer's monthly salary. This is why the entire software industry today literally swears in the name of testing: it helps to catch bugs as early as possible, and to keep the development workflow uninterrupted, so it ends up saving huge amounts of money.\nMyth #2: Software testing represents an investment. No, it does not even. Software testing is regarded by our industry as an integral part of software development, so it is meaningless to examine it as an investment separate from the already-recognized-as-necessary investment of developing the software in the first place.\nBeware of the invalid line of reasoning which says that in order to implement a certain piece of functionality all we need is 10 lines of production code which cost 100 bucks, whereas an additional 10 lines, that would only be testing the first 10 lines, and would cost an extra 100 bucks, are optional.\nInstead, the valid reasoning is that in order to implement said functionality we will need 20 lines of code, which will cost 200 bucks. It just so happens that 10 of these lines will reside in a subfolder of the source code tree called \u0026quot;production\u0026quot;, while the other 10 lines will reside in a subfolder of the same tree called \u0026quot;testing\u0026quot;; however, the precise location of each group of lines is a trivial technicality, bearing no relation whatsoever to any notion of \u0026quot;usefulness\u0026quot; of one group of lines versus the other. The fact is that all 20 of those lines of code are essential in order to accomplish the desired result.\nThat's because production code without corresponding testing code cannot be said with any certainty to be implementing any functionality at all. The only thing that can be said about testless code is that it has so far been successful at creating the impression to human observers that its behavior sufficiently resembles some desired functionality. Furthermore, it can only be said to be successful to the extent that it has been observed thus far, meaning that a new observation tomorrow might very well find that it is doing something different.\nThat's a far cry from saying that \u0026quot;this software does in fact implement that functionality\u0026quot;.\nMyth #3: Software testing is just sloppiness management. This is usually not voiced, but implied. So, why can't programmers write correct software the first time around? And why on earth can't software just stay correct once written?\nThere is a number of reasons for this, the most important ones have to do with the level of maturity of the software engineering discipline, and the complexity of the software that we are being asked to develop.\nMaturity Software development is not a hard science like physics and math. There exist some purely scientific concepts that you learn in the university, but they are rarely applicable to the every day reality of our work. When it comes to developing software, there is not as much help available to us as there is to other disciplines by means of universal laws, fundamental axioms, established common practices and rules, ubiquitous notations, books of formulas and procedures, ready made commercially available standardized components to build with, etc. It is difficult to even find parallels to draw for basic concepts of science and technology such as experimentation, measurement, and reproducibility. That's why software engineering is sometimes characterized as being more of an art than a science, and the fact that anyone can potentially become a programmer without necessarily having studied software engineering does not help to dispel this characterization.\nAutomated Software Testing is one of those developments in software engineering that make it more like a science than like an art. With testing we have finally managed to introduce the concepts of experimentation, measurement, and reproducibility in software engineering. Whether testability alone is enough to turn our discipline into a science is debatable, but without testing we can be certain that we are doing nothing but art.\nComplexity The software systems that we develop today are immensely complex. A simple application which presents a user with just 4 successive yes/no choices has 16 different execution paths that must be tested. Increase the number of choices to 7, and the number of paths skyrockets to 128. Take a slightly longer but entirely realistic use case sequence of a real world application consisting of 20 steps, and the total number of paths exceeds one million. That's an awful lot of complexity, and so far we have only been considering yes/no choices. Now imagine each step consisting of not just a yes/no choice, but an entire screen full of clickable buttons and editable fields which are interacting with each other. This is not an extreme scenario, it is a rather commonplace situation, and its complexity is of truly astronomical proportions.\nInterestingly enough, hardware engineers like to off-load complexity management to the software. Long gone are the times when machines consisted entirely of hardware, with levers and gears and belts and cams all carefully aligned to work in unison, so that turning a crank at one end would cause printed and folded newspapers to come out the other end. Nowadays, the components of the hardware tend to not interact with each other, because that would be too complex and too difficult to change; instead, every single sensor and every single actuator is connected to a central panel, from which software takes charge and orchestrates the whole thing.\nHowever, software is not a magical place where complexity just vanishes; you cannot expect to provide software with complex inputs, expect complex outputs, and at the same time expect the insides of it to be nothing but purity and simplicity: a system cannot have less complexity than the complexity inherent in the function that it performs.\nThe value of moving the complexity from the hardware to the software is that the system is then easier to change, but when we say \u0026quot;easier\u0026quot; we do not mean \u0026quot;simpler\u0026quot;; all of the complexity is still there and must be dealt with. What we mean when we say \u0026quot;easier to change\u0026quot; is that in order to make a change we do not have to begin by sending new blueprints to the steel foundry. That's what that you gain by moving complexity from the hardware to the software: being able to change the system without messy, time-consuming, and costly interactions with the physical world.\nSo, even though we have eliminated those precisely crafted and carefully arranged levers and gears and belts and cams, their counterparts now exist in the software, you just do not see them, you have no way of seeing them unless you are a programmer, and just as the slightest modification to a physical machine of such complexity would be a strenuous ordeal, so is the slightest modification to a software system of similar complexity a strenuous ordeal.\nSoftware can only handle complexity if done right. You cannot develop complex software without sophisticated automated software testing in place, and even if you develop it, you cannot make any assumptions whatsoever about its correctness. Furthermore, even if it appears to be working correctly, you cannot make the slightest change to it unless automated software testing is in place to determine that it is still working correctly after the change. That is because you simply cannot test thousands or millions of possible execution paths in any way other than in an automated way.\nMyth #4: Testing has no visible benefit to the customers Yes it does. It is called reliable, consistent, correctly working software. It is also called software which is continuously improving instead of remaining stagnant due to fear of it breaking if sneezed at. It is also called receiving newly introduced features without losing old features that used to work but are now broken. And it is even called receiving an update as soon as it has been introduced instead of having to wait until some poor souls have clicked through the entire application over the course of several days to make sure everything still works as it used to.\nMyth #5: Manual testing can ensure that the software works. No it cannot. That's because the complexity of the software is usually far greater than what you could ever possibly hope to test by hand. An interactive application is not like a piece of fabric, which you can visually inspect and have a fair amount of certainty that it has no defects. You are going to need to interact with the software, in a mind-boggling number of different ways, to test for a mind-boggling number of possible failure modes.\nWhen we do manual testing, in order to save time (and our sanity) we focus only on the subset of the functionality of the software which may have been affected by recent changes that have been made to the source code. However, the choice of which subsets to test is necessarily based on our estimations and assumptions about what parts of the program may have been affected by our modifications, and also on guesses about the ways in which these parts could behave if adversely affected. Alas, these estimations, assumptions, and guesses are notoriously unreliable: it is usually the parts of the software that nobody expected to break that in fact break, and even the suspected parts sometimes break in ways quite different from what anyone had expected and planned to test for.\nAnd this is by definition so, because all the failure modes that we can easily foresee, based on the modifications that we make, we usually examine ourselves before even calling the modifications complete and committing our code.\nFurthermore, it is widely understood in our industry that persons involved in the development of software are generally unsuitable for testing it. No developer ever uses the software with as much recklessness and capriciousness as a user will. It is as if the programmer's hand has a mind of its own, and avoids sending the mouse pointer in bad areas of the screen, whereas that is precisely where the user's hand is guaranteed to send it. It is as if the programmer's finger will never press that mouse button down as heavily as the user's finger will. Even dedicated testers start behaving like the programmers after a while on the job, because it is only human to employ acquired knowledge about the environment in navigating about the environment, and to re-use established known good paths. It is in our nature. You can ask people to do something which is against their nature, and they may earnestly agree, and they may even try their best, but the results are still guaranteed to suffer.\nThen there is repetitive motion fatigue, both of the physical and the mental kind, that severely limit the scope that any kind of manual testing will ever have.\nFinally, there is the issue of efficiency. When we do manual software testing, we are necessarily doing it in human time, which is excruciatingly slow compared to the speed at which a computer would carry out the same task. A human being testing permutations at the rate of one click per second could theoretically test one million permutations in no less than 2 working months, the computer may do it in a matter of minutes. And the computer will do this perfectly, while the most capable human being will do this quite sloppily in comparison. That's how inefficient manual software testing is.\nMyth #6: Manual testing takes less time than writing tests. No it doesn't. If you want to say that you are actually doing some manual testing worth speaking of, and not a joke of it, then you will have to spend copious amounts of time doing nothing but that, and you will have to keep repeating it all over again every single time the software is modified.\nIn contrast, with software testing you are spending some time up-front building some test suites, which you will then be able to re-execute every time you need them, with comparatively small additional effort. So, manual testing for a certain piece of software is an effort that you have to keep repeating, while writing automated test suites for that same piece of software is something that you do once and from that moment on it keeps paying dividends.\nThis is why it is a fallacy to say that we will just test the software manually and with the time that we will save we will implement more functionality: as soon as you add a tiny bit of new functionality, you have to repeat the testing all over again. Testing the software manually is a never ending story.\nThe situation is a lot like renting vs. buying: with renting, at the end of each month you are at exactly the same situation as you were in the beginning of the month: the home still belongs in its entirety not to you, but to the landlord, and you must now pay a new rent in full, in order to stay for one more month. With buying, you pay a lot of money up front, and some maintenance costs and taxes will always be applicable, but the money that you pay goes into something tangible, it is turned into value in your hands in the form of a home that you now own.\nFurthermore, the relative efficiency of manual testing is usually severely underestimated. In order to do proper manual testing, you have to come up with a meticulous test plan, explaining what the tester is supposed to do, and what the result of each action should be, so that the tester can tell whether the software is behaving according to the requirements or not. However, no test plan will ever be as unambiguous as a piece of code that is actually performing the same test, and the more meticulous you try to be with the test plan, the less you gain, because there comes a point where the effort of writing the test plan starts being comparable to the effort of writing the corresponding automated test instead. So, you might as well write the test plan down in code to begin with.\nOf course one round of writing automated software testing suites will always represent more effort than a few rounds of manually performing the same tests, so the desirability of one approach vs. the other may depend on where you imagine the break-even point to be. If you reckon that the break-even point is fairly soon, then you already see the benefit of implementing automated software testing as soon as possible. If you imagine it will be after the IPO, then you might think it is better to defer it, but actually, even in this case you might not want to go this way, more about that later.\nWell, let me tell you: in the software industry the established understanding is that the break-even point is extremely soon. Like write-the-tests-before-the-app soon. (A practice known as Test-Driven Development.)\nMyth #7: You can keep developing new functionality and fixing existing issues without software testing in place. In theory you could, but in practice you can't. That's because every time you touch the slightest part of the software, everything about the software is now potentially broken. Without automated software testing in place, you just don't know. This is especially true of software which has been written messily, which is in turn especially common in software which has been written without any Automated Software Testing in place from the beginning. Paradoxically enough, automated software testing forces software designs to have some structure, this structure reduces failures, so then the software has lesser testing needs.\nTo help lessen change-induced software fragility, we even have a special procedure governing how we fix bugs: when a bug is discovered, we do not always just go ahead and fix it. Instead, what we often do is that we first write a test which checks for the bug according to the requirements, without making any assumptions as to what might be causing it. Of course, since the bug is in the software, the test will initially be observed to fail. Then, we fix the bug according to your theory as to what is causing it, and we should see that test succeeding. If it doesn?t, then we fixed the wrong bug, or more likely, we just broke something which used to be fine. Furthermore, all other tests better also keep succeeding, otherwise in fixing this bug we broke something else. As a bonus, the new test now becomes a permanent part of the suite of tests, so if this particular behavior is broken again in the future, this test will catch it.\nIf you go around \u0026quot;fixing bugs\u0026quot; without testing mechanisms such as this in place, you are not really fixing bugs, you are just shuffling bugs around. The same applies to features: if you go around \u0026quot;adding features\u0026quot; without the necessary testing mechanisms in place, then by definition you are not adding features, you are adding bugs.\nMyth #8: Software testing has no business value Yes it does. The arguments that I have already listed should be making it clear that it does, but let me provide one more argument, which shows how Automated Software Testing directly equates to business value.\nA potentially important factor for virtually any kind of business is investment. When an investor is interested in a software business, and if they have the slightest clue as to what it is that they are doing, they are likely to want to evaluate the source code before committing to the investment. Evaluation is done by sending a copy of the software project to an independent professional software evaluator. The evaluator examines the software and responds with investment advice.\nThe evaluator may begin by using the software as a regular user to ensure that it appears to do what it is purported to do, then they may examine the design to make sure it makes sense, then they may examine the source code to make sure things look normal, etc. After spending not too much time on these tasks, the evaluator is likely to proceed to the tests. Software testing is so prevalent in the software industry, that it is unanimously considered to be the single most important factor determining the quality of the software.\nIf there are no tests, this is very bad news for the investment advice.\nIf the tests do not pass, this is also very bad news.\nIf the tests succeed, then the next question is how thorough they are.\nFor that, the evaluator is likely to use a tool called \u0026quot;Code Coverage Analyzer\u0026quot;. This tool keeps track of the lines of code that are being executed as the program is running, or, more likely, as the program is being exercised by the tests. By running the tests while the code coverage analysis tool is active, the evaluator will thus obtain the code coverage metric of the software. This is just a single number, from 0 to 100, and it is the percentage of the total number of source code lines that have been exercised by the tests. The more thorough the tests are, the higher this number will be.\nThis is a very useful metric, because in a single number it captures an objective, highly important quality metric for the entirety of the software system. It also tends to highly correlate to the actual investment advice that the evaluator will end up giving. The exact numbers may vary depending on the product, the evaluator, the investor, the investment, and other circumstances, but a rough breakdown is as follows:\nbelow 50% means \u0026quot;run in the opposite direction, this is as good as Ebola.\u0026quot; 50-60% means \u0026quot;poor\u0026quot;, 60-70% means \u0026quot;decent\u0026quot;, 70-80% means \u0026quot;good\u0026quot;, 80-90% means \u0026quot;excellent\u0026quot;, 90-100% means \u0026quot;exceptional\u0026quot;. Of course, the graph of programming effort required vs. code coverage achieved is highly non-linear. It is relatively easy to pass the 45% mark; it becomes more and more difficult as you go past the 65% mark; it becomes exceedingly difficult once you cross the 85% mark.\nIn my experience and understanding, conscientious software houses in the general commercial software business are striving for the 75% mark. In places where they only achieve about 65% code coverage they consider it acceptable but at the same time they either know that they could be doing better, or they have low self-respect. High criticality software (that human life depends on, or a nation's reputation,) may have 100% coverage, but a tremendous effort is required to achieve this. In any case, what matters is not so much what the developers think, but what the evaluator thinks; and evaluators tend to use the established practices of the industry as the standard by which they judge. The established practices call for extensive software testing, so if you do not do that, then your evaluation is not going to look good.\nSo, is there business value in software testing? investment prospects alone say yes, regardless of the technical merits of it. Furthermore, software evaluation may likely be part of the necessary preparations for an IPO to take place, so even if you imagined the break-even point of automated testing vs. manual testing to be after the IPO, there is still ample reason to have them all in perfect working order well before the IPO.\nThe above is applicable for businesses that are exclusively into software development. I do not know to what degree parallelisms can be drawn with companies for which software is somewhat secondary, but I suspect it is to no small extent.\nOld comments\nmetamaker 2020-11-14 23:03:33 UTC\n| ... in a single number it captures an objective, highly important quality metric for the entirety of the software system.\nI wished to find in an article more about spec tests (BDD, Gherkin). Code lines coverage is not always applicable, and even in the case of unit tests where it is applicable, branch+predicate coverage is as relevant as ever.\nSo, devs end up with a need to convert use cases to autotests. I had a great Product Owner (C++ dev in past), who was writing Gherkin scripts inside Jira tickets xD, and team needed to just connect actions to words - then voila! we have autotests for use cases that a user encounters.\nThe excuse - it is difficult to setup runner for specs. The solution - fire knaves, hire pros! :D\n| Software testing has no business value\nThis is THE PLAGUE of modern software engineering - business decides how programmers should do their work. Moreover there is the BELIEF that writing bug-free code is easy. In the end of the day, software rot trumps all business decisions and team ends up with polluted unsupportable code. This is the one single reason why now I don't even consider job offers to random teams that have already 2-3 years old software - just too high risk to end up with already non-fixable $hitcode (was there, seen undocumented SQL scripts with 5000 LOC and zero documentation - never again).\nI wish everyone to end up sooner or later in a team with good practices and low stress! Stay good!\nmetamaker 2020-11-14 23:03:27 UTC\nYo Mike! Big kudos for the great article!\n(I am sure you know all that I want to mention in the next paragraphs, I just need to vent my thoughts and feelings after reading; hope it is thought-provoking, because thinking === GREATER GOOD)\n| ... so it ends up saving huge amounts of money.\nThis is the good reason, but not the BEST (which I will mention below). Be like my old team when one day PM told that single calling code equals single country (+1 CC?), and someone wrote a component relying on this \u0026quot;well-known fact\u0026quot;. After half year we randomly found why some phone numbers were messed up. Ironically, the harm that was done to our company - zero bucks, we haven't lost anything due to this bug. We were b2b company that signed up other companies on board and thus really cared about having more sales and signed contracts rather than a good product.\nTests are useless waste of time for company that is sales driven. Am I right? Or not so?\nThe BEST thing for writing tests is that it documents expectations on code level (not biz, but for us, devs). If you ever need to fix something done by some random dude who now moved to Arctica, test is a good guidance (ofc, if that person wrote a good test and not some mocked up from top to bottom monster).\nA quick thought about code reviews. There is the BELIEF that reviews prevent bad code (poorly written tests including). In fact, I have never seen in my career teams where code reviews were helpful (but I was in a great team without no code reviews and permit to fix random places during the development - we trusted each other and cared about well being). If you have a good tech lead, but unsure about the rest of the team, for the God's sake, let tech lead be the only person who reviews code. By not doing so, if there is less than 51% of team are competent developers, you end up with political circus (been there, seen it, friends get LGTM for $hitcode, foes get comments like \u0026quot;change space, change quote, move comment to next line\u0026quot;; so... you end up making situational friends ;) - needless to say, what happens to code base).\n| ... every time you touch the slightest part of the software, everything about the software is now potentially broken.\nNot mentioning that since you are not a solo developer, you end up with merge conflicts due to other people work (even logical, e.g. Country class starts using 2-letter codes instead of 3-letter and uses same old good String for input of constructor - well, hope that your buddy added invariant to the class constructor). Tests synchronize decision making process, they autofix logical bugs between you and buddy.\n","date":"2019-12-01T20:48:53.074Z","permalink":"https://blog.michael.gr/post/2019-12-on-software-testing/","title":"The case for software testing"},{"content":"In this post I am documenting sightings of these red heart stickers.\nIf anyone knows what these are, who sticks them, what for, please do let me know.\nDiscovered: 2019-10-05 (x2)\n138 Oude Binnenweg, Rotterdam, South Holland https://goo.gl/maps/3bPGoPpKjDUYDDfW6\nPlaced, according to google street view: between Jul 2008 and May 2014. (But on Jun 2015, heart on door is white; heart on mailbox is not there.)\nDiscovered: 2019-10-07\nWateringsevest, Delft, South Holland https://goo.gl/maps/LdJpvUCuVE1yrceA7\nPlaced, according to google street view: between Jul 2014 and May 2015\nDiscovered: 2019-10-07\nWateringseveg, Delft, South Holland https://goo.gl/maps/GMLJL45YKbBr2PhW6\nPlaced, according to google street view: between Jul 2014 and May 2015\nDiscovered: 2019-10-07\nWateringseveg, Delft, South Holland https://goo.gl/maps/c6dahVyLmKaBNe2v5\nPlaced, according to google street view: between Jul 2014 and May 2015\nDiscovered: 2019-10-07\nWateringseweg (Lange Kleiweg), Delft, South Holland https://goo.gl/maps/Ggok9TP8Fd7yd9Ki8\nPlaced, according to google street view: between Jul 2014 and May 2015\nDiscovered: 2019-10-07\nLange Kleiweg, Delft, South Holland https://goo.gl/maps/CB2eC3d59qWuDh3S9\nPlaced, according to google street view: between Aug 2014 and May 2015\nDiscovered: 2019-10-08\nNieuwe Plantage \u0026amp; Wateringsevest, Delft, South Holland https://goo.gl/maps/KFoZXg8dMMScnSbX8\nPlaced, according to google street view: between Aug 2014 and May 2015\nDiscovered: 2019-10-13\nRijnstraat / Koningin Julianaplein 7, Den Haag https://goo.gl/maps/eXD2LTaNhuZqJf8q9\nPlaced, according to google street view: between Jun 2015 and Sep 2018\nDiscovered: 2019-10-14\nKoepoortbrug (Nieuwe Langendijk), Delft https://goo.gl/maps/BuQSkSCiBtb3aqgN7\nPlaced, according to google street view: between Aug 2014 and May 2015\nDiscovered: 2019-10-17\n309 Sir Winston Churchilllaan, Rijswijk https://goo.gl/maps/ss3J86Yy5WzLao7FA\nPlaced, according to google street view: between Aug 2014 and May 2015.\nDiscovered: 2019-10-18\nPrinces Beatrixlaan (S106) \u0026amp; Generaal Spoorlaan, Rijswijk https://goo.gl/maps/TvFSaUQQQWwKh7Rz6\nPlaced, according to google street view: between Aug 2014 and May 2015.\nPlaced, according to google street view: between May 2015 and Jun 2017.\nDiscovered: 2019-10-18 https://goo.gl/maps/YJcuCZgFBdL3gPGg9\nDiscovered: 2019-10-18\nhttps://goo.gl/maps/qFsU8Bw8R28ZNzmt6\nDiscovered: 2019-10-20 https://goo.gl/maps/8pC1rJZ8H3biG6iE7\nDiscovered: 2019-10-20 https://goo.gl/maps/dDZ3AMYdY1nPpHpC8\nDiscovered: 2019-10-20 https://goo.gl/maps/sbrR9fSFzVCSExo29\nDiscovered: 2019-10-20 https://goo.gl/maps/PY8SSQxtx6w1kDEV9\nDiscovered: 2019-10-20 https://goo.gl/maps/pvswadavjtHk14k57\nDiscovered: 2019-10-20 https://goo.gl/maps/Jgir74mJtsrPQSts8\nDiscovered: 2019-10-20 https://goo.gl/maps/z7LBTQoGqhguPC6b9\nDiscovered: 2019-10-20 https://goo.gl/maps/b7kEYdA9bYYetVEq7 (not there at the time of sighting)\nPlaced, according to google street view: after Aug 2018\nDiscovered: 2019-10-21\nPlaced, according to google street view: between Jun 2015 and Aug 2016\nRemoved, according to google street view: between Jul 2017 and Aug 2018 https://goo.gl/maps/MCuy8oq4h8Rx38xJ7\nTwo stickers on the same traffic sign post as the previous sticker. The previous sticker was on the west side of the post, these two used to be on the north and east sides.\nDiscovered: 2019-10-21\nPlaced, according to google street view: between Jun 2015 and Aug 2016\nRemoved, according to google street view: between Aug 2018 and Apr 2019.\nhttps://goo.gl/maps/k6NJ3SqgpjtAtGcX8\nThis is an interesting case: while trying to locate the previous heart sticker on google street view, I could not find it, but I noticed another heart sticker on a traffic light post across the street lane. However, if that heart was there when I visited, I would have seen it.\nSo, after a bit of research it seems that the heart had been placed on the traffic light post back in 2015, and then in 2016 the post was probably hit by a car and became slightly slanted, so today there is no heart sticker on it anymore, but it is standing straight again, so it has probably been replaced.\nDiscovered: 2019-10-20 https://goo.gl/maps/qNsHbsCwZgpQtVvg8\nPlaced, according to google street view: after Aug 2018\nDiscovered: 2019-10-20\nSo, I went to a cafe to grab some breakfast, and from the window I saw this:\nIt may be difficult to see it in the picture, but there is a bicycle parked on the other side of the street, that has a box in front of the steering bar, and on that box there is a big red heart sticker!\nSo, while eating my breakfast, I prepared this:\nAnd then I walked towards that bicycle\nAnd I dropped the note in the box.\nBut I did not receive any phone call.\nThen further down I saw another bicycle:\nAnd further down, at the park, another three stickers:\n(Cannot be seen on google street view)\n(https://goo.gl/maps/Jbfa5mfWLhJv3vSi9)\n","date":"2019-10-08T22:53:01.668Z","permalink":"https://blog.michael.gr/post/2019-10-red-heart-stickers/","title":"Red Heart Stickers"},{"content":"\rIf you try visiting The Pirate Bay from Holland nowadays, you are very likely to be disappointed. Your internet provider will most probably block your access with a page like this: https://blocked.t-mobilethuis.nl/\nThis is, of course, completely unacceptable.\nBut while we are sitting around wondering when the revolution will start so that we can round up all those responsible for this situation and send them to the forced labor camps, it would be nice if we had some means of bypassing the block.\nHere is how:\nFirst, visit this address: https://torrentproxy.io/list/thepiratebay\nThis site offers a list of pirate bay proxies, and even uses browser-side scripting to try each one of them from your own internet connection to figure out which ones are working for you and which ones aren't.\nSo, if you see any proxies with a green check mark, you are good to go: just visit The Pirate Bay through the proxy and you are good. However, most chances are that every single one of these proxies will also be blocked by your ISP.\nSo, go to your network properties, and modify your DNS configuration. Throw away your ISP's DNS servers, and replace them with any other DNS servers that are known to be based outside The Netherlands. For example, google's DNS servers: 8.8.8.8 and 8.8.4.4\nAt this point, thepiratebay.org will still be blocked, because they block it by IP address, but the proxies will not be blocked anymore, because they block them by DNS. So, then, reload the proxylist, and most proxies should now receive a beautiful green little check mark.\nSo, pick one of those proxies, and enjoy your visit to The Pirate Bay!\n","date":"2019-07-05T16:55:19.442Z","permalink":"https://blog.michael.gr/post/2019-07-the-pirate-bay/","title":"Bypassing the blocks to The Pirate Bay"},{"content":"Right out of the box, an Ubuntu 18.04 machine will not see any Microsoft Windows machines on the network, and Windows machines will not see it. Unfortunately, there is an incredible amount of work necessary to get it to work properly. So, here are my notes on setting up an Ubuntu 18.04 Linux workstation to act both as a Samba server and as a Samba client in a Windows Workgroup.\nServer Here is my setup script, which works, but be sure to keep reading, because the script alone is not enough.\nsudo apt-get install samba winbind libnss-winbind sudo ufw allow Samba if [[ ! -f /etc/samba/smb.conf.original ]]; then sudo mv /etc/samba/smb.conf /etc/samba/smb.conf.original sudo write /etc/samba/smb.conf \u0026lt;\u0026lt;EOF # See /etc/samba/smb.conf.original [global] map to guest = bad user server role = standalone server server services = smb unix extensions = no wide links = yes client max protocol = NT1 dns proxy = yes wins support = yes # This is necessary here so that user-specific clones of \u0026#34;[homes]\u0026#34; will be browseable. browseable = yes [homes] read only = no comment = %U\u0026#39;s Home Directory # browseable = no is necessary here, or else \u0026#34;homes\u0026#34; will appear in the browse list. browseable = no valid users = %U EOF fi The above script will install samba and configure it. (If it has not been installed already.) The old configuration file is saved as /etc/samba/smb.conf.original so that it can be consulted in the future.\nAfter this, windows machines will successfully list my Ubuntu machine among other machines under \u0026quot;Network\u0026quot;, and if I click on it I will be prompted for credentials. When entering credentials I must remember the incredible, ages long \u0026quot;Domain\u0026quot; gotcha: do not just enter the username, enter MACHINE\\username instead. Then, Windows will correctly list the shares.\nThe [homes] section in the above script defines a share which will magically have the same name as the authenticated user, and will magically map to the user's home folder. That's all I need.\nA minor annoyance is that testparm will always report the following error:\nrlimit_max: increasing rlimit_max (1024) to minimum Windows limit (16384)\nAfter much searching around on the interwebz I found that:\nIt is allegedly a warning, not an error, so it allegedly does not matter, and There simply seems to be absolutely no way to make it go away, so you better get used to it. Client Things turn out to be considerably more difficult in making the Ubuntu machine successfully act as a client. In the beginning nothing worked.\nUnder Nautilus (the Gnome file manager) going to \u0026quot;Other Locations\u0026quot; and trying to open \u0026quot;Windows Network\u0026quot; would fail. Trying to \u0026quot;Connect to Server\u0026quot; with smb://servername would also fail.\nThe smbtree command would initially list only the local host, and if you let things stand for a while, it would eventually also show other machines, but not their shares.\nThe smbclient command would fail with a very informative error message saying \u0026quot;Error NT_STATUS_UNSUCCESSFUL\u0026quot;.\nThe nmblookup command would fail with a very informative message saying \u0026quot;name_query failed to find name\u0026quot;.\nThe interwebz abound with discussions about the connectivity issues between Ubuntu 18.04 and Windows, but none of the solutions offered seemed to work for me. Some of the discussions even arrive at the conclusion that it is impossible to get it to work.\nSo, I started troubleshooting with smbtree and --debuglevel=3. (Because debuglevel=10 is like trying to drink water from a fire hydrant.)\nThe first thing I noticed was that samba was complaining about it being supposed to use a WINS server, but having no address configured for it. I suppose that if there is no mention of WINS in smb.conf, then by default samba will neither try to act as a WINS server, nor will it know who the WINS server is. Which kind of makes sense, but what does not make sense is that samba does not complain in any way about this erroneous situation during startup, and instead we need to look at debug information in order to discover it happening.\nI have no WINS server on my network, because this is apparently an advanced feature that is only found on Windows Server products, so I added \u0026quot;wins support = yes\u0026quot; to my smb.conf, to tell samba to act as a WINS server for me, because Linux gives these things for free, that's how it rolls.\nBy doing that, the error about WINS went away. Unfortunately, everything else remained as broken as before.\nThen it dawned upon me to look at what is going on with the firewall. I had enabled the firewall and I had added the necessary rules to allow samba, but you never know.\nAgain, because trying to read the logs is like trying to drink water from a fire hydrant, I had to write the following little script:\nufwwatch\n#!/bin/bash function process { local -a array=( ${1// / } ) local time=\u0026#34;${array[2]}\u0026#34; local action=\u0026#34;${array[7]%\\]}\u0026#34; if [[ \u0026#34;$action\u0026#34; != \u0026#34;BLOCK\u0026#34; ]]; then return; fi local -a remainder=(\u0026#34;${array[@]:8}\u0026#34;) local -A map=() for part in \u0026#34;${remainder[@]}\u0026#34;; do local -a subparts=( ${part//=/ } ) local left=${subparts[0]} local right=${subparts[1]} map[$left]=\u0026#34;$right\u0026#34; done echo $time $action ${map[PROTO]} in=${map[IN]} out=${map[OUT]} src=${map[SRC]}:${map[SPT]} dst=${map[DST]}:${map[DPT]} } echo Displaying blocked packets as they happen: tail -f /var/log/ufw.log | while read line; do process \u0026#34;$line\u0026#34;; done By running this script I discovered to my astonishment and horror that some packets were in fact being blocked each time I tried running smbtree. The packets were UDP packets arriving from my Windows machines to my Ubuntu machine. The port on the originating machine's side was 137, but the ports on which they were arriving on my Ubuntu machine were random. This is typical behavior when communicating via UDP, but the problem is that the standard ufw rules for \u0026quot;Samba\u0026quot; do not account for this! (WTF?)\nHere is what happens.\nLet us begin without any rules:\nmichael@pegasus:~$ sudo ufw status numbered Status: active Now, let us add the standard rules for \u0026quot;Samba\u0026quot;:\nmichael@pegasus:~$ sudo ufw allow Samba Rule added Rule added (v6) michael@pegasus:~$ sudo ufw status verbose Status: active Logging: on (full) Default: deny (incoming), allow (outgoing), disabled (routed) New profiles: skip To Action From -- ------ ---- 137,138/udp (Samba) ALLOW IN Anywhere 139,445/tcp (Samba) ALLOW IN Anywhere 137,138/udp (Samba (v6)) ALLOW IN Anywhere (v6) 139,445/tcp (Samba (v6)) ALLOW IN Anywhere (v6) The problem is that with these rules, incoming packets from remote port 137 to random local ports get filtered. So, although the Samba server works, the Samba client does not work. Here is what I had to do in order to fix this:\nmichael@pegasus:~$ sudo ufw allow in proto udp from any port 137,138 to any Rule added Rule added (v6) michael@pegasus:~$ sudo ufw status verbose Status: active Logging: on (full) Default: deny (incoming), allow (outgoing), disabled (routed) New profiles: skip To Action From -- ------ ---- 137,138/udp (Samba) ALLOW IN Anywhere 139,445/tcp (Samba) ALLOW IN Anywhere Anywhere ALLOW IN 137,138/udp 137,138/udp (Samba (v6)) ALLOW IN Anywhere (v6) 139,445/tcp (Samba (v6)) ALLOW IN Anywhere (v6) Anywhere (v6) ALLOW IN 137,138/udp (v6) After this, the smbtree command started listing Windows workstations.\nThe final piece of the puzzle was to get a proper listing of the shares of each Windows workstation.\nUnfortunately I had to do an awful lot of tweaking around in order to get this to work, and I do not remember anymore every step of the process, but I am documenting what I remember, and I will refine this when I get the chance.\nThe final steps that worked were the following:\nGo to Control Panel -\u0026gt; Network and Sharing Center -\u0026gt; Change Advanced Sharing Settings -\u0026gt; All Networks -\u0026gt; Password protected sharing and: Select \u0026quot;Turn off password protected sharing\u0026quot; In the Group Policy Editor (gpedit.msc) Go to Computer Configuration -\u0026gt; Windows Settings -\u0026gt; Security Settings -\u0026gt; Local Policies -\u0026gt; Security Options and modify the following: Enable \u0026quot;Network access: Let Everone permissions apply to anonymous users\u0026quot; And of course make sure the firewall allows samba packets to go through. Of course the above were probably not the only things required to make it work. While troubleshooting, I also did several other things which, although they did not have any immediately visible result, may have contributed to the success of the configuration as a whole. Those included the following:\nIn an elevated command prompt, execute:\nnet user guest /active:yes to enable the guest account. net user guest \u0026quot;\u0026quot; to make sure that the guest account has a blank password. Go to Windows Explorer -\u0026gt; This PC (\u0026quot;My Computer\u0026quot;) -\u0026gt; Uninstall or Change a program -\u0026gt; Turn Windows features on or off\nMake sure \u0026quot;SMB 1.0/CIFS File Sharing Support\u0026quot; is checked. Here is a relevant discussion for reference: https://unix.stackexchange.com/q/453944/141190\n","date":"2019-04-22T18:41:39.908Z","permalink":"https://blog.michael.gr/post/2019-04-the-smb-monstrosity/","title":"Samba (SMB) on Ubuntu"},{"content":"I have been researching clipboard managers for Linux, (Ubuntu with Gnome,) and I am recording my findings here for the benefit of others.\nGpaste Available in https://www.imagination-land.org/tags/GPaste.html As of the time of writing this, the latest version is 3.32.0 (March 12, 2019) but this version is not available via apt. The latest version available via apt is 3.28.0-3, I guess it will have to do.\nDecent looking user interface, but wasteful in terms of screen real estate, clunky, and actually annoying due to buttons whose icons are unintuitive and at the same time do not offer tooltips, so you have no way of knowing what the button does unless you click it. (And it also has buttons that do not seem to do anything, so you will never know.)\nAlso quite buggy. Supports a bunch of hotkeys for various arcane capabilities, but the mechanism for changing them does not work.\nAlso, badly designed: in the dialog that pops up when you invoke the history, the focus is not on the previously copied item, (which is what you want in the vast majority of cases,) the focus is on a stupid search box.\nAlso, it does not automatically paste, so after selecting an entry from the history you still have to press Ctrl+V in order to actually paste it into the application that you are using.\nThumbs down.\nKeepboard (for Linux) Available via https://sourceforge.net/projects/keepboard/\nImmediately after installation, on the very first run, during startup, it dies with a NullPointerException.\nThe author posted a comment here suggesting that I give it another try, so I gave it another try with version 5.5 released in January of 2020.\nNow it does not die with a NullPointerException, it just runs doing nothing. The only way I know it is running is by using \u0026quot;ps -A\u0026quot;, because it is not giving any other sign of life anywhere on my screen. Also, there exists absolutely no documentation for the project, not even a single sentence explaining how to use it, so it is impossible to know whether this behavior is intended or unintended. This means that I had to waste a lot of my precious time trying to find out whether it is working and I just do not know how to use it, or it is simply not working at all and there is no way to use it. I finally decided that it is the latter.\nMy verdict is \u0026quot;don't waste your time with this\u0026quot;.\nParcellite Available via http://parcellite.sourceforge.net/. Back in January of 2017 the author wrote \u0026quot;Nothing for years, then two releases in the same day\u0026quot;. He has been quiet ever since. (And the previous update was like in 2014.) So, forget it.\nGlipper Available via https://launchpad.net/glipper. Last update was in 2013, so, forget it.\nxclipboard A very old program. Comes preinstalled with Ubuntu. When run, it fails with the message \u0026quot;Error: another clipboard is already running\u0026quot;. There is a page from 2009 explaining how to fix this problem, (https://lildude.co.uk/howto-use-xclipboard-with-gnome) but judging by how xclipboard looks in the screenshots, I do not feel compelled to keep trying.\nCopyQ Home page is https://hluk.github.io/CopyQ/. It is available via apt, latest version in apt is 3.2.0. For Ubuntu there is also ppa:hluk/copyq which hosts the latest version, 3.8.0. Updates are frequent, with the latest being just a few days ago.\nCopyq is a monster of a clipboard manager, packed with an awful lot of features and offering a ridiculous degree of control. Besides the menu that drops down from the taskbar indicator allowing you to see your clipboard history and make a selection, it also has an extensive preferences window, and one more window which they call \u0026quot;main\u0026quot; and which allows you to manipulate your history entries, as if that's a very important thing deserving its own window.\nThe application is so over-engineered that it even has its own \u0026quot;task manager\u0026quot; and it supports color theming on the window they call \u0026quot;main\u0026quot;. Yet, it lacks some very basic customization features, for example the ability to get rid of unnecessary menu items in the drop-down menu so as to have a more minimalistic experience.\nThe application is a bit buggy. One bug I found is that if you have the preferences window open, and then you also open the main window, then the main window is not responding unless you close the preferences first. Another bug is that its taskbar icon often gets lost, even though the application is still active and responding to the hotkey. Another bug is that every few seconds the application scans the entire directory where clipboard entries are saved, each in its separate file, so if you have lots of files there, it consumes a lot of CPU (and therefore energy) doing this. The worst problem is that it confuses the numeric keypad keys with their non-numeric keypad equivalents, so there is no way to specify a hotkey on the numeric keypad, like Ctrl + Shift + Numeric-Keypad-Insert.\nTo the author's credit, when I reported the last two issues to him, he fixed them relatively quickly.\nUnfortunately, in each case a couple of months had to pass before the fix appeared in an actual release of the software.\nMost of the exotic features are unintuitive, so you are unlikely to use them because you will probably not even know what they do, but all the basics are there.\nBottom line is that this is a very useful clipboard manager, and the overengineering that has gone into it does not seem to hurt, because\na) it is not ridiculously large, (only about 2MB to download, 7MB on the disk)\nb) the unnecessary extra functionality does not get too much in the way of using the small subset of the functionality that is actually useful, and\nc) the functionality that is actually useful does really work, and it works well.\nIf you do use it, do not forget to immediately go to \u0026quot;Preferences\u0026quot; -\u0026gt; \u0026quot;Items\u0026quot; -\u0026gt; \u0026quot;Synchronize\u0026quot; and add a folder for saving your clipboard, because CopyQ does not do that automatically for you. (So, even though the app has the feature, and you may have made sure that the feature is enabled, your clippings are still not being saved unless you take additional action.)\nUpdate 2020-05-31: Up until today CopyQ had been working fine for the minimal functionality that I have wanted from it, namely to be able to paste old clipboard entries, nothing more. It even survived my upgrade to Ubuntu 20.4. However, today I needed to open up the CopyQ main window to change something in its configuration, and as soon as I would click on any part of the main window, it would just disappear without even saying good-bye. So, I uninstalled it. It was a monstrosity anyway.\nDiodon Home page is https://launchpad.net/diodon.\nThe application icon is a blowfish, which is a bit over the top, and yet its icon on the gnome indicator bar is a tiny paperclip, which is rather underwhelming. (But of course a blowfish on the indicator bar would have looked even worse.)\nDecent, minimalistic, and extensible with plugins.\nAt first glance it disappoints, but if you give it a second chance it seems to come through. For instance, the first time I ran it it just did not appear to store anything in history: no matter what I copied to the clipboard, its history list remained empty. Then, the second time I ran it, I found in its history list everything that I had copied earlier. Then I manually assigned a hotkey for it, (I know, it is a nuisance, but it is okay, because you only do it once,) but the first time I used the hotkey nothing happened. But after the second time it started working.\nSo, I am giving it a try, I will post an update after a while.\nClipit Home page appears to be https://launchpad.net/ubuntu/+source/clipit\nOr perhaps https://github.com/CristianHenzel/ClipIt\nA decent little clipboard manager that does no more than what is necessary. The most delightful aspect of it is that its popup menu does not contain any useless crap, just your clipboard history, as it should.\nUnfortunately, when you select an item from history, there appears to be a half second delay before the text is actually pasted, and there appears to be no option to change this behavior.\nI used to be excited by this utility, because I very much prefer its minimalism over CopyQ's pompousness, but I gave it a try twice, and on both occasions it miserably disappointed me by continuously crashing and generally not working.\nAnamnesis Home page is http://anamnesis.sourceforge.net/\nLast update was in 2013, so, forget it.\nPastie Home page appears to be https://github.com/fmoralesc/pastie/\nHas no README.\nLast update was 4 years ago, so forget it.\nClipman Home page appears to be https://goodies.xfce.org/projects/panel-plugins/xfce4-clipman-plugin\nIt is for \u0026quot;xfce\u0026quot;, so it is a no-go for Ubuntu, which uses Gnome. (Besides, I even tried it, just to be sure, and it just does not work.)\nClipboard Indicator (Gnome Shell Extension) Home page: https://extensions.gnome.org/extension/779/clipboard-indicator\nEither this is a clone of GPaste, or GPaste is a clone of this.\nHas the very nice ability to show the first few characters of the current clipboard content next to its icon on the task bar. Unfortunately, that's the only good thing I have to say about it.\nIt has a maximum history length of only 50, and as if that was not ridiculous enough, it will only show the last 15 of them.\nWhen you open the menu, the first selectable entry is a stupid search box, (as if you will ever need to search through a meager 50 entries,) so you always need one extra press of the \u0026quot;down\u0026quot; arrow to skip the search box to go to the current clipboard entry, and one more to go to the previous clipboard entry, which is what you want like 99% of the time.\nThe author's idea of how favorites should work is that they should be pinned to the top of the list, so each time you add a favorite you are increasing the number of times you will have to press the down-arrow before you can reach the previous clipboard entry.\nWorst of all, when you select an entry, it does not automatically paste it for you, it just places it in the clipboard, so you always need yet one more keystroke to actually paste.\nBottom line: Usable, but annoying.\nClipper (Gnome Shell Extension) Home page: https://extensions.gnome.org/extension/1081/clipper/\nAppears to be abandoned.\nOld comments\nDragan Bozanovic 2020-01-05 20:25:30 UTC\nGood overview. Jfyi, Keepboard startup issue that was appearing in newer Java versions has been fixed.\n","date":"2019-04-12T23:18:39.304Z","permalink":"https://blog.michael.gr/post/2019-04-clipboard-managers-for-ubuntu-as-of/","title":"Clipboard Managers for Ubuntu as of April 2019"},{"content":"This is an article I enjoyed reading. I am in full agreement with every claim made therein. It was very nice to see certain conclusions that I have arrived at in the past being spelled out and illustrated with nice explanations.\nhttps://medium.com/@egonelbre/psychology-of-code-readability-d23b1ff1258a\nOld comments\ngmc 2019-09-16 19:05:24 UTC\nAbout naming, this is a brilliant podcast on the subject that i think should be required listening for any sw engineer: https://www.se-radio.net/2016/12/se-radio-episode-278-peter-hilton-on-naming/\n","date":"2019-01-29T08:30:36.378Z","permalink":"https://blog.michael.gr/post/2019-01-mediumcom-psychology-of-code/","title":"Medium-com: Psychology of Code Readability by Egon Elbre"},{"content":"\rMy apartment is equipped with a Ferroli BlueSense boiler, which I needed to troubleshoot today, but the manual is in Dutch, so I had to find the manual in electronic form on the interwebz, and have it machine-translated to English.\nI am posting both the original and the machine-translated version here, in case others find it useful.\nFerroli BlueSense Boiler 3, 4 and 5 user's manual in Dutch (PDF)\nFerroli BlueSense Boiler 3, 4 and 5 user's manual in English (PDF)\nThe machine translation from PDF to PDF was very nicely done via https://www.onlinedoctranslator.com. By very nicely I mean that the images have been preserved, and the formatting has been preserved as much as is reasonable to expect. Of course, if you have a Ferroli boiler, then you are probably in Holland, and if you are reading this text then you are probably an expat living in Holland, so you probably already know that the state of the art today in machine translation from Dutch to English sucks, so do not expect it to make perfect sense, or even to make sense. Still, it is better than nothing.\nIn a later post I published the Ferroli HR OptiFor OT-V Ventilation Unit User's Manual in English.\nOld comments\nUnspecified 2020-08-16 14:57:04 UTC\nThank you very much, it really was a problem for me too)\n","date":"2019-01-20T18:43:23.772Z","permalink":"https://blog.michael.gr/post/2019-01-ferroli-boiler-manual/","title":"Ferroli BlueSense Boiler User's Manual in English"},{"content":"\rSo, I have this huge mp3 collection that is mostly missing cover art. I embarked on a quest to add cover art to each mp3 file, and before long I realized that I need some software to help me with it. Here I am documenting my findings.\nAlbum Art Downloader version 1.0.3\nfrom http://album-art.sourceforge.net\nVery complicated, unintuitive, and clunky user interface with multiple free-floating windows and no sense of a central application. You often have to kill it via the task manager to stop it from endlessly re-opening windows. Does not remember options from run to run and also from opening a certain window to opening the same window again. Only saves Folder.jpg, so you have to manually embed the image to the mp3 file by some other means. (Luckily, this was not a problem for me, because I was able to use foobar2000's Batch embed pictures feature.) Also, since it only saves Folder.jpg, you have to have all your files in separate folders, otherwise you will get all your cover art as Folder1.jpg, Folder2.jpg, etc. and then good luck figuring out which jpg corresponds to which mp3. Gives somewhat inaccurate results; in about 50% of the cases, the covers I could manually find in google image search were much better than covers it was finding supposedly from the same source. (However, that is to be expected.) When it does not work, you receive no hint as to what went wrong. At some point it just stopped working, probably because some settings were fouled up, so I had to uninstall it and re-install it. Verdict: very bad user experience, but potentially useful, considering how utterly useless the rest of the options are.\nCreevity Mp3 Cover Downloader version 1.4.0\nfrom http://www.creevity.com/mp3coverdownloader\nVery promising at first glance, until you realize that the window does not resize. No configuration at all. Unintuitive buttons with no tooltips to explain what they do. Accurate results, but no information whatsoever as to where it found them. Displays results as thumbnails, with no option to view them larger, and does not even tell you what the actual size of each image is. When it does not work, you receive no hint as to what went wrong. Verdict: Useless.\nAudioShell version 2.3.6\nfrom http://www.softpointer.com\nThis is a Windows Explorer Shell Extension which does not download anything, but it could perhaps be helpful in attaching picture files that have already been downloaded.\nVery promising at first glance, but a) when album art is attached via this program, it is not visible in Windows Explorer, and b) once this program is installed, attaching album art via foobar2000 stopped working. Once I uninstalled it, attaching album art via foobar2000 started working again. Very strange. Verdict: Useless.\nSACAD version 2.1.2\nfrom https://github.com/desbma/sacad\nA command line tool. I thought it would give me lots of options. It didn't give me any.\nVerdict: Useless.\nMusicBrainz Picard version 1.4.2\nfrom https://picard.musicbrainz.org/\nWell written app, with a solid user interface. Its primary purpose is adding proper tags to mp3 collections, and from what I have seen it must be extremely useful in doing that; however, adding album art is only a side feature, and as such it is not as developed as one would hope. So, while it makes it very easy to add cover art to each of your mp3s, it has some important disadvantages:\nIt gives extremely few options for configuring where and how it will look for pictures. For each mp3 file, it yields very few pictures to choose from. If you do not like what it finds, you are completely out of luck. To me, all pictures found seemed to be of a rather low resolution, so I could not use any of them. Verdict: Useless.\nConclusion It is with great sadness that I announce Album Art Downloader from http://album-art.sourceforge.net to be the winner. It is lame, but it helps much more than the other options in getting the job done.\n","date":"2018-07-01T10:53:11.101Z","permalink":"https://blog.michael.gr/post/2018-07-researching-mp3-cover-art-downloaders/","title":"Researching mp3 cover art downloaders"},{"content":"In this post I am documenting the process of rooting my Samsung Galaxy Note 4. It should work on the majority of Android phones out there.\n[\rGo to Settings -\u0026gt; System -\u0026gt; Security and make sure that Reactivation lock is unchecked. Mine was unchecked and disabled, I have no idea what this means. Find CF-Auto-Root for your exact model. Mine is the SM-N910F, (and I have a Windows PC,) so I got CF-Auto-Root-trlte-trltexx-smn910f.zip. No, I am not providing the zip file, and if I did, you should not trust me. Just go to xda-developers.com and locate it yourself.\nExtract the contents of the CF-Auto-Root archive in a folder.\nYou should end up with the following files:\nCF-Auto-Root-trlte-trltexx-smn910f.tar.md5 Odin3.ini Odin3-v3.10.6.exe tmax.dll zlib.dll If not, there is something wrong, abandon the procedure, seek advice elsewhere.\nMake sure your phone is not connected via USB to your computer.\nStart Odin3 and do not touch anything except what the instructions below say.\nOdin looks like this:\nUnder Files [Download] click the AP button and select the CF-Auto-Root-....tar.md5 file. The check mark to the left of the AP button should become checked by itself.\nPower down your phone.\nHold Volume-Down + Home + Power-Button for a whole second or so, until your phone gives a little vibration. This will start up your phone in download mode. If asked, answer that you are sure.\nConnect the phone to your PC via USB. In Odin you should now see something like the following:\nIn the Options tab, make sure Repartition is not checked.\nClick the Start button.\nWait while Odin is doing its thing. When it is done, you will be rooted and you will be able to find SuperSU in your applications tray. Its default settings should be fine.\nFurther steps:\nInstall AdAway from xda-developers: https://forum.xda-developers.com/showthread.php?t=2190753\nInstall File Explorer Root Browser from the Play store: https://play.google.com/store/apps/details?id=com.jrummy.root.browserfree\nInstall System app remover (ROOT) by Jumobile (also known as \u0026quot;System App Safe Remover\u0026quot; or just \u0026quot;Uninstall\u0026quot;) from the Play store:\nhttps://play.google.com/store/apps/details?id=com.jumobile.manager.systemapp\nI somehow also ended up with BusyBox Free, I do not know whether it is necessary:\nMost things worked fine, but some apps (for example, No-frills CPU Control) would fail to start with a message saying that my phone needs to be rooted. To fix this, I had to use Root Browser to copy the su executable from /su/bin to /system/xbin.\nThen, I used System app remover (ROOT) to uninstall a whole bunch of apps. When I restarted my phone, I kept receiving the following message:\nI must have clicked that OK button about a hundred times, but it seemed like there was no end to it. So, I restarted my phone. Luckily, on the 2nd boot the message did not show up again.\nNext step:\nInstall No-frills CPU Control from the Play store:\nhttps://play.google.com/store/apps/details?id=it.sineo.android.noFrillsCPU\n","date":"2018-06-24T14:04:06.109Z","permalink":"https://blog.michael.gr/post/2018-06-rooting-my-samsung-galaxy-note-4/","title":"Rooting my Android Phone"},{"content":"I have a large number of mp3 files, all of them with embedded album art, but when viewing them via Windows Explorer, I see generic music file icons instead of album art thumbnails.\nGeneric music icons. Ugh. I spent a good couple of hours troubleshooting this problem, so I would like to document the solution for anyone who happens to go through the same pain.\n--- Rant Begin ---\nThe interwebz abound with questions posted from people who are having the same problem, but none of the solutions offered seemed to work for me. I have seen suggestions to download a registry file that will restore my mp3 file mapping, wipe my icon cache, go change various options in windows that control the displaying of thumbnails, etc.\nThe most lame solutions are being offered from official Microsoft sources, which appear to have a policy of only giving answers that involve the User Interface of Windows, never going under the hood, never offering a life-saving hack, never suggesting the use of any tools that were not built by Microsoft. I suppose they are being official like that.\nAnd Microsoft has not made things easy at all, because there exist 3 different settings in the Windows User Interface that control the displaying of thumbnails:\nIn \u0026quot;Folder Options\u0026quot;, the \u0026quot;Always show icons, never thumbnails\u0026quot; setting, which must be off.\nIn \u0026quot;System Properties\u0026quot;, under \u0026quot;Performance\u0026quot;, the \u0026quot;Show thumbnails instead of icons\u0026quot; setting, which must be on.\nIn \u0026quot;Group Policy Editor\u0026quot;, the \u0026quot;User Configuration / Administrative Templates / Windows Components / Windows Explorer / Turn off the display of thumbnails...\u0026quot; settings, which must be Disabled or Not configured.\nBut of course the problem is very rarely one of the above. If a user accidentally has one of the above settings wrong, they are likely to notice the problem with image thumbnails long before they notice it with mp3 thumbnails. So, when a user is searching for a solution to mp3 thumbnails, they usually do so because they have thumbnails working on images, and they are wondering why they are not working on mp3 files, which means that they have all of the above settings right.\n--- Rant End ---\nSo, my problem simply turned out to be wrongly written mp3 tags, or to put it differently, mp3 tags written in such a way that Microsoft Windows Explorer could not read them, so it was showing generic icons instead.\nHere is what I did to fix the problem:\nI re-installed Windows Media Player. I am not sure that this was necessary, (since it did not immediately fix the problem,) but it may be a pre-requisite.\nI instructed my favorite mp3 tag editor to save mp3 file tags as follows:\nWrite ID3v2.3 tags\nDo not use padding\nWrite ID3v2 tags only. (No ID3v1 tags.)\nFor completeness, here is a screenshot of all the mp3 tagging options that I used:\nfoobar2000 mp3 tagging options that work Then, in order to cause tags to be rewritten, I selected all my files, I attached some random image as the \u0026quot;Disc\u0026quot; picture, (because I know I have no files with such a picture,) and voila, all of my album art appeared in Windows Explorer's \u0026quot;Icons\u0026quot; view. Then, I removed the \u0026quot;Disc\u0026quot; picture from all files.\nOld comments\nAnonymous 2023-06-20 17:55:53 UTC\nthank you so much!\nAnonymous 2025-04-21 16:58:10 UTC\nHi. Please, do you know how to do the contrary? I see thumbnails and I'd like to see only generic icons. I use WIN 8.1 and AIMP. Thanks!\n","date":"2018-06-23T10:31:21.147Z","permalink":"https://blog.michael.gr/post/2018-06-solved-windows-explorer-shows-generic/","title":"Solved: Windows Explorer shows generic thumbnails instead of embedded album art for music (mp3) files"},{"content":"\rThere is a Chinese proverb which states:\nThe beginning of wisdom is to call things by their proper name.\nThis proverb is generally understood to be a summarization and paraphrase of an actual quote from the \u0026quot;Rectification of Names\u0026quot; section of the Analects of Confucius. (See Wikipedia - Rectification of names)\nThe original quote is as follows:\nIf names be not correct, language is not in accordance with the truth of things. If language be not in accordance with the truth of things, affairs cannot be carried on to success. -- Confucius\nOf course, the value of the quote lies not in that it carries the weight of the name of Confucius, but rather, in that by simply seeing it spelled out in words, one can immediately realize its self-evident truthfulness, and thus, perhaps, appreciate the magnitude of its importance.\nP.S.\nIn the modern western world fashion of \u0026quot;here is some entertainment to go along with your philosophy\u0026quot;, I would point out that for us programmers, this quote should be understood as:\nIf you don't name your variables properly, your programs ain't gonna work.\nAnother thing we can all take home is that:\nConfucius was not fond of confusion.\n","date":"2018-05-21T23:50:55.365Z","permalink":"https://blog.michael.gr/post/2018-05-confucius-on-naming/","title":"Confucius On Naming Things"},{"content":"\rGitHub project: Rumination\nMaking plain old java objects aware of their own mutations.\nNOTE:\nThis project has been retired. The github link does not even work anymore.\nThis page only serves historical documentation purposes.\nWhat is mikenakis-rumination? mikenakis-rumination is a java agent and associated class library for making plain old java objects aware of their own mutations.\nRumination is a term that I have coined to refer to the ability of an object to be aware of mutations on itself.\nRumination is important because it is the first step in making objects observable: Once an object is aware of changes to its own state, it can then offer means by which external observers can register to receive notifications about these changes. Rumination is not concerned with observability, it is only concerned with the narrow concept of self-mutation-awareness.\nThe goal of rumination is to have some overridable method of an object invoked each time one of the setters of the object alters some piece of the state of the object. One can achieve this manually, by hand-coding an invocation to the overridable from within each setter, but that is tedious and error-prone. The idea behind rumination is to systematize this process, so that the overridable gets invoked automatically, without us having to write any additional code.\nThe dictionary offers two meanings for the word ruminate:\nOne is \u0026quot;to chew the cud, as a ruminant\u0026quot;, and it relates to the overridable being invoked to re-process a value which has already been received by a setter, so it is like eating one's own food once again. (And hence the goat in the logo.) The other meaning is \u0026quot;to meditate or muse; ponder\u0026quot; which is also applicable, because the process is introspective in nature. Rumination works by means of a java agent which gets invoked by the JVM to transform classes as they are being loaded. (Actually, it works as an AgentClaire Interceptor, which is essentially the same thing.)\nA class is a ruminant if it is marked with the @Ruminant annotation, or if it is a descendant of a ruminant. This means that you only have to mark a certain base class as a ruminant, and then all of its descendants will automatically be ruminants without the need to add the @Ruminant annotation to each one of them.\nThe rumination processor examines ruminant classes for setters. A setter is a method which is of void return type, has a name starting with set and followed by the name of an existing instance field, accepts only one parameter which is of the same type as the instance field, and ends with a sequence of bytecode instructions that store the value of the parameter into the field before returning. (I am only looking for these instructions at the end of the setter in order to allow for validation prologue.)\nThe rumination processor will replace those last bytecode instructions of the setter with bytecode instructions that do the following:\nCheck whether the new value is same as the current value of the field; if so, return without doing anything. Store the new value into the field; Invoke the ruminator method, passing it the name of the field. return. The ruminator method is of the following form:\nprotected void onMemberChanged( String fieldName ) { ... } The @Ruminant annotation accepts an optional String ruminatorMethodName() parameter, whose default is \u0026quot;onMemberChanged\u0026quot;, so the name of the ruminator method can be changed.\nThe field is identified by name, because java does not (yet?) support field literals.\nTo see how rumination is used, check out the tests.\nHosted on GitHub: https://github.com/mikenakis/mikenakis-rumination\nLicense This creative work is explicitly published under No License. This means that I remain the exclusive copyright holder of this creative work, and you may not do anything with it other than view its source code and admire it. More information here: Open Source but No License.\nIf you would like to do anything more with this creative work, please contact me.\nCover image: The mikenakis-rumination logo. Based on original from free-illustrations.gatag.net Used under CC BY License.\n","date":"2018-04-11T05:17:08.445Z","permalink":"https://blog.michael.gr/post/2018-04-github-project-mikenakis-rumination/","title":"GitHub project: mikenakis-rumination"},{"content":"This is a rant about JUnit, or more precisely, a rant about JUnit's inability to execute test methods in natural method order.\nDefinition: Natural method order is the order in which methods appear in the source file.\nWhat is the problem? (Useful pre-reading: About these papers)\nUp until and including Java 6, when enumerating the methods of a java class, the JVM would yield them in natural order. However, when Java 7 came out, Oracle changed something in the internals of the JVM, and this operation started yielding methods in random order.\nApparently, JUnit was executing methods in the order in which the JVM was yielding them, so as a result of upgrading to Java 7, everybody's tests started running in random order. This caused considerable ruffling of feathers all over the world.\nNow, the creators of the Java language are presumably running unit tests just like everyone else, so they probably noticed that their own tests started running in random order before releasing Java 7 to the world, but apparently they did not care.\nLuckily, the methods are still being stored in natural order in the class file, they only get garbled as they are being loaded by the class loader, so you can still discover the natural method order if you are willing to get just a little bit messy with bytecode.\nHowever, that's too much work, and it is especially frustrating since the class loader is in a much better position to correct this problem, but it doesn't. (The class loader messes up the method order probably because it stores them in a HashMap, which yields its contents in Hash order, which is essentially random. So, fixing the problem would probably have been as simple as using a LinkedHashMap instead of a HashMap.)\nPeople asked the creators of JUnit to provide a solution, but nothing was being done for a long time, allegedly because if You Do Unit Testing Properly?, you should not need to run your tests in any particular order, since there should be no dependencies among them. So, the creators of JUnit are under the incredibly short-sighted impression that if you want your tests to run in a particular order, it must be because you have tests that depend on other tests.\nWhen the creators of JUnit finally did something to address the issue, (it did not take them long, only, oh, until Java 8 came out,) their solution was completely half-baked: the default mode of operation was still random method order, but with the introduction of a special annotation one could coerce JUnit to run test methods either in alphabetic order, (which is nearly useless,) or in some other weird, ill-defined, so-called \u0026quot;fixed\u0026quot; order, which is not alphabetic, nor is it the natural order, but according to them it guarantees that the methods will be executed in the same order from test run to test run. (And is completely useless.)\nSo, apparently, the creators of JUnit were willing to do anything except the right thing, and even though JUnit 5 is said to have been re-written from scratch, the exact same problem persists.\nWhy is this a problem? Well, let me tell you why running tests in natural method order is important:\nWe tend to test fundamental features of our software before we test features that depend upon them, so if a fundamental feature fails, we want that to be the very first error that will be reported. (Note: it is the features under test that depend upon each other, not the tests themselves!)\nThe test of a feature that relies upon a more fundamental feature whose test has already failed might as well be skipped, because it can be expected to fail, but if it does run, reporting that failure before the failure of the more fundamental feature is an act of sabotage against the developer: it is sending us looking for problems in places where there are no problems to be found, and it is making it more difficult to locate the real problem, which usually lies in the test that failed first in the source file.\nTo give an example, if I am writing a test for my awesome collection class, I will presumably first write a test for the insertion function, and further down I will write a test for the removal function. If the insertion test fails, the removal test does not even need to run, but if it does run, it is completely counter-productive to be shown the results of the removal test before I am shown the results of the insertion test. If the insertion test fails, it is game over. As they say in the far west, there is no point beating a dead horse. How hard is this to understand?\nAnother very simple, very straightforward, and very important reason for wanting the test methods to be executed in natural order is because seeing the test method names listed in any other order is brainfuck.\n","date":"2018-04-08T20:15:05.997Z","permalink":"https://blog.michael.gr/post/2018-04-random-order-of-tests/","title":"On JUnit's random order of test method execution"},{"content":"\rA command-line utility for dumping the contents of class files.\nRead the README.md file at https://github.com/mikenakis/Public/tree/master/classdump\nCover image: The mikenakis-classdump logo. Based on an image found on the interwebz.\n","date":"2018-04-07T21:28:50.205Z","permalink":"https://blog.michael.gr/post/2018-04-github-project-classdump/","title":"GitHub project: mikenakis-classdump"},{"content":"\rA lightweight framework for manipulating JVM bytecode.\nRead the README.md file at https://github.com/mikenakis/Public/tree/master/bytecode\nCover image: The mikenakis-bytecode Logo, an old-fashioned coffee grinder, by michael.gr, based on original work by Gregory Sujkowski from the Noun Project. Used under CC BY License.\n","date":"2018-04-07T19:24:50.967Z","permalink":"https://blog.michael.gr/post/2018-04-github-project-bytecode/","title":"GitHub project: mikenakis-bytecode"},{"content":"Foreword In my career I have experimented a lot with coding styles, mostly on pet projects at home, but also in workplaces where each developer was free to code in whatever way they pleased, or in workplaces where I was the only developer.\nMy experimentation has been in the direction of achieving maximum objective clarity and readability, disregarding convention, custom, precedent, and the shock factor: the fact that a particular style element might be alien to others plays very little role in my evaluation of the objective merits of the element.\nThe counter-argument (the argument in favor of following convention) says that whatever benefits might be offered by a coding style cannot possibly outweigh the benefit of presenting others with a familiar coding style. This is of course true, and that's why it makes sense for an organization to choose a traditional coding style. However, I am not a company; I am an individual, and my own projects are mine. Furthermore, my counter-counter-argument is that I firmly believe that tradition is a synonym for progress stopper.\nSo, over the years I have tried many things, once even radically changing my coding style in the middle of a project. (Modern IDEs make it very easy to do so.) Some of the things I tried I later abandoned, others I permanently adopted.\nSo, my coding style today is the result of all this experimentation. If it looks strange to you, keep in mind that every single aspect of it has been deliberately chosen to be this way by someone who was not always coding like that, and who one day decided to start coding like that in the firm belief that this way is objectively better.\nIn moving on with each of these changes over the years, I had to overcome my own subjective distaste of the unfamiliar, for the benefit of what I considered to be objectively better. So, if you decide to judge my coding style, please first ask yourself to what extent you are willing to overcome the same.\nMy Very Own™ Coding Style I use this coding style for languages that belong to the C syntax family, for example C, C++, Java, and C#. These are languages with curly braces, a reduced set of keywords, and a moderate amount of parentheses. I hardly ever program in any other language, but when I do, I apply whatever parts of this coding style are applicable.\nTabs vs. Spaces: Tabs I use tabs for indentation, because this allows different developers to view the code with the amount of indentation that they are accustomed to, without having to reformat the code. Spaces should never be used for indentation. Tabs should never be used for anything other than indentation. Tabular formatting: No Tabular formatting refers to inserting spaces within statements in consecutive lines of code to align parts of the statements into columns across those lines of code. So, for example, in statements that are of the form variable-type variable-name = initializer-expression; spaces would be inserted after the variable-types to align all the variable-names in a column, and more spaces would be inserted after the variable-names to align all the equals-signs in a column. I used to be a big fan of this; however: Generics make this less appealing, because most type definitions might be short, but one generic type definition might be very long, resulting in lots of seemingly unnecessary whitespace. A change in one line of code may result in re-alignment of many lines around it, and diff tools are not smart enough to account for this, so the possibility of merge conflicts skyrockets. Thus, at some point my verdict became to drop tabular formatting. Spaces: Before or after unary operators: Never Around binary operators: Always Around ternary operators: Always Before a comma: Never After a comma: Always Before opening parenthesis of function argument list: Never Before opening parenthesis of flow-control keyword: Never Inside parenthesized expressions: Never Around parameter lists: Always This means that a function call must look like this: foo( a, b ); Note that there is a space after ( and a space before ). This applies not only to function calls, but also to function declarations and to keywords that accept parameters. Parameterless functions can still be coded like this: foo(); because the rule is carefully worded to call for spaces around parameter lists, not spaces inside parentheses. When you are invoking a parameterless function there is no parameter list, therefore no spaces. Note that although parameter lists require spaces, parenthesized expressions require no spaces, and therein lies the advantage of this pair of rules: it suddenly becomes clear which parenthesis belongs to a function call, and which parenthesis belongs to an expression. For example, passing an expression as a parameter to a function looks like this: foo( a, (b + c) ); Note that certain C# constructs like typeof() and nameof() are expressions, not functions, therefore their arguments must not be padded with spaces. Right Margin Column: 120 In May of 2020 Linus Torvalds declared that the number of characters per line in the Code Style of the Linux Kernel was to be increased from 80 characters to 100 characters. That's laughable. We have had widescreen monitors since the beginning of the century. We can easily do 120 characters. I sometimes do 160 characters. Hard right margin: No The right margin is not meant to be a hard limit: if a line needs to be longer, make it longer. It is fine to push uninteresting stuff off the screen horizontally in order to fit more interesting stuff inside the screen vertically. New line after attributes (C#) / annotations (Java): Never This may push the function definition quite a bit far to the right, and that's fine. If a function has lots and lots of attributes/annotations, it might look very ugly, but that's okay, because it happens very rarely, and when it does happen, maybe that is exactly how it should be: beautiful things should look beautiful, and ugly things should look ugly. Empty lines before a block-style comment: One If a block-style comment appears in code, there must always be a blank line before it. This way, we are clearly indicating that the comment refers to the following line. Empty lines within functions: Zero Quite often programmers like to use blank lines to visually separate pieces of code that are conceptually different. The problem is, the blank line gives no hint about the concepts involved, so it is entirely useless to anyone but the person who inserted it. If it is worth leaving a blank line, then it is worth adding a block-style comment explaining why, in which case a blank line before the comment is also necessary due to a previous rule. Better yet, move the conceptually different code into a different function, and give that function a descriptive name, so that you need neither comment nor blank line. Empty lines before a function: One The rule which requires a blank line before a block comment covers all the cases where a function is preceded by a block comment that describes the function. However, quite often functions have descriptive names, rendering explanatory comments unnecessary. For these cases, we mandate separating functions with a blank line. Empty lines between fields: Zero If you really need a blank line between two fields, you must insert a block comment. Empty lines anywhere else: Zero Some people have the habit of leaving one or more blank lines in various odd places according to some ad-hoc rules that exist only in their head. The problem is, it is impossible to teach such rules to an automatic reformatting tool. Therefore, there shall be no such rules. There should never be any spurious blank lines anywhere. Curly braces: Allman See http://en.wikipedia.org/wiki/Indent_style#Allman_style Each opening brace and each closing brace is on a separate line, the braces are at the same indentation level as the controlling statement, and the code in the block is one indentation level deeper. Luckily, this is the curly brace style of C#. Unluckily, this is not the curly brace style of Java. I do not care; this is my coding style even when I code in Java. The Egyptian curly brace style which is so popular in the Java world is absolutely retarded. Braces on single statement blocks: Never (unless the language requires them) The \u0026quot;always\u0026quot; choice seems to be very popular; that's retarded. The \u0026quot;sometimes\u0026quot; choice also seems to be popular, but I strive for consistency. Note that in some languages some keywords have been introduced that require curly braces even if the controlled block consists of a single statement, for example the try-catch-finally clause in C++, Java, and C#. I greatly resent this. Nesting: Always consistent* Some people like writing quick one liners, for example if( x ) return 0; all in one line. That's unacceptable. Some people refrain from nesting the case labels in a switch statement, or if they do, then they refrain from nesting the code under the case labels. That's unacceptable. In C#, people quite often refrain from nesting the classes within their namespaces. That's unacceptable. In C#, people quite often do not nest cascaded using statements. That's unacceptable. The only case where I sometimes violate this rule, and I am not yet completely decided on how to go about it, is with single statement functions in Java, which I sometimes code in one line, not because I believe this is correct, but because I am expressing a wish that Java would offer a functional style of declaring functions the way C# does. Type identifier casing: SentenceCase Even if the type is private. Constant identifier casing: SentenceCase Even if the constant is private. Private member identifier casing: camelCase A very popular choice is prefixing the identifier with _ or m_; that's unacceptable. Public member identifier casing: C#: SentenceCase Java: camelCase The camelCase choice of Java is retarded, but it would be too heretic even for me to go against it, mainly because there exist tools that use reflection to guess what methods are getters and setters, and everything goes haywire if the capitalization is not what these tools expect. Static fields: Same as other fields Note: this explicitly means that static fields must not be named differently from other fields. Some people like doing weird things like prefixing static fields with s_. That's not only mighty ugly, but also entirely unnecessary, because any half-decent IDE will color-code static fields for you. Acronyms: SentenceCase In other words, never use \u0026quot;GUID\u0026quot;; always use \u0026quot;Guid\u0026quot;. The acronym becomes a word, so that it can be added to the spell-checker. Speaking of spell checkers: The spell-checker must always be on Every commit must pass inspection by the spell-checker The spell-checker wordlist must be committed like any other file The spell-checker wordlist must pass code review like anything else. That's how the quality of the codebase can be protected despite contributions from people with poor command of the English language. Explicit this: Never Unless a field is receiving its value from a method or constructor parameter, in which case the parameter must have the exact same name as the field, and subsequently this is necessary in order to refer to the field. Use of var: Rarely Only for non-trivial types, and only when the type is obvious. Of course, you might ask, when is the type obvious? The answer is simple: the type is obvious only when the name of the type is present on the right side of the assignment. Naming of files and classes: One class per file, exact same name In Java this is standard, but there is one exception: Java makes it impossible to access constructor parameters from field initializers. The solution to this is to pass the constructor parameter to the superclass, so that it can be stored in a protected member, so that it can be accessed by the field initializers of descendants. Quite often, we invent superclasses for no reason other than to be able to do just that. In these cases, it is okay (preferable even) if the superclass is package-private, and declared in the same file as the descendant. In C# one class per file with exact same name is not standard, so it is worth stating. Again, there are a few exceptions: It is okay to declare all the classes that make up a small class hierarchy in a single file, as long as the file is named after the base class of the hierarchy. It is also okay to declare trivial types like enums and delegates in the same file as the class that they conceptually belong to. Namespace imports (C# only): Inside namespace declarations Most people import their namespaces outside of their namespace declarations. This style guide mandates the opposite: namespaces must be imported inside namespace declarations. In other words, first we open our namespace, then we declare our imports, then we declare our class. This is in accordance with the Principle of Smallest Scope, i.e. any given thing must have the smallest scope that it can possibly have. Namespace Aliases (C# only) For namespaces defined in the solution: Never If you have defined a namespace in your solution, then you should never need to alias it. If it conflicts with a namespace defined outside your solution, then you should alias the external namespace. For namespaces defined outside of the solution: Almost always I have the habit of aliasing all external namespaces so as to make it evident exactly where each type is coming from. So for example, I never do using System.Text and reference Encoding.UTF8; I always do using SysText = System.Text and then I reference SysText.Encoding.UTF8. I make an exception for namespaces System and System.Collections.Generic. Non-ANSI characters: Via Unicode Escape Sequences That's because every once in a while some tool will garble non-ANSI characters by accident, and a) that's the kind of error that you will usually have no tests for, while b) even if there is a test, the non-ANSI character in the test might be also garbled, causing the test to pass, while it should fail. Miscellaneous If something can be private, it must be private. If something can be final/readonly, it must be final/readonly. If something can be final/sealed, it must be final/sealed. If something can be of a less-derived type, it must be of a less-derived type Unless you want to document something important; for example, you may want to use a List instead of a Collection to indicate that order matters. If a string literal can be replaced with nameof, it must be replaced with nameof. If a pair of parentheses can be omitted, it must be omitted. Unless operator precedence is unclear and requires clarification. Note that this means that the expression after the return keyword must never be parenthesized. Overriding methods must not have documentation comments. The documentation comment of an override is the documentation comment of the method it overrides. ","date":"2018-04-05T17:29:00.11Z","permalink":"https://blog.michael.gr/post/2018-04-on-coding-style/","title":"My Very Own™ Coding Style"},{"content":" The mikenakis-agentclaire logo\nbased on a piece of clip art found on the interwebz. GitHub project: AgentClaire\nA Java Agent to end all Java Agents.\nNOTE:\nThis project has been retired. The github link does not even work anymore.\nThis page only serves historical documentation purposes.\nThe Problem Java Agents are a feature of the Java Virtual Machine which is used for transforming the bytecode of classes as they are loaded and before they are used. We register a Java Agent with the JVM at startup, and each time a class is loaded, the JVM will invoke our java agent to give it a chance to transform the class. The JVM supplies the Java Agent with each class to be transformed in the form of an array of bytes, and expects the Java Agent to return the transformed class also as an array of bytes.\nThis means that the Java Agent has to parse the array of bytes in order to build some kind of object graph representing the class, manipulate the object graph to apply the intended transformation, and then re-pack the object graph into another array of bytes before returning it to the JVM.\nFurthermore, the class being transformed may reference other classes, which the Java Agent may need to examine in order to perform the intended transformation, so for each class being transformed the java agent may need to load multiple arrays of bytes from the filesystem, and do more parsing of bytes and building of object graphs in order to make sense out of them.\nNow, a program may be started with multiple Java Agents attached to it, so all this parsing and repacking will be repeated by each Java Agent.\nThis represents an insane amount of overhead.\nMost people do not mind this overhead, because it is only incurred during application startup, and for some reason it has come to be that the entire computing industry today is resigned to slow-as-molasses startup times that are simply to be endured as a fact of life.\nI beg to differ. I like things to be snappy, especially at startup, because as a programmer, waiting for the application that I am developing to start up tends to represent a considerable portion of my working day.\nAlso, the same bytecode transformations are usually performed when running tests, and I want my tests to be running as quickly as possible, without unnecessary overhead.\nThe Solution AgentClaire is a Java Agent that can be used to simplify and optimize the process of examining and transforming bytecode during application startup.\nWith AgentClaire, instead of writing Java Agents, you write AgentClaire Interceptors.\nThe difference between a Java Agent and an AgentClaire Interceptor is that instead of receiving an array of bytes and returning an array of bytes, the interceptor is given an instance of ByteCodeType (see the mikenakis-bytecode project) which has been constructed by parsing the original array of bytes just once.\nThen, once each AgentClaire Interceptor has had a chance to modify the ByteCodeType, AgentClaire will take care of repacking it just once into an array of bytes before returning it to the JVM\nThis way, a considerable amount of time is saved during startup by parsing and repacking bytes only once per class, instead of once per class per java agent.\nFurthermore, the Umbilical interface that is provided by AgentClaire contains a ByteCodeService service which can be used to obtain an instance of ByteCodeType given a class name and a ClassLoader. The ByteCodeService achieves this either by looking up the ByteCodeType from a cache of all those that have already been loaded, or by loading the class bytes and parsing them to construct a new instance of ByteCodeType.\nThis way, an interceptor can very easily and very efficiently obtain classes that are referenced by the class being transformed. Furthermore, the ByteCodeService is also available to the main program, so it can have read-only access to bytecode information about its own classes. This way, not a single class ever needs to be parsed from bytes more than once throughout the lifetime of the JVM.\nUsage AgentClaire is started via the -javaagent option of the JVM. The option looks like this:\n-javaagent:\u0026lt;AgentClaire-jar\u0026gt;=\u0026lt;interceptor-jar\u0026gt;[\u0026lt;another-interceptor-jar\u0026gt;...]\nThis will cause the JVM to load AgentClaire before the main program runs, and AgentClaire will then load each of the specified interceptor jars. Each interceptor jar must contain a MANIFEST.MF with an AgentClaire-Interceptor-Class entry that gives the fully qualified name of a class to be instantiated by AgentClaire. (No stinkin' static entry points.)\nThis class must have a constructor which accepts two parameters:\na Umbilical interface (See the mikenakis-umbilical project)\nan AgentClaire interface (See the mikenakis.agentclaire-main project.)\nThe constructed class can use the AgentClaire interface to register itself as an Interceptor. Once this is done, then the interceptor's entrypoint will be invoked for each class that is loaded by the JVM, giving the interceptor a chance to modify it.\nHosted on GitHub: https://github.com/mikenakis/mikenakis-agentclaire\nLicense This creative work is explicitly published under No License. This means that I remain the exclusive copyright holder of this creative work, and you may not do anything with it other than view its source code and admire it. More information here: Open Source but No License.\nIf you would like to do anything more with this creative work, please contact me.\n","date":"2018-04-04T20:14:46.355Z","permalink":"https://blog.michael.gr/post/2018-04-github-project-agentclaire/","title":"GitHub project: mikenakis-agentclaire"},{"content":"I have posted some small projects of mine on GitHub, mainly so that prospective employers can appreciate my skills. I am not quite ready to truly open source them, so I published them under \u0026quot;No License\u0026quot;. This means that I remain the exclusive copyright holder of these creative works, and nobody else can use, copy, distribute, or modify them in any way, shape or form. More information here: choosealicense.com - \u0026quot;No License\u0026quot; (https://choosealicense.com/no-permission/).\nPretty much the only thing one can legally do with these creative works is view their source code and admire it.\nGitHub says that one can also make a copy of my projects, (called fork in GitHub parlance,) but I am not sure what one would gain from doing that, because you cannot legally do anything with the forked code other than view it and admire it. Even more information here: Open Source SE - GitHub's “forking right” and “All rights reserved” projects (https://opensource.stackexchange.com/q/1154/10201)\n(Okay, if you compile any of my projects and run it once or twice in order to check it out, I promise I will turn a blind eye.)\nIf you want to do anything more with any of these projects, contact me.\n","date":"2018-04-04T05:16:09.09Z","permalink":"https://blog.michael.gr/post/2018-04-open-source-but-no-license/","title":"Open Source but No License"},{"content":" Inntel Hotel at Amsterdam, Zaandam Table of Contents What is full-stack development\nWhy is full-stack development necessary today\nWhat is wrong with full-stack development\nConclusion\n(Useful pre-reading: About these papers)\nWhat is full-stack development The predominant web application development model today requires splitting application logic in two parts:\nThe front-end, running on the browser. The back-end, running on the server. The front-end is typically written in JavaScript, while the back-end is typically written in Java, Scala, C#, or some other programming language. The two ends invariably communicate with each other via REST. The choice of JavaScript and REST is not due to any technical merit inherent in these technologies, (there is none,) but purely due to historical accident; see The Wild, Wild Web.\nA web application developer can either focus on one part of the stack, or work on both parts. Due to reasons that will be explained further down, more often than not, web developers are asked to work on both parts simultaneously. When this happens, it is known as full-stack development.\nFor the purposes of this paper, we will call full-stack development not just this mode of work, but also this architectural style as a whole: full-stack development is when application logic must be written both on the server and on the client.\nFull-Stack Development is a paradox, since it suggests a way of work which is contrary to what common sense dictates. Common sense calls for specialists each working on their own area of specialization, so one would expect to see different developers focusing on different layers of the stack, and nobody ever attempting something as preposterous as working on all layers simultaneously. However, there is a technological hurdle which renders this necessary today.\nWhy is full-stack development necessary today Normally, (outside of web application development,) in a system that consists of multiple layers, only one of the layers tends to be application-specific, while all other layers tend to be general purpose infrastructure layers that are agnostic of any application that might put them to use. Under such an arrangement, the functionality offered by each layer is dictated by what makes sense for that layer to be doing, so the work to be done at each layer tends to be rather self-contained and straightforward. In this scenario, each specialist can indeed work on the layer that they specialize in.\nHowever, in web development we have a server, and we have a client, and so far we have been unable to find a solution that would allow us to confine all of our application logic to only one of them. (There have been some attempts in that direction, but they were only moderately successful, and virtually none of them survived the transition from monolithic architectures to microservices architectures.) As a result, in modern web applications, both layers are application-specific.\nIn the early days people did try to apply specialization and division of labor to web application development, and they found that when all the layers are application-specific, collaboration between teams working on different layers suffers, resulting in low productivity. There are too many details that have to be agreed upon by people working on different layers; too much waiting for the guys working on the layer below to finish their part before the guys working on layer above can do their job; too much disagreement as to whose fault it is when the system is not working as expected; in general, too much back and forth, too much friction.\nFor this reason, full-stack development was invented: instead of dividing the workforce horizontally, it ends up being less inefficient to divide them vertically: when each developer works on a different feature of the product from top to bottom, they do not have to interact too intensively with other developers, and this represents a gain which seems to offset the loss of not having specialists working on their respective areas of specialization.\nWhat is wrong with full-stack development In brief, full-stack development has the following disadvantages:\nThe front-end: Has limited capabilities. Is confined within the sand-boxed execution environment of the browser. Admittedly, browsers today are pretty feature-rich, (actually, monstrously so,) but still, you are writing code which is running out there, on browsers, and is therefore out of your control, instead of here, on the server, where you do have control. So, there are always things that you would like to accomplish, but you cannot on the client, so you have to suffer the additional bureaucracy of having the client communicate what you are trying to accomplish to the server, having the server do it for you, and receiving the results back on the client. That?s an awful lot of work for something as simple as, say, obtaining the current date and time regardless of client configuration or misconfiguration. Suffers from incidental complexity. Peculiarities of the browser environment such as URLs, HTML, the DOM, HTTP, REST, Ajax, etc. Cross-browser incompatibilities and cross-browser-version incompatibilities. Security hazards. Code on the client must not only accomplish application goals, but it must do so while avoiding various commonly known and not-so-commonly known security pitfalls. Each time a new security hazard is discovered by the security community, vast amounts of application code must be meticulously audited and painstakingly fixed. Must be re-written on each targeted format (web, mobile, desktop.) When targeting a new format besides the web (e.g. desktop, mobile) we have to re-engineer not only the presentation markup, but also all of the application logic which is inextricably mangled with it. This necessitates the creation and maintenance of multiple separate code bases that largely duplicate the functionality of each other. These code bases are liable to diverge, thus causing user workflows and overall user experience to unwantedly differ across formats. Is usually written in a scripting language. The code is error-prone due to scripting languages being untyped. The code is hard to maintain due to untyped languages being impervious to refactoring. The code is messy due to scripting languages invariably being inferior to real programming languages. The code is transmitted in source code form to the browser, thus exposing potentially sensitive intellectual property. Is usually written in JavaScript in particular. JavaScript was originally intended for no more than a few, tiny, and isolated snippets of code per HTML page. The haphazardness of the language design reflects this intention. However, modern web applications tend to contain tens of thousands of lines of application-specific JavaScript. That is an awful lot of code in a language which is defective by design. Excludes artists. Artists are prevented from actively participating in the creation and maintenance of web pages, because HTML is inextricably mangled with JavaScript, so they cannot touch it. Thus, artists are resigned to creating mock-ups showing how they want web pages to look like, and programmers are then tasked with making the web pages look like the mockups. (As if the programmers did not already have enough in their hands.) The back-end: Is inextricably tied to REST This is because REST is impervious to abstraction. REST forces reliance on binding-by-name, which undermines the coherence of the entire system and prevents static code analysis, invariably resulting in a big unknown chaos. Duplicates part of the client-side application logic. This is necessary in order to perform validation on the server-side too, because from a security standpoint the client must always be considered compromised. This translates to additional development and maintenance cost. Inevitable discrepancies between the validation done on the client and the validation done on the server are a continuous source of bugs. The application as a whole: Is split in two parts. Usually having each part written in a different programming language. Having The Internet interjected between the two parts. Having the point of split dictated not by business considerations, but by technological limitations instead. Mixes application with presentation. A fundamental principle of graphical user interface application development is that application logic should be kept completely separate from presentation logic. This principle warns against inadvertently allowing application logic to bleed into the presentation layer; however, with full-stack development we have application logic not just bleeding to the presentation layer, but actually embarking on a massive deliberate large-scale exodus to the presentation layer. One might naively think that full-stack development accomplishes separation by keeping application logic on the server and presentation logic on the client, but this is demonstrably not so: The server is largely reduced to a bunch of dumb REST endpoints that perform not much more than Create, Read, Update, Delete, List (CRUDL) operations with validation. That is not application logic; that's mostly just querying and updating the data store. The client not only decides how things should look, but it also decides what options should be available to the user at any moment, and what new options will become available to the user as a result of user actions. Essentially, all application workflows are implemented on the client. That's application logic par excellence. Is hard to test. The front-end is not functional without the back-end, so the two ends usually have to be tested in integration, necessitating such monstrosities as Selenium. Prevents specialization and division of labor. Full-stack development necessitates The Full-stack Developer, who is: a front-end programmer, a back-end programmer, a network programmer, a security expert, a user experience expert, an accessibility expert, and a graphic artist ? all rolled into one, thus running the risk of being a *jack of all trades, master of none*. Conclusion By its nature, web application development requires systems that consist of multiple layers; the current state of affairs is such that application-specific code must be running on each of these layers, and this is called full-stack development. However, as I have shown, full-stack development has a list of disadvantages which is rather extensive, and each of these disadvantages is rather severe.\nEssentially, we are suffering the consequences of a technological limitation: we currently have no means of confining all application logic to the server, so we have to be placing application logic on the client too, so we have no option but to be engaging in full-stack development.\nTechnological limitations require technological solutions, but companies with commercial goals do not usually take it upon themselves to solve the world's technological problems. Instead, they tend to make do with the existing problems, providing non-technological workarounds to them, such as throwing more manpower into the development effort. This might make sense for each individual company, but from a global perspective, we have collectively been too busy mopping the floor to turn off the faucet.\nA solution that would confine all application logic to the server and thus eliminate full-stack development has the potential of being very beneficial to the industry as a whole.\n","date":"2018-04-01T14:37:37.806Z","permalink":"https://blog.michael.gr/post/2021-12-full-stack-development/","title":"What is wrong with Full Stack Development"},{"content":" Here is Douglas Crockford,\ntalking patent nonsense about Java and about exceptions,\nneither of which he understands, obviously.\nStart playing at 27':42''. The insanity lasts until 32':00''.\nEnjoy responsibly.\n","date":"2018-03-24T14:30:34.424Z","permalink":"https://blog.michael.gr/post/2018-03-douglas-crockford-talking-nonsense/","title":"Douglas Crockford talking nonsense"},{"content":"\rI will try to make a list of items here, but I could probably write a book on this.\n(Useful pre-reading: About these papers)\nAssert everything Assertions take care of white-box testing your code, so that automated software testing can be confined to the realm of strictly black-box testing, as it should. Assertions do not execute on release builds / production runs, so they essentially cost nothing. This means that you can go wild with them:\nGo ahead and assert that your array is sorted before performing binary search on it. Verify that your binary search worked correctly by comparing its result against the result of a linear search for the same item. Yes, the time complexity of these assertions is far greater than the time complexity of the operation that they guard, and this is perfectly fine, because:\nRemember, assertions do not execute on release runs, so they cost nothing. On test runs, you are not supposed to be using large amounts of data anyway. When N is small, then O(N) and even O(N2) are not very different from O(log2(N)), which means that even when assertions do execute, they do not matter. To the small extent that assertions might nonetheless slow you down during development, you can see it as one more reason why you, as a developer, should have a computer which is much more powerful than the computers of mere mortals --er, I mean, users. When I look at code, I don't ask myself \u0026quot;should I assert that?\u0026quot; Instead, I ask myself \u0026quot;is there anything that I forgot to assert?\u0026quot; The idea is to assert everything that could possibly be asserted, leave nothing assertable unasserted. I call this The Maximalistic Approach to Error Checking, in contrast to the predominant minimalistic approach, where programmers decide on a case by case basis whether to assert something or not, based on completely-oblivious-of-Murphy's-law assumptions about how likely it is to go wrong, inappropriately mixed with misguided performance considerations.\nFor more information, see Assertions and Testing.\nAlso note that the attention horizon of code is the function, so if function f1() asserts some condition and then invokes function f2(), it is perfectly fine for f2() to also assert the same condition. In other words, whether something has already been asserted or not by some other function is irrelevant: each function must assert every condition that pertains to it.\nDo black-box testing, avoid white-box testing Heed the advice that says test against the interface, not the implementation. Unit Testing is testing against the implementation, so despite the entire software industry's addiction to it, it should be avoided. Incidentally, this means that mocking, despite being an admirably nifty trick, should never be used: if you are using mocks then you are doing white-box testing, so you are doing it wrong.\nFor more on why Unit Testing is white-box testing, and why white-box testing is bad, read this: White-Box vs. Black-Box Testing For more on why mocks in particular are especially bad, read this: If you are using mock objects you are doing it wrong. For what to use instead of mocks, read this: Testing with Fakes instead of Mocks For what to do instead of unit testing, read this: Incremental Integration Testing If for some reason you must do white-box testing, then you can at least avoid having to do it in code; read this: Audit Testing and this: Collaboration Monitoring Avoid non-determinism in tests Testing must be completely free from non-determinism under all circumstances. Since testing code exercises production code, this means that production code must also be free from non-determinism, or at the very least any source of non-determinism in production code must be replaceable during testing with a fake which is completely deterministic. For example:\nNever rely on the garbage-collector doing anything other than reclaiming memory; specifically, never rely on any cleanup operations being initiated by the garbage-collector. Perform all cleanup explicitly. For more information, see Object Lifetime Awareness. Never allow any external factors such as file creation times, IP addresses resolved from DNS, etc. to enter into the tests. Fake your file-system; fake The Internet if necessary. Never use wall-clock time; always fake the clock, making it start from some arbitrary fixed origin and incrementing by a fixed amount each time it is queried. Never use random numbers; if randomness is necessary in some scenario, then fake it using a pseudo-random number generator seeded with a known fixed value. This includes all constructs that utilize randomness, for example GUIDs/UUIDs. Never allow any concurrency during testing; all components must be tested while running strictly single-threaded, or at the very least multi-threaded but in lock-step fashion. Minimize state, maximize immutability Design so that as much code as possible is dealing with data that is immutable. Re-examine every single class which contains mutable members, and many chances are you will find that it could be replaced with an immutable class. Even if not, you might discover that many of its members could be immutable.\nEschew frameworks, technologies, and techniques that prevent or hinder immutability. For example, if you are using some dependency-injection (DI) facility that provides you with auto-wiring, use constructor injection only, so that you can always store in final/readonly members. If your DI facility does not support constructor injection, throw away everything and start from scratch with one that does.\nNote, however, that immutability is not important in function-local variables. There is absolutely nothing wrong with function-local mutation if it serves the slightest purpose. Which brings us to the next point:\nDo overwrite function parameters There exists a widespread cargo cult habit among programmers, of never overwriting the value of a parameter to a function within the function. This habit is so unquestioned that it enjoys \u0026quot;best practice\u0026quot; status, despite being completely misguided. Some languages (e.g. Scala) even prohibit it, which is deplorable. Go ahead and overwrite function parameters (if your language allows it) when the original parameter value should not be used in the remainder of the function. In doing so you are minimizing the number of variables that are in scope, and preventing accidental use of the original value.\nThe historical origins of the practice of never overwriting function parameters are actually quite funny: some early versions of Fortran (the first programming language) used to pass everything by reference, including constants. So, if you had function F(X) which was invoked with 3 for X, and within F(X) you assigned 5 to x, then from that moment on the constant 3 would actually have the value 5 in your entire program. As a result, early computer scientists decreed that function parameters should never be reassigned. Fortran was soon fixed to correct this problem, but the advise kept being passed from generation to generation of programmers, who have been accepting it without rethinking it. This is cargo cult programming at its finest.\nAvoid Hail-Mary Initializations Contrary to what many people falsely think of as \u0026quot;common knowledge\u0026quot; and \u0026quot;best practice\u0026quot;, you should never initialize a variable before you have a meaningful value to assign to it. For more information, see Hail-Mary Initialization.\nAvoid \u0026quot;b-to-a\u0026quot; style conversions, use \u0026quot;a-from-b\u0026quot; style instead When I see A = AfromB( B ) I can immediately tell that it looks correct, since A is on the side of A and B is on the side of B. However, when I see B = AtoB( A ) I have to stare at it for a few milliseconds longer before I can tell whether it is correct or not. Of course, this is a trivial example: in real-world situations, the identifiers, as well as the call chain, could be much longer and much more complicated. This is related to Joel Spolsky's notion that wrong code should look wrong, and it is especially important since the entire industry has traditionally been doing it in precisely the wrong way with B-to-A style conversions.\nAvoid Yoda conditionals This is the practice of reversing the terms around the equality operator when one of the terms is a constant. You might have seen it the following forms:\nif( 5 == a ) instead of the normal if ( a == 5 ). if( \u0026quot;x\u0026quot;.equals( b ) ) instead of the normal if( b.equals( \u0026quot;x\u0026quot; ) ). Don't do this. The Principle of Least Surprise is not just violated by this construct, it is gang-raped. Plus, in doing this you are most probably engaging in the cardinal sin of silent failure. Here are the reasons often cited for using Yoda conditionals, and their rebuttals:\nAlleged reason #1\nStatement: It will catch accidental use of the assignment operator where the equality operator was intended.\nRebuttal: Such accidental use should be impossible because your compiler or your IDE should be issuing a warning if you try to do this. If you are not receiving a warning, then you have other, much bigger problems in need of solving, i.e. using the wrong programming language, using the wrong IDE, or trying to write code without first having figured out how to enable all warnings.\nAlleged reason #2\nStatement: It works even if the variable accidentally happens to be null.\nRebuttal: No, it does not work; it silently fails. If you follow offensive programming, the definition of \u0026quot;it works\u0026quot; is that it produces correct results when given valid input, and it decisively fails when given invalid input.\nSo, there are two possibilities: either the variable may legitimately be null, or it may not.\nif the variable may legitimately be null, then make it evident by explicitly checking against null. if the variable may not legitimately be null, then write the code so that it will not fail to fail if the variable ever turns out to be null. Avoid unnecessary braces Doing so keeps the code more compact, making more statements fit within the screen. The cargo-cult programming convention of enclosing even single-statement blocks within curly braces allegedly avoids bugs caused by trying to add a second statement to the block while forgetting to introduce curly braces.\nThis has actually happened to me once, and the programmer who introduced the bug in my code did not even apologize, because he considered it my fault for not having provided the curly braces for him to insert his second statement in.\nThe fact of the matter is that a decent IDE will point out such a mistake as a formatting violation, so this is not a problem today. Of course, in order to enable the IDE to point out formatting violations you must be keeping a consistent indentation style everywhere, right? Right?\nAvoid Egyptian-style curly braces People who use Egyptian-style curly braces essentially treat them as noise. I would very much favor a programming language where nesting is based on indentation alone, thus requiring no curly braces; unfortunately, the only such language that I know of is Python, which is a scripting language, and therefore out of the question; so, for as long as we are using programming languages that require curly braces, we have to pay attention to them and we cannot just treat them as noise; therefore, absolutely all curly braces must absolutely always be perfectly aligned; period, end of story, discussion is locked and comments are closed.\nMinimize flow control statements Especially the if statement. If there is any opportunity to structure a piece of code so as to eliminate an if statement, the opportunity should be pursued tenaciously.\nOf course, by this I do not mean replacing if statements with the conditional operator ( a ? x : y ); the conditional operator is nice, because it makes code more expressive and compact, but it is equivalent to an if statement, so it too should be eliminated when possible.\nThe if statement can be avoided in many cases with the use of calculations, with lookup tables, with the judicious application of inheritance and polymorphism, etc.\nFavor one and only one way of doing any given thing If you ask a hundred programmers to write some code that accomplishes a certain simple task, you will get a hundred different solutions. These solutions will reflect different ways of thinking, which is inevitable, but they will also reflect different coding conventions, which is entirely unnecessary. So, it is a good idea to establish coding conventions that minimize unnecessary differences.\nOne easy way to achieve this is to stipulate that any construct which is optional must be omitted. For example:\nDisallow extra parentheses in expressions.\nUnfortunately, compilers by default allow superfluous parentheses without complaining. This has fostered the development of some truly bizarre habits among programmers, such as the construct return (x); which is so common that some folks are under the impression that this is the correct syntax, and that return x; would constitute a syntax error. Well, guess what: return x; is the correct syntax, whereas return (x); contains a pair of superfluous parentheses. Configure your compiler or your code analysis tool-set to disallow unnecessary parentheses, so that all code that accomplishes the same thing looks the same.\nIf you do this, then the tooling will also complain about parentheses that you might be using elsewhere to clarify the order in which calculations are to be performed when you are unsure about the operator precedence rules of the language. Here is what I have to say about that:\nYour programming language has a very specific, very well documented, and rather small set of rules that govern operator precedence; these rules are fundamental, and this programming language is your bread and butter; so, learn them. Learn them all by heart, so that you are never unsure about operator precedence, so that you never need extra parentheses for clarification.\nDisallow optional keywords.\nIn many languages, certain keywords are implied by default and can be omitted. Unfortunately, in virtually all example code out there, such keywords tend to always be included, which leads people to form the impression that they must be mandatory.\nFor example, did you know that in C# every class is internal by default? This means that you never have to say internal class Foo { ... }, you can simply say class Foo { ... }.\nFurthermore, did you know that in C# every class member is private by default? This means that you never have to say private int foo() { ... }, you can simply say int foo() { ... }.\nAgain, it is fundamental rules of the language that govern these things, which means that every programmer should know them by heart, which in turn means that nobody should be surprised to see int foo() { ... }, and nobody should be wondering what the visibility of foo() is.\nDisallow the var keyword.\nIf we were to mandate that two lines of code should look identical if they accomplish the same thing, we have two options: either always require the var keyword, or completely disallow it.\nAlways requiring the var keyword is not an option, because in many cases the type cannot be inferred from the right hand side, so it must be specified. Thus, we are only left with the option of completely disallowing it, and that is the way to go.\nFurthermore, as I explain elsewhere, \u0026quot;absolutely any choice that makes code easier to read is absolutely always preferable over absolutely any choice that makes code easier to write\u0026quot;, and the var keyword is a prime example of a choice which is easy to write but makes code harder to read, so we should not even be debating this.\nIf you are not sure about the exact type of the right-hand side of an assignment, or if you do not want to be bothered with having to type it, is perfectly okay to begin with var x = ..., and once you have written your entire statement you go back to the var keyword, and ask your IDE to refactor it and replace it with the actual type.\nThe var keyword is only useful in type casts; I would rather say var x = (int)y; than int x = (int)y; however, the benefits of being able to disallow var with a rule outweigh the convenience of being able to use it in type casts.\nPut the complexity in the design, not in the code If the code does not look so simple that even an idiot can understand it, then the code is too complex. When this happens, it usually means that shortcuts were taken in the design, which had to be compensated for with overly complex code. Make the design as elaborate as necessary so that the code can be as simple as possible. Overly complex code is usually the result of violations of the Single Responsibility Principle. Which brings us to the next point:\nAdhere to the Single Responsibility Principle like your life depends on it Often, what you think of as a single responsibility can in fact be further sub-divided into a number of more fundamental responsibilities. Almost all of the code that we write performs, or can be thought of as performing, some kind of transformation, involving a certain number of participants. For example:\nAt the lowest level, an assignment operation transforms each bit of the destination variable into the corresponding bit of the source variable. Obviously it involves two participants: the source and the destination. At the highest level, a shopping web site transforms relational data and user input into pixels on the user's browser window and purchase orders in the logistics department. In this simplified view we have four participants, realistically we have many more. Most transformations are of the simplest kind, involving only two participants, transforming one into the other. That's great, that's a single responsibility: convert A to B. Many transformations involve three participants, A, B and C, and they tend to be appreciably complex. In some cases they can be simplified into successive operations, one to go from A to B and another to go from B to C, meaning that there were in fact two different responsibilities which were identified and realized as separate steps. However, quite often they cannot be simplified, as for example when we are converting A to C by consulting B. That's a single responsibility which cannot be further broken down. All to often, people manage to involve four or more participants in a single transformation. These tend to be grotesquely complex, and they invariably constitute violations of the single responsibility principle. It goes without saying that they must be avoided at all costs. Luckily, operations that involve more than 3 participants can always be refactored into multiple successive transformations of no more than 3 participants each, by introducing intermediate participant types if necessary. (I have never heard of this being suggested by anyone before, so this could perhaps be The Mike Nakis Postulate for Simplification.)\nRefactor at the slightest indication that refactoring is due Do not allow technical debt to accumulate. Avoid the situation of being too busy mopping the floor to turn off the faucet. Allow a percentage of sprints to explicitly handle nothing but technical debt elimination. Do not try to spread the task of refactoring over feature development sprints, because:\nThe refactoring effort will not magically disappear. Focus will be diluted. Time estimations will suffer. Managers who feel that every sprint must involve some feature development or else it does not look good on their report should be removed from their positions and be given jobs milking goats.\nStrive for abstraction and generalization The urge to abstract and generalize is often mistaken as having reusability as its sole aim, so it is often met with the YAGNI objection: \u0026quot;You Ain't Gonna Need It\u0026quot;. The objection is useful to keep in mind so as to avoid over-engineering, but it should not be followed blindly, because abstraction and generalization have important inherent benefits, regardless of the promise of reusability.\nEvery problem of a certain complexity and above, no matter how application-specific it might seem to be, can benefit from being divided into a specialized, application-specific part, and an abstract, general-purpose part. Strive to look for such divisions and realize them in the design.\nThe application-specific part will be simpler to write and to understand, because it will be free from the incidental complexity represented by the general-purpose part. The general-purpose part will be simpler to write and to understand, because it will be implementing a self-contained abstraction that can be independently reasoned about. Also, the general-purpose part will be fully testable on its own, so you will have assurances that it works, regardless of how the application-specific part uses it, and regardless of how the application-specific part evolves over time. Note that the above benefits come in addition to the potential benefit of reusability.\nIn other words, if you can choose between the following two:\nadding 5 lines of application code, vs. adding only 2 lines of application code but a whole 10 lines of infrastructure code then opt for the latter, even if these 10 lines of infrastructure code are unlikely to ever be reused. Saving 3 lines of application code is worth writing an extra 10 lines of infrastructure code.\nUse abstraction even in the spoken language People have the unfortunate tendency of using the most specific term for any given thing, rather than the most abstract term. I am not sure why people do this, perhaps it is addiction to technicality, perhaps it is trying to sound smart, but it often ends up causing miscommunication. For example:\nif your application has a settings file, and this file happens to be a json file, people are likely to form a habit of calling it \u0026quot;the json file\u0026quot; instead of \u0026quot;the settings file\u0026quot;. if your application stores session state information in a key-value store, and that store happens to be a Redis instance, people are likely to say \u0026quot;send it to Redis\u0026quot; instead of \u0026quot;send it to the session state store\u0026quot;. Identify such unwarranted technicalisms and encourage people to switch to using the abstract terms instead. Tell them that the json file was replaced with a yaml file today, and when they all start calling it the yaml file, tell them that it is now an xml file. When they start complaining, tell them that the particular file format of the settings file is none of their business, and they should be calling it by its proper name, which is \u0026quot;the settings file\u0026quot;.\nAvoid false abstractions Sometimes programmers give abstract names to things that are not really abstract. For example:\nso-called serialization frameworks which expose details of the underlying file format, meaning that they are only capable of serializing to and from that specific file format. A serialization framework which exposes XML-specific details should not be called a \u0026quot;Serialization Framework\u0026quot;; it should be called an \u0026quot;XML Serialization Framework\u0026quot;. in NuGet (the predominant package manager in DotNet) a version is said to consist of a version prefix and a version suffix, however the toolset interprets the two in a very specific way: the version prefix is not really a prefix, it is the actual version, and the version suffix is not really a suffix, it is a pre-release version identifier. These are examples of pretending that things are more abstract than they really are, which causes misinformation and suffering.\nUse domain-specific interfaces Encapsulate third party libraries behind interfaces of your own devise, tailored to your specific application domain. Strive to make it so that any third-party library can be swapped with another product without you having to rewrite application logic.\nConventional wisdom says the opposite: we have all heard arguments like:\n\u0026quot;The best code is the code you don't write\u0026quot; (makes me want to invest in the business of not writing software) \u0026quot;A third-party library will be better documented than your stuff\u0026quot; (presumably because documentation is a skill your developers have not mastered) \u0026quot;If you run into trouble with a library, you can ask for help on Stack Overflow, whereas with something you have developed in-house, you are stuck\u0026quot; (presumably because your developers know nothing of it, despite working with it every day.) The truth with application development is that the more you isolate the application logic from peripheral technologies, the more resilient your application logic becomes to the ever changing technological landscape, a considerable part of which is nothing but ephemeral fashions, the use of which is dictated not by actual technological merit, but by C.V. Driven Development1 instead.\nIncidentally, this also means one more thing:\nFavor libraries over frameworks The difference between a framework and a library is, simply speaking, that a library is something that your code invokes, whereas a framework is something that invokes your code. The problem with frameworks is that it is impossible to abstract them away behind custom interfaces; therefore, any code you write using a particular framework will forever be a prisoner of that framework: it will be extremely difficult to replace that framework with a different one without rewriting all your code.\nStrive for what is simple, not for what looks easy The simple often coincides with the easy, but sometimes the two are at odds with each other. Eschew languages and frameworks that provide the illusion of easiness at the expense of simplicity. The fact that a particular toolset makes \u0026quot;hello, world!\u0026quot; an easy one-liner probably means that the hundred-thousand liner that you are actually aiming for will be unnecessarily complicated and hard to write.\nWatch this: https://www.infoq.com/presentations/Simple-Made-Easy\nAvoid binding by name like the plague Avoid as much as possible mechanisms whose modus operandi is binding by name: use them only for interfacing with external entities, never for communication between your own modules. REST enthusiasts can cry me a river.\nNote that binding by name must be avoided even in comments. If you need to refer to an identifier from within a comment, use whatever special notation is offered by the language at hand ({@link ...} in java, \u0026lt;see cref=\u0026quot;...\u0026quot;\u0026gt; in C#) so that when you later refactor the name of that identifier, the IDE will also update any comments that mention that identifier.\nAlways use strong typing Avoid any kind of weak typing (euphemistically called dynamic typing) and avoid languages and frameworks that require it or even just sympathize with it. Yes, this includes all scripting languages. Scripting language enthusiasts can cry me a river. (And yes, this includes Typescript too, because it sympathizes with JavaScript.)\nRead this: On Scripting Languages.\nStrive for debuggability For example, do not overdo it with the so-called \u0026quot;fluent\u0026quot; style of invocations, because they are not particularly debuggable. Do not hurry to adopt this or that cool new programming language before you have made sure that debugger support for it is complete and working properly.\nResist the idiomatic craze Contrary to popular belief, doing things in whatever way is considered idiomatic for the programming language at hand is never an end in and of itself; Avoid the use of idiomatic ways of doing things unless you are convinced they are superior. Many of them are, but some of them are not.\nStrive for testability Design interfaces that expose all functionality that makes sense to expose, not only functionality that is known to be needed by the code that will invoke them. For example, the application may only need an interface to expose a register() and unregister() pair of methods, but isRegistered() also makes sense to expose, and it will incidentally facilitate black-box testing.\nEnable all warnings that can be enabled The fact that a certain warning may on occasion be issued on legitimate code is no reason to disable the warning: the warning must be enabled, and each occurrence of the warning must be dealt with on a case-by-case basis.\nThe best way to deal with a warning is to resolve it. For example: If your compiler is warning you that a certain cast is redundant, remove that redundant cast. (Duh!) If the compiler is warning you that you are dereferencing a pointer which might be null at that point, then add a null check before dereferencing it. (Duh!) If your compiler is warning you that you are invoking an overridable method from within the constructor of a base class, then do whatever restructuring is needed, throw it all away and rewrite it from scratch if necessary, so that no such thing is happening. Another way of dealing with warnings is by suppressing them. Of course, this approach should only be used on perfectly legitimate code that would become less perfect if it was to be restructured so as to resolve the warning. Suppression should always be as localized as possible, meaning that it should be done on the individual statement where the warning is issued, instead of the entire function or the entire class. Note, however, that there are certain warnings that should always be properly resolved and never suppressed; take the invocation of an overridable method from within the constructor of a base class for example. Some warnings, like \u0026quot;unused identifier\u0026quot;, occur on legitimate code too often for selective suppression to be practical. For those warnings, consider using an IDE that supports a \u0026quot;weak warning\u0026quot; or \u0026quot;suggestion\u0026quot; level, which is highlighted inconspicuously, so it can be easily filtered out by your eyes, but the visual clue is still there in case it points to something unexpected. Also consider using a better programming language, which supports a construct known as a \u0026quot;discard variable\u0026quot;, allowing the programmer to explicitly state their intention to let a variable go unused, so that the warning can remain a warning. Of course some silly warnings occur on legitimate code all the time, so it goes without saying that they need to be disabled, but in my experience they are far fewer than the average programmer thinks they are.\nThou shalt not suffer a warning to live Every single warning must always be resolved immediately upon being introduced. Nobody should ever commit code that contains warnings, and therefore nobody should ever check out code that already contains warnings.\nThis is because a warning always is (or ought to always be) a cause of alarm; however, long-standing warnings constitute long-standing false alarms, so their continued existence causes two things:\nAll programmers in the house start becoming insensitive to the alarms, so the alarms start going unnoticed. (The \u0026quot;cry wolf\u0026quot; effect.) Those programmers who are perfectionists (and those are the best kind of programmers) start becoming mighty annoyed. Which brings us to the next point:\nTreat Warnings as Errors Always use the \u0026quot;treat warnings as errors\u0026quot; option of your compiler. If your compiler does not have such an option, throw away everything and start from scratch with a compiler that has it.\nThe conventional understanding of what the difference is between warnings and errors is that with an error, you have to fix it before you can proceed, whereas with a warning, you can just ignore it and proceed.\nThis understanding is technically correct, in the sense that this is in fact how compilers tend to behave by default, and this is in turn what most programmers expect, since dumb defaults seem to always suit mindless majorities. However, this conventional understanding, and therefore this default behavior of compilers, is wrong. It has been wrong since the dawn of our discipline, and it continues to be wrong today. The magnitude of the wrongness, multiplied by the pervasiveness of the wrongness, is truly staggering.\nThe difference between warnings and errors should be that you can suppress a warning if you must, whereas you cannot suppress an error; however, you should absolutely have to address and eliminate both, meaning that you should have to either explicitly suppress or otherwise resolve every single warning before being allowed to proceed.\nThe \u0026quot;treat warnings as errors\u0026quot; option corrects the wrong behavior of compilers, and exists precisely for the benefit of those (apparently very few) people in our discipline who happen to have their reasoning right on this issue.\nBe one of those people. Use that option.\nStrive for readability Readability is one of the most important qualities of code, second only to correctness. Code is generally read far more often that it is written. We tend to read code:\nseveral times as we write it; at least once more as we review it; many more times throughout its lifetime as we: extend it; refactor it; tweak it; as we write nearby code; as we browse through code to understand how things work; as we perform troubleshooting; etc. In other words, over time, the reads-to-writes ratio of any piece of code approaches infinity. Therefore:\nAbsolutely any choice that makes code easier to read is absolutely always preferable over absolutely any choice that makes code easier to write.\nThis means that languages that achieve great terseness of code are not really delivering anything of value by this alone, (I am looking at you, Scala,) because verbosity of code is not one of the major problems that our profession is faced with; unreadable code is. This also means that certain languages whose grotesquely arcane syntax has earned them the \u0026quot;write-only language\u0026quot; designation are not to be touched with a 10 ft. pole. Perl enthusiasts can cry me a river.\nAvoid using elements of prose in code Identifiers should be pedantic, not creative, and unless they pertain to the problem domain, they should come from the realm of engineering, not from the realm of literature. Think twice before using a term like \u0026quot;drop\u0026quot; instead of \u0026quot;delete\u0026quot;, or \u0026quot;payload\u0026quot; instead of \u0026quot;content\u0026quot;, because \u0026quot;drop\u0026quot; and \u0026quot;payload\u0026quot; are metaphors. Metaphor should be avoided unless it helps to express something that would otherwise require an entire sentence to express, for example \u0026quot;Factory\u0026quot; instead of \u0026quot;Object-that-creates-other-objects\u0026quot;.\nUse an IDE with a spell checker Avoid anything that fails to pass the spell check.\nAdd the spell-checking dictionary of the IDE to source control and review any commits to it just as you review any other code.\nThis specifically means abandoning certain old habits; all of the following are wrong:\nnrPoints; pointsNr; nPoints; pointsN; noPoints; pointsNo; lenPoints; pointsLen\nOnly the following are right:\nnumberOfPoints; pointCount; pointsLength\nAvoid acronyms and abbreviations Acronyms and abbreviations are cryptic for the uninitiated, and even if they are not, they make the code look unnecessarily technical and foreboding. Use fully spelled-out words of the English language instead. Modern IDEs have formidable auto-completion features, so fully spelling out every word does not necessarily mean that you will have to type more, but even if it did, typing is not one of the major problems that our profession is faced with; unreadable code is.\nThis means that a huge number of abbreviations which have traditionally been staple terms in programming, should never be used, or their use should be seriously reconsidered. This includes all of the following: abs, addr, alloc, alt, app, arg, async, attr, auth, avg, bg, bat, bin, bool, buf, buff, btn, calc, cert, char, cls, clr, col, coll, cmd, com, cmp, comp, cfg, conf, config, const, ctx, ctrl, conv, coord, cos, cnt, cur, curr, db, dbg, dec, decl, def, deg, del, desc, dest, dev, diff, dim, dir, disp, div, doc, drv, dyn, env, eq, err, exe, exp, expr, ext, fac, fig, fg, fmt, frac, freq, fn, fun, func, gen, geom, hdr, hex, img, imp, impl, inc, idx, info, init, ins, inst, int, iter, lang, len, lib, lnk, max, mem, msg, mid, min, misc, mod, mul, mut, nav, net, num, obj, org, pkg, param, perf, pic, ptr, pos, pow, pwr, pred, pref, prev, priv, proc, prof, pub, rand, rnd, recv, rec, rect, ref, regex, rel, rem, rm, repo, req, res, ret, rev, sel, seq, svc, sess, sin, sln, src, spec, sqrt, std, stmt, stat, str, sub, sync, tan, tmp, temp, txt, usr, util, var, val, vec, ver, win, wiz.\nIf a particular acronym is understood by every programmer, then it might be okay to use it in code, but if it is only understood by domain experts, then it is not okay. This is because programmers often work on software for domains on which they are not experts, and even if they do eventually become domain experts, in the beginning they are not, but the beginning is when everything is difficult, so that is precisely the time that you do not want to be adding any extra difficulty to them. This means that very few acronyms are actually okay.\nLet me stress this to make sure it is understood: Domain Experts may protest that it is awkward to see a particular term fully spelled out in the code, because the term is so well known, that it appears as an acronym in the entirety of the literature in their field; let them find it awkward, and let them protest. Your code is not part of the literature in their field.\nIf the choice is made to keep a certain acronym in the code, then the acronym must be turned into a word, meaning that only the first letter may be written in upper-case, while all subsequent letters must always be written in lower-case. For example, if you have decided that you are not going to replace GUID with GloballyUniqueIdentifier, I am totally with you, but then you must replace it with Guid, so that the spell-checker can recognize it as a word and spell-check it. Otherwise, the spell-checker will consider each capital letter individually, and each individual letter passes spell-checking, so anything written in all-capitals essentially circumvents the spell-checker. If \u0026quot;Guid\u0026quot; as a word violates your English-language sensitivities, then please remember that you are writing code, not prose. There is a reason it is called code: it is specifically not prose.\nAlso beware of abbreviations that do not look like abbreviations. For example:\nThe word \u0026quot;out\u0026quot; can be a word on its own, but more often than not, it is used as an abbreviation of \u0026quot;output\u0026quot;. Spell out the full word. Let the computer hacker of the seventies go gently into the good night; be modern and sophisticated; use the fully spelled-out term StandardOutput instead of the cryptic abbreviation StdOut.\nIn the methods ToUpper() and ToLower(), the terms \u0026quot;upper\u0026quot; and \u0026quot;lower\u0026quot; have no inherent meaning of their own; if you think about it, they make no sense; what happens in fact is that they are abbreviations of the proper terms, which are UpperCase and LowerCase respectively. The proper terms make sense; use the proper terms.\nPay attention to naming Every single concept must have the best name that it could possibly have. Not just a good name, but an excellent name. Unfortunately, finding the right name for things is hard. It is not a coincidence that naming things is One of the Two Hard Problems in Computer Science. (https://martinfowler.com/bliki/TwoHardThings.html)\nStrive for a variety of names that uniquely and accurately reflect each concept that you are dealing with. A Thesaurus is an indispensable programming tool.\n(I once worked in a metrology environment where both the main entity of interest was called a \u0026quot;Measurement\u0026quot;, and the main thing that you could do with it was to perform a \u0026quot;Measurement\u0026quot;; that's deplorable.)\nIf a certain domain-specific term is problematic in code, then do not use that term in code. Completely ignore the domain experts who will protest that the original term is the established term in the field and it is awkward to see it replaced with something else.\n(In that same metrology environment, the goal of the software was to measure and report how something differs from its ideal form; the term used in that field for this kind of difference was \u0026quot;error\u0026quot;, so the software was full of identifiers called \u0026quot;error\u0026quot; that did not stand for error as we know it in software; that's deplorable.)\nAvoid zero-information names; invest the necessary amount of thinking so that each name gives at least some hint as to what it is about to someone who sees it for the first time. A good rule of thumb for deciding whether a name is good is to ask yourself the following question:\nCould the same name conceivably also stand for some unrelated entity in my code base?\n(A co-worker of mine once created a namespace called \u0026quot;DataInfo\u0026quot;; that's deplorable.)\nIn special cases, dare to use names that you may have never heard anyone using before. For example, if you need a Factory of Factories, why not call it Industry?\nRead Chapter 2: Meaningful Names of the book Clean Code by Robert C. Martin.\nAlso read this: Confucius On Naming Things\nAny code written by a programmer whose English language skills are poor should be reviewed by a programmer whose English language skills are good.\nWhen words need to be combined to form an identifier, the combination must follow general English grammar rules, except for English grammar special cases.\nRead this: Software Engineering Stack Exchange: Clean Code: long names instead of comments\nIn the following discussion when we speak of a noun or an adjective or a verb we actually mean a sequence of various parts of speech that effectively constitute a noun or an adjective or a verb. For example, reticulated_spline is a noun (spline), reticulated_before_dive is an adjective (reticulated), and dive_for_moog is a verb (dive).\nTypes: Classes: The name of a class must always be a noun; it must never be an adjective or a verb; no exceptions. Also, the name of a class must always be in singular form; no exceptions. If you need to signify plurality, do not use plural! Instead, append a plurality-signifying term which is in turn a singular noun. For example, if you have a class that stands for a group of entities, do not call it 'Entities', call it 'EntityGroup' instead. (Duh!) Interfaces: The name of an interface must be either an adjective, (e.g. Comparable,) or a noun, (e.g. Serializer,) no exceptions. Singular form goes without saying. Enums: The name of an enum type must always be a noun in singular form, no exceptions. (E.g. WeekDay.Monday instead of WeekDays.Monday.) Variables: Single-value: The name of a single-value variable must always be a noun in singular form, unless it is of Boolean type, in which case it may signify a condition, such as isEmpty. Collection: The name of a collection variable must either be a noun in plural form, (e.g. Gizmos,) or it must be suffixed with a collection-signifying term (e.g. GizmoList.) Functions: Pure: The name of a function that returns a result without mutating anything must always be a noun unless it returns Boolean, in which case it may signify a condition, such as hasChildren(). The name must be in singular form, unless a collection is returned, in which case the name must either be in plural form, (e.g. GetGizmos(),) or be suffixed with a collection-signifying term. (e.g. GetGizmoList().) Impure: The name of a function that performs an operation (has side effects) must be a verb, no exceptions. Fallible: If a function returns a result indicating success or failure, the name must begin with 'try', for example TryGetGizmos() or TryAddGizmo(). If the name does not begin with 'try' then the absolutely inviolable rule is that the function will signal failure by throwing an exception. When multiple words are combined to form an identifier, they must still make sense. As an example of what to avoid, take the INotifyPropertyChanged interface of WPF. This name is deplorable because notify is a verb, not a noun or an adjective, and because an object implementing this interface is not a property-changed notification, it is an object which may issue property-changed notifications.\nAdmittedly, it is difficult to come up with a good name to describe such objects; a decent choice might be PropertyChangedNotificationIssuer, but this might be a bit too long for some people's taste. An alternative is to use a familiar term of broader scope if there is no possibility of confusion. So, another decent choice here might simply be Mutable. It is true that all kinds of different classes are mutable without issuing property-changed notifications, but then again the only thing that different mutable classes could have in common simply by virtue of being mutable, so as to warrant a common interface for all of them, is issuing notifications about their mutations. The point to take home from all this is that although it is difficult to come up with good names, the application of some actual thinking should produce a name which is at least a bit better than nonsense.\nAs mentioned earlier, special cases of the English grammar can, and should, be ignored. An example of this is the simplification of plurals: choose \u0026quot;indexes\u0026quot; instead of \u0026quot;indices\u0026quot;, \u0026quot;schemas\u0026quot; instead of \u0026quot;schemata\u0026quot;, and, even though I know this is a tough proposition for some, \u0026quot;companys\u0026quot; instead of \u0026quot;companies\u0026quot;. See Software Engineering Stack Exchange: Does it make sense to use \u0026quot;ys\u0026quot; instead of \u0026quot;ies\u0026quot; in identifiers to ease find-and-replace functionality?\nNever begin a function name with the prefix 'check'. Doing so is a typical example of a developer choosing names according to fleeting notions in their head, without the slightest concern as to how these names will be understood by others. The word check means nothing; a function that only checks something and then does nothing about it would serve absolutely no purpose; presumably, whatever checking the function does culminates in taking some kind of action, or returning some kind of result; this is an extremely important piece of information that the name of the function should not fail to convey; therefore, the name of the function must indicate what kind of action is performed, or what kind of result is returned.\nAvoid conventions that make code look unnecessarily technical Code is, by definition, already quite technical; we do not need to be making it look even more technical than it already is. Abandon the abhorrent practice of prefixing static variables with \u0026quot;s_\u0026quot;, prefixing member variables with \u0026quot;m_\u0026quot;, and prefixing private member variables with \u0026quot;_\u0026quot;. Modern IDEs can be configured to provide sufficient visual clues about these things via syntax highlighting. If your IDE does not support this, throw it away and find one that does. If you are not using an IDE, then please switch to the arts and humanities.\nAvoid Hungarian Notation. (https://en.wikipedia.org/wiki/Hungarian_notation.) One thing that greatly helps in avoiding Hungarian Notation is The Maximalistic Approach to Typing, (see next item below,) where the nature of a variable is fully determined from its data type without the need for name adornments.\nOne more thing: no matter how popular it is in the DotNet world, the practice of prefixing interface names with I is an instance of Hungarian notation, and is therefore a bad practice. It is indicative of uncertainty and uneasiness on behalf of the programmers who first started using interfaces a long time ago, perhaps without fully understanding what they are. Obviously, they wanted to have that I clearly visible in every interface name, to make it evident that this is one of those new interface things. This is laughable today. There is absolutely no reason to view an interface as something so special as to deserve its own naming convention.\nFollow the Maximalistic Approach To Data-Typing Many programmers tend to use general-purpose data types everywhere, even when values conceptually belong to specialized types. This is a bad practice, known as Primitive Obsession. The classic example of this is using an int number of milliseconds instead of a Duration data type, which is more specific, and unit-agnostic, and therefore far more type-safe than an int.\nNow, in most modern programming languages, a data type like Duration already exists, so using it instead of int is a no-brainer; however, there is a lot to be gained by creating user-defined data types to represent quantities for which there are no pre-existing data types; for example:\nYour height is not of type double; it is of type Length. A value indicating whether you are married or not is not a Boolean; it is an instance of MarriedStatus. A customer id and a product id are not both of type int; one is of type CustomerId, while the other is of type ProductId. ...and so on. The practice of creating user-defined types is such a good thing that we can never have too much of it. So, in every case where we see primitives being used, we must consider whether it would make sense to be using more specific types instead, and if so, to add such types and refactor all code to start making use of them. I call this The Maximalistic Approach To Data-Typing.\nUser-defined data types that stand for physical quantities are called Strongly Typed Units. With them, we can go a step further and make use of user-defined operators to implement an entire type-safe algebra, where a Length divided by a Time yields a Velocity, thus guiding the programmer to make the correct calculations, and sparing the programmer from ever having to worry about unit conversions. (Wrong unit conversions is the stuff that Mars Climate Orbiter mission failures are made of.)\nStrongly Typed Units are such a good thing to have, that I would recommend using only programming languages that have support for them, for example C++, C#, Scala, etc., and avoiding any programming languages that lack support for them, for example Java.\nOh, and, untyped programming language aficionados can cry me a river.\nAvoid defensive programming; engage in offensive programming instead Defensive programming is summarized by Postel's law, otherwise known as the Robustness Principle, which says:\nBe conservative in what you do, be liberal in what you accept from others.\n(See https://en.wikipedia.org/wiki/Robustness_principle.)\nThis principle suggests that besides producing output which adheres to the spec, our software should, as much as possible, be capable of coping with input that is off-spec. In other words, it should be tolerant to error. People imagine that when software behaves like that, it is more robust.\nIf there is one thing that I have learned in several decades of programming, both from my own code and from code written by others, it is that tolerance towards error leads to anything but bug-free software; it invariably results in chaos; and guess what chaotic software tends to be: buggy.\nRead this: http://trevorjim.com/postels-law-is-not-for-you\nSo, instead of defensive programming, I advocate offensive programming, which means:\nNever allow any slack or leeway, require everything to be exactly as expected. Require strict adherence to the spec even if you have no use for the full precision mandated by the spec. Keep tolerances not just down to a minimum, but at absolute zero. Never fail silently; fail loudly instead. Fail fast; fail hard; fail eagerly, and enthusiastically. Examples of offensive programming:\nAvoid conversion functions that return null if given null; always assert that the parameter is non-null. Better yet, avoid nullability altogether, or use a type system with explicit nullability, so as to restrict it via strong typing to only those places where it is meaningful. The same applies to empty strings: if an empty string is not meaningful somewhere, do not simply cope with it; explicitly and categorically disallow it by throwing an exception. Avoid things like a Map.put() method which either adds or replaces, and instead design for an add() method which asserts that the item being added does not already exist, and a replace() method which asserts that the item being replaced does in fact already exist. Force the programmer to clearly indicate their intention, and assert that things are exactly as the programmer assumes they are. In scenarios where an add-or-replace operation seems useful to have, (and in my experience, such scenarios are exceedingly rare,) add such a function but give it a name that clearly indicates the weirdness in what it does: call it addOrReplace(). (Duh!) Avoid things like a close() method which is allowed to be invoked more than once with no penalty: assert that your close() methods are invoked exactly once. Never use the garbage collector for cleanup; always perform explicit and deterministic clean-up at the exact moment when it is supposed to happen; the cleanup function invoked by the garbage collector should only be used for producing a diagnostic message in case we forgot to do explicit cleanup. Read this: Object Lifetime Awareness Use inheritance when it is clearly the right choice The advice that composition should be favored over inheritance was very good advice back in the mid-1990s, because back then people were overdoing it with inheritance: the general practice was to not even consider composition unless all attempts to get things to work with inheritance failed. That practice was bad, and the fact that the predominant language at that time (C++) supported not just inheritance but actually multiple inheritance made things even worse. So the advice against that practice was very much needed back then.\nHowever, the advice is still being religiously followed to this day, as if inheritance had always been a bad thing. This is leading to unnecessarily convoluted designs and much weeping, and wailing, and gnashing of teeth. Even the original advice suggested favoring one over the other, it did not prescribe the complete abolition of the other. So, today it is about time we reword the advice as follows:\nKnow when to use inheritance and when to use composition.\nFor a variety of opinions and a lengthy discussion about this, see https://stackoverflow.com/q/49002/773113\nAlso heed the advice by Josh Bloch to design and document for inheritance or else prohibit it. (See https://blogs.oracle.com/javamagazine/post/java-inheritance-design-document)\nFavor early exits over deep nesting This means liberal use of the break and continue keywords, as well as return statements in the middle of a method whenever possible. The code ends up being a lot simpler this way. Yes, this directly contradicts the ancient \u0026quot;one return statement per function\u0026quot; dogma. I love contradicting ancient dogma.\nAvoid static mutable state like anthrax Yes, this also includes stateful singletons. The fact that it only makes logical sense to have a single instance of a certain object in your world is no reason to design that object, and your world, so that only one instance of them can ever be.\nYou see, I guarantee to you that the need will arise in the future, unbeknownst to you today, to multiply instantiate your world, along with that object in it, which you thought was one-of-a-kind.\nAs a matter of fact, it is quite likely that you will have to do that anyway, for the purpose of testing.\nOptimize performance bottlenecks, not performance penalties The ages-old advice to avoid premature optimization is considered common knowledge, but it is a bit vague, so it does not actually register with many folks, who will not hesitate to optimize any code construct that they consider as representing a performance penalty, under the reasoning that if it represents a performance penalty then its optimization is not premature.\nFor this reason, I like to rephrase the advice as \u0026quot;Optimize performance bottlenecks, not performance penalties\u0026quot; to stress the point that just because something represents a performance penalty, it does not mean that it should be optimized.\nYou see, all code takes clock cycles to run, so every little piece of code that we write represents a performance penalty; if that was sufficient reason to optimize it, then premature optimization would be the order of the day, every day. For something to be considered worthy of optimization, it should not merely represent a performance penalty; it should be proven to represent a performance bottleneck.\nYou do not know whether something is a bottleneck unless you run the completed software system, discover that its performance is unacceptable, and use the profiler to determine exactly where the bottlenecks are. Also, what usually happens in these cases is that you tend to find some nice and formal algorithmic optimizations to apply in just a few places, and make your software meet its performance requirements, without having to go all over the entire source code base and tweak and hack things to squeeze clock cycles here and there.\nPut the tools of the trade into use Armies of very good developers have worked hard to build these tools, don't you dare make their efforts go in vain.\nUse an IDE.\nProgrammers who think that they are better off with their favorite text editor should be admitted to rehabilitation.\nUse the build feature of your IDE, which only compiles modified files.\nProgrammers who habitually perform a full rebuild instead of a plain build should be fired.\nUse your IDE for running tests.\nProgrammers who habitually run tests via separate tools outside of the IDE should be shot.\nThe continuous build pipeline is your second line of defense, not your primary means of building and testing. Your IDE will always be a lot faster, and it has a built-in debugger.\nUse the debugger of your IDE as your first choice for troubleshooting anything, not as the last resort after all other options have been exhausted. This means that you should be using the debugger not only when there is trouble, but always, by default, so that it is ready when trouble occurs. This in turn means that when you want to fire up your creation, or to run the tests, you should never hit the \u0026quot;Run\u0026quot; key on your IDE; you should hit the \u0026quot;Debug\u0026quot; key instead. Always the \u0026quot;Debug\u0026quot; key. Only the \u0026quot;Debug\u0026quot; key. You are a programmer; act like it.\nHaving said all that, I should also add that people who are so attached to their IDE that they program by dragging and dropping code snippets around should perhaps consider that some desktop publishing job might suit them better.\nDo not even think that you are done with testing unless the code coverage tool gives you sufficient reason to believe so.\nHave your IDE perform code analysis, and incorporate even more code analysis in the continuous build.\nDesign with reliability as a foundation, not as an afterthought For example, sharing data in a multi-threaded environment by means of traditional locking techniques (\u0026quot;synchronization\u0026quot;) is both error-prone and untestable, because you cannot test for race conditions. Note that \u0026quot;error prone\u0026quot; and \u0026quot;untestable\u0026quot; is a deadly combination; therefore, this way of sharing data should be abandoned. Instead, design for a lock-free, share-nothing approach that works by passing immutable messages, thus eliminating the very possibility of race conditions.\nDesign with security as a foundation, not as an afterthought Security is not something that you can add on top of an insecure foundation, because there exist no automated tests that can detect security hazards and no amount of carefulness on behalf of programmers that is careful enough. So, what is necessary is architectural choices that eliminate entire classes of security hazards. (Do not worry, there will always be other classes of security hazards to have to worry about.)\nSo, if a certain architectural choice is prone to security vulnerabilities, do not make that choice. An example of a vulnerability-prone architectural choice is putting application code on the web browser, otherwise known as full-stack development. Full-stack developers can cry me a river.\nFor more on this, read: What is wrong with Full Stack Development\nKeep the log clean Do not vex your colleagues, and do not make your own life harder, with torrential info-level or debug-level spam in the log. Keep the info-level messages down to an absolute minimum, and once debugging is done, completely remove all the debug-level log statements. Utilize commit hooks that deliberately fail a commit if it contains debug-level logging statements. Regularly use the \u0026quot;blame\u0026quot; feature of the version control system to remind developers of info-level logging statements that they should remove. Never use the log for capturing metrics or any other kind of structured information; use some separate, specialized instrumentation facility for that.\nMake the best out of the log You should at all times be able to click on a log line in the output window of the IDE and be taken to the source line that generated that log entry, and you should also at all times be able to click on any line of a logged exception stack trace and be taken to the corresponding line of source code. I am appalled by how many programming environments do not offer this as the default mode of operation under all circumstances.\nIn the Microsoft Visual Studio world, for a line to be clickable in the output window it must start with a source pathname, followed by an opening parenthesis, a line number, a closing parenthesis, and a colon. It can optionally be prefixed with whitespace. Fortunately, both C++ and C# support efficient means of obtaining source file name and line number information: In C++ it is the __FILE__ and __LINE__ built-in pre-processor macros, while in C# it is the CallerFilePath and CallerLineNumber attributes. Unfortunately, the pathnames generated by these mechanisms are absolute, meaning that they start from the drive letter and include the kitchen sink, so you might want to programmatically convert them to pathnames relative to the solution folder before logging them. Visual studio also recognizes those, though this is undocumented. In the Jetbrains IntellijIdea world, for a line to be clickable in the output window it needs to contain an identifier, followed by an opening parenthesis, a source filename-plus-extension, (but no path,) a colon, a line number, and a closing parenthesis. The identifier is meant to be a package name, but Idea does not interpret it in any way, so it can be anything. Due to a long-standing bug (which JetBrains refuses to acknowledge or fix) if the word \u0026quot;at\u0026quot; appears in the log line, and if it is in any place other than immediately before the package name, then this mechanism breaks. (Note that this is all entirely undocumented.) Note that this mechanism suffers from ambiguity in the case of multiple source files with the same filename. An alternative mechanism is to include a \u0026quot;file://\u0026quot; URI in the log entry, but in order to produce such a URL you would have to figure out the path from the package name, which is doable, but not easy. Unfortunately, Java does not provide any efficient means of obtaining source file name and line number information, so one has to generate a stack trace in order to extract this information from it. Fortunately, generating a stack trace in the java world is not anywhere near as expensive as in the Microsoft world. Unfortunately, it is still unreasonably expensive. You can see this performance penalty as one more reason to keep logging to a minimum. Take maxims with a grain of salt (Especially quantitative maxims, which offer specific numerical limits for things.)\nWhen someone says \u0026quot;no function should accept more than 4 parameters\u0026quot; or \u0026quot;no class should be longer than 250 lines\u0026quot; they are usually talking nonsense.\nA class should be as long as necessary to do its job, and if that is 2000 lines, so be it. I would much rather keep some ugly code confined in a single class than split it into multiple classes and thus propagate the ugliness in the design.\nA function should accept as many parameters as necessary to do its job, and if that is 15 parameters, so be it. I would much rather have a long constructor than a mutable object.\nBreaking things down to smaller units should be done because there is some actual tangible merit in doing so, not because some prophecy said so.\nPrivate static methods are fine. Really! An instance method has the entire object state at its disposal to read and manipulate, and this state may be altered by any other instance method, including instance methods that this method may invoke. The complexity of this is mind-boggling. A static method on the other hand is obviously not in a position to read nor alter any of the object's state, and it is unable to invoke any instance methods that would do that. By its nature, a static method has to rely exclusively on parameters, which are all clearly visible at each call site. Thus, a static method is an immensely less complex beast than an instance method. What this means is that private static methods are not the slightest bit evil as some folks believe they are, and we should have more of them.\nPersonally, when I have a class that has both complex logic and mutable state, I tend to move the complex logic into private static methods, reducing the instance methods to doing nothing but invoking private static methods, passing instance fields to them and storing results into instance fields as necessary.\nDo not fix it unless there is a test for it I do not yet have an opinion about test-driven development, but what I have found to be immensely useful, is test-driven maintenance. So, when a bug is discovered, which obviously passed whatever automated tests you already had in place, do not hurry to figure out what causes it and fix it. First, write a test that tests for the bug, being completely agnostic of any theory that you might already have as to what is causing the bug. This test should initially fail; if it does not fail, then the bug is not what you think it is, so you have more research to do. If the test fails as it should, then fix the bug according to your theory as to what is causing it. If the test now passes, then your theory was correct. If not, then not only you have not fixed the bug, but you have probably broken something else which used to be fine.\nAvoid death by ten thousand little methods Again and again I see code bases with multitudes of tiny methods having cryptic names, each containing just one or two lines of trivial code, aiming to ensure that not a single line of code is duplicated anywhere. The downside of this is that it increases the complexity of the call tree and therefore the amount of mental effort required to make sense out of it. A new function is worth introducing if it has a well-defined, meaningful role to play. Difficulty in coming up with a name for a function, or having many functions with names that differ only slightly and fail to readily convey the difference between them, are both good indicators that these functions have no role to play other than to avoid code duplication. Of course there is merit in reducing code duplication, but not when the code in question is trivial. And when you see the possibility to de-duplicate non-trivial code, then the well-defined, meaningful role of the function tends to be immediately obvious, as well as the appropriate name for it.\nMake the best out of break-on-exception Set up your development tooling, and use whatever runtime mechanisms are necessary, so that the debugger always stops at any statement that throws an unexpected exception.\nMany programmers have the bad habit of doing all their troubleshooting by examining logs and postmortem stack traces and theorizing as to what went wrong, instead of having the debugger break on exception and actually seeing what went wrong. This is extremely counter-productive.\nUnfortunately, exceptions are a somewhat complex topic, programming languages and their run-times behave in complex ways when exceptions are thrown, and debuggers have complex mechanisms for dealing with them, none of which helps. As if that was not enough, it is not always easy to tell when a certain exception should be expected and when it should not be expected.\nThus, there exist several obstacles to accomplishing proper, usable, break-on-exception:\nOur code throws and catches expected exceptions all the time, or uses external libraries that do so, internally, all the time; clearly, we do not want the debugger to stop on any of those. One might think that the solution to this problem would be to configure the debugger to ignore caught exceptions and only stop on uncaught exceptions; unfortunately, that will not work either, because quite often we have exceptions that we consider as uncaught, but technically they are caught; for example: An external library invokes our code, and our code throws an exception, which is uncaught as far as our code is concerned, but it is caught by the external library. A typical example of this is event-driven frameworks, i.e. virtually all GUI frameworks, which invoke our code to handle events, and almost always do so from within a try-catch block. Thus, any exception thrown by our event handlers is actually a caught exception, and the debugger will not stop on it. In many languages, the try-finally clause internally catches exceptions and re-throws them at the end of finally, meaning that any exception thrown within the try block is technically a caught exception. Thus, a debugger configured to stop on uncaught exceptions will break at the end of the finally block, which is completely useless and counter-productive. The same problem is encountered with other constructs which are internally implemented using try-finally, such as the synchronization clause, the automatic disposal clause, etc. To complicate matters even further, an exception which is unexpected and unhandled under normal circumstances may temporarily become expected and handled during testing. This happens when a test deliberately causes malfunction to ensure that the component-under-test detects it and responds by throwing an exception, which is then caught by the test and examined to ensure that it is the correct exception and it has been correctly filled-in; when this happens, we do not want the debugger to stop, because we do not want our tests to be interrupted by the debugger while everything is proceeding according to plan. Here is a Stack Overflow question and answer which simplifies things a lot: https://stackoverflow.com/q/71115356/773113\nWrite code as if it will be reviewed by someone, even if it never will Always try to take one more look at the code from a completely agnostic point of view, supposing that you know nothing about what it does, why it does it, how it does it. Does the code still make sense? Is everything obvious? If not, refactor it until it is as plain as daylight. If comments are necessary to explain what is going on, can the code be refactored so that the comments become unnecessary?\nWhich brings us to the next point.\nAvoid writing code comments Never add a comment in the code unless absolutely necessary. (Note that this applies to code comments, not to public interface comments, which can be nice to have.)\nThe purpose of a code comment should be to alert the reader that something special is happening here, which is not obvious, and cannot be explained by any means other than written prose. This should only be necessary in exceptional situations, while the norm should be that the code is always so simple, and so self-explanatory, that no comments are necessary. An example of an exceptional situation is provenance comments, see related section. Code comments that simply state what the code does are unwarranted causes of alert, and if you repeat them enough they will force the reader to start treating your comments as noise, and may thus cause the reader to miss that rare comment which was actually important to note. - Comments tend to be necessary when a piece of code does something unexpected, which is usually code that takes special measures to circumvent some anomalous behavior of some other code. In these cases, explaining what the code does is not even the goal; the goal is to explain why it does it, and in order to explain that you have to describe the anomalous behavior, which may even necessitate listing various usage scenarios that have been tried and results that have been observed. This in turn means that comments worth writing tend to be entire multi-paragraph-long essays explaining strange and complicated situations. In my experience, one-liners are usually of no value. - Note that when documenting code that circumvents anomalous behavior it is a good idea to assert, if possible, that the anomalous behavior is in fact still present, so that if it gets fixed in the future, you will take notice so you can remove the code that circumvents it. - If you find yourself adding a code comment, first ask yourself whether there is anything you can do to avoid that. Instead of adding a comment to some piece of code explaining what it does, extract that code into a separate function that has a self-explanatory name. However, it is even better to restructure the code, if possible, so that even the explanatory name becomes unnecessary. For example, in old C code you might come across a pointer-returning function whose documentation says that the caller is responsible for freeing the pointer. This is deplorable. Do whatever it takes to avoid this; use a callback, use an allocator parameter, have the caller supply the memory, throw it all away and rewrite it in Java, anything but requiring people to read comments or else they get punished with memory leaks. Instead of adding a comment to a hard-coded value, extract that value into a constant that has a self-explanatory name. When performing a calculation which involves a certain fixed value, it goes without saying that you will not hard-code some magic number in the calculation; instead, you will declare a constant with a nice descriptive name for that value, and use the constant in the calculation. Note that this must be done even in fairly trivial cases, for example const int BitsPerByte = 8; and can only be skipped in an exceedingly small number of special cases, for example when directly multiplying something by 2 in order to double it, or by -1 in order to negate it. - If a comment can be coded as an assertion statement, that's all the better. Comments saying \u0026quot;x must be greater than y here\u0026quot; are retarded. Assert the darn thing, and spare us from the comment, or perhaps use a comment to explain the why, but not the what. The assertion takes care of the what, and it does so unambiguously and definitively, because it compiles and passes the tests, which is something that no comment will ever do. - If you modify some code, and there is a comment attached to that code, do not forget to do something about the comment: Ideally, your modifications should make the comment redundant, so you should remove it. If not, then at least make sure that the comment is still valid after the modifications. Unfortunately, programmers often leave comments unchanged while changing the code around them, thus making every single comment in the entire code base liable to devolving into being inaccurate, or even misleading, and thus constituting an instance of sabotage. This is happening because: Programmers treat comments as noise, and therefore do not even notice their presence. (This is why comments should be used very rarely, in exceptional situations only.) Comments are poorly written, so programmers do not understand them. When a programmer does not understand a comment, they obviously cannot modify it, but it gets even worse: they do not dare to remove it either, because they assume that it must have some special meaning to some other programmer. Thus, poorly written comments are very similar to Persistent Organic Pollutants (POPs) a.k.a. forever chemicals: once created, they stay in the environment, causing harm for all eternity. If a comment does not make sense to you, then find the author, ask them what it means, and update it accordingly. If the author is not around anymore, then ask any other experienced programmer in the shop. If they cannot tell what it means either, then trust me, this comment will never make sense to anyone, so go ahead and remove it.\nIf you must write doc-comments, make them good Ideally, an entity (class or method) should have a well-chosen name and a very simple and straightforward interface or prototype, so that everything is clear at a glance, and therefore no doc-comment is needed. If things are not so simple, then it may be necessary to clarify them with a doc-comment.\nA doc-comment must be as simple and as brief as possible.\nDo not try to follow templates, or if you do, then treat all template fields as optional: skip any information that is not strictly speaking necessary. Some bureaucratic documentation guidelines require the doc-comment of a function to follow a specific template which begins with a summary line, is followed by one line for each parameter, and includes one line for the return value. If your function really needs all this information to be explained in a doc comment, then your function must be doing something extremely bizarre. If your function is not doing anything bizarre, then a single summary line might suffice to explain what it does; if so, then skip the extra lines explaining each parameter, as well as the extra line explaining the return value. As an example of what to avoid, see the IEnumerable\u0026lt;T\u0026gt;.GetEnumerator() method of C#/dotnet. The doc comment says: Description: Returns an enumerator that iterates through the collection. Returns: An enumerator that can be used to iterate through the collection. As you can see, the documentation is repeating itself. This is wasting the time of anyone attempting to read this documentation. This is annoying. Do not do this. A doc-comment is a public interface comment, not an implementation comment. As such, a doc-comment on an entity should explain, in the most brief and abstract terms possible, the following:\nWhat task it accomplishes. What input it accepts. What output it produces. Note that it does not need to address each one of those items separately; a doc-comment on a method which simply says that it \u0026quot;sorts a file in-place\u0026quot; explains all three items in one go. A documentation comment should not make the slightest attempt to explain any of the following:\nHow the task is accomplished. Which entities are expected to invoke the entity. Which entities are invoked by the entity. The above points are important to state because many misguided practices from the infancy of our discipline have it all wrong by stipulating that documentation comments should include preposterous things such as who invokes whom, completely missing the whole point behind the notion of general-purpose, reusable software and even the fundamental notion of abstraction.\nIf you are asking \u0026quot;but shouldn't documentation describe the how?\u0026quot; the answer is no, that's what we write code for. By definition, the only authoritative source of information as to how something is done is the code that does it. As I have already explained, the code must be so simple and so easy to read that English-language prose on top of it should be bringing no added value. As a matter of fact, the presence of prose is dangerous, because quite often people modify the code without bothering to also modify the documentation, which leads to situations where the documentation is misleading.\nIf, after looking at the code, something is still unclear, then place a breakpoint and launch the tests; the debugger will make things pretty clear to you. If you are wondering how the code works under a case which is not covered by the tests, then fix this by adding a test for that case! (Duh!) Also note that even if there was a \u0026quot;how\u0026quot; section in the doc-comment, it probably would not have covered that special case anyway. Always maintain provenance When you copy some code from the interwebz, always add a comment containing a link to the original source. Of course this is not necessary if the code that you copied is something fairly standard, like reversing a string; but if the code is anything but standard, (do you have any idea what it takes in Microsoft Windows to have a progress dialog shown while copying files?) then citing your sources is an absolute must.\nSources can include:\nExamples from the official documentation (provide a link to the example page) Stack Overflow (provide a link to the answer) GitHub (provide a link to the source file(s)) ChatGPT (give the exact prompt which yielded the code) etc. This applies not only to code, but also to any piece of information, including individual values. Why did you choose this particular value and not some other value? Unless the value in question is a Fundamental Constant (e.g. static readonly Velocity SpeedOfLight = Velocity.FromMetersPerSecond( 299792458.0 )) you should add a comment to it explaining exactly why this particular value was chosen, or where it came from. For example, if you need to use the population of Mexico City, then const int MexicoCityPopulation = 9209944; is not enough; it must be followed by a comment saying //2020 data from https://en.wikipedia.org/wiki/Mexico_City.\nStick with UTC everywhere Use UTC and only UTC for any purpose that involves storing, retrieving, communicating, converting, calculating, and doing really anything whatsoever with time, except for the following two cases, and only the following two cases:\nParsing a string that was entered by the user into a UTC time variable. Converting a UTC time variable to a string to be shown to the user. However: When dealing with events that happen in the future, make sure to also store the targeted time-zone along with the UTC coordinate, because every few years various countries around the world decide to change their daylight savings policy, which means that the mapping from UTC to local time may change in the future, and you have no way of knowing that in advance.\nKeep technical implementation concerns separate from application concerns Application code should not be making assumptions about the technical details of the system, so that the technical details are free to change with minimal changes to application code, and vice versa.\nFor example, the multi-threading regime under which a system operates (whether the system utilizes a single thread, or multiple discrete threads, or a thread-pool) is a technical implementation concern. As such, all knowledge of how multi-threading is done should be isolated in the relatively small body of code which wires up the system, and all application code should be capable of operating regardless of the multi-threading regime. Incidentally, this facilitates running tests either under a strictly single-threaded regime, to ease debugging, or under a multi-threaded regime, to maximize speed.\nAsync/await aficionados can cry me a river.\nMaximize the consistency of code formatting I would be tempted to say \u0026quot;format code with absolute consistency\u0026quot;, but I cannot, because we usually lack the tools to achieve this, so the goal is to strive to get as close as possible to achieving absolute formatting consistency.\nIn the preface of the highly acclaimed book \u0026quot;Clean Code\u0026quot; by Robert C. Martin, the author mentions some experimental findings indicating that \u0026quot;consistent indentation style was one of the most statistically significant indicators of low bug density.\u0026quot; The author also states that \u0026quot;style distinguishes excellence from mere competence\u0026quot;, which I think is a very good observation; however, the conclusion at which the author arrives is unwarranted, because correlation does not imply causation: it is probably not the consistent indentation style that causes fewer bugs, it is the kind of mindset of programmers who strive for a consistent indentation style which also happens to be the kind of mindset that produces fewer bugs. Be the programmer who has that mindset.\nIf you are one of those programmers who do not particularly care for consistent formatting, I know what you are thinking right now: you are thinking that you are the rare exception to the rule, and that your code is of course awesome and bug-free despite looking sloppy; well, you have every right to think in any way you like about yourself, but I hope you understand that nobody else will be particularly willing to give you the benefit of the doubt.\nNote that this does not mean that every programmer must be forced to follow a specific set of formatting guidelines; on the contrary, by using tools to do the formatting for us, we do not have to worry about formatting. The corollary to this is that as an employer, the only kind of code formatting that you have the right to require from programmers is that which can be achieved by means of automatic code reformatting tools that you already have in place.\nThe point to take home from all this is that the formatting style must be specified in the highest detail possible, the tools must be painstakingly configured to reformat code according to that style, and the guidelines of how to work around limitations of the tools must be laid down and agreed upon before any work is done on a software project, no matter how much effort all of this represents.\nUse tight abstractions In other words, avoid leaky abstractions.\nJoel Spolsky's original 2002 article formulating the Law of Leaky Abstractions stated that \u0026quot;All non-trivial abstractions, to some degree, are leaky\u0026quot;. The article focused on examples where implementation details of the underlying layer are exposed not by the interface itself, but by observing the performance characteristics of the underlying layer. For example, the interface of two-dimensional arrays is generic enough to allow us to iterate over them either row-first or column-first without having to know their internal memory layout; however, which way we choose can have drastic performance implications, due to memory cache utilization. This means that we do of course have to keep in mind the technicalities of the layer which implements the abstraction; it does not, however, mean that the interface should be compromised in any way.\nMore often than not, in our daily jobs we have the misfortune of dealing with abstractions that are leaky at the interface level. A glaring example of this, in languages like C# and Java, is class Object, whose public interface contains a hash-code function, which is entirely out-of-place and unwarranted, because it has to do with an implementation detail of hash-maps.\nThis mishap could have been avoided in a number of different ways, for example:\nRequire the programmer to supply, upon hash-map construction, the hashing function to use. Require objects intended to be used as keys in a hash-map to implement a Hashable interface which defines a hash-function. Unfortunately, neither of these approaches was chosen, either by Java or by C#, due to some misguided notion of convenience. Instead, the implementation detail of hash-maps that they need a hash function to work with has leaked into Object, requiring every single class to have a hash-code method, even if the class will never be used as a key in a hash-map, and even if the class is mutable, and should therefore never be used as a key in a hash-map.\nAnother example is serialization frameworks that leak details about the underlying file format that they work with: every single XML or JSON serialization framework that I have come across exposes XML-specific functionality or JSON-specific functionality; therefore, it is an XML serialization framework, or a JSON serialization framework, but not a general-purpose serialization framework.\nA proper general-purpose serialization framework would expose no file format details in its interface, thus being replaceable with a different implementation which serializes to and from some other file format, without any changes necessary to the code that uses the framework. I have written such a framework, and I assure you it was not easy, but here is the thing: Doing it right ™ is never easy.\nLeaky abstractions are the source of untold suffering in software development, and they must be avoided at all costs. Creating air-tight abstractions is often omitted in the interest of saving time, and people make do with leaky abstractions instead, but this invariably results in orders of magnitude more time wasted over the long run in dealing with the disastrous consequences of the leaky abstractions.\nI would dare to propose that the term abstraction has (or ought to have) an inherent notion of absoluteness; just as one can be either pregnant or non-pregnant but not slightly pregnant or almost pregnant, so can an interface either be an abstraction or not an abstraction; it cannot be somewhere in-between. Thus, an incomplete or leaky abstraction should, for all practical purposes, be regarded as not an abstraction. (Because that's what the almost absolute is: non-absolute.)\nThoroughly emulate any and all hardware Hardware emulation is a special case of abstraction, where instead of abstracting software we are abstracting hardware. Incomplete hardware emulations are a curse for the same reasons that leaky abstractions are a curse. Hardware emulations must be 100% complete so that any software performing high level operations with the hardware can make use of all of the functionality of the hardware while remaining completely agnostic of whether it is connected to the real hardware or to an emulation thereof.\nOnly use absolute file-system paths All file-system paths must be absolute. It is fine to provide the user with the convenience of entering a relative path, but the relative path must be converted to absolute immediately upon entering the system.\nRelative paths are based on the notion of a \u0026quot;current directory\u0026quot;, which is one of the most ill-conceived, misused, and treacherous notions in the history of programming, because it is a global mutable variable. (I hope I do not need to explain why a global mutable variable is evil, right?) Note that the \u0026quot;current directory\u0026quot; is global not only across all classes of your application, but also across all threads of your application, and, in DotNet, even global across all AppDomains of your application, which were supposed to be completely isolated. Duh!? What were they thinking?\nAvoid GUIDs (also known as UUIDs) Never use GUIDs if you can avoid them. If you must use them, then make sure they are an implementation detail and that they constitute a side-note of your design, not a predominant feature of your design. Read this: What is wrong with UUIDs and GUIDs.\nDo It Right ™ Avoid taking shortcuts in the name of immediate savings now but at the expense of headaches later, because the later headaches invariably end-up costing orders of magnitude more than the immediate savings. When some colleague, manager, or decision-maker suggests to \u0026quot;make it simple now, and worry about making it right later\u0026quot; they are imagining that they are being smart and they are helping optimize things, while in fact they are being a smart-ass, and they are suggesting that a technical crime be committed.\nAn example of this, which has already been mentioned, is finding proper names for identifiers. If you want to introduce a new identifier, finding a proper name for it may require opening up the thesaurus, spending a considerable amount of time creating a list of candidate words, opening up the dictionary, looking up the exact meaning of each candidate word, applying the process of elimination, etc. So, you can save lots of time right now by skipping all this and simply calling it something meaningless, or worse yet, something inaccurate and therefore misleading. It is a fact that you will indeed experience immediate time savings right now if you do this. However, it is also a fact that the time you save now by performing this act of sabotage against yourself will invariably be paid a hundredfold later, when you and your coworkers will be wondering what on earth was meant by this meaningless name, or struggling with the realization that it is being used in the code in ways that are in conflict with its meaning.\nOf course, Do It Right ™ does not apply only to naming, it applies to everything. And when I say everything, I mean E V E R Y T H I N G. The practice of Do It Right ™ must be a conditioned reflex; it must be the default, reliable, fail-safe, look-no-further choice that we always make, without spending time calculating the costs vs. savings of Do it Right ™, debating whether we should Do It Right ™ or not Do It Right ™, etc. The term Do It Right ™ contains in it the reason why we should Do It Right ™.\nC.V. Driven Development: See Martin Jee's blog - CV Driven Development (CDD)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2018-02-05T15:55:42.32Z","permalink":"https://blog.michael.gr/post/2018-02-code-craftsmanship/","title":"On Code Craftsmanship"},{"content":"\rA high-tech, sci-fi horror short-story written on the evening of January 25, 2018.\nThere was a guy who got in a quarrel with his girlfriend, and she kicked him out of her apartment without even throwing his clothes out the window for him. So there he was, naked on the street, not knowing what to do. Out of necessity, he grabbed a tablecloth from a restaurant, draped himself with it, and started to go home, trying to look as if everything was normal and under control.\n(Useful pre-reading: About these papers)\nPeople saw him walking down the street draped with a tablecloth, and thought that he must be making some sort of fashion statement. Some of them decided to imitate him by also wearing tablecloth, and lo and behold, before you knew it there was a tablecloth-wearing movement that was gaining ground like wildfire.\nNow, imagine that all this happened a long time ago, and you are now living in a society in which a large part of the population is regularly wearing tablecloth. This form of attire is considered perfectly normal, even by people who do not wear it. A multitude of explanations have been invented after the fact to try and explain why tablecloth is better than conventional clothing, as if wearing tablecloth was invented on purpose. They will try to convince you to also wear tablecloth with truly disarming statements like the following:\nTablecloth is easy: you don't have to learn how to use complicated buttons and zippers and belts and buckles and what not; just hold it in place with your hand.\nTablecloth is simple: it is one size fits all, one shape suits all, no need for designs, no need for cutting and sewing parts together, and the best thing of all? no seam lines!\nTablecloth is convenient: when putting it on, you don't have to make your hands fit through sleeves and your legs fit through trousers; you just throw the tablecloth over you, and you are good to go. Want to take it off? no need for complicated motions, just let it fall off of you.\nTablecloth is fashionable. Tablecloth is hip. Tablecloth is cool. Who can argue with that?\nAnd that, ladies and gentlemen, was my JavaScript analogy.\nP.S.\nTablecloth is a source of innovation. Every six months or so, someone comes up with a new pattern for printing on tablecloth, which invariably becomes an instant fashion hit, thus revolutionizing the way we dress.\nAlso see: On Scripting Languages.\nCover image: Deerlux 100% Pure Linen Washable Tablecloth Solid Color from target.com\n","date":"2018-01-25T18:47:57.109Z","permalink":"https://blog.michael.gr/post/2018-01-tablecloth/","title":"Tablecloth"},{"content":" Simplicity is the art of hiding complexity\nRob Pike, \u0026quot;Simplicity is Complicated\u0026quot;, dotGo 2015\n(https://www.youtube.com/watch?v=rFejpH_tAHM)\n","date":"2017-12-07T12:44:13.553Z","permalink":"https://blog.michael.gr/post/2017-12-simplicity/","title":"Simplicity"},{"content":"Scenario:\nYou are an administrator on your machine. Your machine is either: In a Windows Domain, and you don't want the domain admins messing with it. Not in a Windows Domain, and you just don't want useless services running. In this case, what you probably want to do is prevent the Group Policy Client Service from running on your machine. Unfortunately, that's not a straightforward task to accomplish, because if you go to \u0026quot;services\u0026quot; and try to stop or disable this service, Windows doesn't let you.\nHere is how to do it.\nThese instructions have worked for me on Windows 7; they might also work on other versions of windows. If there is anything in these instructions that you don't quite understand, what it means is that these instructions are not for you; don't try to follow them, you are going to wreck things. Ignore this post, move on.\nUsing regedit go to HKLM\\SYSTEM\\CurrentControlSet\\services\\gpsvc and: Change the owner to yourself. Grant Administrators (not just you) full control. Change the value of “Start” from “2” to “4”. Now go to HKLM\\SYSTEM\\CurrentControlSet\\Control\\Winlogon\\Notifications\\Components\\GPClient and: Change the owner to yourself. Grant Administrators (not just you) full control. Delete the entire key. (Possibly after exporting it so as to have a backup.) Restart your machine. Old comments\nAnonymous 2025-05-13 12:31:49 UTC\nThank you! Still functional as of May 2025.\nSide note: This sent be down the rabbit hole of \u0026quot;Per-User Services\u0026quot;. Which if you aren't familiar are services created upon user login (booting windows) and share the same of their parent service with random alpha-numeric suffixes (ie 'MessagingService' and MessagingService_4b77e).\nI see you were still on Windows 7 when you published this even though Windows 11 released 2 years earlier, I'm starting to see why.\nAnonymous 2024-01-05 12:01:50 UTC\nAfter doing this it shows error editing value. Error writing the value's new content\nA 2026-01-04 12:54:00 AM GMT+1\nDoesnt work on windows 11?\nAlexander 2025-04-08 07:55:01 UTC\nGreat! (win11)\nAnonymous 2025-05-13 12:30:22 UTC\nThank you! Still functional as of May 2025.\nSide note: This sent me down the rabbit hole of \u0026quot;Per-User Services\u0026quot;. Which if you aren't familiar are services created upon user login (booting windows) and share the same name of their parent service with random alpha-numeric suffixes (ie 'MessagingService' and 'MessagingService_4b77e').\nAt first glance these are extremely suspect, even more suspect when attempting to disable the service results in an error, and EVEN MORE sus if you notice the service is running under the current user rather than 'Local System Account' that the parent service uses.\nI notice you were still on Windows 7 when you published this even though Windows 10 was released in 2015. I'm starting to see why...\n","date":"2017-11-19T18:14:41.574Z","permalink":"https://blog.michael.gr/post/2017-11-disabling-group-policy-client-service/","title":"Disabling the Group Policy Client Service in Windows"},{"content":" Greg Young - The Long Sad History of Microservices\nFrom the \u0026quot;Build Stuff\u0026quot; event of April 2017.\nTalk begins at 9:45.\nHighlights of the talk:\n27:00 Placing a network between modules simply to enforce programmer discipline\n29:05 There is other levels of isolation I can go to. I can run a docker container per service. That's the coolest stuff right? What that means is I can make it work on my machine so I send my machine to production.\n29:52 Now, one thing that's very useful is I don't necessarily want to make this decision up front. And I don't necessarily want to make the same decision in dev as in production. I may want in dev to have a different way that we run things, why? because bringing up 19 docker containers on your laptop is not very much fun. I may prefer to host everything inside a single process to make debugging and such a lot easier when I am running on dev in my laptop. Whereas in production we may go off to multiple nodes.\n34:16 If you have maintenance windows, why are you working towards getting rid of your maintenance windows? Is this a business drive or is this you just being like C.V. driven development?\nMy notes:\nUnfortunately his shrieky voice makes him sound like he is bitching about things, which in a sense he is, but it would help his cause to deliver his criticism in a more palatable tone. Also, in order to make his point about microservices being nothing new he seems to disregard statelessness.\nResources referenced in the talk:\nhttps://en.wikipedia.org/wiki/Queueing_theory\nhttps://en.wikipedia.org/wiki/π-calculus\nhttps://en.wikipedia.org/wiki/Actor_model\nLeslie Lamport - Time, Clocks, and the Ordering of Events in a Distributed System\n(available on the interwebz)\nC.A.R. Hoare - Communicating Sequential Processes\n(available on the interwebz)\n","date":"2017-10-27T08:32:53.495Z","permalink":"https://blog.michael.gr/post/2017-10-my-notes-on-greg-young-long-sad-history/","title":"My notes on \"Greg Young - The Long Sad History of Microservices\""},{"content":"\rNow that Java 9 is out, I decided to migrate to it my pet project, which is around 120K lines of java.\nThe first step is to just start compiling and running against jdk9, without using any of its features yet.\nThis is an account of the surprisingly few issues that I encountered during this first step and how I resolved them.\nIssue #1: Object.finalize() has been deprecated. The javadoc of Object.finalize() explains why it has been deprecated and suggests the use of java.lang.ref.Cleaner and java.lang.ref.PhantomReference as alternatives.\nSolution: For now, an acceptable solution is to just mark any overrides of Object.finalize() as also deprecated, so that no warnings are issued.\nIssue #2: AccessibleObject.isAccessible() has been deprecated. This means that method.isAccessible() and field.isAccessible() should not be used anymore.\nSolution:\nReplace this:\nObject callMethod( Object instance, Method method ) { boolean access = method.isAccessible(); with this:\nObject callMethod( Object instance, Method method ) { boolean access = method.canAccess( instance ); Issue #3: com.sun.xml.internal.stream.XMLInputFactoryImpl is not visible anymore. Solution:\nReplace this:\nXMLInputFactory xmlInputFactory = new XMLInputFactoryImpl();\nwith this:\nXMLInputFactory xmlInputFactory = XMLInputFactory.newDefaultFactory();\nIssue #4: Class.newInstance() has been deprecated. Solution:\nReplace this:\nClass.forName( \u0026quot;com.mysql.jdbc.Driver\u0026quot; ).newInstance();\nwith this:\nClass.forName( \u0026quot;com.mysql.jdbc.Driver\u0026quot; ).getDeclaredConstructor().newInstance();\nIssue #5: com.sun.nio.file.ExtendedOpenOption does not exist anymore. I was making use of ExtendedOpenOption.NOSHARE_WRITE, but the ExtendedOpenOption enum has now been moved into some module called jdk.unsupported and even though IntelliJ IDEA somehow does see the type, the compiler does not see it.\nSolution:\nStop using com.sun.nio.file.ExtendedOpenOption. I wish I knew of an alternative.\nIssue #6 The java compiler now resolves symbolic links of source files This is a problem for me, because I have all my projects in C:\\Users\\Michael\\Projects, but this is actually a link to D:\\Michael\\Docs\\Projects, which I normally never touch. This has been working fine for years, but now with java 9 when an error occurs in a source file, javac reports the error using the resolved source pathname, which is on drive D:, and this completely confuses the IDE which does not know of any source file with that pathname.\nSolution:\nAbandon the practice of accessing the project from a path which contains a symbolic link, work on the project in its actual location.\nIssue #7 ClassLoader.getSystemClassLoader() cannot be cast to URLClassLoader anymore. This was definitely to be expected.\nSolution:\nReplace this:\nURLClassLoader urlClassLoader = (URLClassLoader)ClassLoader.getSystemClassLoader(); for( URL url : urlClassLoader.getURLs() ) { ...\nwith this:\nClassLoader classLoader = ClassLoader.getSystemClassLoader(); for( URL url : Collections.list( classLoader.getResources( \u0026quot;\u0026quot; ) ) ) { ...\nIssue #8 Jackson fails to deserialize exceptions from JSON. For debugging purposes, I used to have exceptions serialized into JSON and deserialized later. For some unknown reason, this does not work anymore. Now each element of the stackTrace contains several additional fields, like classLoaderName, moduleName and moduleVersion, and although Jackson serializes them just fine, for some reason it fails to deserialize them. Upgrading to the latest version of jackson-jaxrs-json-provider (version 2.9.0) did not fix this problem.\nSolution: stop serializing and deserializing exceptions.\nAt the end of all this, my 817 tests passed, so I consider the migration complete.\n","date":"2017-09-30T20:27:45.123Z","permalink":"https://blog.michael.gr/post/2017-10-migrating-project-from-java-8-to-java-9/","title":"Migrating a project from java 8 to java 9"},{"content":" Screenshot of Borland Turbo Debugger found on the interwebz, possibly the same version that I was using back then. This is a hacking story from my University years. It ends with a nice bit about human qualities.\nParts of this text are very deeply technical, to the point where they might only be understood by experienced programmers with a deep, low-level understanding of how computers work. This is in a sense intentional: if you do not understand a sentence, it is because very advanced hacking stuff is happening. Feel free to skim through such sentences.\nA Hacker's Tale The University had several computer labs, most of them equipped with Unix workstations, a few with PCs. I would often be found in the PC lab, since I was already quite familiar with that kind of machine and operating system. It was the early nineties, and PCs back then were running MS-DOS. Networking was done by connecting them to Novell? servers via coaxial Ethernet cable, which delivered a (decent, for that time) 10 megabits per second.\nEach PC in the lab was running a network driver, which was making parts of the server's filesystem visible locally as DOS drives. These drives were available only at the filesystem level: if you bypassed the OS and invoked the BIOS to enumerate the physical hard disks on the system, they did not show up, because they did not physically exist.\nFilesystem access to these drives was subject to security checks performed by the Novell server, which was running some proprietary Novell operating system, so the whole setup was fairly secure, and for even higher security, the server was kept locked in a cabinet, so nobody but the administrator had physical access to it. The administrator of the lab was Dr. \u0026quot;A\u0026quot;, and he had appointed as co-administrator a fellow student and friend of mine, Bashir.\nBack in those days, if you were a power user, (let alone a computer lab administrator,) you absolutely had to be using the Norton Utilities.\nScreen capture of the main menu of the Norton Utilities; found on the interwebz. Of course, most of these utilities required physical access to the disk, so it was impossible to use them on the server, but they could be used on workstations, so Bashir had stored them on the server in order to be able to access them from any workstation.\nOne day Bashir had to troubleshoot something on one of the workstations in the lab, so he connected to the server using his password, he launched the Norton Utilities from the server, and then I noticed that upon startup the utilities asked him for a password again.\nI asked Bashir what is the deal with that second password, and he told me that Dr. \u0026quot;A\u0026quot; had made use of a password-protection feature of the Norton Utilities so as to prevent people from using them to destroy the server. I told him that this was absurd, because the utilities could only be launched from the server, they could not be used against the server, and he told me that he had tried to explain that to Dr. \u0026quot;A\u0026quot;, but he would not understand.\nOne of the following days I was at the computer lab very late, like 2 o'clock in the morning. I was there not because I had some assignment to complete, but because you know, if you are mad about programming, then the computer lab is where you are likely to be found late at night. The only other person in the lab at that time was Yoshi. Me and Yoshi were the best students in the class. I could not exactly call him a friend, because he kind of kept a distance, but I did respect the fact that he was the most knowledgeable among the other students, and I hoped that he thought more or less the same about me.\nI was thinking about the password with which the Norton Utilities had been protected. I was thinking that in choosing to protect the Norton Utilities with a password, Dr. \u0026quot;A\u0026quot; may have inadvertently compromised the security of the server, because:\nGiven the lax security practices of the era, the password that the Norton Utilities had been protected with was in all likelihood the same as the administrator's password for the Novell server. For as long as the password was kept only by the Novell server, it was fairly secure; however, the moment the password was given to the Norton Utilities, the safety of the password depended on how secure the Norton Utilities were. By nature, the Norton Utilities could not be anywhere near as secure as the server itself, because the Norton Utilities resided on the server's filesystem, so the only storage medium available to them for storing the password was that same filesystem, meaning that anyone who had sufficient access to launch the Norton Utilities (even if prevented, by password, from actually using them,) also had access to the password, if only one knew how to find it. I looked at the directory where the Norton Utilities were installed, but I could not find any file that looked like it may contain a password. I wondered whether the password was stored within the executable itself. Normally, nobody writes data into executables, but for very special purposes, such as password protection, one could conceivably do that. I checked the time-stamp of the executable, and sure enough, it was more than an hour later than the time-stamp of the installation directory. This did not exactly prove anything, but it was a very good indication that something had been written into the executable after installation took place.\nSo, I rolled up my sleeves and got to work. I took out the floppy disk that I always carried with me, and launched Borland's Turbo Debugger from it.\nI loaded the Norton Utilities executable with the debugger, and I let it run. When it prompted me for the password, I entered my name instead. Then, I hit Ctrl+Break to switch to the debugger. The debugger stopped deep inside the BIOS service which waits for the next keystroke. I managed to single-step out of the BIOS service and into the Norton Utilities, and I continued single-stepping, hoping to see it doing something with the password that I had just entered, but that did not seem to be happening. I was going over hundreds upon hundreds of instructions which I had no idea what they were doing, but one thing I could tell was that they were not dealing with passwords in any way. I reasoned that since the Norton Utilities had a text-mode Graphical User Interface, I was probably deep inside GUI code, which had been invoked from some application logic that was thousands, possibly even tens of thousands of instructions away. So, I decided to take a different approach.\nI performed a global memory search for my name, and I found the place within the application's data segment where it had been stored after being read from the BIOS. I placed a \u0026quot;memory access\u0026quot; breakpoint on that memory location, and I let the Norton Utilities resume execution. When it stopped, I was finally looking at code that was about to do something with the password that I had entered. I started single-stepping again.\nAfter not too many more instructions, I reached a loop that was reading characters from my name, XORing each character with 01Ah, essentially turning it into gibberish, and storing it in a buffer. I realized that I was looking at the user-entered password being encrypted prior to being compared against the encrypted administrator's password. I found it amusing that the Norton Utilities, one of the top commercial tools of that era, created by some of the brightest programmers around, was using one of the simplest imaginable and most woefully insecure encryption mechanisms, the letter-by-letter XOR method, for password encryption. I single-stepped further, and sure enough, there was a loop that was comparing the contents of that buffer against the contents of some other, unknown buffer, which also contained gibberish, and was in all certainty the encrypted administrator's password.\nThe comparison failed at the very first character, so I was taken out of the loop and to the end of the function. I stepped out of the function, and I noticed that the caller was checking whether the AX register contained a zero or non-zero value. This is usually the machine code that gets emitted by a compiler when you have an if-statement which invokes a boolean function and does one thing if the function returned true, or another thing if the function returned false. My guess was that the if-statement was something like if comparison-succeeded then proceed else show-error-message. Clearly, the comparison had not succeeded, and AX contained non-zero; so, I changed it to zero in order to pretend that the comparison had in fact succeeded; I knew there was no need to continue single-stepping anymore; I just resumed program execution from that point, and voila, I was running the Norton Utilities as if I was the administrator.\nThat was one of the most gratifying moments of my life.\nOf course, the next step was to find out exactly what the administrator's password was. \u0026quot;In a few minutes I will have the administrator's password\u0026quot;, I said to Yoshi, who was completely unaware of what I had been doing all that time. Yoshi turned his head in my direction by barely an inch, said \u0026quot;uh-uh,\u0026quot; and turned back to his screen. Obviously, he highly doubted my claim.\nI reloaded the Norton Utilities in Turbo Debugger, I repeated the same process until I reached the encryption routine, and took a note of its address. I allowed the routine to encrypt my name again, and proceeded to the comparison routine. Once I had the address of the buffer that contained the administrator's encrypted password, I instructed the debugger to invoke the encryption routine on that buffer, reasoning that a simple XORing function will decrypt when reapplied. This turned the contents of the buffer from gibberish into plain text. I was looking at the administrator's password.\n\u0026quot;I have it!\u0026quot; I said to Yoshi.\nThat did get his attention.\nHe came over to see. I was so excited that I could not sit on my chair, I showed my feat to him while we were both standing in front of the computer. I relaunched the Norton Utilities, typed the magic letters, and I was in. Yoshi was impressed. He asked me how I did it, and I explained everything to him.\n\u0026quot;I bet it is the same as the network administrator's password\u0026quot;, I said, and tried to log into the Novell server as admin using this password. Sure enough, I was in the Novell server's administrator account.\n\u0026quot;It may even be the same as the personal password of Dr. \u0026quot;A\u0026quot;\u0026quot;, I said.\nI tried it, and yes, I was also in Dr. \u0026quot;A\u0026quot;'s personal account.\nYoshi had a huge grin in his face. He was really impressed.\nAnd then, these words came out of his mouth:\n\u0026quot;Dude, I thought you were an asshole!\u0026quot;\nI laughed, because I kind of knew why Yoshi was saying that. Back then I was an arrogant bastard, I liked to show off my knowledge, and I tended to be derisive when I knew I was right and the other person was wrong in a technical argument. I probably even was snobbish at times, certainly not to Yoshi, but to others, and I suppose Yoshi had noticed.\nI also laughed because of the sincerity with which Yoshi had said that, the purity of his intentions in saying it. Obviously, for Yoshi to say that to my face, there was not even a trace of a suspicion of me being an asshole anymore; it was not just left behind, it was as if it had never happened. I was just totally not an asshole, period.\nI also laughed because I knew that hacking prowess does not really tell us anything as to whether someone is an asshole or not. I thought that maybe I did not really deserve that much credit. I kept that in mind for the rest of my life.\n","date":"2017-08-27T16:32:09.686Z","permalink":"https://blog.michael.gr/post/2017-09-a-hackers-tale/","title":"A Hacker's Tale"},{"content":"\u0026quot;Simple Made Easy\u0026quot; presentation by Rich Hickey from the InfoQ Software Development Conference, recorded at Strangeloop 2011\nWatch the presentation here:\nhttps://www.infoq.com/presentations/Simple-Made-Easy\n(the slideshow plays alongside with the video.)\nMy notes on the presentation:\n\u0026quot;Simplicity is prerequisite for reliability\u0026quot; - Edsger W. Dijkstra\nSimple vs. Complex, Easy vs. Hard\nSimple: one role; one task; one concept; one dimension. (But not one instance; one operation.) Lack of interleaving, but not lack of cardinality. Objective notion.\nEasy: near, at hand; near to our understanding / skill set; familiar; near our capabilities. Relative notion.\nRegarding \u0026quot;at hand\u0026quot; and \u0026quot;familiar\u0026quot;:\nI think that collectively we are infatuated with these two notions of easy.\nWe are just so self-involved in these two aspects, it's hurting us tremendously.\nAll we care about is, can I get it instantly and start running it in five seconds?\nIt could be this giant hairball that you got, but all you care for is can you get it?\nIn addition, we are fixated on \u0026quot;oh, I can't, I can't read that.\u0026quot;\nI can't read German. Does this mean that German is unreadable? No! I don't know German!\nSo, you know, this sort of approach is definitely not helpful.\nIn particular, if you want everything to be familiar you will never learn anything new\n'cause it can't be significantly different from what you already know, and not drift\naway from the familiarity.\nRegarding \u0026quot;near to our capabilities\u0026quot;:\ndue to a combination of hubris and insecurity we never really talk about whether or not something is outside of our capabilities.\nRegarding easy being \u0026quot;relative\u0026quot;:\nPlaying the violin and reading German are really hard for me; they are easy for other people; certain other people.\nSo, unlike simple, where we can go and look for interleavings, look for braiding, easy is always going to be easy for whom; or hard for whom; it is a relative term. The fact that we throw these things around, sort of casually saying, oh I like to use that technology cause it is simple, and when I am saying simple I mean easy, and when I am saying easy I mean because I already know something that looks very much alike that, is how this whole thing degrades, and we can never have an objective discussion about the qualities that matter to us in our software.\nConstructs and artifacts\nWe program with constructs. We have programming languages, we use particular libraries, and those things in and of themselves, when we look at them, when we look at the code we write, have certain characteristics in and of themselves. But we are in a business of artifacts, right? We don't ship source code, and the user doesn't look at our source code and say \u0026quot;oh, this is so pleasant\u0026quot;. They run our software. And they run it for a long period of time. [...] all that stuff, the running of it, the performance of it, the ability to change it, all is an attribute of the artifact. Not the original construct. But again, here we still focus so much on our experience of the use of the construct. --Oh look, I only had to type 16 characters. Wow, that's great, no semicolons. Or things like that. This whole notion of programmer convenience. Again, we are infatuated by it, not to our benefit.\nSo, we are going to contrast this with the impacts of long term use. What does this mean to use this long term? And, what's there? What's there is all the meat, right? Does the software do what it is supposed to do? Is it of high quality? Can we rely on it doing what it's supposed to do? Can we fix problems when they arise, and if we are given a new requirement, can we change it? These things have nothing to do with the construct as we typed it in, or very little to do with it, and have a lot to do with the attributes of the artifact.\nWe have to start assessing our constructs based around the artifacts, not around the look and feel of the experience of typing it in, or the cultural aspects of it.\nWe can only hope to make reliable those things we can understand.\nWe can only consider a few things at a time.\nIntertwined things must be considered together.\nComplexity undermines understanding.\nWhat's true of every bug found in the field?\nIt passed the type checker.\nWhat else did it do?\nIt passed all the tests.\nYour ability to reason about your program is critical to debugging.\nDevelopment speed:\nNow, of course everyone is going to start moaning, but I have all this speed, I am agile, I'm fast, this easy stuff is making my life good because I have a lot of speed.\nSo, what kind of runner can run as fast as they possibly can from the very start of the race?\n(Audience: Sprinters.)\nRight. Only someone who runs really short races!\nBut of course we are programmers, and we are smarter than runners, apparently, because we know how to fix that problem, right? We just fire the starting pistol every hundred yards; and call it a new sprint. Right?\n(Applause)\nI don't know why they haven't figured that out.\nIt's my contention, based on experience, that if you ignore complexity you will slow down, you will invariably slow down over the long haul. Of course if you are doing something that is really short term, you don't need any of this, you can write it, you know, in ones and zeros.\nEmphasizing ease gives early speed.\nIgnoring complexity will slow you down over the long haul.\nOn throwaway or trivial projects, nothing much matters.\nIf you focus on ease, and ignore simplicity (so I am not saying you can't try to do both, that'd be great) but if you focus on ease you will be able to go as fast as possible from the beginning of the race, but no matter what technology you use, or sprints or firing pistols or whatever, the complexity will eventually kill you; it will kill you in a way that will make every sprint accomplish less, most sprints being about completely redoing things that you have already done, and the net event is that you are not moving forward in any significant way.\nNow, if you start by focusing on simplicity why can't you go as fast as possible right from the beginning? Cause some tools that are simple are actually as easy to use as some tools that are not, why can't you go as fast then? You have to think --you have to actually apply some simplicity work to the problem before you start, and that's going to give you this ramp-up.\nSo, one of the problems I think we have is this conundrum that some things that are easy actually are complex.\nBut we don't care about that, right? Again, the user does not look at our software and they don't actually care very much about how good a time we had when we were writing it. What they care about is what the program does, and if it works well it will be related to whether or not the output of those constructs were simple --in other words, what complexity did they yield.\nWhen there is complexity there, we are going to call that _incidental complexity_. It was not part of the user asked us to do, we chose the tool, it had some incidental complexity.\nThe mental capability part\nThe fact is we can learn more things, we actually can't get much smarter. We are not going to move our brain closer to the complexity, we have to make things near by simplifying them. The truth here is that there are not these super bright people who can do these amazing things and everyone else is stuck. Because the juggling analogy is pretty close. The average juggler can do three balls. The most amazing juggler in the world can do like nine balls or twelve, or something like that. They can't do twenty, or a hundred. We are all very limited. Compared to the complexity we can create, we are all statistically at the same point in our ability to understand it, which is not very good.\n(section about parens rather uninteresting to anyone not using them.)\nYou never see in these discussions was there a tradeoff? Is there any downside? Is there anything bad that comes along with this? Never, nothing; we are looking all for benefits. So I think that as programmers now I think we are looking all for benefits and we are not looking at the byproducts.\nComplect vs. Compose\nComposing simple components is the key to writing robust software. State is Never Simple\nComplects value and time It is easy, in the at-hand and familiar senses Interweaves everything that touches it, directly or indirectly Not mitigated by modules, encapsulation The Complexity Toolkit\nConstruct Complects State Everything that touches it Objects State, identity, value Methods Function and state, namespaces Syntax Meaning, order Inheritance Types Switch/matching Multiple who/what pairs var(iable)s Value, time Imperative loops, fold what/how Actors what/who ORM OMG Conditionals Why, rest of program The Simplicity Toolkit\nConstruct Get it via... Values final, persistent collections Functions a.k.a. stateless methods Namespaces language support Data Maps, arrays, sets, XML, JSON etc Polymorphism a la carte Protocols, type classes Managed refs Clojure/Haskell refs Set functions Libraries Queues Libraries Declarative data manipulation SQL/LINQ/Datalog Rules Libraries, Prolog Consistency Transactions, values Complexity that you have no control over: Environmental Complexity (Inherent)\nNot part of the problem, part of the implementation. (You cannot go back to the customer and say the thing you wanted is not good because I have GC problems.)\n\u0026quot;Programming, when stripped of all its circumstantial irrelevancies, boils down to no more and no less than very effective thinking so as to avoid unmastered complexity, to very vigorous separation of your many different concerns\u0026quot;. --Edsger W. Dijkstra\nStrictly separating what from how is the key to making how somebody else's problem.\n\u0026quot;Simplicity is the ultimate sophistication\u0026quot;. --Leonardo da Vinci\nHere is a somewhat shorter and somewhat different version of this presentation:\n","date":"2017-07-12T10:46:10.23Z","permalink":"https://blog.michael.gr/post/2017-07-rich-hickey-simple-made-easy/","title":"Rich Hickey - Simple Made Easy"},{"content":"A couple of weeks ago some of us went to the TechSummit conference organized by LeaseWeb. Here is a list of the talks that I attended, along with a short description for each.\nThe first presentation was “Shaving my head made me a better programmer” by Alex Qin, which was about what it is like to be a woman, and specifically a programmer, in the U.S. tech industry. (And in the University before that.) She talked about the inequality, the sexism, and the harassment. She mentioned that she once gave a talk in a really big conference about accessibility in the U.S., and afterwards she was asked “How do I talk to women at bars?” The head-shaving part refers to how changing her appearance resulted in being taken more seriously. It was quite an interesting talk, though I suspect that in Amsterdam, she was to a large extent preaching to the choir.\nThe next presentation was about “Least privilege container deployment” by D. Monica from Docker.\nAs you understand, it was mostly about security. He spoke about six tools that have been developed by the guys at Docker that can be used to provision and manage resources in a distributed system while maintaining security. These tools are infraKit, linuxKit, runC, containerD, Notary and swarmKit, none of which I had ever heard of before.\nThe next presentation was “Why you should run performance tests in the cloud or on CDN”, by J. van Gaalen.\nI attended this one because I have an interest in performance testing, as I am participating in the Performance CoP, but I did not find it as interesting as I had hoped. (No surprises and no eye-opening revelations there.) The talk focused on the single big-bang event scenario, where you normally have no load on your web site, and suddenly a specific event (e.g. a football game) causes millions of users to visit your site simultaneously. Admittedly, this scenario has some exotic performance test requirements, but it is largely irrelevant to us.\nThe next presentation was “Deploying Image Recognition with TensorFlow and Kubernetes” by C. West from Google.\nThis was an interesting presentation because of the subject, and because the presenter was quite good at it. Within half an hour we were taken through a tutorial on setting up a machine learning application using google technologies.\nThe next presentation was “Containerize everything: Stateful apps on Kubernetes” by Chris Madden.\nIt was mainly about using Persistent Volumes of Kubernetes to provision storage for persisting the state of (otherwise stateless) scalable web applications. Fairly narrow scope, fairly convincing.\nThe next presentation was “Kubernetes in production” by K. Bollen, (who comes from game development so he likes things to run fast,) and it was a description of how they deploy their web app using kubernetes. A considerable part of the talk had to do with features and peculiarities of the google infrastructure that they use for this.\nMind you, these were just the talks that I attended. There were more, some happening in parallel, and at least one more happening after I left. (By the time 6th talk was over, my brain was fried.)\n","date":"2017-06-16T10:11:13.102Z","permalink":"https://blog.michael.gr/post/2017-06-techsummit-amsterdam-2017-jun-1st/","title":"6 videos from TechSummit Amsterdam 2017 (Jun 1st)"},{"content":"Introduction Universally Unique Identifiers (UUIDs) otherwise known as Globally Unique Identifiers (GUIDs) are 128-bit numbers that are often used to identify information. In its canonical representation, a UUID looks like this: 2205cf3e-139c-4abc-be2d-e29b692934b0.\nThe Wikipedia entry for Universally Unique Identifier says that they are for practical purposes unique and that while the probability that a UUID will be duplicated is not zero, it is so close to zero as to be negligible. Wikipedia then does the math and shows that if 103 trillion UUIDs are generated, the chance of duplication among them is one in a billion.\nDespite the infinitesimally small chances of receiving a duplicate UUID, there exist programmers out there who are afraid of this actually happening, and who will not hesitate to suspect duplicate UUIDs as being responsible for an observed malfunction rather than first look for a bug in their code. Clearly, these folks do not understand the meaning of infinitesimally small chance, so let me explain it:\nInfinitesimally small chance means practically impossible to happen, and the practically part is only mentioned for scientific correctness: practically, you can disregard the word practically and consider it as simply impossible to happen.\nGreat.\nNow, let me tell you why I hate UUIDs.\n(Useful pre-reading: About these papers)\nKnown disadvantages Disadvantages of UUIDs that are unanimously recognized are the following:\nA UUID is 4 times larger than a regular 32-bit integer. This undeniably affects the performance and storage demands of a system. Apparently, the industry has decided that the benefits of UUIDs are so great that they are worth the sacrifice. The randomness of UUIDs is technically unsuitable in certain scenarios, for example in database clustered indexes, where the record ids must be sequential. When a UUID is needed in such applications, a special kind of UUID is used which contains a sequential part, but its uniqueness guarantees are severely limited. (Remember that one-in-a-billion chance of duplication mentioned earlier? Well, you may forget it now.) UUIDs are cumbersome to debug with, because they are unreadable, non-sequential, and non-repeatable. Debugging is a notoriously difficult process, so we do not need anything that makes it harder than it already is, but the use of UUIDs imposes an additional burden on debugging. In the paragraphs that follow I will address some of those disadvantages in greater detail, and I will also address some disadvantages that I have personally identified with UUIDs.\nThe entropy When looking at a table of columns, I find that the UUID column is always the angry column. This is because the 32 hexadecimal digits that make up a UUID have a higher concentration of entropy than anything else that I deal with during a regular working day. (It helps that IntelliJ IDEA spares me from having to see git commit hashes.) This is to say that the overwhelming majority of all the entropy that I am exposed to nowadays is due to seeing UUIDs. This was not happening in the days before the UUID; entire weeks could pass without seeing something as hopelessly nonsensical as a UUID, requiring me to coerce my brain to ignore it because there is no sense to be made there.\nThe higher the entropy of the visual stimulus we are exposed to, the higher the cognitive effort required to process it, even if just to dismiss it as un-processable. This makes UUIDs very tiresome to work with.\nThe Undebuggability Ben Morris says in The Problem with GUIDs :\nThis readability issue is often dismissed as mere inconvenience, but it's a real problem for anybody who has to support applications or trouble-shoot data. GUIDs are often a lazy solution selected by developers who will not have to deal with the support consequences.\nIf you're going to replicate or combine disparate data sources then you really will need some globally unique identifiers. However, this is an implementation detail that does not have to be baked into data design. There's nothing to stop you from adding separate identifiers onto your data rows in response to replication requirements.\nLet me explain in a bit more detail what the problem is with troubleshooting in a system that identifies entities using UUIDs instead of regular sequentially issued integers.\nWith sequentially issued integers you can take a mental note of the id of the entity that you are troubleshooting, and then see when and where it pops up. This means noting say, the number 1015, and then looking for a 1015 to appear again. With UUIDs you cannot do that, because a UUID is impossible to memorize. You literally cannot tell that the UUID that you are seeing now is the same as a UUID that you saw a few seconds earlier. Even if you write down the UUID that you are looking for, there is still considerable difficulty in visually comparing a UUID on the screen with a copy you made earlier. While you are looking for that 1015, if you see 1010, you know you are close. When you see 1020, you know you passed it. With UUIDs, you cannot do that, because they do not form a sequence. Even when UUIDs are of the special sequentially issued kind, the sequential part is hidden among random digits, making extraction difficult, and even if you detect the subset of the digits that make up the counter, it is in hexadecimal instead of decimal, so it is hard to make sense out of it. In the mean time, when the ids of some other entity increment from 2100 to 2200, you know that for every entity of the kind you are troubleshooting, 10 entities of the other kind are being generated. So, if you suddenly see a newly issued id of the other kind in the 3000 range, you know that something for some reason generated more of that kind of entity than expected. No such hint is available when using UUIDs, because they are just random numbers. Most importantly, on a subsequent test run, starting with the same initial database state, you can expect the exact same sequential ids to be issued, so you have the exact same ids to troubleshoot. Not so with UUIDs, which are entirely different from run to run. So, what it boils down to is that none of the most common lines of reasoning are applicable when troubleshooting UUIDs: you are constantly in the dark about most aspects that have to do with the identifiers of the entities that you are dealing with.\nLet that sink in for a moment:\nThe identifier of an entity is what you use to identify the entity with.\nIt is a very important piece of information.\nArguably, in most scenarios, it is the most important piece of information about an entity.\nUUIDs invalidate all previously known methods of reasoning about identifiers.\nThey are essentially useless to humans.\nWe don't want that.\nAs a matter of fact, let me put it in blunt terms to drive home a point:\nWhat kind of idiot thought that this would be a good idea?\nThe Needlessness I agree that UUIDs have certain usages, but quite often I see them being used in situations where they are not needed, or they are rather unwanted. Here is a stackoverflow question where some genius is assigning names to his threads, and he is using UUIDs as names: Stack Overflow - Writing a custom ThreadPool\nThe only scenario where you really need UUIDs is when you have a decentralized system (consisting of \u0026quot;nodes\u0026quot;) in which all of the following conditions hold true:\nYou want to have no single point of failure and therefore no single node issuing unique identifiers. You have such high performance requirements that you do not want the nodes to have to coordinate with each other in order to issue unique identifiers. You are for some reason unable to issue a guaranteed unique node id to each node, so as to trivially solve the problem of unique keys by making each key consist of node id + node-local sequential number. If you do not have a situation that meets all of the above criteria, then you are only using GUIDs because you heard of some really smart and successful guys using them on some really monstrous systems, and you want to be like them.\nThe only kind of scenario that I can think of that would actually meet the above criteria would be a system with such a large number of nodes, and such a high new node join rate, that negotiation for a unique node id for each new node would be impractical. There are probably not very many systems in existence on the planet with such requirements, which in turn means that every single one of them is a special case. There is really no point in imposing a worldwide curse on computing just because a few special cases benefit from it.\nIf you are using a database, then you probably already have a single point of failure. So, go ahead and use an SQL SEQUENCE, which is very efficient because it caches thousands of ids at a time, and has been available in RDBMS products since the eighties, and part of the standard since SQL2003.\nMany people appear to be under the impression that UUIDs are necessary for replication, but that is not true. What is necessary for replication is row identifiers that are unique over all nodes that participate in the replication of a specific table. That is \u0026quot;system-wide per-table unique identifiers\u0026quot;, which not even system-unique identifiers, and certainly a far cry from \u0026quot;globally-unique\u0026quot; identifiers. A unique row identifier could be created by concatenating a unique node identifier with a node-local, table-specific, sequential row number. It is an arbitrary choice of Microsoft SQL Server to require a ROWGUIDCOL of the xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx format for merge replication, (and transactional replication with queued updating subscriptions,) and if we are to believe the documentation, this requirement can be circumvented by creating your own GUIDs instead of using Microsoft's newid() function.\nAnother thing that is sometimes cited as a benefit of UUIDs is their alleged ability to be issued off-line. \u0026quot;Off line\u0026quot; was a condition that computing systems could suffer from in the old times. It is generally not an issue today, and the vast majority of those who cite this as a benefit of UUIDs do not really have an application at hand which really needs to be able to issue ids off-line. However, even in the extremely rare case where being \u0026quot;off-line\u0026quot; is an issue today, it can be taken care of with special handling. We really do not need to pollute everything everywhere with nonsensical entity identifiers just because some exceedingly rare special cases might benefit from them.\nThe paradigm shift When sequentially incrementing integers are used as identifiers, they represent an absolute guarantee that every identifier will be unique. When UUIDs are used, they represent an almost-absolute guarantee.\nThus, UUIDs have introduced a fundamental and completely unwanted paradigm shift in programming: we have gone from systematic absolute determinism (never leaving anything to chance) to systematic non-absolute determinism (regularly leaving something to, a however minuscule, chance.)\nYou see, that's what the almost-absolute is: non-absolute. I am not sure all these people who are so happily using UUIDs realize this. I find it sacrilegious, like picking a buffer size which is not a power of two.\nThe technological compromise Furthermore, I am not sure people realize that UUIDs represent a technological compromise. Why are UUIDs only 128 bits instead of 256 bits? 256 bits would give even more guarantees of uniqueness, right? How about 512 bits to really make sure no duplicate ever gets issued in this universe and in all parallel universes that we might one day somehow come in contact with? Wouldn't that be the ultimate? Well, obviously, there will always be an even higher number of bits that will always be better, so what it boils down to is that a compromise has been made.\nThe thing with GUIDs is that we don't want them to be huge, because then they would be wasteful, so someone had to come up with a number of bits that is small enough to not be too wasteful and yet large enough to give a reasonable guarantee against collisions. And so, 128 bits it is.\nHowever, if history has taught us anything, it is that technological compromises always seem very reasonable at the time that they are made, and invariably turn out to be unreasonable at a later point in time. There is really no difference between saying \u0026quot;128 bits should be enough for everyone\u0026quot; and saying \u0026quot;640K should be enough for everyone\u0026quot;. At the time that the decision was made to make 640K the absolute upper limit for the amount of memory that the IBM PC could be equipped with, this amount was considered so astronomically large, that nobody was expected to ever have a use for it. Similarly, in our century 128-bit UUIDs seem to be a good compromise, but with almost mathematical certainty there will be another century when this compromise will not be so good anymore.\nEpilogue I do believe that there will be a time, maybe in a couple of thousand years from now, maybe sooner, when we will be colonizing the galaxy, our population will be in the trillions, the individual devices embedded everywhere will number in the quadrillions, and every single one of those devices will be generating UUIDs at rates that are unthinkable today. When that time comes, we will inevitably start running into trouble with duplicate UUIDs popping up every once in a while in distant areas of the galaxy, and then it will be like 640k of memory all over again, two-digit-year millennium bug all over again, DLL hell all over again, all of them combined.\nWhen that time comes, I hope that we as a species still have some sufficiently low-level understanding of how our computers work, so as to be able to fix them. I fear we might not.\nThis post was inspired by a Stack Overflow answer that I wrote, here: https://stackoverflow.com/a/8642874/773113\n","date":"2017-06-12T17:47:55.631Z","permalink":"https://blog.michael.gr/post/2017-06-on-uuids-and-guids/","title":"What is wrong with UUIDs and GUIDs"},{"content":"These are my notes on Roy T. Fielding's famous Ph.D. dissertation \u0026quot;Architectural Styles and the Design of Network-based Software Architecture\u0026quot;\nWhat follows are excerpts from the dissertation, with my notes usually in parentheses.\nRoy Thomas Fielding is: chief Scientist in some tech company; Chairman, Apache Software Foundation; Visiting Scholar, W3C @ MIT CS Lab; etc; Publications, Honors, Awards, Fellowships etc. Involved in the authoring of the Internet standards for the Hypertext Transfer Protocol (HTTP) and Uniform Resource Identifiers (URI).\nAbstract:\n\u0026quot;The World Wide Web has succeeded in large part because its software architecture has been designed to meet the needs of an Internet-scale distributed hypermedia system.\u0026quot;\n(He makes it sound as if it was designed this way on purpose.)\n\u0026quot;In order to identify {...} aspects of the Web that needed improvement and avoid undesirable modifications, a model for the modern Web architecture was needed to guide its design, definition, and deployment.\u0026quot;\n(So, he admits the need to build a model after the fact.)\n\u0026quot;An architectural style is a named, coordinated set of architectural constraints.\u0026quot;\nThis dissertation defines a framework for understanding software architecture via architectural styles and demonstrates how styles can be used to guide the architectural design of network-based application software.\nA survey of architectural styles for network-based applications is used to classify styles according to the architectural properties they induce on an architecture for distributed hypermedia.\nI then introduce the Representational State Transfer (REST) architectural style and describe how REST has been used to guide the design and development of the architecture for the modern Web.\nREST emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems.\nI describe the software engineering principles guiding REST and the interaction constraints chosen to\nretain those principles, contrasting them to the constraints of other architectural styles.\nFinally, I describe the lessons learned from applying REST to the design of the Hypertext Transfer Protocol and Uniform Resource Identifier standards, and from their subsequent deployment in Web client and server software.\nIntroduction\nThe guideline that “form follows function” comes from hundreds of years of experience with failed building projects, but is often ignored by software practitioners.\n\u0026quot;The hyperbole of The Architects Sketch may seem ridiculous, but consider how often we see software projects begin with adoption of the latest fad in architectural design, and only later discover whether or not the system requirements call for such an architecture. Design-by-buzzword is a common occurrence.\u0026quot;\n(Aa-aa-aa-aa-meeeen, brutha!)\n\u0026quot;This dissertation explores a junction on the frontiers of two research disciplines in computer science: software and networking. Software research has long been concerned with the categorization of software designs and the development of design methodologies, but has rarely been able to objectively evaluate the impact of various design choices on system behavior. Networking research, in contrast, is focused on the details of generic communication behavior between systems and improving the performance of particular communication techniques, often ignoring the fact that changing the interaction style of an application can have more impact on performance than the communication protocols used for that interaction.\u0026quot;\n\u0026quot;My work is motivated by the desire to understand and evaluate the architectural design of network-based application software through principled use of architectural constraints, thereby obtaining the functional, performance, and social properties desired of an architecture. When given a name, a coordinated set of architectural constraints becomes an architectural style.\u0026quot;\n\u0026quot;Over the past six years, the REST architectural style has been used to guide the design and development of the architecture for the modern Web, as presented in Chapter 6.\u0026quot;\n(so, I am not sure I understand: did the author come up with REST, or is he just documenting it?)\nChapter 1\n\u0026quot;This raises an important distinction between software architecture and what is typically referred to as software structure: the former is an abstraction of the run-time behavior of a software system, whereas the latter is a property of the static software source code\u0026quot;\nChapter 2\n\u0026quot;The primary distinction between network-based architectures and software architectures in general is that communication between components is restricted to message passing {6}, or the equivalent of message passing if a more efficient mechanism can be selected at run-time based on the location of components {128}.\u0026quot;\n\u0026quot;Tanenbaum and van Renesse {127} make a distinction between distributed systems and network-based systems: a distributed system is one that looks to its users like an ordinary centralized system, but runs on multiple, independent CPUs. In contrast, network-based systems are those capable of operation across a network, but not necessarily in a fashion that is transparent to the user. In some cases it is desirable for the user to be aware of the difference between an action that requires a network request and one that is satisfiable on their local system, particularly when network usage implies an extra transaction cost {133}. This dissertation covers network-based systems by not limiting the candidate styles to those that preserve transparency for the user.\u0026quot;\n\u0026quot;An interesting observation about network-based applications is that the best application performance is obtained by not using the network. This essentially means that the most efficient architectural styles for a network-based application are those that can effectively minimize use of the network when it is possible to do so, through reuse of prior interactions (caching), reduction of the frequency of network interactions in relation to user actions (replicated data and disconnected operation), {...}\u0026quot;\n\u0026quot;Scalability refers to the ability of the architecture to support large numbers of components, or interactions among components, within an active configuration.\u0026quot; I do not think that's a good definition of scalability.\n\u0026quot;Scalability can be improved by simplifying components, by distributing services across many components (decentralizing the interactions), and by controlling interactions and configurations as a result of monitoring.\u0026quot;\n(I think scalability is something better thought of as achieved or not achieved, rather than something sort of achieved and then improved upon.)\n\u0026quot;Styles influence these factors by determining the location of application state, the extent of distribution, and the coupling between components.\u0026quot;\n(Interestingly enough, even though the author appears to have gotten the previous two\ntwo sentences wrong, this conclusion appears to be correct.)\n\u0026quot;Generality of connectors leads to middleware.\u0026quot;\n(I do not object to that, but I have no idea what the author is on to with it.)\n\u0026quot;Modifiability is about the ease with which a change can be made to an application architecture. Modifiability can be further broken down into evolvability, extensibility, customizability, configurability, and reusability, as described below. A particular concern of network-based systems is dynamic modifiability {98}, where the modification is made to a deployed application without stopping and restarting the entire system.\u0026quot;\n\u0026quot;the system must be prepared for gradual and fragmented change, where old and new implementations coexist, without preventing the new implementations from making use of their extended capabilities\u0026quot;\n(I think this may be a fallacy, or at least only possible in trivial scenarios. I think that what is far more likely to happen is that the introduction of a new feature will be incompatible with keeping an old feature around in any way shape or form. Essentially, the only way for the old functionality to remain available will be by re-implementing the associated module so that it emulates the old functionality using the new functionality. (Providing an \u0026quot;illusion\u0026quot; of the old functionality.) And then, should this completely rewritten reincarnation of the old implementation be allowed to keep the old version number? In theory, if your testing is not just extremely robust but actually perfect, then yes. In practice, no.)\n\u0026quot;Evolvability represents the degree to which a component implementation can be changed without negatively impacting other components.\u0026quot;\n\u0026quot;Extensibility is defined as the ability to add functionality to a system. Dynamic extensibility implies that functionality can be added to a deployed system without impacting the rest of the system.\u0026quot;\n\u0026quot;Customizability refers to the ability to temporarily specialize the behavior of an architectural element, such that it can then perform an unusual service. A component is customizable if it can be extended by one client of that component’s services without adversely impacting other clients of that component. {...} Customizability is a property induced by the remote evaluation and code-on-demand styles\u0026quot;\n\u0026quot;Configurability is related to both extensibility and reusability in that it refers to post-deployment modification of components, or configurations of components, such that they are capable of using a new service or data element type.\u0026quot;\n\u0026quot;Reusability is a property of an application architecture if its components, connectors, or data elements can be reused, without modification, in other applications. The primary mechanisms for inducing reusability within architectural styles is reduction of coupling (knowledge of identity) between components and constraining the generality of component interfaces.\u0026quot;\n(*Constraining* the generality of component interfaces? Is that an error? I thought that\nthe more general the interface, the more reusable the component.)\n\u0026quot;Visibility {...} refers to the ability of a component to monitor or mediate the interaction between two other components.\u0026quot;\n\u0026quot;Software is portable if it can run in different environments.\u0026quot;\n\u0026quot;Reliability, within the perspective of application architectures, can be viewed as the degree to which an architecture is susceptible to failure at the system level in the presence of partial failures within components, connectors, or data. Styles can improve reliability by avoiding single points of failure, enabling redundancy, allowing monitoring, or reducing the scope of failure to a recoverable action.\u0026quot;\nChapter 3\n\u0026quot;The purpose of building software is not to create a specific topology of interactions or use a particular component type — it is to create a system that meets or exceeds the application needs. The architectural styles chosen for a system’s design must conform to those needs, not the other way around.\u0026quot;\nChapter 4\n\u0026quot;Working groups within the Internet Engineering Taskforce were formed to work on the Web’s three primary standards: URI, HTTP, and HTML. The charter of these groups was to define the subset of existing architectural communication that was commonly and consistently implemented in the early Web architecture, identify problems within that architecture, and then specify a set of standards to solve those problems.\u0026quot;\n(yup, that pretty much sums it up: it began as chaos, and any attempts to put the chaos into order were post-hoc.)\n\u0026quot;The next chapter introduces and elaborates the Representational State Transfer (REST) architectural style for distributed hypermedia systems, as it has been developed to represent the model for how the modern Web should work. REST provides a set of architectural constraints that, when applied as a whole, emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems.\u0026quot;\nChapter 5\n\u0026quot;The REST interface is designed to be efficient for large-grain hypermedia data transfer, optimizing for the common case of the Web, but resulting in an interface that is not optimal for other forms of architectural interaction.\u0026quot;\n\u0026quot;REST is defined by four interface constraints: identification of resources; manipulation of resources through representations; self-descriptive messages; and, hypermedia as the engine of application state.\u0026quot;\ncontinue on last paragraph of page 83\n","date":"2017-06-03T21:02:34.863Z","permalink":"https://blog.michael.gr/post/2017-06-my-notes-on-fielding-dissertation-rest/","title":"My notes on the Fielding Dissertation (REST)"},{"content":"A YouTube videoclip titled \u0026quot;Roy T. Fielding: Understanding the REST Style\u0026quot;\nQuote: \u0026quot;It's really an accessible piece of work. It is not full of equations. There is one equation. The equation is there just to have an equation, by the way.\u0026quot;\nFor my notes on REST, see other post: My notes on the Fielding Dissertation (REST)\n","date":"2017-05-23T07:21:30.333Z","permalink":"https://blog.michael.gr/post/2017-05-my-notes-on-fielding/","title":"Roy T. Fielding: Understanding the REST Style"},{"content":"\rForeword Historically, the difference between scripting languages and real programming languages has been understood as the presence or absence of a compilation step. However, in recent decades the distinction has blurred; from time to time we have seen:\nInterpreters for languages that were originally meant to be compiled. Compilers for languages that were originally meant to be interpreted. Scripting engines internally compiling source code to bytecode before interpreting it. Real languages compiling to bytecode which is then mostly interpreted and rarely converted to machine code. So, compiled vs. interpreted does not seem to be the real differentiating factor; nonetheless, we can usually tell a scripting language when we see one. So, what is it that we see?\n(Useful pre-reading: About these papers)\nFirst, let us identify the three different kinds of error that can potentially occur in program code:\nSyntax Error: this represents a violation of fundamental rules governing the form of the language; for example, in most programming languages the statement a = ; is a syntax error, because something is obviously missing between the equals sign and the semicolon. Semantic Error: this represents failure to respect the meaning of things; for example, in most languages the statement a = \u0026quot;x\u0026quot; / 5; is syntactically correct but semantically incorrect, because dividing a string by a number does not make sense. As another example, the statement a.increment(); may represent a semantic error if object a has no method called increment. Logic Error: this corresponds to a mistake in our reasoning. For example, the statement circumference = radius * π can be correct both syntactically and semantically, but it is nonetheless flawed, because this is not how you calculate a circumference given a radius; the correct formula also involves a multiplication by 2. From the three types of error that we have identified, the first and the last are unaffected by our choice of programming language:\nSyntax error will be detected by any halfway decent IDE regardless of whether we are using a scripting language or a real programming language. Logic error is just as easy to make in any programming language, and the way we protect ourselves against it is by writing copious amounts of automated software tests. Semantic Error is where different kinds of languages take vastly different approaches. This type of error is closely associated with the concept of data types:\nThe expression \u0026quot;x\u0026quot; / 5 is flawed because the left operand is of type string, while the right operand is of a numeric type.\nThe validity of the statement a.increment() depends upon the type of a, and whether that type defines an increment() method or not.\nIf the programming language is strongly typed, then semantic error will always be detected during compilation, so there is never any danger of attempting to run (or ship to the customer) a program containing this kind of error; however, if the programming language is weakly typed, then all semantic error will go undetected until an attempt is made to execute code containing such error.\nIn light of the above, it turns out that the actual differentiating factor between real programming and scripting languages is nothing but the presence or absence of semantic checking, in other words the use of strong vs. weak typing.\nTypeScript is the odd exception to the rule, and this is to be expected, because the impetus for the creation of TypeScript was vastly different from that of other scripting languages. Virtually all scripting languages except TypeScript are one-man efforts that have come into existence as nothing more than toy projects. In contrast, TypeScript was the result of a deliberate group effort backed by a big company (Microsoft) starting with the realization that JavaScript is unfortunately here to stay, and setting out specifically to correct one of its major deficiencies, namely the lack of strong typing.\nThe trend of real programming languages to be compiled and of scripting languages to be interpreted can be explained in full as a consequence of the primary choice of strong vs. weak typing:\nIf a language is strongly typed, then a compilation step is very useful to have, because it will unfailingly locate all errors that are detectable via semantic analysis before attempting to run.\nIf a language is weakly typed, then semantic errors are undetectable, so there is no need to parse code in advance. A compilation step would only reveal syntactic errors, which can also be detected by any halfway decent IDE.\nSo, allowing for the exception of TypeScript, this leaves us with the following soft rule:\nReal languages are strongly typed, employ semantic checking, and are therefore usually compiled.\nScripting languages are weakly typed, lack semantic checking, and are therefore usually interpreted.\nAnd yet, many people like scripting languages, and write lots of code in them, supposedly because they are \u0026quot;easier\u0026quot;. This brings to mind the famous quote by Edsger W. Dijkstra:\n{...} some people found error messages they couldn't ignore more annoying than wrong results, and, when judging the relative merits of programming languages, some still seem to equate \u0026quot;the ease of programming\u0026quot; with the ease of making undetected mistakes.\n(Edsger W. Dijkstra, On the foolishness of \u0026quot;natural language programming\u0026quot;.)\nNote that the above quote is from a paper about Natural Language Programming (NLP) but the particular passage containing the quote pertains to programming languages in general. Dijkstra wrote against NLP back in the 1980s because at that time it was being considered by some fools as a viable prospect; luckily, it failed to catch on, (or naturally, if you would permit the pun,) but little did ol' Edsger know that in the decades that would follow his nightmares would come true, because scripting languages did catch on. Apparently, people love making undetected mistakes.\nArguments in favor of scripting languages Argument: It is easy to write code in it; look, the \u0026quot;hello, world!\u0026quot; program is a one-liner.\nRebuttal: What this means is that this scripting language is a very good choice, possibly even the ideal choice, for writing the \u0026quot;hello, world!\u0026quot; program.\nThe ease with which you may write \u0026quot;hello, world!\u0026quot; is no indication whatsoever about the ease with which a non-trivial system may be collaboratively developed, tested, debugged, maintained, and extended.\nArgument: No, I mean it is really terse. There are many things besides \u0026quot;hello, world!\u0026quot; that I can write in one line.\nRebuttal: Sure, you can write them in one line; but can you read them?\nOne of the most important aspects of code is readability, (second only to correctness,) but terse code is not necessarily easy to read; if that was the case, then Perl would be the most readable language ever, but instead it enjoys the dubious distinction of being the least readable among all programming languages in general use.\nTerseness usually represents a tradeoff between verbosity and understandability: the more terse the code, the less of it you have to read, but also the harder it is to untangle its complexity. Thus, it is debatable whether terseness correlates with readability. Terseness appears to be the modern trend, so as real programming languages keep evolving they are also receiving features that make them more and more terse, for example tuples, lambdas, the fluent style of invocations, etc. So, terseness is not the exclusive domain of scripting languages, and to the extent that scripting languages go further in this regard it is debatable whether it is an advantage or a disadvantage.\nArgument: There are lots of libraries for it.\nRebuttal: Seriously? There are more libraries for your scripting language than there are for Java?\nArgument: I don't have to compile it; I just write my code and run it.\nRebuttal: I also just write my code and run it. When I hit the \u0026quot;launch\u0026quot; button, my IDE compiles my code in the blink of an eye and runs it. The difference between you and me is that if I have made any semantic mistakes in my code, I will be told so before wasting my time trying to run it. But what am I saying, being told that there are semantic mistakes in your code probably counts as a disadvantage for you, right?\nThe ability to just write your code and run it without any semantic checking is causing real harm in scripting languages because it prevents them from evolving.\nThis is, for example, the major reason why Python version 2.x is still enjoying widespread use despite the language having moved on to version 3.x by now: people are afraid to make the transition to version 3.x in existing projects, even though it is mostly backwards compatible with version 2.x, because it is not 100% compatible, and lack of semantic checking means that there is no way of knowing which lines of code will break unless those lines happen to get executed.\nArgument: I can modify my program as it runs.\nRebuttal: I can also modify my program as it runs; the ability to do this is available in most real programming languages, and it is called \u0026quot;edit and continue\u0026quot; or \u0026quot;hot reload\u0026quot; depending on the language; look it up.\nModification of running code is not always applicable in real programming languages, and it does not always work, but then again nor does it always work when you modify running code in a scripting language, because usually, you already have data structures in memory that were created by the code before it was modified. In real programming languages, you are prevented from making edits to running code that would seriously foul things up; in scripting languages, you are allowed to do whatever you please, and the catastrophic consequences of doing so are your own problem.\nArgument: I do not like to have to declare the type of every single variable because it is a pain.\nRebuttal: This is akin to arguing against seat belts because putting them on and taking them off is a pain. Do you have any idea of what kind of pain you are looking at if you get in a traffic accident without a seat belt?\nFurthermore, the ability to not have to declare the type of every single variable is not the exclusive privilege of scripting languages, because in recent years type inference has been gaining ground in real programming languages, allowing us to omit declaring the type of many of the variables that we use. The difference is that in real programming languages this is done right, by means of type inference instead of type ostrichism:\nType inference is deterministic extra work that the compiler does for us, and it relies on having already assigned specific types to other variables, so that we do not have to repeat things that the compiler already knows or can easily figure out.\nType ostrichism is scripting language programmers preferring to not see types and to not deal with types, as if that will make the types go away.\nIt might be worth taking a look at Python Enhancement Proposal (PEP) 483 which acknowledges that behind the scenes every variable is of course of a specific type, and discusses the potential benefits of adding a type annotation system to the language which will allow programmers to make their intentions about types explicit, so as to be able to at least partially, and at least as an afterthought, enjoy some of the benefits of strong typing. I quote:\nThese annotations can be used to avoid many kind of bugs, for documentation purposes, or maybe even to increase speed of program execution.\nWell, guess what: the main difference between scripting languages and real programming languages is that real programming languages have such \u0026quot;type annotations\u0026quot; and all of their advantages already built-in: it is called The Type System.\nArgument: I am not worried about errors, because I use testing.\nRebuttal: Oh really? Are your tests achieving even a mere 60% code coverage as we speak? And supposing that they do, how do you feel about the fact that in the remaining 40%, every single line is liable to break due to reasons as trivial and yet as common as a typo?\nTesting is an indispensable quality assurance mechanism for software, but it does not, in and by itself, guarantee correctness. You can easily forget to test something, and you can easily test \u0026quot;around\u0026quot; a bug, essentially creating tests that pretty much require the bug to be in place in order to pass. Despite these deficiencies, testing is still very important, but it is nothing more than a weapon in our arsenal against bugs. This arsenal also happens to include another weapon, which is closer to the forefront in the battle against bugs, and it is 100% objective, and definitive. This weapon is called strong typing.\nArgument: My scripting language has lots and lots of built-in features.\nRebuttal: Sure, and that's why scripting languages are not entirely useless. If the only thing that matters is to accomplish a certain highly self-contained goal of severely limited scope in as little time as possible, then please, by all means, do go ahead and use your favorite scripting language with its awesome built-in features. However, if the project is bound to take a life of its own, you are far better off investing a couple of minutes to create a project in a real programming language, and to include the external libraries that will give you any extra features that you might need.\nBuilt-in features do not only come with benefits; in contrast to libraries, they are much more difficult to evolve, because even a minute change in them may break existing code, resulting in people being reluctant to migrate to the latest version of the language. (Take the Python 2 vs. 3 conundrum for example.)\nFurthermore, built-in features usually have to be supported forever, even after better alternatives have been invented, or after they simply go out of style and fall out of grace, so over time scripting languages tend to gather lots of unnecessary baggage. We have tried feature-bloated programming languages before, (with ADA for example,) and the consensus is that they are not the way to go.\nArgument: But really, my scripting language is so much easier! Look here, in one statement I obtain a list and assign its elements to individual variables!\nRebuttal: That's great, I bet this has slashed your time-to-market by half. What happens if the number of elements in the list differs from the number of variables that you decompose it into? I bet there is no error, because you do not like being bothered with errors, right?\nIn any case, my compiled language of choice has its own unique, arcane syntax quirks that I could, if I wanted to, claim that they make things so much easier for me.\nSome of them are not even that arcane; for example, instead of using clunky annotations to hint to the IDE the types of my variables, so that it can then provide me with some rudimentary type checking, I get to simply declare the type of each variable as part of the actual syntax of the language! Imagine that!\nArgument: I like dynamic typing. It gives me freedom.\nRebuttal: Yes, freedom to shoot yourself in the foot. Also please note that there is no such thing as \u0026quot;dynamic\u0026quot; typing; this term is just a euphemism invented by scripting language aficionados to down-play the detrimental nature of this practice. The proper term is weak typing.\nArgument: I do not need type safety. I am better off without it.\nRebuttal: Right. So, you are the technological equivalent of an anti-vaxxer. (Credit: danluu)\nArgument: I do not have to use an IDE, I can just use my favorite text editor.\nRebuttal: Oh sure. You are also the technological equivalent of an Amish farmer.\nArgument: My scripting language is trendy. It is hip.\nNo contest here. I can't argue with hipsters.\nThe problems with scripting languages The nonsense I don't need to say much here, just watch the legendary \u0026quot;Wat\u0026quot; video by Gary Bernhardt from CodeMash 2012, it is only 4 minutes long:\nThe reason for all this nonsense is that all these languages are hacks.\nWhen the foundation that you are working on is a hack, then either anything you will build on top of it will in turn be a hack, or you are going to be putting an enormous effort to circumvent the hackiness of the foundation and build something reasonable over it. Why handicap yourself?\nThe errors Lack of semantic checking means that the mistakes that will inevitably be made will not be caught by a compilation step. Therefore, lack of semantic checking necessarily means that there will be more errors.\nIt is an established fact that a certain percentage of errors will always pass testing and make it to production, which in turn inescapably means that there will be a somewhat increased number of bugs in production.\nThis alone is enough to classify scripting languages as unsuitable for anything but tinkering, and the debate should be over right there.\nThe crippled IDE Lack of semantic checking means that your IDE cannot provide you with many useful features that you get with strongly typed languages. Specifically, you either have limited functionality, or you do not have at all, some or all of the following features:\nContext-sensitive argument auto-completion. Since any parameter to any function can be of any type, the IDE usually has no clue as to which of the variables in scope may be passed to a certain parameter of a certain function. Therefore, it has to suggest everything that happens to be in scope. Most of these suggestions are preposterous, some are even treacherous. Member Auto-completion. Since a variable does not have a specific type, the IDE usually has no clue as to what member fields and functions are exposed by that variable. Therefore, either it cannot give any suggestions, or it has to suggest every single member of every single known type and the kitchen sink. Listing all usages of a type. Since any variable can be of any type, the IDE usually has no clue as to where a given type is used, or if it is used at all. Contrast this with strongly typed languages where the IDE can very accurately list all usages of any given type and even provide you with visual clues about unused types. Type sensitive search. If you have multiple different types where each one of them contains, say, a Name member, you cannot search for all references of the Name member of only one of those types. You have to use text search, which will yield all irrelevant synonyms in the results. This can be okay in tiny projects, but it very quickly becomes non-viable as the project size increases. Refactoring. When the IDE has no knowledge of the semantics of your code, it is incapable of performing various useful refactoring operations on it. IDEs that offer refactoring on untyped languages are actually faking it; they should not be calling it refactoring, they should be calling it Cunning Search and Replace. Needless to say, it does not always work as intended; it does sometimes severely mess up the code, and when that happens, it is called Search and Destroy. Furthermore, since there is no compiler, you will not know that your source code base has been damaged unless you run your software, and even then, for every line of code that has been damaged, you will only discover the damage if that line gets executed; on lines that rarely get executed, you might never discover the damage. (But your customers of course will.) That little performance issue Performance is generally not an issue for scripting languages, because they tend to be used in situations where performance is not required.\n(There are of course some situations where people opt to use a scripting language despite the fact that performance matters, and in those situations people do in fact suffer the consequences of poor performance, take web servers written in node.js for example.)\nIn today's world where the majority of personal computers are running on precious battery power, it can be argued that even the tiniest bit of performance matters, but we can let that one slide, since battery technology is constantly improving.\nIn cases where performance matters but the task at hand is well-defined and relatively isolated, performance is again not an issue for scripting languages because external libraries tend to be quickly developed to handle those tasks. (These external libraries are written in guess what: real programming languages.)\nHaving explained that performance is usually not an issue, let us also quickly mention before moving on that on computationally expensive tasks, such as iterating over all pixels of an image to manipulate each one of them, and assuming a competent programmer in each language, the following statements hold true:\nthere is no way that a scripting language will perform as well as Java, just as: there is no way that Java will perform as well as C++, just as: there is no way that C++ will perform as well as Assembly. Stop arguing about this.\nThe horrendous syntax Most scripting languages suffer from a severe case of capriciously arcane and miserably grotesque syntax. No, beauty is not in the eye of the beholder, and there is only a certain extent up to which aesthetics are subjective.\nThe syntax of scripting languages tends to suffer due to various reasons, the most common being:\nTheir priorities are all wrong to begin with. They were hastily hacked together in a very short amount of time. Plain incompetence on behalf of their creators. Scripting languages that have their priorities wrong are, for example, all the shell scripting languages. These languages aim to make strings (filenames) look and feel as if they are identifiers, so that you can type commands without having to enclose them in quotes, as if the convenience of not having to use quotes was the most important thing ever. If all we want to do in a shell script is to list a sequence of commands to execute, then this convenience is perhaps all we care for, but the moment we try to use any actual programming construct, like variables and flow control statements, what we have in our hands is a string-escaping nightmare of epic proportions.\nObligatory XKCD comic: \u0026quot;Backslashes\u0026quot;\nA scripting language that owes its bad syntax to being hastily hacked together is JavaScript. Its creator, Brendan Eich, has admitted that the language was developed within a couple of weeks, and that it was not meant for anything but short isolated snippets of code. He is honest enough to speak of his own creation in derogatory terms, and to accept blame. (See TEDxVienna 2016, opening statement, \u0026quot;Hello, I am to blame for JavaScript\u0026quot;.) Also, pretty much anyone deeply involved with JavaScript will admit that it has serious problems. One of the most highly acclaimed books on the language is JavaScript: The Good Parts, authored by Douglas Crockford and published by O'Reilly; you can take the title of the book as a hint.\nAs a matter of fact, here is a photo that has become a meme in and of itself, showing the relative sizes of two of the most popular books about JavaScript: The Definitive Guide and The Good Parts, both published by O'Reilly:\nA scripting language that owes its horrific syntax to lack of competence is PHP. Its creator, Rasmus Lerdorf, is quoted on the Wikipedia article about PHP as saying \u0026quot;I don't know how to stop it, there was never any intent to write a programming language {...} I have absolutely no idea how to write a programming language, I just kept adding the next logical step on the way.\u0026quot;\nSo, from the above it should be obvious that most scripting languages are little toy projects that were created by individuals who simply wanted to prove that they could build something like that, without actually intending it to be used outside of their own workbench.\nThe cheapness The lack of semantic checking in scripting languages is usually not a deliberate design choice, but instead a consequence of the very limited effort that has gone into creating them. In many cases the creators of scripting languages would not know how to add semantic checking to the language even if they wanted to. In all cases, the amount of work required to add semantic checking would have been orders of magnitude greater than the total amount of work that went into the creation of the language in the first place.\nIn this sense, the comparison between scripting languages and real programming languages is a lot like comparing children's tinker toy tools with tools for professionals: sure, a plastic screwdriver is inexpensive, lightweight and easy to use, but try screwing anything but plastic screws with it.\n(I was going to also add \u0026quot;you cannot hurt yourself with it\u0026quot;, but this analogy does not transfer to programming: you can very easily hurt yourself with a scripting language.)\nWhat scripting languages are good for Scripting languages used to be an easy way to write cross-platform software. This does not hold true anymore, since most major real programming languages are pretty much cross-platform nowadays. Scripting languages are useful when embedded within applications, (applications written in real programming languages,) as evaluators of user-supplied expressions. (E.g. spreadsheet cell formulas.) Scripting languages are useful when shortening the time from the moment you fire up the code editor to the moment you first run your program is more important than everything else. By \u0026quot;everything else\u0026quot; we really mean everything: readability, understandability, maintainability, performance, even correctness. Scripting languages are useful when the program to be written is so trivial, and its expected lifetime is so short, that it is hardly worth the effort of creating a new folder with a new project file in it. The corollary to this is that if it is worth creating a project for it, then it is worth using a real programming language. Scripting languages are useful when the code to be written is so small and simple that bugs can be detected by simply skimming through the code. The corollary to this is that if the program is to be even slightly complex, it should be written in a real programming language. (Adding insult to injury, many scripting languages tend to have such a cryptic write-only syntax that it is very hard to grasp what any piece of code does, let alone skim through it and vouch for it being bug-free.) The most important thing about scripting languages (and the main reason why they have become so wildly popular in recent years) is that they are useful in getting non-programmers into programming as quickly as possible. Most of us programmers have had a friend, who was not a programmer, and who one day asked us how to get into programming. The thought process should be familiar: you think about it for a moment, you start making a mental list of things they would need in order to get started with a real programming language, and you quickly change your mind and suggest that they try Python, because this answer stands some chance of fitting within our friend's attention span. However, the truth of the matter is that this recommendation will only save our friend from maybe a few hours of preparatory work, and it would be a crime if it condemns them to thousands of hours wasted over the course of a several year long career due to the use of an inferior programming language. This brings us to the following realization:\nScripting languages are a lot like teething rings (pacifiers):\nIt is okay to start with one; you must get rid of it as soon as you grow some teeth.\nConclusion The fact that some scripting languages catch on and spread like wildfire simply shows how eager the industry is to adopt any contemptible piece of nonsense without any critical thinking whatsoever, as long as it helps optimize some short-sighted concern, such as how to get non-programmers into programming as quickly as possible. It is a truly deplorable situation that kids nowadays learn JavaScript as their first programming language due to it being so accessible: all you need is a web browser, and one day instead of F11 for full-screen you accidentally hit F12 which opens up the developer tools, and you realize that you have an entire integrated development environment for JavaScript sitting right there, ready to use. The availability of JavaScript to small children is frightening.\nUsually, once a language becomes extremely popular, tools are created to lessen the impact of its deficiencies. Thanks to the herculean efforts of teams that develop scripting engines, and through all kinds of sorcery being done under the hood in these engines, the most popular scripting languages are considerably faster today than they used to be. However, the sorcery is not always applicable, even when it is applicable it is imperfect, and besides, it incurs a penalty of its own, so scripting languages will never match the performance of real programming languages. Also, modern IDEs have evolved to provide some resemblance of semantic checking in some scripting languages, but since this checking has been added as an afterthought, it is always partial, unreliable, hacky, and generally an uphill battle.\nSo, you might ask, what about the hundreds of thousands of successful projects written in scripting languages? Are they all junk? And what about the hundreds of thousands of programmers all over the world who are making extensive use of scripting languages every day and are happy with them? Are they all misguided? Can't they see all these problems? Are they all ensnared in a monstrous collective delusion?\nYep, that's exactly it. You took the words from my mouth.\nAlso read: Tablecloth (A high-tech, sci-fi horror short-story)\nNote: This is a draft. It may contain inaccuracies or mistakes. There are bound to be corrections after I receive some feedback.\nCover image: Teething rings (pacifiers) found on the great interwebz.\nScratch (please ignore)\nSee:\nhttp://stackoverflow.com/questions/397418/when-to-use-a-scripting-language\nFrom http://wiki.c2.com/?SeriousVersusScriptingLanguages\nScripting Languages emphasize quickly writing one-off programs\nSerious languages emphasize writing long-lived, maintainable, fast-running programs.\nlight-duty \u0026quot;gluing\u0026quot; of components and languages.\nFrom https://danluu.com/empirical-pl/\n\u0026quot;I think programmers who doubt that type systems help are basically the tech equivalent of an anti-vaxxer\u0026quot;\nThe effect isn't quantifiable by a controlled experiment.\nMisinformation people want to believe spreads faster than information people don't want to believe.\nThe Stack Overflow Blog: Minimizing the downsides of dynamic programming languages\n","date":"2017-05-19T19:46:34.745Z","permalink":"https://blog.michael.gr/post/2017-05-on-scripting-languages/","title":"On Scripting Languages"},{"content":"My notes on Devoxx 2016 Belgium - Microservices Evolution: How to break your monolithic database by Edson Yanaga (I attended this conference)\nReduce maintenance window\nAchieve zero downtime deployments\n\u0026quot;Code is easy, state is hard\u0026quot;\nChanges in a database schema from one version to another are called database migrations\nTools: Flyweight Liquibase\nMigrations require back and forward compatibility\nBaby steps = Smallest Possible Batch Size\nToo many rows = Long Locks\nShard your updates (not updating the entire table in one go)\nRenaming a column\nALTER TABLE customers RENAME COLUMN wrong TO correct; becomes:\nALTER TABLE customers ADD COLUMN correct VARCHAR(20); UPDATE customers SET correct = wrong WHERE id \u0026amp;lt; 100; UPDATE customers SET correct = wrong WHERE id \u0026amp;gt;= 100 AND id \u0026amp;lt; 200; {...}\n(later)ALTER TABLE customers DELETE COLUMN wrong; Adding a column\nADD COLUMN, setting NULL/DEFAULT value/computed value\nNext release: Use Column\nRenaming / Changeing Type / Format of a Column: Next version: ADD COLUMN, Copy data using small shards Next release: Code reads from old column and writes to both Next release: Code reads from new column and writes to both Next release: Code reads and writes from new column Next release: Delete old column\nDeleting a column\nNext version: Stop using the column but keep updating the column Next version: Delete the column\nFor migrating from a monolithic application with a monolithic database to many microservices with own database each:\nUsing Event Sourcing\ntool: debezium.io\nYou tell it which tables you want to monitor, and from then on it monitors them and generates an event for each DDL/DML statement you issue. The event is propagated to as many event consumers as you want. So, microservices can receive these events and update their own databases.\n\u0026quot;HTTP and REST are incredibly slow\u0026quot;\n","date":"2017-05-18T19:08:00.22Z","permalink":"https://blog.michael.gr/post/2017-05-devoxx-2016-belgium-microservices/","title":"Devoxx 2016 Belgium - Microservices Evolution: How to break your monolithic database by Edson Yanaga "},{"content":"My notes on Devoxx US 2017, Knowledge is Power: Getting out of trouble by understanding Git by Steve Smith\n\u0026quot;If that doesn't fix it, git.txt contains the phone number of a friend of mine who understands git. Just wait through a few minutes of 'It's really pretty simple, just think of branches as...' and eventually you'll learn the commands that will fix everything.\u0026quot;\n","date":"2017-05-18T18:33:34.257Z","permalink":"https://blog.michael.gr/post/2017-05-devoxx-us-2017-knowledge-is-power/","title":"Devoxx US 2017, Knowledge is Power: Getting out of trouble by understanding Git by Steve Smith"},{"content":"My notes on GOTO 2016 - Microservices at Netflix Scale: Principles, Tradeoffs \u0026amp; Lessons Learned - R. Meshenberg\nThey have a division making a layer of tools for other teams to build their stuff on top of it.\nExceptions for statelessness are persistence (of course) but also caching.\nDestructive testing - Chaos monkey -\u0026gt; simian army - in production, all the time. (During office hours)\nTheir separation of concerns looks like a grid, not like a vertical or horizontal table.\nThey have open sourced many of their tools, we can find them at netflix.github.com\n","date":"2017-05-18T18:20:46.077Z","permalink":"https://blog.michael.gr/post/2017-05-goto-2016-microservices-at-netflix/","title":"GOTO 2016 - Microservices at Netflix Scale: Principles, Tradeoffs \u0026 Lessons Learned - R- Meshenberg "},{"content":"My notes on GOTO 2015 - Progress Toward an Engineering Discipline of Software - Mary Shaw\nNotes\n17:28 past the bridges and into software engineering\nSoftware Engineering is all design. Production used to be printing the CDs, and nowadays it is hitting the \u0026quot;deploy\u0026quot; button.\n\u0026quot;scaling the costs to the consequences\u0026quot; -- the point is not to minimize the cost, the point is to scale it to the consequences. Risks must be taken, and if the potential gains are huge, then the risks can be correspondingly large.\n","date":"2017-05-18T18:20:02.433Z","permalink":"https://blog.michael.gr/post/2017-05-goto-2015-progress-toward-engineering/","title":"GOTO 2015 - Progress Toward an Engineering Discipline of Software - Mary Shaw"},{"content":"My notes on GOTO 2015 - DDD \u0026amp; Microservices: At Last, Some Boundaries! - Eric Evans\nMicroservices and Netflix - what is the connection?\nIsolated data stores\n\u0026quot;A service is something that can consume messages and can produce messages\u0026quot;\n","date":"2017-05-18T18:19:17.31Z","permalink":"https://blog.michael.gr/post/2017-05-goto-2015-ddd-microservices-at-last/","title":"GOTO 2015 - DDD \u0026 Microservices: At Last, Some Boundaries! - Eric Evans"},{"content":"My notes on GOTO 2014 - REST: I don't Think it Means What You Think it Does - Stefan Tilkov\n\u0026quot;People decide they want to build something in a RESTful fashion, so they spend all their time arguing about where the slashes go\u0026quot;.\n\u0026quot;It is the first litmus test for your REST API whether you depend on specific characters in your URIs for things to work.\u0026quot;\n(From the client's point of view.)\n\u0026quot;Version numbers in URIs just suck. Everybody does it which doesn't make it any less sucky. It is a stupid idea. Don't do that.\u0026quot;\n\u0026quot;The version number is in the URI because the URI is the API\u0026quot;. \u0026lt;-- ? I would assume the URI is NOT the API.\nVersioning: \u0026quot;Version your documentation documents. Wait what? --Yes, no versioning\u0026quot;.\nPostel's law \u0026quot;TCP implementations should follow a general principle of robustness: Be conservative in what you do, be liberal in what you accept from others.\u0026quot; http://tools.ietf.org/html/rfc761\nClient rules Don't depend on URI structure Support unknown links Ignore unknown content\nServer rules Don't break URI structure unnecessarily Evolve via additional resources Support older formats\nDiscovery/Discoverability: \u0026quot;JSON Home\u0026quot; http://tools.ietf.org/html/draft-nottingham-json-home-03\nHypermedia APIs \u0026quot;give you flexibility\u0026quot;, \u0026quot;are cool\u0026quot;, \u0026quot;are neat\u0026quot; \u0026lt;-- no explanation\n\u0026quot;Excellent question, do I know any examples of widely used public APIs that fully follow this model? No.\u0026quot;\n","date":"2017-05-18T18:16:57.43Z","permalink":"https://blog.michael.gr/post/2017-05-goto-2014-rest-i-dont-think-it-means/","title":"GOTO 2014 - REST: I don't Think it Means What You Think it Does - Stefan Tilkov"},{"content":"My notes on GΟΤΟ 2014 - Microservices - Martin Fowler\nCharacteristics of Microservices\nComponentization\nOrganized around business capabilities\nProducts not Projects\nSmart endpoints and dumb pipes\nDecentralized Governance\nDecentralized Data Management\nInfrastructure Automation\nDesign for failure\nEvolutionary Design\nWith services we typically use some kind of interprocess communication facilities such as web service calls or messaging or something of that kind.\nHow big should a microservice be?\n\u0026quot;It should have one responsibility\u0026quot; --too vague\n\u0026quot;It should fit in my head\u0026quot; --fairly good\n\u0026quot;You should not have a team that you cannot feed with 2 pizzas\u0026quot;\n","date":"2017-05-18T18:14:12.069Z","permalink":"https://blog.michael.gr/post/2017-05-g-2014-microservices-martin-fowler/","title":"GΟΤΟ 2014 - Microservices - Martin Fowler"},{"content":"\rIf you were thinking of installing the \u0026quot;Andy\u0026quot; android emulator on your PC, think again. Here is my experience with it:\nThe installable file (Andy_46.2_207_x64bit.exe) is a whole 431 MB. It installed VMware without asking me, so the installation took a really long time. It installed some \u0026quot;Bonjour Service\u0026quot; by Apple, Inc. without asking me. It replaced all my .apk icons with its own icon without asking me. (I am using apk shell extension and I much prefer it that way.) During installation, there were 15 attempts to call home by \u0026quot;Andy\u0026quot; and/or by other crapware that it installed. (I have a firewall, so I didn't let any of that happen.) At the end of the installation, it popped up a message box saying that the installation failed because it could not detect my internet connection, and that it requires internet access in order to install. Despite the failed installation message, \u0026quot;Andy\u0026quot; was found under \u0026quot;installed programs\u0026quot; so I uninstalled it. During uninstallation there were a couple of more attempts to call home. After uninstallation it left \u0026quot;Bonjour Service\u0026quot; installed, so I had to go find it and uninstall it too. After uninstallation it left an \u0026quot;Andy\u0026quot; folder on the root of my user folder, which I had to delete. What a piece of crapware!\n","date":"2016-05-15T08:17:33.39Z","permalink":"https://blog.michael.gr/post/2016-05-andy-android-emulator-avoid-it-like/","title":"\"Andy\" android emulator - Avoid it like the plague"},{"content":"So, while trying to create a new user account on my Windows 10 computer, I get this:\nThe solution:\nStart a command prompt as administrator.\nType the following command:\nnet user \u0026lt;username\u0026gt; \u0026lt;password\u0026gt; /add\nVoila, the user has been created. What in fact went wrong is that Microsoft completely broke Windows after Windows 7. ","date":"2016-05-02T21:23:32.388Z","permalink":"https://blog.michael.gr/post/2016-05-solved-something-went-wrong-trying-to/","title":"Solved: \"something went wrong\" trying to create new user account on Windows 10"},{"content":"So, the brightness keys on my Asus Laptop do not work anymore. All other Fn keys still work, but the Fn+F5 and Fn+F6 keys which control brightness do not work anymore.\nThe way to solve this problem is as follows:\nInitiate an update of the driver of your monitor. This can be accomplished in many ways, for example:\nRight-click on the desktop\nSelect \u0026quot;Display settings\u0026quot;, then\nSelect \u0026quot;Advanced display settings\u0026quot;\nSelect \u0026quot;Display adapter properties\u0026quot;\nSwitch to the \u0026quot;Monitor\u0026quot; tab\nClick on \u0026quot;Properties\u0026quot; for the monitor\nSwitch to the \u0026quot;Driver\u0026quot; tab\nClick \u0026quot;Update driver\u0026quot;.\nAlternatively, you can:\nHit Win+Pause to open the \u0026quot;System\u0026quot; window\nClick \u0026quot;Device Manager\u0026quot;\nFind your monitor under \u0026quot;Monitors\u0026quot;\nRight-click on the monitor and select \u0026quot;Update Driver\u0026quot;.\nOnce the \u0026quot;Update Driver Software\u0026quot; dialog is up:\nIn the wizard which prompts you whether you want to search automatically or browse your computer, lie and say that you want to browse your computer. (Windows is so messed up that you have to lie to it to coax it to work.) On the next screen, do not browse anything, select \u0026quot;let me pick from a list of drivers on my computer\u0026quot;. On the next screen, select \u0026quot;Generic PnP Monitor\u0026quot; and click \u0026quot;Next\u0026quot;. You are done. Source: http://visihow.com/Restore_Lost_Brightness_Control_app_in_ASUS_Laptops_After_Updating\n","date":"2016-03-20T22:39:39.708Z","permalink":"https://blog.michael.gr/post/2016-03-solved-brightness-control-keys-do-not/","title":"Solved: Brightness control keys do not work on Asus Laptop"},{"content":"So, I just bought a new phone, a Samsung Note 4, (Android,) and boy, was it a lousy experience!\nToo complicated bundling options To start off, the experience was becoming a bit lousy even before the purchase, with all the incredibly complicated options on offer, which make selecting a plan quite an ordeal. Frankly, I think bundling services (mobile connectivity) with products (the phone) should be prohibited.\nUnwanted applications Anyhow, so I bought the phone. Upon booting, it went through a lengthy process of installing applications I never asked for.. Finally, I arrived at the first interactive screen, which promptly went blank.\nScreen times out while charging. So, here is a riddle for you: if my phone is plugged into the USB port and charging, then why the hell is the screen switching off every 15 seconds?\nMisleading \u0026quot;Dexterity\u0026quot; options On this first interactive screen the phone asked me to select a language, and allowed me to either \u0026quot;Start\u0026quot;, (whatever this may mean,) or set up accessibility first. I tapped accessibility just out of curiosity. In the following \u0026quot;Accessibility settings\u0026quot; screen, there were entries for \u0026quot;Vision\u0026quot; and \u0026quot;Hearing\u0026quot;, which were of no interest to me, but there was also an entry for \u0026quot;Dexterity and interaction\u0026quot;, which I was very pleased to see, as I happen to be left-handed. So, I selected that entry, only to discover, to my disappointment, that it actually has nothing to do with dexterity. There is no option where the user can specify whether they belong to the unexciting right-handed majority or the very special, if I may say so myself, left-handed minority. Modern mobile phones still appear to simply not care at all to switch a few screen elements around on the screen so as to accommodate left-handed people.\nAnnoying popups While in that menu I received the following annoyance: a popup asking me whether I would like to allow google to regularly check device activity for security problems, and prevent or warn about potential harm, with the option of declining or accepting. I had no idea what that meant, and there was no direct way of finding out, so I had no option but to decline, which is of course very annoying. About a minute later, the exact same screen popped up again, asking me the same question, so I had to decline again. This popup appeared in front of me a total of 4 times while using my phone and writing this text. On at least one occasion the popup disappeared all by its own, without me having to select \u0026quot;Decline\u0026quot;. This brings annoyance to a whole new level.\nStuff in Dutch At the bottom of the \u0026quot;Accessibility settings\u0026quot; screen there was a very interesting \u0026quot;Protect\u0026quot; entry, which lead to a screen all written in Dutch, presumably because I happen to be located in Holland. I had just selected that my language is English, so I was not expecting to see any Dutch in front of me. At all. Ever.\nSo, maybe I would have been interested to know what that protection stuff was all about, but if that protection stuff was created by people who can't get a simple language selection right, then it cannot possibly be any good at protecting me from anything, right?\nLame navigation designed by idiots, for idiots At the bottom of the \u0026quot;Accessibility settings\u0026quot; screen there was a \u0026quot;Next\u0026quot; button. This is somewhat confusing and quite annoying, because I came to this screen from the main menu, which had its own \u0026quot;Next\u0026quot; button, so I would expect to have to first go back to the main menu and then go to whatever \u0026quot;Next\u0026quot; is.\nThe \u0026quot;Next\u0026quot; button led to a \u0026quot;Wi-Fi\u0026quot; screen. There was no \u0026quot;Back\u0026quot; button. Pressing the \u0026quot;back\u0026quot; hardbutton took me back to the main screen. Clicking the \u0026quot;Start\u0026quot; button from the main screen brings me again to the \u0026quot;Wi-Fi\u0026quot; screen. Whatever. I guess I have to get used to navigation for idiots.\nThe monstrous EULA The next screen was \u0026quot;EULA \u0026amp; Diagnostic Data\u0026quot;. The EULA must have been like 20k of plain text, and it was deliberately shown through a small window covering about one-third of the total screen area as a hint that you should not even attempt to read it.\nI don't know and I don't care what kind of radical reformation of the Legal systems of the Western World it would take to get rid of this madness, but really, we should all be working hard towards a world where legal documents of this kind are prohibited by law. If you can't say it in five short sentences that the average 18-year old can understand, then you should not be allowed to even try to convince people to consent to it.\nSo, I had to accept it in order to proceed, and I have no idea what I accepted, I may have given my home over to some Korean guy for all I know. Perhaps my wife, too.\nLies about NFC After a brief pause to \u0026quot;check connection\u0026quot; and another for a \u0026quot;software update\u0026quot;, I was taken to a \u0026quot;Tap \u0026amp; Go\u0026quot; screen, which suggests that I can quickly copy any google accounts, backed up apps and data from my old Android device, by having my two devices touch each other. Which is of course a damned lie, because that would only work if my old device was also equipped with NFC. But I suppose that \u0026quot;truth\u0026quot; is sometimes a bit too technical and a bit too uncool to mention. I wonder if they burst out in evil laughter, thinking of the millions of people who will be wasting their time touching their devices and trying to get \u0026quot;Tap \u0026amp; Go\u0026quot; to work despite the fact that it can't. The scourge of the modern world is information design done by marketing people. So, okay, skip.\nFirst application crash during device setup On the next \u0026quot;Setup your account\u0026quot; screen, I entered my email, and when I hit next, I was told that \u0026quot;unfortunately, google play services has stopped\u0026quot; and I was taken back to the EULA screen. Need I mention how disillusioning it is to experience your first application crash as early as during device setup?\nWhat is my e-mail and password being used for? Fortunately, a second attempt worked. The screen said \u0026quot;By signing in, you agree to the Terms of Service and Privacy Policy\u0026quot;, which I could have clicked to read if I wished to, but of course I did not, in order to spare myself from the aggravation. There was also a \u0026quot;Don't sign in\u0026quot; option, which begs the question why it is only shown to me after I was asked to enter my e-mail and password. Or maybe this means that the email and password I just entered will be used for some unknown stuff, and if I now do this unknown thing they call \u0026quot;sign in\u0026quot; it will be used for some additional unknown stuff. This is driving me nuts.\nSo, I am their bitch now After a \u0026quot;Checking info\u0026quot; pause, I was taken to a \u0026quot;Set up payment info\u0026quot; screen, where I suppose filthy rich people who don't know what to do with their money actually do go ahead and enter their credit card info. They enter it. On the phone. Just because they were asked to. Right. Note that there is no \u0026quot;Skip\u0026quot; option; the best you can choose is \u0026quot;Remind me later\u0026quot;. Because of course you will be spending money on their shit; you bought their phone, you are their bitch now.\nStupid software trying to look smart After a \u0026quot;Just a sec...\u0026quot; pause, I was taken to a \u0026quot;Get your apps \u0026amp; data\u0026quot; screen, where I was told that I could setup my phone from a backup of my old phone which was last used 1 day ago. The problem here is that my old phone was not last used 1 day ago, it was used just now. So, this is software trying to look smart by boasting knowledge of things it knows nothing about, and of course getting it wrong. Anyhow, I agreed, and I edited the list of apps that I would like to have transferred, to see what kind of magic google will do to move all that stuff from one phone to the other.\nSecond application crash during device setup The next screen was \u0026quot;Google services\u0026quot;, where I agreed to back up my phone's apps, app data, settings, personal dictionaries, and Wi-Fi passwords using my Google Account so I can easily restore later. I also agreed to use Google's location service to help apps determine location. I unchecked \u0026quot;Help improve location services\u0026quot; and \u0026quot;Help improve your Android experience\u0026quot;. On this screen, I also received a popup saying \u0026quot;Unfortunately, Google Calendar Sync has stopped\u0026quot;. Who is Google Calendar Sync, what is he doing on my phone, and why has he stopped doing it?\nExtremely annoying chaotic navigation Then I decided to tap \u0026quot;\u0026lt;\u0026quot; to review what apps of the old phone will be transferred to the new phone, but instead of the \u0026quot;Get your apps \u0026amp; data\u0026quot; screen I was taken to an as of yet unseen \u0026quot;Account added\u0026quot; (\u0026quot;Your account is ready for use\u0026quot;) screen. Hitting \u0026quot;\u0026lt;\u0026quot; once again took me all the way back to the EULA screen. Hitting \u0026quot;Next\u0026quot; from there went through the \u0026quot;Checking connection\u0026quot; and \u0026quot;Software update\u0026quot; screens, and then landed me on the \u0026quot;Account added\u0026quot; screen again. So, there is no way to go back to the \u0026quot;Get your apps \u0026amp; data\u0026quot; screen. Apparently the wiz kids at google have invented the \u0026quot;Back\u0026quot; button that takes you to new places, and the important settings screen that vanishes and cannot be found again.\nYes, I am sure The next screen was the \u0026quot;Samsung account\u0026quot; screen. Of course I hit skip, and then the annoying assholes at Samsung presented me with an \u0026quot;Are you sure?\u0026quot; popup. So, skip times two, it is.\nSpeaking in riddles The next screen was some \u0026quot;Find my mobile\u0026quot; screen, by Samsung again, because I suppose the previous two taps on \u0026quot;Skip\u0026quot; were not enough for them to understand that besides buying their hardware, I want to have nothing to do with them. The text reads: \u0026quot;Protect your device in case it is lost or stolen by stopping other people from reactivating your device after it has been reset\u0026quot;. Which, of course, requires an understanding of several unknown words in order to make sense. Like, what does \u0026quot;protect\u0026quot; mean? What does \u0026quot;stopping\u0026quot; mean? What does \u0026quot;reactivating\u0026quot; mean? What does \u0026quot;reset\u0026quot; mean? And no, I am not an idiot, nor was I born yesterday, I am a Software Engineer with 30+ years of experience, and I have this very important message for you: no matter who you are, if you think you know what these terms mean, you don't. Also, statistically, as a member of the general public, you should not even pretend to know what these terms mean. Anyhow, my mom taught me to never agree to things I don't understand. So, \u0026quot;Next\u0026quot;.\nBotched device name The next screen reads \u0026quot;Samsung Galaxy\u0026quot; and \u0026quot;Thank you!\u0026quot;. Then it shows the \u0026quot;device name\u0026quot;, which is my first name followed by \u0026quot;(Galaxy Note\u0026quot;. Apparently, my name and the entire phone name did not fit; however the device name was being presented to me as a statement of fact, there was no option to change it to something else. As I later found out, it is possible to replace this name with something shorter by renaming the mobile device when you see it on Windows Explorer.\nOh they have selected some apps for me! After all this I clicked \u0026quot;Finish\u0026quot;, and the home screen of the phone appeared in front of me, only to disappear again immediately and be replaced by some \u0026quot;Vodafone Start\u0026quot; screen, where Vodafone is suggesting that I tap on \u0026quot;Next\u0026quot; to discover the Apps Vodafone has selected for me. Fuck off Vodafone!\nLies about internet connectivity So, then, finally, the home screen of my phone appears in front of me, and I can use it.\nI accidentally tap on what I thought was empty screen, but as it turns out, it was part of the weather widget, so the weather widget informs me that the weather is updated once every 6 hours, and I may increase the frequency of the updates, but this may incur additional charges. Which is a fucking lie, because I am on Wi-Fi.\nThe annoying bubbles Then, I let my screen timeout, and upon bringing the phone back to life again, the home screen has disappeared, and it has been replaced by some annoying animated bubbles. I do not find the bubbles offending per se, but what I do find offending is that the designers of the bubbles took it for granted that their sense of aesthetics should be found agreeable by me. I tap on the phone and it does not react, it only beeps, so it looks like I have been locked out of my own phone. To add insult to injury, the bubbles do react to my tapping, as if they are mocking me: they are dancing inside my phone, I am locked outside.\nIt turns out that a swipe lets me use the phone again, but the fact that a swipe is required was not advertised anywhere, which is annoying. I mean, they went through the trouble of adding the fucking animated bubbles, and they did not bother adding some kind of visual indication that a swipe is needed at that point?\nFound my location. Now what? So, after the swipe, a new screen appears out of nowhere, and a popup on top of it asks me to consent to providing my current location. I consent, so it searches for my current location, it displays it, and that's it. Just a screen showing the name of the city I am in, and nothing else. No \u0026quot;Done\u0026quot;, no \u0026quot;Next\u0026quot;, no \u0026quot;Select this and proceed\u0026quot;. In certain areas the user interface is very unpolished.\nThe fucking \u0026quot;google hangouts\u0026quot; A huge number of notifications is on the notification bar. One of them says \u0026quot;new messages from...\u0026quot; and lists a number of friends. I click on it, and suddenly google hangouts appears in front of me. I try to go \u0026quot;Back\u0026quot;, but it does not react to the \u0026quot;Back\u0026quot; hardbutton. The only option it leaves me with is \u0026quot;Okay, got it\u0026quot;, but I do not want to consent that I got it, because I did not get it, I do not even want to get it, I want google hangouts to leave me alone. I go to settings, application settings, application manager, I swipe all the way to the \u0026quot;running\u0026quot; tab, I search for google hangouts, it does not appear to be in the list. Then I notice an entry which says \u0026quot;hangouts\u0026quot;. I try to click it, but it disappears before I touch. I search for it in the list, it is nowhere to be found.\nI back out of the application manager, I am in hangouts again. I hit the \u0026quot;recent apps\u0026quot; button to go back into the application manager, but a new popup appears, informing me about new multi window features. (FUCK YOU!) I search for google hangouts in the running apps, it is still not there. I tap the \u0026quot;Active applications\u0026quot; button in the \u0026quot;recent apps\u0026quot; screen, the list is empty. Google hangouts somehow decided to leave me alone, but all this makes for a very chaotic experience.\nMore lies about internet connectivity To my horror I discover that mobile data is turned on. I go turn it off, and I am presented with a popup which says that I will no longer be available to use applications such as the internet. Which is a fucking lie, because I am on Wi-Fi.\nUndiscoverable user interface features One of the most useful icons on my home screen has always been the little counter which counts how many applications are running. I go to \u0026quot;Apps\u0026quot; and try to find it, but it is not there. I reckon it must be a widget, not an app. I look around for widgets, but there is no menu of any kind that mentions widgets anywhere. Finally, I realize that in order to get to the widgets I have to find an area of the screen which is unused, and long-tap on it to enter a special mode that allows placement of pages and insertion of widgets. Undiscoverable user-interface features are beyond lame.\nWhen \u0026quot;none\u0026quot; is not really \u0026quot;none\u0026quot; Page flipping has some fancy kitsch transition effect that causes me nausea. I set it to \u0026quot;None\u0026quot;. Actually, even \u0026quot;None\u0026quot; is not really \u0026quot;None\u0026quot;, it is a horizontal animation with an inertial effect. But it is the simplest available, and that makes it the most agreeable as far as I am concerned.\nThe fucking \u0026quot;flipboard briefing\u0026quot; While flipping the pages this annoying \u0026quot;Briefing\u0026quot; thing keeps showing up even though I did not ask for it. A closer examination reveals that the leftmost page is not really a page, it is this whole brand spanking new awesome spiffy and utterly useless to me feature for reading stuff. So every time I swipe to the leftmost page, this annoying thing is popping up. After much searching, I finally find a way to remove it: in the \u0026quot;Home screen settings\u0026quot; page there is a \u0026quot;Flipboard Briefing\u0026quot; setting which needs to be unchecked.\nI hate this modern fashion of coming up with \u0026quot;cool\u0026quot; short names for apps that give no hint as to what they are about. Sure, once you know what the app is about, the name might make sense and act as a mnemonic. But to the uninitiated, \u0026quot;Flipboard Briefing\u0026quot; means absolutely nothing. I know, for the guys who built this app, it was their daily everything for possibly a year or more. But really, to me, it means nothing, and I find it extremely annoying that I am essentially being invited to learn what this nonsensical name actually stands for before deciding whether I am interested in it or not. So, I think I will just skip directly to not being interested in it. Which was my default condition before hearing of its existence, by the way.\nThe fucking \u0026quot;profile sharing\u0026quot; A notification pops up that says \u0026quot;Profile Sharing\u0026quot; and \u0026quot;Show your profile image to your friends\u0026quot;. There is no hint as to who is showing this notification, what technologies it involves, who the fuck do they think my friends are, why the fuck would I be showing any information at all to them, let alone my profile picture, etc.\nI would very much like to know what bloody application is showing this notification, in order to uninstall it, but I do not even want to click on the notification, so I just dismiss it.\nThe fucking \u0026quot;beaming service\u0026quot; Another notification says \u0026quot;Beaming Service is Ready\u0026quot; and \u0026quot;The Samsung preinstalled Beaming Servic...\u0026quot; -- who the fuck asked for this service to be preinstalled, who the fuck requested that it be made \u0026quot;ready\u0026quot;, and what the fuck is this beam -- no, actually, I don't even care to know what the fuck it is beam, I just wish I was not bugged with this overwhelming pile of crap that I did not ask for and do not want to have anything to do with.\nThe fucking \u0026quot;vodafone start\u0026quot; Another notification says \u0026quot;Resume Vodafone Start\u0026quot;; apparently, dismissing the annoying uninvited app was not enough, so now it will be pestering me with notifications.\nThe fucking \u0026quot;samsung quick connect\u0026quot;. So, on the notification menu there are two new options, which are probably samsung-only. One says \u0026quot;S.Finder\u0026quot; and the other one says \u0026quot;Quick connect\u0026quot;. If you click on \u0026quot;Quick connect\u0026quot;, you are taken to a \u0026quot;Quick connect\u0026quot; screen which gives a few hints about the magic it supposedly does, has a \u0026quot;Turn on quick connect\u0026quot; button, and gives two rather confusing options: to turn it on as always visible, or to turn it on only when quick connect is turned on. Of course this makes no sense, so you read the text more carefully, and now it appears that it is asking whether the device should be visible to other devices at all times, or only when quick connect is turned on. Which, in light of the fact that you are about to turn on quick connect, does not make much more sense either. If you hit \u0026quot;Back\u0026quot; from that screen, the pestering little fuckers hit you with an annoying popup saying \u0026quot;Update Quick Connect\u0026quot;, and \u0026quot;A new version is available. Update now?\u0026quot; \\[Cancel\\] \\[Update now\\]. A screenshot that was captured earlier is now adorned in the notification menu with some crap which I believe was not there before, about sharing it or editing it and stuff. So, going back to the \u0026quot;Quick connect\u0026quot; and \u0026quot;Turn on quick connect\u0026quot; screen, I select \u0026quot;Only when quick connect turned on\u0026quot; and click on \u0026quot;Turn on quick connect\u0026quot;, and I see the \u0026quot;Update Quick connect\u0026quot; popup again. I cancel it, and nothing else happens. So I go back to the notification menu, select \u0026quot;Quick connect\u0026quot; again, and some previously unseen screen opens up, but it is immediately covered by the \u0026quot;Update Quick connect\u0026quot; popup so I cannot use it. I cancel the popup, and everything disappears. So, it is impossible to go back to the \u0026quot;Quick connect\u0026quot; screen again. The fucker secured my consent to turn itself on, and now it won't run unless I consent to also updating it. Also, I cannot find any way of disabling it, since there is no \u0026quot;Quick connect\u0026quot; app in the apps. This is a serious time waster and seriously driving me nuts.\n","date":"2016-03-17T17:05:01.392Z","permalink":"https://blog.michael.gr/post/2016-03-17-new-mobile-phone/","title":"New Mobile Phone"},{"content":"So, I bought a brand new Samsung Android phone, and it was a huge disappointment due to all the distracting, annoying, and completely useless crapware from Google, Samsung, Vodafone, and even Yahoo, which came pre-loaded with the phone and which I am not allowed to uninstall. I mean, never mind that a certain application is useless; suppose it is in fact very useful; and yet, suppose that despite it being so awesomely useful, I for some reason still want to uninstall it. It is my phone, I should be able to do it, right? But no, the powers that be have decided that I am not allowed to uninstall apps from my own phone. Even when they are not only useless, but actually harmful, since some of them are always running, thus consuming memory, CPU cycles, battery, and communications bandwidth. Some apps can be uninstalled, but many others cannot be uninstalled. They have to stay on the phone. Whether I like them or not.\nAfter this horrible experience I am very seriously considering the possibility that next time I buy a phone it will be an iPhone. But for now, I am stuck with Android, so I am now learning how to root my phone so that I can be somewhat in control of the situation. I am experimenting with my old phone first, a Samsung Galaxy S2. Here are my notes.\nFirst of all, make sure that your S2 is updated to the latest version of Android by going to settings -\u0026gt; About device and selecting Software update. Then, come to the same screen and check your Android version. The last time the powers that be bothered to distribute a new android version to the S2 was in 2012, and it was android version 4.1.2 \u0026quot;Jelly Bean\u0026quot;. In the future I will probably be upgrading to some custom ROM with some newer version of Android, but for now I only care to root the phone, and that is what this document deals with. So, make sure that your Android version is 4.1.2. If not, then these instructions are not applicable to you.\nInstalling the Samsung USB drivers was unnecessary, since windows had already detected my Samsung phone and downloaded the necessary drivers for it. But just in case, the drivers can be found here: http://developer.samsung.com/android/tools-sdks/Samsung-Andorid-USB-Driver-for-Windows (yes, it says \u0026quot;Andorid\u0026quot;. But it works. Go figure.) If you have ever installed Samsung Kies, then you already have the drivers, but rumor has it that Kies must not be running during this procedure.\nDownload and run adb-setup-1.4.3.exe from http://forum.xda-developers.com/showthread.php?t=2588979 It copies some files under C:\\adb without asking (How lame is that!) and adds that directory to the path. When prompted to install the Google USB drivers, the answer is \u0026quot;No\u0026quot;, because the Samsung Galaxy S2 is a Samsung phone, not a Google phone.\nGo to developer options, turn them on, enable USB debugging, then on the PC run cmd.exe and execute \u0026quot;adb devices\u0026quot;. The phone should be listed. In retrospect, I think that installing ADB simply to check the connection makes it a rather unnecessary step for rooting the phone, but never mind, having adb will certainly come handy for doing other things with the phone once it has been rooted.\nDownload a kernel. The process of rooting the phone involves replacing the kernel of the system, and rumors have it that when doing so you have to find the right kernel flavor for the \u0026quot;build number\u0026quot; of the device. Under Settings -\u0026gt; About device my phone reports its build number as JZO54K.I9100XWLSE. The JZO54K part seems to be irrelevant, the I9100XW part says that I have a Samsung Galaxy S2, (if not, then these instructions are inapplicable,) and the LSE part tells which flavor of the kernel is needed. Luckily, the Jaboo kernel that I found on the interwebz is said to cover many flavors, including LSE. So, visit http://forum.xda-developers.com/galaxy-s2/development-derivatives/kernel-v2-3a-jb-i9100-t2062208 and you will be directed to download a jeboo_kernel_i9100_v2-3a.zip or similar, which is useless, because it is a zip file. So, extract zImage from it, and then use 7zip to create a tar file containing zImage. Luckily, 7zip creates the file in \u0026quot;ustar\u0026quot; format, which is the correct format. Let's call it jeboo_kernel_i9100_v2-3a.tar\nDisconnect the USB cable from the phone.\nBoot your phone into \u0026quot;Download Mode\u0026quot; with the following voodoo magic: power-off the phone, (and wait for its final death throe,) then press and hold the [Volume Down] and [Home] buttons together and then press and hold the [Power] button for a few seconds. This should cause a screen to appear, at which point you may release all these buttons. The screen is warning you that what you are about to do is oh-so-scary, and asking you to press [Volume Up] to continue. Press [Volume Up]. The phone enters \u0026quot;Download Mode\u0026quot;.\nRun Odin.\nConnect the USB cable to the phone, and one of the boxes in Odin under ID:COM will turn yellow, indicating that it found your phone.\nIn Odin, make sure Auto Reboot and F. Reset Time are checked.\nIn Odin, click the PDA button, and select jeboo_kernel_i9100_v2-3a.tar. When done, there should also be a check next to PDA.\nIn Odin, click Start. Odin will flash the jaboo kernel onto the phone, and the phone will restart. Your phone now has ClockworkMod Recovery installed on it, but it is still not rooted.\nVisit http://d-h.st/9WL and download UPDATE-SuperSU-v1.30.zip.\nCopy UPDATE-SuperSU-v1.30.zip to your mobile phone.\nBoot your phone into CWM Recovery mode with the following voodoo magic: power-off the phone, (and wait for its final death throe,) then press and hold the [Volume Up] and [Home] buttons, and then press and hold the [Power] button for several seconds. The \u0026quot;Jaboo Recovery\u0026quot; screen should now appear, at which point you may release the buttons. The screen has instructions on how to navigate using swipes, but you can also use the volume keys to navigate and the power button to make a selection.\nSelect \u0026quot;Install Zip\u0026quot;, then navigate and select UPDATE-SuperSU-v1.30.zip. The file will be installed, and you will also see a log describing that everything went fine, undoubtedly.\nReturn to the main menu and select \u0026quot;reboot system now\u0026quot;.\nAt the end of all this, your device is rooted. While booting, a yellow warning triangle will be displayed under the Samsung logo, indicating that you have tampered with the flash storage of the device. If you want to get rid of this triangle, download the \u0026quot;Triangle Away\u0026quot; app from here: http://forum.xda-developers.com/galaxy-s2/orig-development/2014-01-15-triangleaway-v3-26-t1494114 and if you like it, consider buying the app from the Google Play store, thus buying the developer a beer.\nOld comments\nmichael.gr 2016-03-28 14:34:51 UTC\nFirst adventure: downloaded \u0026quot;JRummy Root Browser\u0026quot; and tried to get rid of some apps from /system/app by moving them into a new folder /system/app/away. I moved a whole bunch of seemingly irrelevant stuff, then went back to the home screen, and my google calendar was not working. It was saying \u0026quot;Oops, your calendars and events can't be displayed because Calendar Storage is disabled\u0026quot; and \u0026quot;Turn Calendar Storage back on in Settings\u0026quot; and it had a button \u0026quot;Enable in Settings\u0026quot; which would take me to some settings, but there was nothing there to enable. I started troubleshooting, and I found that apparently some com.android.providers.calendar component was missing, but nothing more helpful. Finally, I realized that I should not have moved \u0026quot;SecCalendarProvider.apk\u0026quot; and \u0026quot;SecCalendarProvider.odex\u0026quot; to \u0026quot;away\u0026quot;. I put them back in their original place, restarted the device, and things went back to normal.\n","date":"2016-02-21T08:34:52.61Z","permalink":"https://blog.michael.gr/post/2016-03-rooting-my-samsung-s2/","title":"Rooting my Samsung Galaxy S2 mobile phone"},{"content":"Summary This is a story about the most elusive and sinister software bug I ever came across in my decades-long career as a programmer.\nThe setting At some point early in my career I was working for a company that was developing a hand-held computer for the area of Home Health Care. It was called InfoTouch™. The job involved daily interaction with the guys in the hardware department, which was actually quite a joy, despite the incessant \u0026quot;It's a software problem!\u0026quot; -- \u0026quot;No, it's a hardware problem!\u0026quot; arguments, because these arguments were being made by well-meant engineers from both camps, who were all in search of the truth, without egoisms, vested interests, or illusions of infallibility. That is, in true engineering tradition.\nThe manifestations During the development of the InfoTouch, for more than a year, possibly two, the device would randomly die for no apparent reason. Sometimes it would die once a day, other times weeks would pass without a problem. On some rare occasions it would die while someone was using it, but more often it would die while sleeping, or while charging. So, the problem seemed to be completely random, and no matter how hard we tried we could not find a sequence of steps that would reproduce it.\nWhen the machine died, the only thing we could do was to give it to the hardware guys, who would open it up, throw an oscilloscope at it, and try to determine whether it was dead due to a hardware or a software malfunction. And since we software guys were not terribly familiar with oscilloscopes, we had to trust what the hardware guys said.\nLuckily, the hardware guys would never say with absolute certainty that it was a software problem. At worst, they would say that it was \u0026quot;most probably\u0026quot; a software problem. What did not help at all was that one out of every dozen times that they went through the drill, they found that it did in fact appear to be a hardware problem: the machine was just dead; there was no clock, no interrupts, no electronic magic of the kind that makes software run. But what was happening the rest of the times was still under debate.\nThis situation was going on for a long time, and we had no way of dealing with it other than hoping that one day someone either from the software department or the hardware department would stumble upon the solution by chance. The result was a vague sense of helplessness and low overall morale, which was the last thing needed in that little startup company which was struggling to survive due to many other reasons having to do with funding, partnerships, competitors, etc.\nThe discovery Then one day as I was working on some C code somewhere in our code base, I stumbled upon a function which was declaring a local variable of pointer type and proceeding to write to the memory location pointed by it without first initializing it. This is a silly little bug which is almost guaranteed to cause a malfunction, possibly a crash.\nTo this day still I do not know (or do not remember) whether that early version of Microsoft C did not yet support warnings for this type of mistake, or whether the people responsible for our build configuration had such hubris as to believe that \u0026quot;we don't need no stinkin' warnings\u0026quot;.\nI quickly fixed the bug, and I was about to proceed with my daily work, when I thought to take a minute and check precisely what were the consequences and ramifications of that bug before the fix.\nFirst of all, I checked to see whether the function was ever being called, and it turned out that it was; however, the InfoTouch was running fine for 99.9% of the time, so obviously, due to some coincidence, this bug did not seem to cause any problems.\nOr did it?\nThe astonishment I decided to see exactly what was the garbage that the pointer was being initialized with.\nThe problem was in function cfunc(). Function cfunc() was invoked from function afunc(), which had just previously invoked function bfunc().\nstatic void afunc() { bfunc( arg1, arg2 ); cfunc( arg1 ); // \u0026lt;-- function with uninitialized local variable } Function bfunc() had two arguments and one local variable . Function cfunc() had one argument and two local variables, of which the second was the uninitialized pointer. So, the uninitialized pointer in cfunc() shared the same stack word as the local variable in bfunc().\nTo my astonishment, I discovered that the local variable in bfunc() was a timestamp, where the function stored the current time.\nThis is how the stack looked:\nSo, the uninitialized pointer contained a bit pattern that represented a date and time. This resulted in random memory corruption during different hours of the day and different days of the month. The function was not being invoked very frequently, so the memory corruption was building up slowly, until some vital memory location would be affected and the software would crash. It is amazing that the machine ever worked at all.\nAfter this bugfix the InfoTouch never again experienced any problems of a similar kind.\nThe lesson learned What do we learn from this? Warnings are your friend. Enable as many warnings as you can, and use the \u0026quot;treat warnings as errors\u0026quot; option to ensure that not a single warning goes unnoticed.\nOld comments\nUnspecified 2015-12-14 13:11:39 UTC\nnice story...\nmichael.gr 2015-12-14 13:39:21 UTC\nThanks, Divyesh! C-:=\nAnonymous 2019-09-16 18:56:39 UTC\nClassic :)\n","date":"2015-10-21T00:20:15.194Z","permalink":"https://blog.michael.gr/post/2015-10-the-mother-of-all-bugs/","title":"The Mother of All Bugs"},{"content":"\rBack in 1999-2000 the state of the art in computer telephony was called Interactive Voice Response (IVR). Nowadays when we speak of \u0026quot;voice\u0026quot; we usually mean voice recognition, but all that those telephony systems did back then was to playback recorded messages and wait for the caller to press digits on their phone. Sometimes, we would ask the caller to speak on the phone, and we would record their voice, for a human operator to listen to later.\nThe hardware had special filters on it to recognize the DTMF digits, probably because the CPU was thought of as too wimpy to do it by itself. I experimented writing WAV-file processing filters on my own, and discovered that it took less than 10% of CPU time per phone line to run such filters in software, so it could certainly be done, but then again there existed systems out there in configurations of 30 or even 100 lines per computer, and of course the CPU was not enough in these cases. We only worked with configurations of four lines per computer, but still, since the filters were made available by the hardware, I made use of them for the work project, and I only re-invented the wheel at home, for fun.\nMy employer at that time managed to secure a number of computer telephony contracts for a couple of big clients; he gave me a rough description of what the projects were supposed to do, and he had my coworkers slide pizza under my office door for as long as it took me to complete them. He probably charged his clients the equivalent of a dozen programmers for this, and it was all done by me. The only external help that went into these projects was messages recorded by a professional at a recording studio.\nWhat follows is some screenshots of the telephony application that I created to run these projects, in Microsoft Visual C++ using MFC and the Dialogic Telephony API.\nAll applets waiting to start. Click to enlarge. This is a Multiple Document Interface (MDI) application (remember those?) which opened one MDI client window per phone line made available by the underlying hardware, and allowed the user to select a telephony applet to run on each phone line. The operator could see the status of each line and the log messages produced by each applet, and he could start, stop, and select applets to run.\nAll applets running. Click to enlarge. Each applet was instantiated by the framework to run on its own thread, and it was given a simple abstraction of the telephony API to do work with, and another simple API to display its status and log messages. I also wrote an emulated implementation of my telephony API abstraction, so as to be able to test the applets without having to place actual phone calls. The emulator would run in a special MDI client window with a few additional controls on it for calling the applet, sending DTMF digits, hanging up, and even faking a timeout so that I did not have to wait while testing.\nThe emulator. Click to enlarge. The main application also provided a uniform means of specifying configuration parameters for the applets, so that the applets did not need to have any GUI of their own.\n","date":"2015-10-18T13:39:42.072Z","permalink":"https://blog.michael.gr/post/2015-10-computer-telephony-in-c-with-mfc/","title":"Computer telephony in C++ with MFC"},{"content":"![[media/crossword-compiler-screenshot.png]]\nThis is old-style artificial intelligence in action, solving within seconds a problem that would normally take eons to complete.\nYou give it a crossword grid, and a long list of words, and it finds ways to mesh words into the grid so as to form a complete crossword puzzle. The final working version was done in 2003 using C# version 1.2 with a minimalistic UI in WinForms.\nThe following 30-second video shows the crossword compiler in action, filling multiple successive crosswords using a word list taken from actual crosswords that have been published on the interwebz by various sources through the years. The video is in real time, showing that the crossword compiler is, in most cases, extremely fast.\nTechniques demonstrated: Solving an intractable problem using a scoring heuristic Super-indexing data structures for ultra-fast domain-specific queries A longer description for those who like reading For my Graduation Project at California State University, San Bernardino in 1993 I chose to develop a Crossword Puzzle Compiler. That's a program that you give it a crossword grid and a long list of words, and it weaves the words into the grid to produce a Crossword Puzzle. I wrote the program in C, on the killer machine that I had at home, an IBM PS/2 Model 80 running DOS, with an Intel 80386 clocked at 16Mhz, and equipped with an entire megabyte of RAM. I had not taken any Artifical Intelligence courses, so poor me was under the impression that I could just brute force it and it would work, since the machine was so incredibly fast. Afterall, DOS directory listings scrolled by so much faster than on my old 8Mhz Intel 8088!\nThe program did work, but only in the sense that it would begin to fill the grid, and it would keep working on it without any errors. However, it would never actually complete the grid, because this is not one of those problems that you can just brute force: you need to be more clever than that.\nThe test grid was American-style 15 by 15 symmetrical, divided into 9 compartments that were roughly 4 by 4 each, each one of them communicating with the rest via only one or two longer words. Back then we had text-mode VGA displays, so storing a single byte in the video RAM caused a character to appear on the screen, which made it very easy to have the program display its progress. The program would fill the first 4 by 4 compartment within a fraction of a second, but then it would get stuck in the second compartment, unable to find a way to mesh the words. It would back-track to the first compartment, undo a word, and try the second compartment again for several seconds, only to back-track again. Hours would pass, and it was not getting any further than the second compartment. I once let it work overnight, and by the next day it had completed the second compartment and was now struggling with the third. As I was watching it, it back-tracked from the third to the second, and to my great dismay after a few seconds it back-tracked to the first. I was sure that it would complete all 9 compartments one day, but I understood that the day was several centuries in the future.\nI admitted failure to my professor, and he agreed to let me pick another subject, which I promptly did and graduated. However, the desire to get that Crossword Puzzle Compiler to work stayed with me.\nBy the time I was back in Greece in the late nineties I hard learned about heuristics and search tree pruning, so I thought I would give it another go. I decided that I would first rewrite the whole thing in C++, and then I would try to find some heuristic to make it cut corners and complete crosswords in a timely manner. When I had just gotten it to roughly the same point where the older version was, and before I had the chance to try any heuristics, my apartment was broken into and my laptop was stolen, so I lost all of the work that I had done.\nIn early 2003 I decided to give it another go, this time in a brand-new language called C#. (That was .NET Framework 1.1 on Visual Studio .NET 2003.) Either because the language made it so much easier to be productive, or because by that time I had learned so much more about Object Oriented Programming, I was able to re-create the program very quickly, and then I used a simple technique to cut corners, and thus avoid brute-forcing: when it had a list of words to consider, it would first assign a score to each word based on how many words could be found perpendicular to it, then it would sort the words by score, and then it would start trying each word in the list starting from the word with the highest score. So, one day I hit the \u0026quot;Run\u0026quot; key on my keyboard, and after only a few seconds of waiting, a completely filled Crossword Puzzle appeared in front of me for the first time. That must have been the most magic moment in my life. (Well, excluding certain moments with girls.)\nThe generated crosswords were not of a very good quality, because the Crossword Compiler would keep picking words that have high perpendicular counts, like \u0026quot;ADD\u0026quot; and \u0026quot;DAD\u0026quot;. So, in 2011 I revisited the project with the intention improving the heuristic. However, I knew that would be a time consuming process, involving a lot of trial and error, so I decided to first improve the linear running time of the program. I did many optimizations, all of them structural and algorithmic, without any small-scale hacking and tweaking of the kind which shaves clock cycles but renders the code unmaintainable.\nBy far the greatest optimization that I did was what I called the Super Index. The Crossword Dictionary Model of the Crossword Puzzle Compiler needs to be able to very quickly answer queries like \u0026quot;give me all 5-letter words with a the letter 'G' on the 3rd position.\u0026quot; A simplistic approach is to have words already grouped by word length, and to just enumerate all words of the requested length, looking for ones that have the requested letter in the requested position. However, we can do a lot better than that. The Super Index is an index which has a sub-index for each word length, which has a sub-index for each position along that word length, which has a sub-index for each letter of the alphabet, which has a list of all words of that length, that contain that letter, at that position. Therefore, search through words is completely eliminated. If I remember correctly, the introduction of the super index cut the running time of the Crossword Compiler to one twentieth of what it used to be.\n■\n","date":"2015-10-18T01:16:02.251Z","permalink":"https://blog.michael.gr/post/2015-10-my-crossword-compiler/","title":"Crossword Puzzle Compiler"},{"content":"This question was asked on Programmers SE on Jun 12, 2015. I answered it, but after a few days the question was closed as primarily opinion-based and then deleted, along with all answers. Since I now have sufficient reputation to view deleted questions, I was able to find it, so I am posting the question and my answer here for posterity.\nThe Question Original link: https://programmers.stackexchange.com/questions/286508/is-my-mentors-concern-for-code-quality-excessive\nIs my mentor's concern for code quality excessive? \\[closed\\] Score: 75 (79 upvotes, 4 downvotes) Favorites: 28\nTo tell you a little about myself: I'm a newbie programmer working internships and learning a lot from experienced programmers. I can't believe I used to think I was good in college.\nThe one I'm doing right now is pretty great due to the amount of time and resources that the company is putting into helping and mentoring me and another intern. I'm learning a whole lot and for the first time, I feel like I get close to being competent.\nThe only \u0026quot;problem\u0026quot; are the massive code quality concerns of one of my mentors. It's to the point that anything takes a whole lot of time because I have to find the best way to do it or else it's a waste of time. It also feels like my creativity doesn't matter because there is only one right way to do everything. I don't mind any of this at all but I wonder, and this is mainly what I'm asking, if it's normal in the industry.\nAlso, when I get assigned a little feature and this guy reviews my code, he actually reviews the whole codebase I'm working on, pointing out loads of mistakes, most of them from before I was even hired. I have spent this whole week fixing code (that worked) written by their full-time programmers, even some things that are best practice according to other mentors.\nTags: [javascript] [web-development] [programming-practices] [object-oriented-design]\nasked Jun 11 2015 at 18:51 by CyborgFish\nMy Answer: Let me quote from Page xxii (Foreword) of \u0026quot;Clean Code\u0026quot; by Robert C. Martin from Prentice Hall.\nBack in my days working in the Bell Labs Software Production Research organization (Production, indeed!) we had some back-of-the-envelope findings that suggested that consistent indentation style was one of the most statistically significant indicators of low bug density. We want it to be that architecture or programming language or some other high notion should be the cause of quality; as people whose supposed professionalism owes to the mastery of tools and lofty design methods, we feel insulted by the value that those factory floor machines, the coders, add through the simple consistent application of an indentation style. To quote my own book of 17 years ago, such style distinguishes excellence from mere competence.\nI would like to suggest that what the author observed as \u0026quot;consistent indentation style\u0026quot; was most probably in fact a \u0026quot;consistent coding style\u0026quot;, which in turn shows discipline. So, is discipline important? You bet!\nYes, you may be grunting now; yes this is probably a drag; but it is short-term, within less than a year you will have learned to do things correctly, and this will stay with you for decades, so it's actually very good.\nThe author says it is what distinguishes excellence from mere competence. So, none of this means that you are incompetent; but your mentor actually wants excellence from you; and that's something you should be glad about.\nEarly in my career, (actually, precisely when I was fresh out of college,) I also had to work for some people who were hell bent on telling me how to do things down to the comma, and I am glad now that I went through that process back then.\nAlso please keep in mind that your creativity will best manifest itself in building designs, not in formulating code. Creativity in formulating code is a synonym for disorder; creativity in designs is what all the good stuff are made of.\nIt is worth also including some of the comments to my answer, which were quite insightful. Most importantly, dodgethesteamroller's comment is about an issue which I should have addressed: correlation vs. causality.\n26^ \u0026quot;consistent indentation style was one of the most statistically significant indicators of low bug density.\u0026quot; Totally true for python (: – ecoologic Jun 11 at 23:49\n23^ However, beware Goodhart's law (en.wikipedia.org/wiki/Goodhart%27s_law). Once you notice that indentation is strongly correlated with good code, and argue on that basis that the code should be indented consistently in order to pass review, then typically you should expect the correlation to weaken and eventually disappear, because ill-disciplined people now learn to fix their indentation without becoming more disciplined in any other respect. – Steve Jessop Jun 12 at 9:35\n8^ \u0026quot;Also please keep in mind that your creativity will best manifest itself in building designs, not in formulating code. Creativity in formulating code is a synonym for disorder; creativity in designs is what all the good stuff are made of.\u0026quot; This :) – TheCatWhisperer Jun 12 at 12:45\n4^ Programming is actually quite tolerant in that respect. Creativity in chess is all at the strategic level; inventing new moves for your chess pieces is frowned upon ;) – MSalters Jun 12 at 15:16\n1^ @SteveJessop: Right on. Very perceptive. That quotation does nothing to improve my already low opinion of Uncle Bob and his ilk. Sounds like a classic case of correlation mistaken for causation. – dodgethesteamroller Jun 13 at 0:33\n@dodgethesteamroller: in particular, you can get consistent indents just by running a linter over whatever junk some cowboy throws out. Clearly this will not reduce bug density, although I suppose it might help future maintainers avoid introducing more bugs. – Steve Jessop Jun 13 at 0:59\n@SteveJessop Good, then we will move to the next offender and teach them one good practice at the time. – Davidmh Jun 14 at 14:24\n","date":"2015-09-25T21:07:12.456Z","permalink":"https://blog.michael.gr/post/2015-09-is-my-mentors-concern-for-code-quality/","title":"Is my mentor's concern for code quality excessive?"},{"content":"\rOracle talks a lot about 11g Express Edition, and how it differs from the full (paid) versions of their database, but it does not say a word about the most important thing:\nIs it compatible at the SQL syntax level?\nNo way to find out other than to try it. So, let's try it.\n(Useful pre-reading: About these papers)\nThe downloaded installation zip file contains a folder called DISK1. Now, Oracle Database Express Edition 11g Release 2 bears the date June 4, 2014, so 14 years into the 3rd millennium Oracle is still masturbating with the concept of \u0026quot;disks\u0026quot;.\nThe downloaded installation zip file is 316MB, it contains a setup.exe which is 317MB, and when you run it, it extracts a contained .msi file, which in turn extracts its contents.\nThe installer asks me to specify a database password. It says that this password will be used for the SYS and SYSTEM accounts. If both accounts are to have the same password, then clearly, one of them must be redundant, no?\nOracle 11g Express installs an icon on my desktop which is literally hideous.\nI double-click the icon, and an error message dialog pops up. The title of the dialog is:\nhttp://127.0.0.1:%HTTPPORT%/apex/f?p=4950\nand the message in the dialog is:\nWindows cannot find 'http://127.0.0.1:%HTTPPORT%/apex/f?p=4950'. Make sure you typed the name correctly, and then try again.\nI open up the start menu, find a \u0026quot;Start Database\u0026quot; icon, double click it, and an elevated command prompt shows up. The command prompt appears to do nothing at all, it just sits there waiting for me to type something. I haven't the slightest clue as to what I am supposed to type in there. After some troubleshooting it turns out that the command prompt executed a command which (might have) silently started the database. But they used the /k option, so the command prompt stayed open, so it appears as if nothing was done.\nAnd then, that's it. Some database must have started somewhere, but you are given no means of working with it. There is a \u0026quot;Get Started\u0026quot; icon in the start menu, but clicking it gives the same stupendous message as the one on the desktop:\nWindows cannot find 'http://127.0.0.1:%HTTPPORT%/apex/f?p=4950'. Make sure you typed the name correctly, and then try again.\nSo, you have to guess that you must download and run Oracle SQLDeveloper.\nWhen Oracle SQLDeveloper starts, it begins with a \u0026quot;Confirm Import Preferences\u0026quot; box which asks me if I want to import preferences from a previous SQL Developer installation. The dialog has a list of \u0026quot;previous installations\u0026quot;, but it is empty. You have to use your magic powers to guess that you need to click on a tiny magnifying glass icon on the side, to search for a previous installation. It opens a file selection dialog which starts pointing to \u0026quot;roaming/SQLDeveloper\u0026quot;, and then you have to guess that you need to doubleclick one of its subdirectories.\nThe process of importing preferences from the previous version takes forever, (entire minutes on a 4GHz i7-4790 with SATA3 SSD,) even though I hardly used that previous version: I just launched it once and closed it.\nMy firewall shows that as soon as Oracle SQLDeveloper launches, it calls home, even as it is prompting me whether I would like to participate in their data collection scheme. (\u0026quot;Allow automated usage reporting to Oracle.\u0026quot;)\nWhen trying to create a connection, I am presented with a dialog with far more options than should be necessary, and I have to guess what to enter in various fields. Trying with \u0026quot;SYS\u0026quot; for username yields an error message saying \u0026quot;ORA-28009: connection as SYS should be as SYSDBA or SYSOPER\u0026quot;, but then trying with \u0026quot;SYSDBA\u0026quot; and with \u0026quot;SYSOPER\u0026quot; gives \u0026quot;ORA-01017: invalid username/password; logon denied\u0026quot;. So, again, I have to guess that \u0026quot;SYSTEM\u0026quot; must be the magic word.\nAlso, it is not enough to check \u0026quot;Save password\u0026quot;, I have to also remember to click \u0026quot;Save\u0026quot; to save the saved password.\nUpon opening a brand spanking new (empty) database, it contains dozens of system tables. A filter has already been applied, but it only excludes tables in some \u0026quot;recycle bin\u0026quot;. It is possible to extend the filter to exclude most system tables, but it takes quite a bit of work, because they do not all obey a consistent naming pattern.\nThere is a \u0026quot;manage database\u0026quot; option which opens up a purely informational page, nothing to manage there.\nSo, it turns out that Oracle 11g Express Edition installed with a \u0026quot;sid\u0026quot; of \u0026quot;xe\u0026quot;, which is different from the one that I needed. I do not know what this \u0026quot;sid\u0026quot; thing is, and I do not care, and frankly, it should not exist, but it does. So, I have to change the sid of my local instance. Luckily, there is an article on stackoverflow on how to do it: http://stackoverflow.com/a/3424544/773113 I decide that this is insanely complicated and opt to uninstall my local instance and re-install it from scratch, only to discover that nowhere during installation does oracle ask me for the sid that the local instance will have.\nThe first time I invoke databaseMetaData. getImportedKeys() it takes an exorbitant amount of time for the server to respond, (14 seconds on an i7-4790K with SATA3 SSD,) even on a freshly created, empty database.\nIn Oracle SQL Developer, If you select a single table, the context menu allows you to drop it. But if you select multiple tables, a different context menu is shown, which does not contain an option to drop them. So, you have to do it table by table.\nIn Oracle SQL Developer, there is a \u0026quot;Quick DDL\u0026quot; entry in the context menu. If you select multiple tables and activate this menu entry, you find out that it is anything but quick: it takes 4 minutes for 20 small tables.\nThe dialog shown for lengthy operations has a \u0026quot;run in background\u0026quot; option. If you select this option, the dialog disappears and not a single hint of it remains anywhere to be seen. So, you don't know if it is running, and you have no way of checking its progress.\nSupposedly, \u0026quot;run in background\u0026quot; means \u0026quot;take this application-modal dialog out of my face so that I can continue using the application\u0026quot;. But if you send a lengthy operation to the background, and then you try to do anything with the application, you get a message saying that \u0026quot;the connection is currently busy\u0026quot;.\nThe \u0026quot;connection is currently busy\u0026quot; dialog has two buttons: one says \u0026quot;Retry\u0026quot; and the other says \u0026quot;Abort\u0026quot;. This makes you wonder what is going to happen if you click \u0026quot;Abort\u0026quot;: is it going to terminate the application? Is it going to terminate the background process? No, it turns out that it just cancels the dialog. How callous must an engineer be call the button \u0026quot;Abort\u0026quot; when they just mean \u0026quot;Cancel\u0026quot;?\nIn Oracle SQL Developer, whenever you issue a command which fails, you get extensive but completely lame error messages. For example, the following:\nError starting at line : 3 in command - ALTER TABLE \u0026quot;APV_OWNER\u0026quot;.\u0026quot;CONNECTIBLES\u0026quot; DROP FOREIGN KEY \u0026quot;SYS_C0010516\u0026quot; Error report - SQL Error: ORA-00905: missing keyword 00905. 00000 - \u0026quot;missing keyword\u0026quot; *Cause: *Action:\nThere is a problem with every single line of the above error report:\n\u0026quot;Error starting at line : 3 in command -\u0026quot;: When it says \u0026quot;line 3\u0026quot; it does not mean line 3 of the failed command, it means that the failed command was taken from line 3 of the original script. But I chose to execute only that particular command from the script, so I really do not care to hear which line of the script it came from: it is the one and only one command that I chose to execute. Furthermore, if the failed command was several lines long then this blatant lie about line 3 would have sent me on a wild goose chase, looking for an error at line 3 of the failed command. ALTER TABLE \u0026quot;APV_{...}\u0026quot;: I know what command I issued, you see, I just issued it, so I do not need to see its full text again, inside the error report. At most, the command should have been echoed when it was about to be executed. Furthermore, Oracle does not help me at all to pinpoint the precise location within the command where the error occurred, (see further down,) so including the full text of the command with the error report without telling me where the error occurred is only adding insult to injury. \u0026quot;Error report -\u0026quot;: I know that this is an error report, I do not need to be told that it is an error report. This is a Z.I.S. (Zero Information Statement.) \u0026quot;SQL Error: ORA-00905: missing keyword\u0026quot;: A useless message which is giving me absolutely no hint as to what went wrong, and where. This is as good as saying \u0026quot;an error occurred\u0026quot;. 00905. 00000 - \u0026quot;missing keyword\u0026quot;: Oracle, you are repeating yourself. \u0026quot;*Cause:\u0026quot; So, what is the cause? How lame is this? \u0026quot;*Action:\u0026quot; So, what is the action? How lame is this? So, as it turns out, Oracle Express Edition luckily does understand the exact same SQL syntax as the paid versions. Now, let's look at how Oracle SQL differs from other relational databases:\nNumeric data types do not correspond to machine data types. Instead, they are all stored in Binary-Coded-Decimal (BCD) form, and they must be declared in ancient SQL syntax. So, there is no such thing as INTEGER and BIGINT, there is only NUMBER(10) and NUMBER(19). The corresponding stupidity holds true for real numbers. This comes straight from the nineteen sixties. The middle of the previous frigging century. Identifiers can only be up to 30 characters long. I do not know whether the folks at Oracle have realized this, but for more than a decade now, we live in the third millennium. Unlimited identifier length is what we have grown accustomed and expect for decades now. We use long identifiers, quite commonly in excess of 30 characters long, and we also use tools and techniques that automatically construct identifier names from other identifiers. For example, a foreign key name is commonly constructed as \u0026quot;fk_\u0026quot; + tableName + \u0026quot;_\u0026quot; + keyColumnName. This all breaks under Oracle. There are no catalogs, and schemas correspond to users. Which means that in order to create a schema, you need to create a user. With a password. And when creating a new user, you must specify something called a tablespace, which must have the magic name \u0026quot;USERS\u0026quot;, which will hopefully work on any installation out there. All this means that automated schema creation suddenly becomes very tricky business with Oracle. I understand that features like this are very good for the job security of the sysadmins, but they are very bad for anyone wishing to get any useful work done. The empty string is treated the same as NULL. This is the most devastating difference between Oracle and other RDBMSes, and it alone is enough for me to never recommend Oracle for anything, to anyone. (Except my enemies.) What it means is that if you write NULL, you get NULL, but if you write an empty string, you also get NULL. There is no option to change this behaviour, and no easy workaround. This is incredibly lame. Actually, words cannot express how lame this is. The lameness of this is of epic proportions. There is no such thing as an AUTOINCREMENT column. Oracle supports SEQUENCEs, but it does not offer any easy way of specifying that a column should obtain its value from a sequence. There is a Byzantine way of declaring a TRIGGER which gets triggered once a row is inserted and fills the value of a column with the next value from a sequence, but good luck in getting that bitch to work. There is no LIMIT or TOP clause. In order to limit the number of rows returned by a query you have to embed it as a subquery of an absolutely idiomatic \u0026quot;SELECT * FROM (query) WHERE ROWNUM \u0026lt;= N\u0026quot;. The syntax is generally very poor and lacking in features to make the life of the programmers easier. For example, there are no niceties such as DROP TABLE IF EXISTS and CREATE OR REPLACE SEQUENCE, forcing you to write (and debug) tons of script, which in turn locks you with their product. Old comments\nAnonymous 2017-08-18 11:22:14 UTC\nMasterpiece...\nUnspecified 2018-11-19 10:22:05 UTC\nOracle never pleases users. All that company is good at is bureaucracy.\n","date":"2015-09-14T11:40:29.958Z","permalink":"https://blog.michael.gr/post/2015-09-why-oracle-sucks/","title":"Why Oracle Sucks"},{"content":"The question is on a controversial topic, and as usual, I take the controversial stance. So, it is no wonder that people do not agree with me.\nHere is the question:\nprogrammers.stackexchange.com: Does it make sense to use “ys” instead of “ies” in identifiers to ease find-and-replace functionality?\nUpdate: It turns out that the question was closed as \u0026quot;primarily opinion based\u0026quot;, so I am saving the question and my answer here for posterity.\nThe question:\nDoes it make sense to use “ys” instead of “ies” in identifiers to ease find-and-replace functionality? {closed} Although grammatically incorrect, when writing identifiers for functions, variables etc. does it make sense to simply append an \u0026quot;s\u0026quot; to plurals of words ending in Y? My reason for this would be that if you need to find-and-replace, for example, replacing \u0026quot;company\u0026quot; with \u0026quot;vendor\u0026quot;, \u0026quot;company\u0026quot; would match both singular and plural forms (\u0026quot;company and \u0026quot;companys\u0026quot;), whereas if the plural was spelled correctly, you would have to do two separate searches.\nMy answer:\nYes! Yes! Yes! It makes perfect sense to do that. And I have been doing it for years.\nDisclosure 1: English is not my native language.\nDisclosure 2: My knowledge of English grammar is considerably better than that of the average native speaker.\nDisclosure 3: When it comes to communicating with humans, I am a vehement grammar Nazi.\nAnd now that these disclosures are out of the way, let me state that English grammar has no place in code. You see, that's why it is called code and not prose. It is supposed to have some resemblance to a language understood by humans, for the purpose of readability, but other than that, what we mostly need from code is not the qualities of prose; it is other, more technical qualities, like precision, unambiguity, and terseness. That's why the C syntax of if( x != y ) y++; is much preferable to the IF X IS NOT EQUAL TO Y THEN ADD 1 TO Y END-IF. syntax of Cobol. The alleged desirability of compilers that understand natural language is a fallacy, and don't take my word for it, see what ol'Edsger has to say about it: Edsger W. Dijkstra, On the foolishness of \u0026quot;natural language programming\u0026quot;.\nAnother quality which is of importance is computability of identifiers. The fact that a property called Color can always be read via a method called getColor() and written via a method called setColor() is of paramount importance. These identifiers are computable from the name of the property, so you do not have to know them by heart. If a programmer was to choose a pair of methods called getColor() on one hand, but colorize() on the other hand, their colleagues would rightfully consider this sabotage. That's how important identifier computability is.\nAlso, programming tools can be written (and plenty of them have in fact been written, for example, *Hibernate*) which can compute these names. Without identifier name computability you would have to use additional syntax (e.g. in Hibernate, extra annotations) to specify to each tool precisely how to create every single identifier name, or precisely which ad hoc name you have given to each entity.\nSo, identifier computability is important, while at the same time English grammar is irrelevant, (since we are not doing natural language programming,) so to be able to compute the name of a collection of entities by *always* appending \u0026quot;s\u0026quot; to the name of a single instance makes perfect sense, never mind the fact that it violates most people's (mine included) English language sensitivities.\nAnd whether we like it or not, this is the trend of the future. The native language of the majority of programmers on the planet is not English anymore, and the trend is to continue *very strong* in this direction. (Also, I would not even be willing to bet money on the suggestion that English is the native language of the majority of programmers working in the USA right now.) These are people who, to a large extent, when trying to calculate the name of a collection from the name of a single instance of \u0026quot;company\u0026quot;, will simply append an \u0026quot;s\u0026quot;, and the form \u0026quot;companies\u0026quot; will not even cross their mind. To a great and ever increasing percentage of programmers in the world, knowledge of the peculiarities of the English language does not add any value to their work, it only makes it slightly harder.\n","date":"2015-07-28T13:26:37.304Z","permalink":"https://blog.michael.gr/post/2015-07-woohoo-one-more-of-my-programmers-se/","title":"Woohoo! One more of my \"Programmers SE\" answers has received a score of -5 !"},{"content":"So, with Windows 8 sporting these god-awfully ugly opaque square boxes, which are not adorned in any way whatsoever, (not even the decent in all its simplicity gradient of Windows 98,) it seems like a confession on Microsoft's behalf that the spiffy \u0026quot;Aero\u0026quot; look of Windows 7 was nothing but an unnecessary gimmick after all.\nSo, are you sticking with Windows 7 but want to disable Aero in order to enjoy considerable gains in performance, memory, power consumption, and GPU temperature? Here is how:\nOpen up \u0026quot;Services\u0026quot;. Locate the service \u0026quot;Desktop Window Manager Session Manager\u0026quot;. Disable it and stop it. Enjoy!\n","date":"2015-07-26T14:27:56.769Z","permalink":"https://blog.michael.gr/post/2015-07-how-to-completely-disable-aero-in/","title":"How to: Completely disable \"Aero\" in Windows 7"},{"content":"To enable the Administrator account in Windows:\nOpen up an elevated command prompt. (If you do not know what this is, you should not be even thinking of enabling the Administrator account.)\nType the following command:\nnet user administrator /active:yes\n","date":"2015-07-24T23:47:05.606Z","permalink":"https://blog.michael.gr/post/2015-07-how-to-enable-administrator-account-in/","title":"How to: Enable the Administrator account in Windows"},{"content":"To disable the administrative shares in Windows, (default shares like C$, ADMIN$, etc.) follow these steps:\nRun the Registry Editor and go to the following key:\nHKEY\\_LOCAL\\_MACHINE\\SYSTEM\\CurrentControlSet \\Services\\LanmanServer\\Parameters\nCreate a new DWORD, name it AutoShareWks, and leave the default value of 0.\nReboot Windows.\nThis will disable things like C$ and ADMIN$.\nI am not sure how to also delete print$ and IPC$. (net share ipc$ /delete appears to work temporarily, but the share automagically re-appears after the next reboot.)\n","date":"2015-07-24T23:45:12.153Z","permalink":"https://blog.michael.gr/post/2015-07-how-to-disable-administrative-shares-in/","title":"How to: Disable the administrative shares in Windows"},{"content":"In order to have your windows computer connect to your favorite WiFi access point immediately after booting, (without you having to first login to your computer,) follow these simple steps:\nFirst of all, make sure that this computer has previously connected to the WiFi spot of interest. (This is necessary, because a so-called wireless profile is created as a result of this manual process.)\nRun cmd.exe and issue the following command:\nnetsh wlan show profile\nIn the list of wireless profiles that are displayed, locate the one you want, and copy its name to the clipboard.\nRun regedit.exe and navigate to this key:\nHKEY\\_LOCAL\\_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\nAdd a string value to this registry key. Name it anything you like, and give it the following value:\n%comspec% /c netsh wlan connect name=\u0026quot;\u0026amp;lt;profile name\u0026amp;gt;\u0026quot;\nVoila, next time you restart, your computer will immediately connect to this WiFi spot if it is in range.\nMany thanks to user Soumya of superuser.com for posting this: http://superuser.com/a/133935/111757\n","date":"2015-07-20T18:13:07.307Z","permalink":"https://blog.michael.gr/post/2015-07-solved-wifi-connect-immediately-after/","title":"Solved: Windows: WiFi connect immediately after boot and before logon"},{"content":"Millions of people all over the planet continuously have this problem: they try to eject their USB drive so as to avoid data loss, but windows does not allow them. Which is very frustrating, because whatever reasons windows thinks it has for not allowing the removal should have lower priority than the user's explicitly stated direct wish to remove the damned thing!\nAnyway, suggestions for solutions abound on the interwebz, but they are mostly misguided and ineffective. Here are some that I found with a quick search:\n1. Put your computer to sleep. When going to sleep, Windows flushes all buffers to all devices, so while the computer is asleep, it is safe to remove the device. The problem with this approach is that I don't want to have to put my bloody computer to sleep each time I want to take a USB device out.\n2. Use SysInternals Process Explorer to search for the drive name, find which process has a handle open on the drive, and terminate it. The problem is that drive names are only one a letter and a colon, a search for \u0026quot;D:\u0026quot; may yield thousands of handles from several dozen processes, all of which are unrelated to your drive \u0026quot;D\u0026quot;. Unfortunately, Process Explorer does not offer any option to search for a whole word only. Another problem is that this approach hardly ever works. Apparently, there are ways in which devices can be locked which do not show in Process Explorer. Even if you remember to launch it as Administrator.\n3. Select \u0026quot;Quick removal\u0026quot; policy as opposed to \u0026quot;Better performance\u0026quot; policy in the drive properties. The problem with this is that it is not really a solution: we generally do specifically want the better performance, and we are willing to sacrifice quick removal for it, accepting slow removal instead. But this issue is not one of slow vs. fast removal, it is about not being allowed to remove your device at all, ever!\nHere is the solution which worked for me, after the above failed:\nMake sure that the drive, or any folders in it, are not shared on the network! (Use \u0026quot;NET SHARE\u0026quot; on the command prompt to quickly find out what you are sharing.)\nOld comments\nUnknown 2015-09-25 03:56:30 UTC\nWell, that's helpful, except I want to share the drive on the network most of the time!\nUnknown 2015-09-25 03:56:56 UTC\nAny way to unshare to eject drive??\nmichael.gr 2015-09-25 08:14:39 UTC\nYes, of course. You can either stop sharing the drive from the \u0026quot;host\u0026quot; computer (the one on which the drive physically resides,) or from each \u0026quot;client\u0026quot; computer (that makes use of the shared drive.) Right-click on the drive, and there should be sharing-related options. They differ depending on whether you are on the host or on one of the clients, on your version of windows, and also depending on what is your configuration, (whether you are in a domain, a workgroup, or a homegroup and whether you have \u0026quot;advanced sharing\u0026quot; enabled or not,) but the options are there.\n","date":"2015-07-14T00:40:18.274Z","permalink":"https://blog.michael.gr/post/2015-07-solved-cannot-eject-usb-device-this/","title":"Solved: cannot eject USB device: \"This device is currently in use-\""},{"content":"So, I am on computer A, trying to map a local drive to a shared folder of computer B, and Windows gives me this message: \u0026quot;The network folder specified is currently mapped using a different user name and password\u0026quot;.\nMicrosoft supposedly addresses this issue here: https://support.microsoft.com/en-us/kb/938120 and concludes that it is so by design.\nBullshit. They are missing the point. The error message is wrong. The network folder specified is not currently mapped using a different user name and password. Instead, what is happening, is that another folder of the same computer B is already mapped on computer A using a different user name and password.\nSo, apparently, with windows, if you connect from computer A to any share of computer B, then all subsequent attempts to connect from computer A to other shares of computer B must be done using the same credentials. Go figure.\nOld comments\nUnspecified 2018-09-21 20:43:15 UTC\nSounds like what I'm experiencing, except that I believe I have used the same username password for all connections. I'm afraid that if I change them all to make sure they're the same, none will work.\n","date":"2015-07-14T00:30:36.7Z","permalink":"https://blog.michael.gr/post/2015-07-the-network-folder-specified-is/","title":"Solved: The network folder specified is currently mapped using a different user name and password"},{"content":"\rSo, Microsoft has pushed this trojan horse to every single user of Windows on the planet. It displays a Windows 10 logo on the taskbar, and it is basically telling you that not only you will get Windows 10 as soon as it comes out, but you should right now begin acting as if you are looking forward to it. Because, you know, you are their biatch.\nSo, let's see:\nIt installs on your machine without your consent; Once installed, you cannot \u0026quot;quit\u0026quot; it or \u0026quot;exit\u0026quot; it; and You cannot find it anywhere to uninstall it. So, it fits squarely within the definition of malware.\nGWX is delivered by means of an innocent looking optional update, KB3035583. The update is designated as \u0026quot;Recommended\u0026quot;, and its description says:\nInstall this update to resolve issues in Windows. For a complete listing of the issues that are included in this update, see the associated Microsoft Knowledge Base article for more information.\n(I am not making this up, that's exactly what it says; I suppose English is not their forte at Microsoft.)\nOf course, with dozens upon dozens of updates each time you check, which give you no clue as to what they are about, and instead each one of them refers you to some web page for more information, you don't bother checking, and so you passively consent to this trojan horse being downloaded and installed on your computer. If you click on \u0026quot;More information\u0026quot;, you are taken to a web page which admits that update KB3035583 is nothing but an incredibly lame and stupid publicity stunt about the upcoming Windows 10.\nAnd so, as it turns out, that bit about resolving issues in Windows was a fucking lie.\nHow to get rid of the KB3035583 trojan horse:\nIn Windows Update select Change settings, and make sure that \u0026quot;Give me recommended updates the same way I receive important updates\u0026quot; is unchecked. This way, the fact that this is a \u0026quot;Recommended\u0026quot; update will not cause it to jump from the \u0026quot;Optional\u0026quot; updates list to the \u0026quot;Important\u0026quot; updates list.\nIn Windows Update select View update history, find KB3035583, and uninstall it.\nIn Windows Update click on \u0026quot;Check for updates\u0026quot;.\nIn Windows Update go to Optional Updates, find KB3035583, and specify that you want it to be hidden.\nAnd then in 2017 there is this: betanews.com: Microsoft will never again sneakily force Windows downloads on users\n(Though it is unclear to me whether this is about Germany only or the whole world.)\nOld comments\nAnonymous 2015-09-27 05:24:14 UTC\nYea but it still lists and keeps the check mark in it.So why hide it with the check mark in it.It won't go away.Keeps always reloading with the check mark in it.So next time you do check for updates or have it set for auto then it will sneak in.Now I have to keep an eye on this every time now.I already deleted the patch.Someone should tell them to do a delete it off the list patch.Oh since I did delete the patch it now came up as an important one to install now.\n","date":"2015-07-12T23:43:43.78Z","permalink":"https://blog.michael.gr/post/2015-07-the-gwx-get-windows-10-kb3035583-trojan/","title":"The GWX (Get Windows 10) KB3035583 trojan horse"},{"content":"So, I am firing up a remote desktop (RDP) connection, and the little box appears saying that it is trying to connect:\nAnd then the box just disappears, without an error message or any other hint as to what might have gone wrong.\nThe interwebz abound with reports of this happening, and people asking how to fix it, and various solutions being offered that range from the complex to the hopelessly complex.\nLuckily, every time I have encountered this problem, the solution for me was very simple: just wait and try again later. The remote computer is probably busy booting, or perhaps booting and installing updates.\nNow, I will tell you a little secret: when something goes wrong, software can ALWAYS give a meaningful error message. If it does not, then someone, somewhere, is being either EVIL or INCOMPETENT.\nIf it could not find the server, it would have said that the server was not found.\nIf the server was not responding, it would have said that the server is not responding.\nIn this case, the server was actively refusing the connections, and someone at Microsoft was either so evil that they decided that in this specific case the software should not say anything, or, more likely, someone at Microsoft was so incompetent that they could not simply write their world-class software to give a meaningful error message in a very frequently occurring usage scenario.\n","date":"2015-07-09T08:52:58.595Z","permalink":"https://blog.michael.gr/post/2015-07-remote-services-rdp-quits-without-word/","title":"Remote Desktop Connection quits without a word"},{"content":"Intel advertises NUC5CPYH - (Celeron N3050) as being compatible with Windows 7. It is not. All my attempts to install Windows 7 Ultimate 64 bit on it failed. The installation would start, and when it would reach the first interactive screen (language selection) the keyboard and mouse would be dead, (not even NumLock would work on the keyboard,) even though both keyboard and mouse work fine on the BIOS screens of the NUC. On other occasions I managed to overcome this problem, (I don remember how,) and the installation would proceed to 99%, only to fail in the end with some error about not being able to update my computer's boot configuration or something.\nEdit: be sure to read nucblog's comment below for a link to an article with instructions that might bring you more luck than I had.\nIn general, my overall experience of interacting with the BIOS of the NUC5CPYH was not pleasant at all. The spiffy looking BIOS screens were not welcome by me, as they were not structured in the perfect logical order that I would expect of them, and certain parts of them were clunky. Attempts to introduce an aesthetic upgrade which result in something technically more flawed than its ugly predecessor generally make a very bad impression, at least to me.\nPerhaps the most annoying thing is that the initial screen with the prompts to press F2 to enter setup, F10 to enter the boot menu, etc. appears as if it requires not one, but two key-presses. It probably requires only one, but there is an initial period of time during which if you press a key, that key is ignored, and that's extremely annoying. Hey you, Intel BIOS programmers, I have a hint for you: if you are not ready to accept key-presses, do not display a prompt for them!\nOld comments\nnucblog 2015-07-27 21:08:45 UTC\nIt's indeed a bit tricky, but can be done: http://nucblog.net/2015/07/installing-windows-7-on-the-nuc5cpyh-or-nuc5ppyh/\nmichael.gr 2015-07-27 22:59:42 UTC\nThank you, nucblog, for the excellent article. I have installed Windows 8.1 now, so it is too late for me, but hopefully it will be helpful to others.\nAnonymous 2016-04-02 04:33:30 UTC\nI'm having the same hassle with my $500 \u0026quot;NUC from Hell\u0026quot; ( a NUC5CPYH).\nI was hoping to buy a bunch of these to use in the school where I'm the IT manager. I can't install a working Windows 7 Pro 64-bit OS\n\u0026lt; khemelberg@kurnhattin.org \u0026gt;\n\u0026lt; https://kurnhattin.org/ \u0026gt;\n","date":"2015-07-08T20:23:30.058Z","permalink":"https://blog.michael.gr/post/2015-07-intel-nuc5cpyh-incompatible-with/","title":"Intel NUC5CPYH incompatible with Windows 7 Ulitmate 64 bit?"},{"content":"I once had a colleague who had a higher rank than me, and who was not only unconvinced by my rational and articulate arguments for doing a certain thing in a certain way, but he concluded the discussion by stating that --and I am not paraphrasing here, this is what he actually said-- the way he wanted it done was mentioned in a book, so unless I could find a book backing up my proposal, it was to be done his way.\nI did not argue with him at that time, (how can you argue with that?), but I would now like to quickly jot down my thoughts on why saying such a thing is incredibly stupid:\n(Useful pre-reading: About these papers)\nEngineering books are there for you to learn things from them, not to be taken as truth by revelation.\nAn engineering book makes (or fails to make) a point by analyzing it in detail until it is self-evident that the point holds true, not by proclaiming the point and putting the name of the author underneath it.\nThe value of an engineering book lies in providing justification for the claims that it makes, not in making the claims available as ink impressed onto paper.\nTherefore, engineering books are supposed to help you win arguments by equipping you with valid points to bring in defense of your positions, not by playing the books as trump cards.\nFor all I know, you may have read the whole book and misunderstood every single bit of it, or you may be completely wrong about the applicability of the points made in the book to the particular situation that you have at hand.\nSo, unless you can use what you have learned from a book to reason, any insistence on things written in a book on the grounds that they are written in a book is blatantly stupid dogma.\nOf course, I understand that other concerns may be at play. For example, that particular guy, having been tasked with being a team leader in a team of relatively senior engineers while being only in his early thirties, and having knowledge of the fact that he is good at his job but not stellar, he was probably insecure about the possibility of large scale project failure, in which case (so he thought) he wanted to be able to save face by claiming that he had followed \u0026quot;best practices\u0026quot;, and here are the books to prove it. Oh, the cowardice.\nOld comments\nAnonymous 2015-05-09 19:30:31 UTC\nIn response you should pick up the bible, quran or - even better - some random rambling published by yourself and cite something incoherent from it. If he frowns, just yell at him \u0026quot;IT'S WRITTEN IN A GOD DAMN BOOK!!\u0026quot;\nmichael.gr 2015-05-09 19:50:51 UTC\n+1 C-:=\nAnonymous 2015-07-17 08:20:38 UTC\nRespond by proving the opposition is not true. Say that unless his ideas are backed up by a book (I know you dont like the logic), then his ideas are not valid. Use his own logic against him.\n","date":"2015-04-30T10:11:59.497Z","permalink":"https://blog.michael.gr/post/2015-04-on-dogma-in-engineering/","title":"On dogma in engineering"},{"content":"I have this answer on programmers.stackexchange.com which, at the time of writing these words, has 5 upvotes and 10 downvotes, and in all likelihood it will continue collecting downvotes, while I adamantly refuse to remove it, standing 100% by my ideas. I am dumbfounded, as such a thing has never happened before.\nHere is the programmers.stackexchange.com question:\nIs it okay to have objects that cast themselves, even if it pollutes the API of their subclasses? I have a base class, Base. It has two subclasses, Sub1 and Sub2. Each subclass has some additional methods. For example, Sub1 has Sandwich makeASandwich(Ingredients... ingredients), and Sub2 has boolean contactAliens(Frequency onFrequency).\nSince these methods take different parameters and do entirely different things, they're completely incompatible, and I can't just use polymorphism to solve this problem.\nBase provides most of the functionality, and I have a large collection of Base objects. However, all Base objects are either a Sub1 or a Sub2, and sometimes I need to know which they are.\nIt seems like a bad idea to do the following:\nfor (Base base : bases) { if (base instanceof Sub1) { ((Sub1) base).makeASandwich(getRandomIngredients()); // ... etc. } else { // must be Sub2 ((Sub2) base).contactAliens(getFrequency()); // ... etc. } } So I came up with a strategy to avoid this without casting. Base now has these methods:\nboolean isSub1(); Sub1 asSub1(); Sub2 asSub2(); And of course, Sub1 implements these methods as\nboolean isSub1() { return true; } Sub1 asSub1(); { return this; } Sub2 asSub2(); { throw new IllegalStateException(); } And Sub2 implements them in the opposite way.\nUnfortunately, now Sub1 and Sub2 have these methods in their own API. So I can do this, for example, on Sub1.\n/** no need to use this if object is known to be Sub1 */ @Deprecated boolean isSub1() { return true; } /** no need to use this if object is known to be Sub1 */ @Deprecated Sub1 asSub1(); { return this; } /** no need to use this if object is known to be Sub1 */ @Deprecated Sub2 asSub2(); { throw new IllegalStateException(); } This way, if the object is known to be only a Base, these methods are un-deprecated, and can be used to \u0026quot;cast\u0026quot; itself to a different type so I can invoke the subclass's methods on it. This seems elegant to me in a way, but on the other hand, I'm kind of abusing Deprecated annotations as a way to \u0026quot;remove\u0026quot; methods from a class.\nSince a Sub1 instance really is a Base, it does make sense to use inheritance rather than encapsulation. Is what I'm doing good? Is there a better way to solve this problem?\nTags: java, inheritance, type-casting\nasked by codebreaker\nThe highest scoring answer so far has collected 21 upvotes and 3 downvotes, and it dismisses the OPs question stating that his design is wrong. Basically, it draws a straw man of the OP's description and proceeds to attack it. The strawman is terrible, so he is mighty successful at destroying it. Needless to say, one of the 3 downvotes is mine.\nThen, here is my answer, which turned out to be highly unpopular:\nWhat you are doing is perfectly legitimate. Do not pay attention to the naysayers who merely reiterate dogma because they read it in some books. Dogma has no place in engineering.\nI have employed the same mechanism a couple of times, and I can say with confidence that the java runtime could have also done the same thing in at least one place that I can think of, thus improving performance, usability, and readability of code that uses it.\nTake for example java.lang.reflect.Member, which is the base of java.lang.reflect.Field and java.lang.reflect.Method. (The actual hierarchy is a bit more complicated than that, but that's irrelevant.) Fields and methods are vastly different animals: one has a value that you can get or set, while the other has no such thing, but it can be invoked with a number of parameters and it may return a value. So, fields and methods are both members, but the things you can do with them are about as different from each other as making sandwiches vs. contacting aliens.\nNow, when writing code that uses reflection we very often have a Member in our hands, and we know that it is either a Method or a Field, (or, rarely, something else,) and yet we have to do all the tedious instanceof to figure out precisely what it is and then we have to cast it to get a proper reference to it. (And this is not only tedious, but it also does not perform very well.) The Method class could have very easily implemented the pattern that you are describing, thus making the life of thousands of programmers easier.\nOf course, this technique is only viable in small, well-defined hierarchies of closely coupled classes that you have (and will always have) source-level control of: you don't want to be doing such a thing if your class hierarchy is liable to be extended by people who are not at liberty to refactor the base class.\nHere is how what I have done differs from what you have done:\nThe base class provides a default implementation for the entire asDerivedClass() family of methods, having each one of them return null. Each derived class only overrides one of the asDerivedClass() methods, returning this instead of null. It does not override any of the rest, nor does it want to to know anything about them. So, no IllegalStateExceptions are thrown. The base class also provides final implementations for the entire isDerivedClass() family of methods, coded as follows: return asDerivedClass() != null; This way, the number of methods that need to be overridden by derived classes is minimized. I have not been using @Deprecated in this mechanism because I did not think of it. Now that you gave me the idea, I will put it to use, thanks! C# has a related mechanism built-in via the use of the as keyword. In C# you can say DerivedClass derivedInstance = baseInstance as DerivedClass and you will get a reference to a DerivedClass if your baseInstance was of that class, or null if it was not. This (theoretically) performs better than is followed by cast, (is is the admittedly better named C# keyword for instanceof,) but the custom mechanism that we have been hand-crafting performs even better: the pair of instanceof-and-cast operations of Java, as well as the as operator of C# do not perform as fast as the single virtual method call of our custom approach.\nI hereby put forth the proposition that this technique should be declared to be a pattern and that a nice name should be found for it.\nGee, thanks for the downvotes! A summary of the controversy, to save you from the trouble of reading the comments:\nPeople's objection appears to be that the original design was wrong, meaning that you should never have vastly different classes deriving from a common base class, or that even if you do, the code which uses such a hierarchy should never be in the position of having a base reference and needing to figure out the derived class. Therefore, they say, the self-casting mechanism proposed by this question and by my answer, which improves the use of the original design, should never have been necessary in the first place. (They don't really say anything about the self-casting mechanism itself, they only complain about the nature of designs that the mechanism is meant to be applied to.)\nHowever, in the example above I have already shown that the creators of the java runtime did in fact choose precisely such a design for the java.lang.reflect.Member, Field, Method hierarchy, and in the comments below I also show that the creators of the C# runtime independently arrived at an equivalent design for the System.Reflection.MemberInfo, FieldInfo, MethodInfo hierarchy. So, these are two different real world scenarios which are sitting right under everyone's nose and which have demonstrably workable solutions using precisely such designs.\nThat's what all the following comments boil down to. The self-casting mechanism is hardly mentioned.\nSo, I have copied all this here for posterity, because I really do stand by my convictions and I will continue to do so no matter how many people disagree with me, for as long as I don't see any valid arguments against my convictions.\nAcceptance of some strategy and dismissal of some other strategy without objective reasons for doing so is just plain good ol' dogma.\nOld comments\nAnonymous 2015-07-05 13:06:52 UTC\nSee your note on \u0026quot;Clean Code\u0026quot;, Page 291 - “G7: Base Classes Depending on Their Derivatives” here: My Notes On 'Clean Code' By Robert C. Martin. Seems you changed your tune by admitting there are exceptions. How... unpopular!\nmichael.gr 2015-07-05 13:11:58 UTC\nYes, I changed my mind. There are some exceptions. I do have the right to change my mind, right? Have you never changed your mind about anything? How do you like it when people make snide remarks about changes of tune? Anyhow, thanks for pointing that out to me, I will now go fix it.\nmichael.gr 2015-07-05 13:20:14 UTC\nSo, I fixed it. The comment that I removed about Page 291 \u0026quot;G7\u0026quot; was: \u0026quot;A class should not know anything about its derivatives, period. I do not believe that there should ever be exceptions to this rule.\u0026quot; Well, it turns out, there are some cases where this rule can have some useful exceptions.\n","date":"2015-04-30T08:47:49.86Z","permalink":"https://blog.michael.gr/post/2015-04-wow-2-upvotes-6-downvotes-and-counting/","title":"Wow, 5 upvotes, 10 downvotes and counting"},{"content":"This article started as a stackoverflow answer, and then I copied it over here to expand on it.\nFor a discussion of the same issue but in java-oriented terms, see this Stack Overflow answer of mine: Is overriding Object.finalize() really bad? http://programmers.stackexchange.com/a/288724/41811\nThere is this practice which is unfortunately very prevalent in the C# world, of implementing object disposal using the ugly, clunky, inelegant, ill-conceived, and error prone idiom known as IDisposable-disposing. MSDN describes it in length, and lots of people swear by it, follow it religiously, write walls of text discussing precisely how it should be done and precisely how it works, and precisely how they arrived at this particular way of doing it, etc.\n(Please note that what I am calling ugly here is not the object disposal pattern itself; what I am calling ugly is the particular idiom of implementing an extra Dispose method with a bool disposing parameter.)\nThis idiom was invented under the assumption that the invocation of IDisposable.Dispose() is something optional, or in any case something which might be OK to forget, in combination with the fact that it is impossible to guarantee that our objects' destructor will always be invoked by the garbage collector to clean up resources. So, people tend to make their best effort to invoke their IDisposable.Dispose() methods, and in case they forget, they also give it one more try from within the destructor. You know, just in case.\n(Useful pre-reading: About these papers)\nBut then your IDisposable.Dispose() might have both managed and unmanaged objects to clean up, but the managed ones cannot be cleaned up when IDisposable.Dispose() is invoked from within the destructor, because they have already been taken care of by the garbage collector at that point in time, so there is this need for a separate Dispose() method that accepts a bool disposing flag to know if both managed and unmanaged objects should be cleaned up, or only unmanaged ones. In addition to that, you have to guard against disposing your unmanaged resources twice, so you have to either have a disposed member variable to keep track of whether IDisposable.Dispose() has been invoked, or, better yet, invoke GC.SuppressFinalize() from within IDisposable.Dispose() so that finalization can be skipped if the object is known to have been properly disposed.\nExcuse me, but this is just insane.\nI go by Einstein's axiom, which says that things should be as simple as possible, but not simpler. Clearly, we cannot omit the cleaning up of resources, so the simplest possible solution has to include at least that. The next simplest solution involves always disposing everything at the precise point in time that it is supposed to be disposed, without complicating things by bringing into the picture the destructor as an alternative fall back. So, that has to be it, according to my line of thinking. I call it Mandatory Disposal.\nNow, strictly speaking, it is of course impossible to guarantee that every single programmer out there will always remember to make sure that IDisposable.Dispose() will be invoked, but what can be done, is that the destructor can be used to detect such omissions. The crux of the matter is what we do once we have detected such an omission: we do not attempt to correct it; instead, we generate an error message.\nIt is very simple, really. (Duh!) All the destructor has to do is generate a log entry if it detects that the disposed flag of the IDisposable object was never set to true. So, the use of the destructor is not an integral part of our disposal strategy, but it is our quality assurance mechanism. And since this is a debug-mode only test, we can place our entire destructor inside an #if DEBUG block, so we never incur any destruction penalty in a production environment. The IDisposable-disposing idiom prescribes that GC.SuppressFinalize() should be invoked precisely in order to lessen the overhead of finalization, but with Mandatory Disposal it is possible to completely avoid all finalization in our production build.\nWhat it boils down to is the eternal hard error doctrine vs. soft error doctrine argument:\nThe IDisposable-disposing idiom of object disposal is a soft error doctrine approach, allowing the programmer to forget to invoke IDisposable.Dispose() and attempting to somehow (magically) make things right in the end. My Mandatory Disposal idiom is a hard error doctrine approach, requiring that the programmer must always make sure that IDisposable.Dispose() gets invoked, under penalty of error.\nThe error is mitigated in this special case, from the standard assertion failure exception that the hard error doctrine usually calls for, to a mere error-level message in the debug log, since the error is detected during finalization, and by that time it is too late for fail-fast measures anyway.\nThe Mandatory Disposal mechanism works best if the DEBUG build of our application performs a full garbage disposal before quitting, so as to guarantee that all destructors will be invoked, and thus detect any IDisposable objects that we forgot to dispose. Also, the Mandatory Disposal mechanism works best if we avoid using static references to objects, because such references prevent objects from being garbage-collected.\nOnce you start using Mandatory Disposal, one issue you inevitably encounter is that you discover objects which you forgot to dispose of, but you do not know how to fix them because you have no idea where they were allocated. There is a nice way of solving this problem without considerable complications, but this will be the subject of another post.\nMore on this issue, but in java-oriented terms, in this Stack Overflow answer of mine: Is overriding Object.finalize() really bad?http://programmers.stackexchange.com/a/288724/41811\nA whole new paper on this issue is here: Object Lifetime Awareness\n","date":"2015-03-20T09:54:20.64Z","permalink":"https://blog.michael.gr/post/2015-03-on-dispose-bool-disposing-abomination/","title":"Mandatory disposal vs. the 'Dispose-disposing' abomination"},{"content":"\rThese two side-by-side screen captures are from CrystalDiskMark measuring the performance of my brand new Samsung 850 PRO 256GB Solid State Drive (C:) versus the performance of my old Seagate Barracuda 1TB 7200 RPM 32MB Cache Hard Disk Drive (D:). Higher numbers are better.\nBoth devices are capable of SATA III, but my machine only has SATA II. Under SATA III, I would expect the HDD to perform somewhat faster, but the SSD to perform a lot faster. I will update this post when I upgrade to SATA III.\nSSD, where had you been all my life?\n","date":"2015-03-13T18:09:30.934Z","permalink":"https://blog.michael.gr/post/2015-03-solid-state-for-win/","title":"Solid State For The Win!"},{"content":"\rAs a developer, I have configured MySQL to launch automatically at startup, because I will probably be using it, if this is a workday, and if my computer is starting up during work hours, and if my work these days involves databases. That's a lot of ifs. Quite often, I launch it but do not use it, so, it would be nice if it is as unobtrusive as possible. Unfortunately, it is obtrusive in at least one way: it occupies a large amount of memory, which is unreasonably large for a piece of software that is just sitting there doing nothing.\nIn this post I show how to configure it to occupy as little memory as possible.\nThe following image shows the mind-boggling amount of memory occupied by MySQL 5.6 server on Windows 7 64-bit.\n(This is despite the fact that during installation I explicitly specified that this MySQL server is going to be used for development, not for production.)\nA quick search on the web shows that this preposterous amount of memory can be reduced to something less preposterous by editing my.ini (usually found in some place like ProgramData\\MySQL\\MySQL Server 5.6) and replacing the following line:\ntable_definition_cache=1400\nwith this line:\ntable_definition_cache=400\nUnfortunately, even though the savings are huge, the memory footprint of mysql is still nothing short of gargantuan:\nSo, I tried to trim it down as much as possible by doing some more reading up on the subject and tweaking with more settings of my.ini, and this is what I finally managed to get it down to:\nSo, the \u0026quot;private bytes\u0026quot; went from 630MB down to 20MB and the \u0026quot;working set\u0026quot; went from 450MB down to 21MB. Not bad, eh?\nThe server is functional, too, meaning that I ran the tests of a medium-sized project, which do heavily hit the database, and they all completed as easily as they used to with the bloated out-of-the-box server.\nThis is the my.ini file that achieves these savings:\n# MySQL Server Instance Configuration File [client] no-beep # pipe socket=0.0 port=3306 [mysql] default-character-set=utf8 # server_type=3 [mysqld] # skip-networking enable-named-pipe # shared-memory # shared-memory-base-name=MYSQL socket=MYSQL port=3306 # basedir=\u0026#34;C:/Program Files/MySQL/MySQL Server 5.6/\u0026#34; datadir=C:/ProgramData/MySQL/MySQL Server 5.6/Data character-set-server=utf8 default-storage-engine=INNODB sql-mode=\u0026#34;STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION\u0026#34; # plugin-load=authentication_windows.dll log-output=FILE general-log=0 general_log_file=\u0026#34;PEGASUS.log\u0026#34; slow-query-log=1 slow_query_log_file=\u0026#34;PEGASUS-slow.log\u0026#34; long_query_time=10 # log-bin log-error=\u0026#34;PEGASUS.err\u0026#34; server-id=1 # *MB* original: 151 max_connections=20 query_cache_size=0 # *MB* original: table_open_cache=2000 table_open_cache=100 tmp_table_size=44M # *MB* original: 9 thread_cache_size=0 myisam_max_sort_file_size=100G # *MB* original: 80M myisam_sort_buffer_size=1M # *MB* original: 8M key_buffer_size=8 # *MB* original: read_buffer_size=64K # *MB* minimum appears to be 8K. read_buffer_size=8K # *MB* original: read_rnd_buffer_size=256K read_rnd_buffer_size=8K # *MB* original: sort_buffer_size=256K # *MB* minimum appears to be 32K sort_buffer_size=32K # innodb_data_home_dir=0.0 # skip-innodb # *MB*: If this setting is enabled, the server fails to start with message \u0026#34;this option is deprecated and will be removed in a future release.\u0026#34; # *MB* original: innodb_additional_mem_pool_size=6M innodb_additional_mem_pool_size=1M # *MB* original: innodb_flush_log_at_trx_commit=1 innodb_flush_log_at_trx_commit=0 # *MB* original: innodb_log_buffer_size=4M # *MB* minimum appears to be 256K innodb_log_buffer_size=256K # *MB* original: innodb_buffer_pool_size=255M # *MB* minimum appears to be 5M innodb_buffer_pool_size=5M innodb_log_file_size=48M innodb_thread_concurrency=9 innodb_autoextend_increment=64 # *MB* original: innodb_buffer_pool_instances=8 innodb_buffer_pool_instances=2 # *MB* original: innodb_concurrency_tickets=5000 innodb_concurrency_tickets=10 innodb_old_blocks_time=1000 # The minimum value is 10. # *MB* original: innodb_open_files=300 innodb_open_files=10 innodb_stats_on_metadata=0 innodb_file_per_table=1 innodb_checksum_algorithm=0 # *MB* original: back_log=80 back_log=5 # *MB* original: flush_time=0 flush_time=10 # *MB* original: join_buffer_size=256K join_buffer_size=16K # *MB* original: max_allowed_packet=4M max_allowed_packet=1M max_connect_errors=100 # *MB* original: open_files_limit=4161 open_files_limit=100 # *MB* original: query_cache_type=0 query_cache_type=2 # The minimum and default values are both 400. # *MB* original: table_definition_cache=1400 table_definition_cache=400 # The value should be a multiple of 256. binlog_row_event_max_size=8K sync_master_info=10000 sync_relay_log=10000 sync_relay_log_info=10000 # additional hints from http://www.tocker.ca/2014/03/10/configuring-mysql-to-use-minimal-memory.html host_cache_size=0 thread_stack=128K max_heap_table_size=16K bulk_insert_buffer_size=0 net_buffer_length=1K innodb_sort_buffer_size=64K binlog_cache_size=4K binlog_stmt_cache_size=4K performance_schema=0 ","date":"2015-03-03T22:10:45.668Z","permalink":"https://blog.michael.gr/post/2015-03-minimal-mysql-memory-footprint/","title":"Minimal MySQL Memory Footprint"},{"content":"\rOld comments\nNat 2015-02-16 14:12:53 UTC\nHey Michael, do you have a contact email? I'd like to run a few things by you but I can't seem to find any contact details.\nmichael.gr 2015-02-16 14:39:37 UTC\nOkay, I guess I need to fix that. I suppose I should post a picture containing my email address on the blog. In the mean time, you can contact me by sending an email to any address at this domain.\nkoyan 2015-02-11 10:23:49 UTC\nNasty mate, nasty :-P\n","date":"2015-02-11T09:50:58.004Z","permalink":"https://blog.michael.gr/post/2015-02-a-stackoverflow-question-that-received/","title":"A stackoverflow question that received no love"},{"content":"This post does not contain any spoilers, unless you would consider as a spoiler my opinion on how the quality if the movie varies as the movie progresses. (Or the image below.)\nPicture source: cgmeetup.com So, I watched The Dawn of the Planet of the Apes yesterday, and what can I say, wow, it blew my mind. I started watching it having very low expectations, and I was very pleasantly surprised for about one hour and fifty minutes of its total two hour and ten minute duration, which includes the end titles. Then, starting with the \u0026quot;I am saving the human race\u0026quot; incident, it transformed into the crap that I had expected from the beginning, perhaps even worse, but that does not annul the fact that the first one hour and fifty minutes were one of the most pleasant movie watching experiences I have had in quite some time.\nThe movie has it all: an epic adventure, adequately developed characters, complex interpersonal dynamics among the main characters, awesome animations thanks to extensive use of motion capture technology, extremely well done \u0026amp; never-seen-before expressiveness in the faces of the animated characters due to the use of facial expression tracking technology, the right amount of suspense, the right amount of sentiment, and the list goes on.\nPicture source:hollywoodreporter.com Why am I writing this post? Well, I wanted to share one little thing which really caught my attention when I realized it was going on (well into the movie) and which really spoke to my inner geek. It was the way that the apes spoke. It caught my attention because it had to do with scientific accuracy: you rarely see Hollywood bothering to do one of the little things right, so this was quite unexpected and quite welcome.\nThroughout the movie, when you hear apes talk, you will notice that they say one, maybe two words, then they pause, then they say another one or two words, pause again, and so on. In the beginning you might think that the director is trying to show that the apes are a bit too dumb to properly articulate speech, but as the movie progresses you realize that these apes are clearly capable of quite complex mental tasks, so it does not seem to add up. Inevitably, the realization dawns upon you: Apes in this movie speak like that because that's precisely how apes would actually speak regardless of how dumb or smart they were. It is the movie trying to be realistic. (And being awesomely successful at it, at least with respect to this little detail.)\nPicture source: graffitiwithpunctuation.net You see, the fact of the matter is not only that real apes today, in the labs, do in fact speak their limited vocabularies exactly like that, but most importantly, that even if they were to become much smarter, (as the apes in The Planet of the Apes saga supposedly are,) they would still speak exactly like that. They are incapable of speaking the way we do, and that's due to a simple biological reason which has nothing to do with intellect: apes cannot control their breath as we do. It is very difficult to be eloquent when half the words come out while exhaling, while the other half come out while inhaling. This is all very well documented fact, ask any primatologist about it, and I just happen to know it due to my side-interest in evolutionary biology and biological anthropology.\nSo, what do the smart apes in The Dawn Of The Planet Of The Apes do? They only speak while exhaling. This only gives them enough time to utter a word or two; then they pause while inhaling; and then they say another couple of words; and so on. Of course, this situation compels them to be rather laconic in anything they say, and to resort to speaking in sign language among themselves. Fantastically well rendered.\n","date":"2014-12-29T23:32:57.341Z","permalink":"https://blog.michael.gr/post/2014-12-movie-dawn-of-planet-of-apes/","title":"Movie: Dawn of the Planet of the Apes"},{"content":"Exceptions are the best thing since sliced bread. If you use them properly, you can write code of much higher quality than without them. I think of the old days before exceptions, and I wonder how we managed to get anything done back then. There is, however, one little very important thing missing from implementations of exceptions in all languages that I know of, and it has to do with transactions.\nAt a high level, exception handling looks structurally similar to transactional processing. In both cases we have a block of guarded code, during the execution of which we acknowledge the possibility that things may go wrong, in which case we are given the opportunity to leave things exactly as we found them. So, given this similarity, it is no wonder that one can nicely facilitate the other, as this sample code shows:\n(Useful pre-reading: About these papers)\nTransaction transaction = transactionable.newTransaction(); try { ... } finally { transaction.close(); } Note that the transaction illustrated above is a simple type of open-close transaction, not a database-style transaction. Database-style transactions are a bit more complicated, because they cannot be just closed, they need to be either committed, or rolled back. The collaboration between exception handling and database-style transactional processing is just slightly more involved:\nTransaction transaction = transactionable.newTransaction(); try { ... transaction.commit(); } catch( Exception e ) { transaction.rollback(); throw e; } That's all very nice, but it is a bit too verbose. So, the good folks who design languages have come up with a nifty feature: the try-with-resources construct of Java, or the using-IDisposable construct of C#, which allows you to have a method automatically invoked at the end of a block regardless of whether an exception was thrown or not, and greatly helps in keeping things neat by keeping cleanup code hidden.\nI like this construct a lot, and I use it all the time. Actually, I practically use it in every single situation where I need to remember (or otherwise not fail) to do something at the end of something. The mere presence of such a need tells me that I have a transaction pattern in my hands, and I like to always code it as such.\nFor example, I have this special collection which issues a notification whenever it is changed, and I am about to make multiple changes to it, but I only want notifications to be issued once I am done with all the changes, not while I am doing the changes, so I have to somehow suppress notifications for a while, and then re-enable them. Someone else in my shoes would simply add a notification suppression flag to the notifying collection, and then code a statement at the beginning of the block which sets that flag, and another statement at the end of the block which clears that flag. But not me. No, I have to have an AutoCloseable (in Java, or IDisposable in C#) object called NotificationSuppression instantiated at the beginning of the block, which sets the notification suppression flag from within its constructor, and gets automatically invoked at the end of the try-with-resources block (in Java, or using-IDisposable in C#) to clear the flag. The beauty of it is that if an exception is ever thrown in the mean time, the notifying collection will not be left in a permanent notification suppression state.\nI suppose that the days when I used to care about clock cycles are long gone, and what I care most about now is reliability and elegance.\nSo, one might be tempted to guess that database-style transactions could also benefit from this construct, by having the transaction object automagically invoked at the end of the try-with-resources (in Java, or using-IDisposable in C#) block to provide proper termination for the transaction, saving us from having to write all that committing, exception-catching, rollbacking boilerplate code each time. Right? Well, unfortunately, wrong. You cannot use try-with-resources (in Java, or using-IDisposable in C#) with database-style transactions, at least not in any elegant way.\nThe reason why you cannot do this is because your close() (in Java, Dispose() in C#) method has no way of knowing whether it is being invoked as a result of normal termination of the controlled block, or whether an exception has occurred, and so it has no way of deciding whether it should commit or rollback the transaction.\nI suppose there are various different ways to fix this in future versions of Java and C#; one way that I can think of would be to introduce a new interface, say AutoCloseable2 or IDisposable2, which accepts a Throwable (in Java. or Exception in C#) as a parameter, and operates in collusion with the compiler, as the case already is with the existing AutoCloseable and IDisposable. If this throwable parameter is null, it would mean that the method is being invoked as a result of normal termination, while if it is not null, then it would mean that the method is being invoked as a result of abnormal termination, and the throwable is being passed just in case the method has any use for it.\nTrue, in the majority of try-with-resources (in Java, or using-IDisposable in C#) blocks you do not need this special functionality, and true, such a feature would probably be misused by some. But that's all irrelevant; the point is that there is this very important and frequently used type of processing, namely, database-style transactional processing, which cannot be done the way things are now, at least not in an elegant way. I think that's worth fixing.\n","date":"2014-11-26T20:14:08.125Z","permalink":"https://blog.michael.gr/post/2014-11-the-transaction-pattern-and-feature/","title":"The transaction pattern and the feature badly missing from exceptions"},{"content":"Here is a check list for creating useful bug reports:\nBefore reporting, make sure that you are using the latest version of the software.\nBefore reporting something as a bug, make sure it is in fact a bug.\nBefore reporting, play around with the bug to identify it with at least a bit of accuracy.\nReport through the proper channel, usually an issue tracking system.\nMake sure that the bug has not already been reported.\nInvest some time to find a proper title for the bug.\nState the name of the product in which you found the bug.\nState the version of the product in which you found the bug.\nDescribe your environment (Operating System name and version, etc)\nList the steps to reproduce the problem.\nState the \u0026quot;observed result\u0026quot; of following the steps.\nState the \u0026quot;expected result\u0026quot; of following the steps.\nState the reproducibility of the bug. (Or your best estimate of it.)\nStick to the facts.\nAddress only one bug per bug report.\nInclude any useful attachments.\nHere is a slightly more detailed discussion of each one of the above items:\n1. Before reporting, make sure that you are using the latest version of the software. If not, then your bug report has a high probability of being irrelevant. Nobody wants to waste time with irrelevant stuff.\n2. Before reporting something as a bug, make sure that it is in fact a bug. Just because the software is not doing what you expected it to do, it does not mean that the software is not working correctly. It is quite possible that the software is working precisely as intended, so what you think of as a bug is not really a bug, it is a change request. The main difference between bug reports and change requests is that in the case of bug reports, the title of the issue describes something which is unwanted, e.g. \u0026quot;my newly created item does not appear at the top of the list\u0026quot; while in the case of change requests, the title of the issue describes something which is wanted, e.g. \u0026quot;sort item list by creation date instead of name\u0026quot;. If you mistakenly report a change request as a bug you are going to be sending the programmers on a wild goose change looking for recent changes in the code that might have caused the observed behavior, while there are none to be found.\n3. Before reporting, play around with the bug to identify it with at least a bit of accuracy. Programmers tend to not appreciate it very much when users report just the first superficial manifestation of anything that appears wrong. A guy filed a bug report once about an application failing to open a document which he had just created, without first bothering to check whether the document file actually existed on his disk. As it turned out after a lot of wasted time and frustration, it was perfectly normal for the application to not be able to open the file, because the file did not exist, and the bug report should instead have been about the application failing to create the file in the first place. Another guy filed a report which included a complicated sequence of steps for arriving at a particular rarely used screen which contained a text edit box whose cursor was not blinking. He did so without noticing that no cursor was blinking on any text edit box in the entire application, not even in the search field of the main screen. So, the number of steps to reproduce could have been 2 instead of 12. Do not be the guy who sends the programmers on wild goose chases: try to investigate the bug before reporting it, so as to file a meaningful report.\n4. Report through the proper channel, usually an issue tracking system. If you have been explicitly informed that proper channels for reporting bugs in your organization include sending direct emails to the programmers, sticking post-it notes on their office doors, or striking up conversations with them by the water cooler, then fine, so be it. But with most software development teams, the one and only acceptable channel for reporting bugs is a specialized issue tracking system. Yes, you will need to first learn how to use it, there is no way around that.\n5. Make sure that the bug has not already been reported. Reporting the same bug multiple times wastes the programmers time and makes them less likely to pay attention to future bug reports submitted by the same person. Decent issue tracking systems tend to scan the text of your bug report and try to find similar bug reports for you, in case one of them is about the same issue. Of course, in order for this feature to work, you need to be verbose while describing the bug, so that the issue tracking system has enough keywords to search for. If your bug has already been reported, amend the original report with whatever new information you might have.\n6. Invest some time to find a proper title for the bug. The title should contain as much information as possible in an as concise form as possible, so as to quickly give an accurate idea of what it is about without delving into unnecessary details. Also, the title should uniquely identify the bug among all existing similar bugs and all future similar bugs. Ideally, if a bug pops up in the future whose title would be exactly the same as the title of the bug you are currently filing, then that future bug should be nothing but a re-appearance of the bug you are reporting. So, for example, \u0026quot;problem with the profile screen\u0026quot; is a very bad title for a bug, because there is a huge number of entirely unrelated things that can go wrong with a profile screen. A better bug title would be \u0026quot;profile picture remains unchanged when a new one is uploaded\u0026quot;.\n7. State the name of the product in which you found the bug. The development team may have produced many different applications, so they need to know precisely which one you are talking about, but even if they have produced only one, chances are that they would still like to see that you are capable of calling it with its proper name before they accept a bug report from you. So, be sure to state the name of the product in which you found the bug, and be sure to use the precise product name as reported by the product itself, (e.g. \u0026quot;Acme Point of Sales For Windows\u0026quot;) instead of the name that you use in every day language, (e.g. \u0026quot;the store app\u0026quot;) or the name that you think is the name of the product even though you have never really looked it up. (e.g. \u0026quot;acme\u0026quot;. Which, in this case, happens to be the name of the company, not the name of the product. Not only programmers, but even technical support personnel hate it when users do that.)\n8. State the version of the product in which you found the bug. It goes without saying that without the version number, the report is useless. Also keep in mind that the bug might persist for several versions or internal releases, so every time you come across the same bug in a later version, amend the bug report to indicate that it is still there.\n9. Describe your environment (Operating System name and version, etc) The programmers will probably need to know at least a little bit, possibly even a lot, about the software and hardware environment that you were using at the time that the bug manifested itself. This will include at the very least your operating system name and version. If the programmers need more than that, then generally, they will have (or they ought to have) produced some guidelines explaining precisely what additional information is required for bug reports on their product. Be thorough and meticulous: give the precise names and version numbers as reported by the systems themselves (e.g. \u0026quot;Windows 7 Ultimate 64 bit, Version 6.1, Build 7601, Service Pack 1\u0026quot;) instead of the names that you would use in every day language (e.g. \u0026quot;64 bit Seven.\u0026quot;)\n10. List the steps to reproduce the problem. Even if the programmers opt to believe you when you say that you \u0026quot;saw a bug\u0026quot;, there is not much they can do about it unless you tell them precisely what you did that caused the bug to manifest itself. Begin with the very first step, no matter how obvious, (e.g. \u0026quot;launch Acme Point of Sales For Windows\u0026quot;) and continue listing steps until the problem occurs. Be sure to list not only what you perceive to be the intended results of your actions (e.g. \u0026quot;save the file\u0026quot;) but also your actual actions (e.g. \u0026quot;save the file by hitting Ctrl+S\u0026quot;) not only because these two may, unbeknownst to you, differ, (Ctrl+S might do something other than saving the file,) but also because different ways of achieving the same thing might have different side-effects, so the programmers need to know about those, too. (Ctrl+S might just save the file, while other ways to save the file might involve opening up a file-save dialog, which in turn involves loading resources needed to display the dialog, instantiating file preview plugins, transitioning the application GUI in and out of a modal state, etc.) Be very clear and precise, and make sure that what you wrote can be understood unambiguously and effortlessly by someone who is not you. If you have found multiple significantly different scenarios to reproduce the same bug, list them all. If you have found some sequence of steps which gets the job done without triggering the bug, be sure to also include it.\n11. State the \u0026quot;observed result\u0026quot; of following the steps. Explain precisely what it is that you consider to be the bug, no matter how obvious it may seem to you. Simply saying \u0026quot;open the properties screen and then boom- there it is!\u0026quot; will not do. The programmers may have stared at the properties screen many times in the past without noticing that which you have noticed, or they may be aware of multiple different issues within the properties screen, which you know nothing about. They may even arrive at a perfectly fine properties screen after following your steps, so they need to have some quick and definitive way of knowing whether they are seeing the bug that you have seen. No programmer likes having to guess something which the bug reporter could have easily stated. Also, a great percentage of bug reports are false reports about things that are the way they are by design and not by accident, so the programmers need to be able to tell right up front whether you are reporting an actual bug, and not some feature which you do not quite like, or not quite understand.\n12. State the \u0026quot;expected result\u0026quot; of following the steps. Explain precisely what you expected to happen in place of that which actually happened and which you consider to be a bug. In many cases it is just a negated form of the statement made in the previous item, (e.g. \u0026quot;Observed result: the application crashes. Expected result: the application should not have crashed.\u0026quot;) so it is not very useful, but in many other cases it can be quite a bit more descriptive than that, and that's where it is really useful. (e.g. \u0026quot;Actual result: nothing happens. Expected result: the text should have started dancing on the screen.\u0026quot;) Diligently filling this in, no matter how obvious it might seem, shows that you are not just any random person out there, but an insider of the art of software testing and bug reporting. If there happen to be any specifications documents describing what the software should do, this is the right place to include a reference to the section of the specification document which states what should have happened.\n13. State the reproducibility of the bug. (Or your best estimate of it.) On most systems, most bugs manifest themselves each time someone follows the steps to reproduce them. Such bugs are said to have a reproducibility of 100%. Unfortunately, some bugs only manifest themselves part of the time, and some particularly nasty ones manifest themselves very rarely. Be sure to state the reproducibility of the bug that you are reporting. If you do not, then the person reviewing your bug report will assume 100% reproducibility, so if the bug does not manifest itself on the first try, they will close your bug report with a \u0026quot;works for me\u0026quot; comment. Nobody will waste time repeating the same steps over and over just in case this is a less-than-100%-reproducible bug which was simply not reported as such. Of course, in order to estimate the reproducibility of a bug, you will need to trigger it a few times. Triggering it only once is generally unacceptable, and besides, if you trigger it only once then clearly you have not played around with it enough in order to identify it with accuracy, as a previous item in this checklist requires.\n14. Stick to the facts. The programmers are primarily interested in the facts, not in your interpretations of the facts, nor in your assumptions about what the facts are. If you would like to include your thoughts besides the facts, that is fine, but be sure to clearly indicate where the facts end and your own opinions begin, and be sure that in your eagerness to start writing your opinions you have not left out any of the facts.\n15. Address only one bug per bug report. A different bug should receive a separate bug report. When in doubt, filing two different bug reports is the way to go, because a bug report can easily be marked as a duplicate of another, whereas multiple bugs rolled into one report are not as easy to split into separate bug reports.\n16. Include any useful attachments. A screenshot showing the problem can be very useful to the people who will be troubleshooting it. If the bug only happens when you use the application with some specific external file, be sure to attach a copy of that file to your bug report. For example, if \u0026quot;the profile picture remains unchanged after uploading a new one\u0026quot;, then please do attach the picture which you tried uploading. Also, if the application you are using has undergone any configuration from your part, then you need to attach your configuration files with your bug report, or else your bug report might not make sense to the programmers. For example, if you have assigned a key to perform a certain action, then stating in your bug report that you pressed that key is meaningless, because the programmers do not know what action you have assigned to that key. Your configuration file will tell them. Also, many applications log everything they do in special \u0026quot;debug logs\u0026quot;. If you have a debug log, be sure to attach it to your bug report. Note that attachments are useful only in addition to the previous items in this checklist, and never as a substitute to any of them, so the fact that you included an attachment does not in any way exempt you from having to fill in all of the above. \u0026quot;Actual result: see attachment\u0026quot; is unacceptable. Please describe the actual result with words, (hint: searchable keywords,) and follow your description with \u0026quot;also see attachment.\u0026quot;\n--michael.gr\n","date":"2014-11-16T13:54:13.315Z","permalink":"https://blog.michael.gr/post/2014-11-bug-reporting-checklist/","title":"Bug reporting: a checklist"},{"content":"I just submitted a feature request for IntelliJ IDEA.\nIt can be found here: https://youtrack.jetbrains.com/issue/IDEA-132626\nFeature request: editor actions for moving the caret left \u0026amp; right with Column Selection. It is a fundamental axiom of user interface design that modes kill usability. Having to enter a special mode in order to accomplish something and then having to remember to exit that mode in order to accomplish anything else is bad, bad, bad user interface design, at least when there is even a slight chance that the same thing could be achieved without a special mode. (Think of VI for example: it is the lamest editor ever, and almost all of its lameness is due to the fact that it relies so heavily on modes.)\nUnfortunately, programmers tend to think a lot in terms of modes, so the first time the user of an editor asked the programmer of that editor for the ability to do block selection (\u0026quot;column selection\u0026quot; in IntelliJ IDEA parlance) the programmer said \u0026quot;sure, I will add a new mode for this.\u0026quot; That's how problems start.\nThen of course users start complaining about this, because all they wanted was the ability to select columns, not a special column selection mode, so what ends up happening is that programmers start adding workarounds. Recently you added the ability to do column selection with Alt + Mouse Drag without the need to manually enter column selection mode first. This is not only especially useful for all of us who come from a Microsoft Visual Studio background, but it also makes sense for everyone, because it does away with a despised mode. At least when working with the mouse.\nUnfortunately, you have not extended the same goodness to selection with keyboard arrow keys, which is pretty bad in light of the fact that expert users tend to stick with the keyboard and rarely touch the mouse, so column selection mode is still being forced upon them.\nThe way to solve this is as follows: you already have four editor actions for moving the caret while extending the selection depending on the current state of the column selection mode: \u0026quot;Left with Selection\u0026quot;, \u0026quot;Right with Selection\u0026quot;, \u0026quot;Up with Selection\u0026quot;, and \u0026quot;Down with Selection\u0026quot;. That's all very fine and dandy, and you do not need to change anything there, so as not to break it for all those poor people who have resigned to having to use a special mode for column selection and would freak out if that mode was taken away from them.\nWhat you need to do is add four more editor actions for moving the caret while extending the column selection: \u0026quot;Left with Column Selection\u0026quot;, \u0026quot;Right with Column Selection\u0026quot;, \u0026quot;Up with Column Selection\u0026quot;, and \u0026quot;Down with Column Selection\u0026quot;. (I would bind them to Alt+Shift+Left Arrow, Alt+Shift+Right Arrow, Alt+Shift+Up Arrow, and Alt+Shift+Down Arrow respectively, as Visual Studio does.)\nThese actions should probably work as follows: if column selection is enabled, they should just work as the existing actions. If column selection is not enabled, then they should enable it, move the caret while extending the column selection, and then set a special flag to clear the column selection mode if any non-selection-extending key is pressed. This way, people will be finally free from the tyranny of column selection mode.\n(By the way, you probably should make Alt + Mouse Drag also set that special 'cancel column selection' flag, so that it does not leave the editor in column selection mode if the editor was not already in column selection mode when the Alt + mouse drag started.)\nThank you very much in advance.\n","date":"2014-11-09T13:39:31.062Z","permalink":"https://blog.michael.gr/post/2014-11-intellij-idea-feature-request-editor/","title":"IntelliJ IDEA feature request: editor actions for moving the caret left \u0026 right with Column Selection"},{"content":"A quick look at the source code that I have written over the past couple of decades in various work projects and hobby projects of mine shows that the percentage of class member variables that I declare as 'final' in Java or as 'readonly' in C# is in excess of 90%. I declare only about 10% of them as non-final. By looking at parameters and locals, a similar ratio seems to apply: their vast majority is effectively final, meaning that even though I do not explicitly declare them as final, the only time I ever write to them is when I initialize them. I would have been declaring them as final, if doing so was not tedious.\nMy percentages may be higher than the percentages of the average programmer out there, but I shall be bold enough to claim that this is probably because I pay more attention to quality of code than the average programmer out there.\nI will even be as bold as to say that the above was an understatement.\nIn my book, there is a simple rule: if it can be made final, it absolutely ought to be made final. If there is even a remote chance of making it final, that chance should be pursued tenaciously.\nTo put it in other words, it is my firm conviction that good code uses 'final' a lot, and bad code uses 'final' sparsely.\nSo, in light of the fact that immutability is a most excellent quality, and the fact that actual usage shows that values in well written code are in fact immutable far more often than not, it seems to me that the 'final' keyword is a bad idea. Not in the sense that things should not be final, of course, but in the sense that 'final' should be the default nature of all values, and therefore unnecessary. A keyword like 'mutable' should be used to explicitly indicate that something is non-final and therefore allowed (and actually expected) to be modified.\nI hope one day we will see a language which implements this realization.\nUPDATE 2015-05-15: It turns out that Rust does this with a 'mut' keyword.\nOld comments\nAnonymous 2015-01-22 13:53:37 UTC\nMany functional languages are immutable by default, so that one day was at least a few decades ago.\nmichael.gr 2015-03-20 23:28:02 UTC\nOkay, mr. smarty-anonymous-pants, I meant to say that I hope one day we will see an _imperative_ language which implements this realization. C-:=\n","date":"2014-10-21T17:56:01.18Z","permalink":"https://blog.michael.gr/post/2014-10-why-final-java-or-readonly-c-keyword-is/","title":"Why the 'final' (Java) or 'readonly' (C#) keyword is a bad idea"},{"content":"So, since we do software testing, we should quit placing assert statements in production code, right? Let me count the ways in which this is wrong:\n(TL;DR: skip to the paragraph containing a red sentence and read only that.)\n1. Assertions are optional. Each programming language has its own mechanism for enabling or disabling assertions. In languages like C++ and C# there is a distinction between a release build and a debug build, and assertions are generally only enabled in the debug build. Java has a simpler mechanism: there is only one build, but assertions do not execute unless the -enableassertions (-ea for short) option is specified in the command line which started the virtual machine. Therefore, if someone absolutely cannot stand the idea that assertions may be executing in a production environment, they can simply refrain from supplying the -ea option; problem solved.\n(Useful pre-reading: About these papers)\nThe mere fact that assertions are optional and not even enabled by default should be enough to quench any objections to their use. Now, in order to convince people to start actively using assertions instead of merely not minding if others do, I need to explain why assertions are awesome. This is what the rest of this document sets out to do.\n2. Assertions check things that testing cannot (and should not.) Testing treats (or should be treating) the production code as a black box, ensuring that given specific input, it produces expected results. Assertions, on the other hand, have a white box view of the code, (of course, since they live in it,) so they perform internal checks to make sure that everything is working as expected under the hood. Therefore, the domain of assertions is different from the domain of software testing, so there is a clear need for both.\nIf there is any uncertainty as to whether software testing should be taking a black box or a white box approach, let me briefly open up a parenthesis to clarify this one:\nWhen tests are tied to implementation details of the production code, situations arise where the production code gets refactored or bugs are fixed in it, and as a result the tests break and have to be modified in order to continue passing. I would call this The Fragile Test Problem. To avoid this, tests should be written having in mind nothing but the operational requirements of the software system, so that they only need to be revised in the event of a change in the requirements. (Also see footnote 1.) Testing against implementation details of the production code renders the tests non-reusable: It should be possible to completely rewrite a piece of production code and then reuse the old tests to make sure that the new code works exactly as the old one did. It should be possible to write a test once and have it test multiple different implementations of a system, created by independently working development teams taking different approaches to solving the same problem. Just as users tend to test software in ways that the developer never thought of, (the well known \u0026quot;works for me but always breaks in the hands of the user\u0026quot; paradox,) software tests written by developers who maintain an agnostic stance about the inner workings of the production code are likely to test for things that were never considered by those who wrote the production code. Yes, of course, black box testing cannot claim that it leaves nothing to chance, and that's precisely why you need assertions! Close parenthesis.\nSo, each assertion can be seen as a little software test embedded within the software.\nVarious techniques that are applicable to software testing are also applicable to assertions. An example of such a technique is the \u0026quot;do not fix it unless there is a test for it\u0026quot; advice. When a malfunction is observed in production, which has obviously passed all existing tests, instead of theorizing as to what went wrong and implementing a fix according to the theory, we add a theory-agnostic test against that malfunction, and we observe it failing. Then, we fix the malfunction according to our theory, and we observe the test passing. If the test still does not pass, then our theory was wrong.\nThe exact same approach is valid with assertions, with the added benefit that the assertion is a one-liner instead of an entire test. The savings here can be huge, both in terms of work to be done by the programmer, and in terms of total tests execution time, because a test may have to do a lot of work to set up the right conditions for the malfunction to be observed.\n3. Assertions are an excellent documentation tool. Unfortunately there is a very prevalent bad habit among software engineers worldwide, the habit of documenting assumptions within comments. This is very bad because:\nEvery time the code gets revised, amended or refactored, someone must remember to also update the comments, but this does not always happen. As a result, comments tend to become out of date as the code evolves, their accuracy and relevance eventually deteriorating so much that they come in conflict with what the code actually does. No comment ever was, or will ever be, as precise and unambiguous as a piece of code stipulating the same thing. Even the most precisely and unambiguously expressed comment is, by its very nature of being a comment, not enforceable in any way. So, if you require a certain condition to be met, or if you have a certain assumption which you believe to be true, put your code where your mouth is and back up your claim with an assertion statement. Since assertions are optional, there is no performance penalty, and even if the code is not even once run with assertions enabled, the assertion is still better documentation than a comment, because at the very least, it compiles.\n4. Assertions can catch errors in the testing code. Sometimes the person coding or maintaining the test code and the person coding or maintaining the production code might have a different understanding of what the operational requirements actually mean. Assertions within the production code help catch any such discrepancies at the earliest point possible, and they show that the production code is the way it is on purpose, and not by accident.\n5. Assertions can be more pertinent than testing. Sometimes the operational requirements are vague on issues on which the software design needs to make specific decisions. If the preferred way of solving a certain problem involves a division by something, then obviously, that something must not be zero, but if the operational requirements say nothing about zero, then the tests might not test for zero, and in any case they cannot be expected to test for zero. The implementation, however, knowing its own limitations, should assert against a zero. Ideally, such an issue of vagueness in the specification would be submitted back to the people responsible for it, and it should receive a definitive answer in the operational requirements document, which should then be translated into an additional test, but these things do not always happen in the real world.\n6. Assertions pinpoint errors that testing only broadly hints at. Consider this scenario: you have a TimeTable object which contains WorkShift objects. The shifts are stored in an array which is sorted by start time, and binary search is used to answer queries such as which employee is working at a certain time. Now, suppose that you have forgotten to sort the array after an insertion, so the binary search fails.\nAll that the failed test will tell you is that it scheduled John to work from 10:00 to 11:00, but a query for who works at 10:30 did not yield anyone. This is not very useful; the bug could be anywhere.\nProper use of assertions mandates that at the very least, immediately prior to performing a binary search on your array, you should ensure that it meets the requirement for it to be searchable via binary search, that is, to be sorted. So, voila! the assertion immediately discovers the nature of the bug.\nEven better, at the end of each operation on your TimeTable object you can assert that the operation is leaving the object in a valid state, which includes the requirement that this array must be at all times sorted. Thus, you will have an assertion failure at the end of the method which inserted an item to the array but forgot to sort it, pinpointing the bug with great accuracy.\nNow imagine the same happening in an immensely more complicated software system, where a maintenance programmer attempts to make a few small changes without comprehending exactly how the entire system works, and as a result the tests of this system start to fail without any indication as to where the problem might be. Sure, those few altered lines of code broke it, but how? What is wrong? Would it not be nice if the system could tell us what is wrong with it? Well, assertions help you achieve precisely that: software systems that can very often tell what is wrong with themselves.\n7. Assertions reduce program complexity. The time complexity and computational complexity of algorithms are subjects which have received extensive study, but most of the code being written on a daily basis all over the planet is not algorithms in an academic sense, so these notions are inapplicable to it. What is pertinent to most code that we regularly write is state complexity, which is a subject that has not received much study yet. I hope that an analytical state complexity algebra will be invented one day, allowing us to accurately calculate the state complexity of any given piece of code, but until then, nothing prevents us from theorizing about it in coarse terms. I hold it as self evident that if you add a variable to a system, the total state complexity of the system is compounded by something akin to the number of bits of that variable multiplied by the total number of statements throughout the system that make use of the value of that variable. If you add an if statement, total system state complexity is compounded by the number of bits of state altered by the body of the if statement, times two for the case that the body of the if statement does not get executed, and therefore the bits do not get altered. From this it should be evident that each time we add the tiniest little something to our program, we are exponentially increasing its state complexity.\nThere are only two constructs that I know of which actually reduce program state complexity instead of increasing it. One is the final keyword, and the other is the assert statement. The final keyword makes bits unalterable, thus excluding them from program state. The assert keyword limits the ranges of values that variables may have, thus also reducing the number of bits that participate in program state, and it even goes one step further, eliminating if statements and guaranteeing that certain paths of execution will never be followed.\nSo, think about it: every time you code an assertion statement you are actually making your program more simple instead of more complicated. I believe that this realization alone should be enough to convince anyone that assert, along with final, are literally the most useful constructs that any programmer could ever use.\n8. Assertions help the compiler make better sense of your code. Please do try this at home:\nvoid foo( Object x ) { assert x != null; if( x == null ) { } } If you have a reasonable number of warnings enabled, (as any decent programmer would,) your compiler should issue a warning telling you that the condition x == null is always false. What this demonstrates is that the preceding assertion gave the compiler knowledge of the fact that x cannot be null, and the compiler is now making use of this knowledge as it compiles the rest of your method, pointing out to you potential flaws in your reasoning. Furthermore, some compilers perform useful optimizations based on knowledge that they gather from assertion statements. (I do not know to what extent java compilers do that, but I know for a fact that the Microsoft Visual C++ compiler does it; see footnote 2.)\nConclusion Test code should always be treating production code as a black box, and knowledge of the inner workings of the production code should be used at most as a hint for testing, never as an instrument for testing. Consequently, assertions are necessary for performing white box tests within the production code so as to ensure that absolutely nothing is left to chance.\nAdditionally, assertions help document the code, reduce its complexity, strengthen it against assumptions made by the testing code, and help build systems that, in the event of an error, can tell you what is wrong with themselves.\nThe fact that assertions are usually disabled on deployed systems means that their use can be thought of as incurring a zero performance penalty, which allows programmers to develop a maximalistic error-checking culture of having every single assumption always checked, never leaving anything to chance. By contrast, runtime checks always incur a performance penalty, and for this reason programmers tend to use them on a minimalistic, \u0026quot;only if necessary\u0026quot; basis. So, with runtime checks, programmers tend to constantly ask \u0026quot;should I check this?\u0026quot; while with assertions, they can develop the habit of asking \u0026quot;is there anything I forgot to check?\u0026quot; (If there is only one paragraph that you should take home from this paper, that was it.)\nAddendum: How to use assertions Assertions can be used in every single place where an unexpected exception would otherwise be thrown. This includes all instances of contract violations such as a null pointer exception or an illegal argument exception, and in general, all instances of errors which a) indicate a bug, (should never happen,) and b) cannot be handled in any meaningful way.\nIn java the assert keyword cannot throw any exception that you might wish it to throw; it only throws the AssertionError exception. Usually this is not a problem, because no piece of production code should ever try to catch an unexpected exception. (That is, after all, precisely why it is called unexpected; it is not that the guest may arrive unannounced, it is that he is not supposed to arrive at all.) Therefore, since it is never meant to be caught, AssertionError is, generally speaking, a suitable one-class-fits-all replacement of all unexpected exceptions.\nHowever, when developing a library, which will be used with assertions presumably enabled at times, you will of course need to have tests which attempt to use your library in various wrong ways, ascertaining that every single one of those attempts gets asserted against, and the problem then with all errors being reported as AssertionErrors is that your tests cannot tell whether the error that was caught was the specific error that you were testing for, and not some unrelated coincidental error. For this reason, when developing libraries, a different approach is needed. One approach is the following:\nassert n != 0 : new IllegalArgumentException( \u0026#34;n\u0026#34; ); This works nicely because according to the java language specification, the expression at the right hand side of the colon of an assertion statement does not have to be of any particular type, but if it happens to be of an exception type, then it will be treated as the 'cause' of the AssertionError exception. (In all other cases, its toString() will be used as the 'message' of the AssertionError exception.) Thus, your testing code can catch the assertion exception and examine its cause to ensure that it is indeed the expected exception.\nSo, your test suite might contain a utility method like the following:\npublic final \u0026lt;T extends Throwable\u0026gt; T expectException( Class\u0026lt;T\u0026gt; exceptionClass, Runnable runnable ) { try { runnable.run(); } catch( Throwable throwable ) { if( throwable instanceof AssertionError \u0026amp;\u0026amp; throwable.getCause() != null ) throwable = throwable.getCause(); assert exceptionClass.isInstance( throwable ) : throwable; //exception of the wrong kind was thrown. assert throwable.getClass() == exceptionClass : throwable; //exception thrown was a subclass, but not the exact class, expected. @SuppressWarnings( \u0026#34;unchecked\u0026#34; ) T result = (T)throwable; return result; } assert false; //expected exception was not thrown. return null; //to keep the compiler happy. } Which might be used as follows:\nIllegalArgumentException e = expectException( IllegalArgumentException.class, () -\u0026gt; myObject.foo( 0 ) ); assert e.getMessage().equals( \u0026#34;n\u0026#34; ); There are times when you have the need to check whether assertions are enabled. For example, in the first thing I always do in my test suites is to make sure that assertions are enabled, because if someone forgot to pass the -ea switch, all tests may pass without checking for a single defect in the code. Here is how to test whether assertions are enabled in java:\npublic static boolean isAssertEnabled() { //noinspection UnusedAssignment boolean assertEnabled = false; //noinspection AssertWithSideEffects,NestedAssignment,ConstantConditions assert assertEnabled = true; //noinspection ConstantConditions return assertEnabled; } Note: the //noinspection comments are understood by IntelliJ IDEA; if the misfortune hath befallen thee of having to use some other IDE such as Eclipse, you will have to use some different warning suppression notation.\nFootnote 1 What makes the Fragile Test Problem especially bad is that the process of fixing tests to make them pass is often carried out under unfavorable conditions, resulting in a sloppy job:\nThe programmer needs to switch his frame of mind back and forth between his core work and the somewhat unrelated and usually less exciting context of tests; The need for these fixes is usually unforeseen, so time for the fixes is rarely allocated in the schedule, which means that the programmer fixing broken tests is usually in a hurry; It is not always clear whether the production code is right and the test is wrong, or whether the test is right and a dormant bug in the production code has been exposed; etc. So, what tends to happen is that tests are quite often fixed sloppily, so over time they tend to evolve to \u0026quot;test around\u0026quot; (specifically pass) long-standing bugs.\nIn the preface of Roy Osherove's The Art of Unit Testing (Manning, 2009) the author admits to having participated in a project which failed to a large part due to the tremendous development burden imposed by badly designed unit tests which had to be maintained throughout the duration of the development effort.\nFootnote 2 In Microsoft Visual C++ the ASSERT(x) macro does not expand to nothing when _DEBUG is undefined; instead, it expands to an assume(x) intrinsic directive which, even though it does not cause any code to be emitted, it allows the asserted expression to survive the preprocessing step and to be considered by the compiler, so that the corresponding optimizations can take place in the release build, too.\nFurther reading Stackoverflow answer by João Manuel Rodrigues https://stackoverflow.com/a/49131673/773113\nStackoverflow answer by me https://stackoverflow.com/a/27622328/773113\n","date":"2014-09-19T15:27:57.305Z","permalink":"https://blog.michael.gr/post/2014-09-assertions-and-testing/","title":"Assertions and Testing"},{"content":"I have been vaping for about two and a half years now, and it has been one of the best things that have ever happened to me. Here are some of my thoughts on the subject, written in the form of a \u0026quot;how-to\u0026quot; guide. It may change as I gain more knowledge.\nLike most people, I started with various odd contraptions of the kind that you receive as presents, and I quickly realized that the way to go is a specific more-or-less-standard type of device which, rather unsurprisingly, is the type of device that you most often see carried by people who have picked up the habit. It consists of a USB-rechargeable battery, a replaceable bit called the vaporizer, and a tank with a mouthpiece. These parts fit together by screwing one into the other, (the mouthpiece snaps onto the tank,) and the dimensions of all the junctions are standard, so you can replace each part as needed, and you can even mix and match components from different brands, since they adhere to the same standard.\nStandard versus non-standard There exists a variety of other types of devices which either require their own special charger, or they store the fluid in a sponge instead of a tank, or they are different in this or that or the other respect which makes them incompatible with standard components. My experience says that it is best to stay as far away from them as possible. Sure, some of them look sleek and exclusive, but lack of interoperability results in an unreasonably high extra cost, for benefits which are usually only aesthetic. You might even find a one-of-a-kind system for a price which might seem comparable to the cost of a bulky and motley system put together out of standard components, but in reality the one-of-a-kind system is far more expensive, because if one aspect of it turns out to not suit you, or if one part of it gets lost or broken, the entire system must usually be tossed, while with standard components you only replace the part that needs replacement. If, in addition to all this, you consider the fact that certain components of electronic cigarettes (namely, the batteries) are known beforehand to have a limited lifetime, buying a special system which is guaranteed to have to be thrown away after a few months makes no sense at all, in my opinion.\nAlso, what does not help at all is the fact that usually, you cannot trust whatever assurances you are given by salespersons. You might ask \u0026quot;will I be able to find spare batteries for this very special model a few months down the road?\u0026quot; and you will invariably receive the answer \u0026quot;but of course!\u0026quot;. You can tell that they are lying because if that was true, they would not have simply assured you, they would have actually shown you the wide range of replacement batteries that they stock. But of course, they don't. Nor do they intend to. When you go there a few months later looking for replacement batteries, it is not like they will blush, or show any shame or remorse; they will inform you with a completely straight face that your very special model was discontinued a long time ago and they do not even carry anything that might betray that it ever existed. Or they might follow the \u0026quot;always say yes\u0026quot; doctrine and tell you that of course they will order your replacement battery for you, and it will cost you twice the price of a brand new model, and it will arrive at about the same time that your grandchildren will be born. Same thing.\nThe beauty of using standard components is that you don't have to take anyone's word that they will carry them: you can see with your own eyes that everyone carries them. The standard kind of batteries have been around in their present form over at least three generations of tanks, (that many I have witnessed, they were probably preceded by more,) so in all likelihood they will continue to be around in this form for a long time to come. The demand for them is probably too high for any manufacturer to ignore.\nThe following discussion pertains only to components that adhere to the standard, and considers only variations within the limits permitted by the standard.\nJargon Electronic cigarette companies use various fancy terms to refer to their products as if they were revolutionary new technology never seen before on the face of earth. You might hear preposterous buzzwords like atomizers, cartomizers, clearomizers, and what not. The good news is that you can safely ignore that gobbledygook. Virtually all of the technology that goes into electronic cigarettes today is old; remember the odourless white fog in the discos back in the seventies? That's basically it, and it was not even new technology back then. Not only nothing about electronic cigarettes is brand new, but also, none of it constitutes a noteworthy invention that was made specifically for electronic cigarettes. So, all this marketingspeak is completely unnecessary. You have every right to refuse to follow a discussion with a salesperson who insists on using buzzwords: demand that they speak plain English. (Or whatever language happens to be spoken where you live.)\nThe only component of an electronic cigarette which could, perhaps, have deserved a brand new name was the original idea of a piezoelectric ultrasound-emitting element that would supposedly vaporize a pressurized jet of vaping fluid. That sounds quite exotic, doesn't it? Luckily, it was pretty quickly discovered that the same end-goal (namely, vaporization) could be accomplished much, much, much more easily and much, much, much more economically with just a coil wrapped around a wick touching a liquid in a --thank goodness-- non-pressurized tank, so no such elements were ever mass produced, nothing of their kind is part of modern electronic cigarettes, and therefore no jargon is necessary.\nThe actual parts of the electronic cigarette are pretty plain and simple; here they are once again: the battery, the vaporizer, the tank, and the mouthpiece. It is that straightforward, really.\nThe battery They come in various sizes and prices. A larger battery has the benefit of lasting longer without a need to recharge, but it is bulkier and it costs more. The higher cost is not to be dismissed, because every once in a while you will inevitably misplace one and lose it, so it is a lot better to lose a €25 device than a €40 one. My personal dogma with batteries is \u0026quot;the cheaper, the better\u0026quot; since I have not witnessed any significant difference between the cheap ones and the more expensive ones. They all look like they come out of the exact same Chinese factory, regardless of the brand name printed on them.\nAlter eGo 900mAh pass-through battery and the two ends of a standard-to-mini USB cable. Activation There exist two types of batteries with respect to activation: push-button activated and breath activated. (Well, actually, inhalation-activated, or suction-activated.) I started with breath activated, and the switch to push-button felt unnatural in the beginning, but I have come to actually prefer the button. Why? Because due to heat stored in the system, the vaporizer keeps vaporizing fluid for a couple of more seconds after power has been cut off, (you can actually hear it fizzing even after you release the button,) so if you have stopped inhaling by that time, then that vapor is lost. With button activation, you can always continue inhaling for a second after the release of the button. (It is not so much the lost vapor that concerns me, as the energy lost in generating that vapor.) The fact that operating the button requires one of your hands is irrelevant, because you are going to need to be using your hand anyway to hold the device to your mouth. E-cigarettes are not small enough yet to be comfortably held by the lips or teeth.\nCharging There exist two types of batteries with respect to charging: batteries that charge via a regular USB cable connected to a standard \u0026quot;type USB mini\u0026quot; receptor located on the far end of the battery, and batteries that charge via a special USB-to-screw adapter which attaches to the near end of the battery. The first kind is also called \u0026quot;pass-through\u0026quot; and it has the significant advantage of being able to charge while you are vaping. The second kind cannot be charged while vaping, because at any given moment the screw at the near end of the battery can accommodate either the charger or the tank, not both. Needless to say, pass-through batteries are the way to go, because there is nothing more frustrating than being in the comfort of your home, and having all the equipment in your hands, and still not being able to vape because the stupid battery needs to charge. Also, that special USB-to-screw adapter is another special item that you have to worry about, while mini USB cables are so common that they can be found everywhere and they are practically given away for free. In light of all that, I do not know why non-pass-through batteries even exist. Duh, what were they thinking?\nVariable voltage There exist two types of batteries with respect to voltage: fixed and adjustable. I have tried an adjustable voltage battery, and I found it to be quite a disappointment. Standard wicks and coils are apparently designed for the voltage produced by regular non-adjustable batteries, so driving them with higher voltage seems to cause the wick to burn. (yikes!) Also, the one adjustable battery that I tried turned out to have an unacceptably low capacity, despite the fact that it was advertised as being of higher capacity than my fixed voltage batteries, and despite the fact that I used it at low voltage. Also, variable voltage batteries tend to not be pass-through, because the far end of the battery where the USB receptor would be located houses the adjusting dial. Variable voltage batteries might be useful for people who like to make their own wicks and their own coils, and there even exist vaporizers that are geared towards that audience, but the vast majority of people who are into vaping do not belong to that audience.\nScrew threads A careful examination of a standard battery reveals that in fact it has two sets of screw threads: a large outer thread (visible in the above photo) and a smaller, inner thread hidden within the top end. Batteries are built like this in order to accommodate both of the two predominant standards of tank screws. However, as far as I know, only tanks of the large screw type accommodate standard vaporizers, so the small screws are useless anyway.\nBattery use and care One of the ways in which vaping is very much unlike smoking cigarettes is that if you run out of cigarettes you can always go buy another pack at the nearest kiosk, while if you run out of battery, you are kind of out of luck. Some people can?t handle this, and the first time they run out of battery they go straight to the kiosk to buy a pack of cigarettes, even though up until that point they were on a good path to kicking that nasty habit. So, take my word on this one: once you start vaping, it is of paramount importance that you never, ever run out of battery.\nThe absolute best strategy for never running out of battery is owning at least two batteries and using one while charging the other. This strategy is not fool-proof, so for better results make sure that at least one of your batteries is pass-through.\nIf you want to be really frugal you can make it a point whenever you are sitting in front of your computer to use in pass-through mode an old battery which is past the end of its lifetime, so that you don?t consume the life of your good batteries. The more you use a battery, the more of its total lifetime you consume, but even when past the end of its lifetime, a pass-through battery is still perfectly usable in pass-through mode.\nAlways keep vaping until your battery is completely drained of energy, and always charge it until it is completely full. This is the optimal charge-discharge cycle which guarantees the best performance and the longest lifetime of modern Li-ion batteries. Of course, this only makes sense if you have another battery fully charged always on stand by, which is another way of saying that having more than one battery is an absolute must.\nThe vaporizer This is the part which draws liquid from the tank and energy from the battery to vaporize the liquid. It just contains a couple of coils wrapped around wicks which are kept moist with liquid from the tank. When you press the button on the battery, electricity is applied to the coil, heating it up and causing the liquid on the wicks to evaporate. That's all, no magic here.\nYou need to replace the vaporizer every few weeks, depending on use, and it has a rather reasonable cost of around ?2.50 apiece. This is probably about a hundred times more than the ?real? cost of manufacture, but the companies that build and market and distribute and sell these things, and keep working on them to come up with an improved new generation of them every couple of years, must somehow stay in business, so I guess it is okay. Sort of.\nWhen I first started vaping, the predominant type of vaporizer had long wicks which extended out of it and floated in the tank. That was pretty lame, and luckily it has now been replaced with dual-bottom-coil, hidden-wick 1.8? metallic capsules. Different electronic cigarette companies in my vicinity sell the exact same thingies in pill-like ?blister packs? of 5, so again, they all probably come out of the exact same factory somewhere in China. Some call them wickless, but that?s wrong: they still have some material that plays the role of the wick, it is just neatly hidden inside the capsule. I will not bother you with what ? stands for and how to choose the right ? for you, nor with the details of why the old top-coil system was lame and why the new bottom-coil system is awesome, because the old system is not a choice anymore anyhow, and impedance appears to have now been standardized to 1.8?, so there is no choice there, either. If someone tries to sell you vaporizers with visible wicks today, just tell them they are so funny.\nAlter eGo bottom-dual-coil vaporizer capsule. When you install a vaporizer for the first time, let it stand for a few minutes before vaping, for the wicks to pre-soak with liquid, otherwise you are going to be burning your wicks the moment you press the button. Even after pre-soaking, the first few puffs for some reason tend to have an awful burned smell, and you do not need an advanced degree to figure out that you better blow these puffs away without inhaling. Better yet, you can try blowing air into the mouthpiece instead of drawing air from it: the foul-smelling vapour will come out of the other end of the vaporizer without entering your mouth at all. After a few puffs the smell should be completely gone.\nI have a bunch of used up vaporizer capsules which I have cleaned by boiling them in water and I am planning to see if they can be reused; I will update this text with the results of the experiment. Update #1: a complete disaster. The vapor has a burned smell which does not go away no matter how much I puff, and the entire device overheats. This vaporizer must be thrown away.\nThe tank\nWe live in the era of plenty, so there exists an astonishing variety here, too. First of all, whatever tank you buy, make sure that it fits standard batteries, and that it is built for the right kind of vaporizer, which, at the time of writing this, is the standard 1.8? dual-bottom-coil, hidden-wick metallic capsule sold by different e-cigarette brands. This most likely means that the tank will be fitting the large, outer screw of the battery, instead of the smaller, inner screw. Also, make sure that the tank accepts standard snap-on mouthpieces.\nAlter eGo tank and mouthpiece. The top half of a fitted vaporizer capsule is also visible just below the centre. If some salesperson tries to sell you their latest and greatest ultra-super-duper™ awesome-o-matic™ device which does not happen to be compatible with the standard, allegedly \u0026quot;because it is better\u0026quot;, clearly state your determination to only buy standard stuff, and if they insist, then they obviously do not carry standard stuff, so you have walked into the wrong shop; walk out.\nAir intake Near the bottom of the cap are the air intake holes. (You can see one in the above picture, and there is another on the opposite side.) They are small, so as to strangulate the air flow, thus providing about the same amount of air resistance as a cigarette would. Unfortunately, they can be clogged with dust. If you notice that puffing has become difficult to the point where the edges of your mouth wrinkle in your effort to achieve suction, suspect them before you suspect the vaporizer. You can usually see if they are clogged by unscrewing the tank from the battery and holding the tank between your eye and a source of light, at a certain angle. Try it when you know they are clean, so as to already know what to look for when it is time to find out if they are clean. Unclog them with a sharp object. I use a sharp-ended wooden toothpick. If the holes are clear, and yet it is still hard to draw air through the device, then it is probably time to replace the vaporizer.\nSealing Perhaps the single most important characteristic of a tank is how well it seals. Anything short of ?perfectly? is not good enough. Make sure that the tank cap fits the tank with a metal-on-metal screw and a ring seal. Completely disregard any salesperson's assurances that their exclusive ultra-tighto-fitto™ snap-o-magic™ neva-eva-leak™ patent is better than a screw. If something better than a screw ever gets invented, you will hear it from the news, not from a salesperson. Make sure both sides of the screw are metallic. I think all tanks today are like that, but I am mentioning this because in the past I have actually come across one tank whose clear plastic walls would end in a thread, and this plastic thread would screw into the metallic thread of the cap; that's a no-no, because after a while it tends to break. The plastic wall of the tank must be press-fitted into a metallic ring, and the ring must be threaded, so that the threads are metal-on-metal. Also, just to be extra sure, somewhere in the area where the tank and the cap meet, usually on the cap, between the threading and the liquid, there must be a tiny ring seal made of rubber or silicone.\nMost tanks are built exactly to the spec that I just described, so there is no need to resort to anything inferior to that. Given the fact that the vaporizers are replaceable, a single tank should last you for years, so there is no need to be frugal with it.\nMilliliter scale Some tanks have a milliliter scale printed on them, while others don't. The printed scale looks kind of cool, but it is useless, so don't worry about its absence if you see a tank that you otherwise like. You will always be filling the tank up to the brim, and you will always be vaping until the liquid-intake holes of the vaporizer capsule are almost exposed, so there is no need for a scale to tell you how many milliliters you have left. You might think that the scale could come in handy if you ever wanted to mix your own liquids, but as someone who is mixing his own liquids I submit to you that it will not, because a) you need a 20/20 eyesight to read the damned thing, and b) mixing minuscule quantities straight into the tank requires such incredibly high precision that it is impractical; it is far better to do your mixing in a larger container and fill up your tank from it. That having been said, it is worth noting that the tank with the milliliter scale is likely to have been manufactured by someone who values meticulousness, thoroughness, and overall quality of engineering more than the one that does not.\nTransparency Some tanks have transparent plastic walls allowing you to check the level of the liquid at a glance; other tanks are made of clear plastic with a colored tint which makes things a bit harder; yet others are made mostly of metal, with only a narrow window along their length. I find anything other than transparent all around walls to be a nuisance: checking the level of the fluid is something that you do very often, so why hinder it even slightly?\nThat having been said, I realize that there may be reasons for choosing a not-completely-transparent-plastic tank that I may not yet be aware of; for example, some ingredient of the vaping fluid, perhaps the nicotine, might be sensitive to (decomposed by exposure to) light; and perhaps there is a case to be made for metallic tank walls because they dissipate heat better than plastic; I will amend this part as I become better educated on the subject.\nThe mouthpiece The mouthpiece is the part that your lips touch. There exist two types of mouthpieces: round ones and flat ones. (Meaning, ones having a round cross-section, and ones having a rectangular or oval cross-section.) One might intuitively guess that flat ones must be better, since they are meant to fit between our lips, which are not exactly shaped like holes, but I personally hate the flat ones with passion, because they force me to hold the device to my mouth in a specific way, prohibiting any rotation. However, I am told that others strongly prefer the flat mouthpieces over the round ones, so what can I say, I guess everyone is entitled to have their own little perversions.\nLuckily, the part of the mouthpiece that snaps onto the top of the tank is standardized, so if there is a tank that you really like, which comes with a mouthpiece that you don't, you can always buy the tank, throw away that mouthpiece, and fit another one in its place. Just make sure beforehand that the shop which is to sell you the tank can also provide you with a mouthpiece of your liking: if they do not have individual mouthpieces for sale, then they should have some spare mouthpiece to give you for free. If they don't, or worse yet, if they suggest that you also buy another tank that you don't care about, just so that you can have its mouthpiece, then perhaps they do not deserve your business.\nVaping fluid The ingredients of vaping fluid typically are: Propylene Glycol (PG), Vegetable Glycerine (VG), water, nicotine, and aroma.\nPropylene Glycol PG emits a faint, almost transparent vapor, which causes a scratchy sensation (also known as ?dryness?) in the throat, similar to the scratchy sensation caused by inhaling tobacco smoke. Both in the case of PG vapour and in the case of tobacco smoke, the scratchy sensation is caused by a mild, fleeting irritation of the insides of our throat. The fact that PG has this effect is the main reason why vaping is so surprisingly similar to smoking.\nVegetable Glycerine VG causes a lot less of that scratchy effect, but its vapour is thick white, so it is used to make the emissions of electronic cigarettes more visually similar to tobacco smoke. Vaping pure VG does not work very well, probably because it is so much more viscous (less runny, thicker) than PG that the rate at which it permeates the wick is lower than the rate at which it vaporizes away from it, causing the wick to run dry and start to burn.\nWater Water is rather neutral as an ingredient, but it is useful for reducing the overall viscosity of the fluid. If you want to use less than 50% PG and more than 50% VG, you generally have to start adding water to keep the viscosity within a workable range. Of course you only have to worry about such things if you decide to start preparing your own liquids: in ready-made liquids all these issues have already been taken care of for you.\nNicotine Nicotine is derived by processing tobacco plants, and it is diluted in the vaping fluid to levels that under normal use roughly correspond to the amount of nicotine that you would be taking in if you were smoking tobacco. Of course one size does not fit all, so each vaping fluid comes in various nicotine concentrations to suit different people's levels of addiction. The concentration of nicotine is measured in mg/ml, (that's milligrams per milliliter,) but scientific literacy does not appear to be the strong point of those involved in the vaping industry, so \u0026quot;mg/ml\u0026quot; is quite often wrongly printed as \u0026quot;mg\u0026quot; on the bottles, as if the entire nicotine content of the bottle was just a meager \u0026quot;8 mg\u0026quot;, when in fact a 20 ml bottle at a 8 mg/ml concentration packs a rather reasonable 160 total mg of nicotine. (Of course it makes you wonder, if they can't get something as basic as printing the units right, what else they may be failing to get right.)\nReady made vaping fluids usually come in bottles of 20 ml, which should last you about a week with heavy use, and typically in concentrations of 8 mg/ml (\u0026quot;low\u0026quot;), 12 mg/ml (\u0026quot;medium\u0026quot;), and 18 mg/ml (\u0026quot;high\u0026quot;). My advice is to start with low, and to try to stay there. If after several days of trying with \u0026quot;low\u0026quot; it seems like vaping does not feel right for you and you start itching to light up a cancerstick, then, and only then, go ahead and up the dosage.\nNicotine is generally harmless in the levels of dilution found in ready-made vaping fluids, but it is best to play it safe and avoid touching the liquid for prolonged periods of time, because it does get absorbed through the skin, and so you do run the danger of a nicotine overdose, which can be rather unpleasant, and may even require hospitalization. If your fingers become wet with fluid, do not let a very long time pass before you wipe them thoroughly or wash them. If a sufficient quantity of liquid gets spilled on your clothes so as to permeate the fabric and come in contact with your skin, change your clothes and take a shower. Never buy or keep around vaping fluid bottles that are not child-safe. As a matter of fact, companies that sell vaping fluids in non child-safe bottles deserve everyone's scorn (not to mention criminal prosecution) : don't buy anything from them. That having been said, you do not need to be paranoid about vaping fluid: you would probably have to swim in a pool of it and then sit by the pool to dry before you would die from nicotine overdose. Briefly getting your hands wet with vaping fluid does not constitute a medical emergency, and you can safely wipe small amounts of it with the outer side of your t-shirt without any harm. The fluids are not oily at all, so there will be no stain, either.\nOf course, the above discussion about handling nicotine only pertains to concentrations of the kind that you find in ready-to-vape fluids. People who make their own fluid generally buy nicotine at higher, sometimes much higher concentrations, and the rules that apply there are completely different: nicotine in higher concentrations is a highly toxic substance which needs to be handled with great precaution.\nAroma There exist two broad categories of aromas: those that imitate the smell of burning tobacco, (at varying levels of success,) and those that are not trying to pretend anything of that sort, and instead aim to offer completely unrelated sensations, such as chocolate, red apple, bubble gum, etc.\nIn my experience there exists a very small number of individuals who may be bothered by the tobacco aroma of your electronic cigarette, but virtually nobody seems to be bothered by a non-tobacco aroma. Still, if you want to be extra safe, and considerate towards those around you, (for example, if you want to be vaping in your workplace,) the best choice is no aroma at all.\nAlso, in my experience, the use of aroma has certain disadvantages. First of all, it is a major source of impurities and unknown substances in the vapor. Given the fact that electronic cigarettes are still largely unregulated, manufacturers are not accountable to anyone, so they can put any harmful substance they like in the aroma with complete impunity. It is said that aroma is a huge gray area where manufacturers can add whatever they please to the vaping fluid and simply put it under the aroma umbrella instead of having to list it as a separate ingredient. Secondly, I think that the use of aroma may be significantly reducing the operating life time of the vaporizers. What may be happening is that some of the substances that make up the aroma are less volatile than the rest of the fluid, so they stick to the wick instead of emanating away with the vapours, and over time the build-up of these substances clogs the wick and results in a premature end of lifetime for the vaporizer. I am not sure about this, but it is my suspicion. Furthermore, it is a well known fact that we tend to become desensitized to olfactory stimulations that persist over long periods of time, which means that after a couple of days of using a certain aroma we tend to sense its smell to a much lesser extent than in the beginning. Essentially, most of it goes wasted as far as our smelling pleasure is concerned, but all of it still passes through our lungs, with whatever implications that may have for our health.\nNeck strap The yearly cost of a vaping habit is only a small fraction of the yearly cost of a smoking habit, but there is one important thing to keep in mind: if you lose a pack of cigarettes, the cost of replacing it is generally not so high as to cause you to go bankrupt, but that is not equally true with vaping devices. So, it is best to never lose one. The one sure way to never lose a device is to buy a neck strap and hang it from your neck, if you do not mind the aesthetics of such an arrangement. There also exist pouches that hang from the neck, but they are ugly, they don't look very secure to me, and their use must be rather cumbersome. They are probably intended as a one-solution-fits-all for the multitude of non-standard devices out there, but for standard devices, (which are the only kind of devices that I advocate buying,) there exist neck straps that end in a special ring which nicely grips the neck of a standard battery, leaving room for the button. Again, you can buy them from different stores and they will all match, because they adhere to the standard. The neck straps that I have found in vaping stores tend to be outrageously expensive for what they are, (just a ribbon with a clip and a ring at the end of it,) and they also tend to be badly engineered, (no way to adjust, no way to fix if damaged, the clip sometimes unfastens,) so it is best to obtain one from elsewhere, or to make your own. But while you are working on that, do go ahead and buy a lame one from the vaping store, because a) its cost is still much lower than the cost of a single lost device, and b) you will probably need that special ring anyway.\nStores If you are wondering how to get started with vaping, one simple thing you can do is locate a vaping store in your neighborhood, walk in, and tell them that you are curious and want to try. Don't worry, they do not expect you to already have any prior knowledge of that stuff, nor do they demand that you simply state what you want to buy, pay, and get out; they are there to help. They have lots and lots of sample tanks containing fluids with different aromas and different ratios of PG to VG for you to try; they have throw-away mouthpieces so that your lips won't touch a mouthpiece that other lips have touched; they have the patience to keep screwing and unscrewing tanks on batteries for you to try different fluids to your heart's content; and they know that all this is part of what they have to do in order to generate more customers. So, don't be shy, pay them a visit.\nAlso, look around for shops in your vicinity and locate the one with the most knowledgeable and helpful personnel. The actual products don't have such huge differences from store to store as to really matter, but the salespersons can make a huge difference. Be suspicious of any claims pertaining to matters of subjective perception, such as flavor. Demand of the salespersons nothing less than thorough technical knowledge of the devices that they are trying to sell to you, plus the ability and willfulness to transfer this knowledge to you in a clear and easy to understand manner. If a salesperson has a hard time explaining something, or if they sound not too sure of what they are talking about, then they are probably in the wrong business, which means that you are probably in the wrong store.\nVersus smoking I stated earlier that vaping is surprisingly similar to smoking. I should add that this is especially true if you consider that in actuality it has nothing to do with smoking: in one case you are setting dried plant leafs and paper on fire and inhaling their combustion by-products, while in the other case you are electrically heating up some liquid and inhaling its vapour. Literally, the only thing in common is that in both cases nicotine is involved. Therefore, it is not wise to expect vaping to accurately emulate the experience of smoking; instead, it is advisable to expect it to provide an (altogether different) experience which suits you as a replacement of the cigarette experience. Approach it with this attitude, and you will be pleasantly surprised by how familiar it will feel.\nNow, if you still have issues with the fact that the vaping experience is not completely identical to the smoking experience, then you have to view it as a compromise and evaluate the benefits. If you are absolutely fine with spending a considerable portion of your yearly income on something which is ruining your health while making you stink, then understandably, you must see absolutely no reason to want, or accept, any kind of compromise. Apparently you are fine with the cancersticks, so continue with them, power to you. If, on the other hand, you happen to realize that there is a problem with this habit of yours, then obviously, freeing yourself from that habit must be worth a certain amount of inconvenience, right? Well, it just so happens that the inconvenience caused by the transition from smoking to vaping is so small that it really could not be any smaller. If this step is not small enough for you, then no step will ever be. They really have done everything they could to make it as easy for you as possible: it has the shape of a cigarette, it emits white fumes like a cigarette, it delivers nicotine like a cigarette, it has the aroma (but not the stench) of a cigarette, it scratches the throat like a cigarette, it even makes a fizzing sound like a cigarette. What else do you want from it? To actually be a cigarette?\nI once came across a lady who said that she had tried electronic cigarette and it was not bad, but she did not go for it because \u0026quot;it is too heavy.\u0026quot; Heavy, as in, weighing too much. Well, obviously, that lady never felt the slightest need to quit smoking. Vaping is for those who slightly do.\nIn the future I am going to write more about electronic cigarettes, mainly on the following two topics:\nHealth effects and related controversy around electronic cigarettes\nMaking your own liquid\nOld comments\nJohn 2016-06-08 07:44:54 UTC\nThere is so much more to learn about e cigarettes, people don't know how they will be in the future, also e-cigs \u0026amp; e-liquids, are getting to gimmicky, check out these flavours for example - kiss e-liquid, are they made to attract kids?\n","date":"2014-08-24T23:04:40.164Z","permalink":"https://blog.michael.gr/post/2014-08-on-electronic-cigarettes/","title":"On Electronic Cigarettes"},{"content":"Now that Java 8 is out, I was toying in my mind with the concept of a new assertion mechanism which uses lambdas. The idea is to have a central assertion method that works as follows: if assertions are enabled, a supplied method gets invoked to evaluate the assertion expression, and if it returns false, then another supplied method gets invoked to throw an exception. If assertions are not enabled, the assertion method returns without invoking the supplied method. This would provide more control over whether assertions are enabled or not for individual pieces of code, as well as over the type of exception thrown if the assertion fails. It would also have the nice-to-have side effect of making 100% code coverage achievable, albeit only apparently so.\nNaturally, I wondered whether the performance of such a construct would be comparable to the performance of existing constructs, namely, the 'assert expression' construct and the 'if( checking \u0026amp;\u0026amp; expression ) throw {...}].' construct. I was not hoping for equal performance, not even ballpark equal, just within the same order of magnitude.\nWell, the result of the benchmark blew my mind.\nCongratulations to the guys that made Java 8, because it turns out that all three constructs take roughly the same amount of time to execute!\nHere is my code:\npackage saganaki; public class TestProgram { public static void main( String[] arguments ) { Benchmark benchmark = new Benchmark( 100 ); for( int i = 0; i \u0026lt; 3; i++ ) { run( benchmark, true ); run( benchmark, false ); System.out.println(); } } interface Checker { boolean check(); } private static boolean assertionsEnabled = true; private static void run( Benchmark benchmark, boolean enableAssertions ) { TestProgram.class.getClassLoader().setClassAssertionStatus( TestProgram.class.getName(), enableAssertions ); assertionsEnabled = enableAssertions; String prefix = \u0026#34;assertions \u0026#34; + (enableAssertions? \u0026#34; enabled\u0026#34; : \u0026#34;disabled\u0026#34;); benchmark.runAndPrint( prefix + \u0026#34;: if-statement\u0026#34;, new Runnable() { @Override public void run() { if( assertionsEnabled \u0026amp;\u0026amp; System.out == null ) throw new IllegalArgumentException(); } } ); benchmark.runAndPrint( prefix + \u0026#34;: assert \u0026#34;, new Runnable() { @Override public void run() { assert System.out != null; } } ); benchmark.runAndPrint( prefix + \u0026#34;: assertTrue()\u0026#34;, new Runnable() { @Override public void run() { assertTrue( () -\u0026gt; System.out != null, () -\u0026gt; { throw new IllegalArgumentException(); } ); } } ); } static void assertTrue( Checker checker, Runnable thrower ) { if( !assertionsEnabled ) return; if( checker.check() ) return; thrower.run(); } } And here is the output:\nassertions enabled: if-statement 28237.0 iterations per millisecond assertions enabled: assert 29037.9 iterations per millisecond assertions enabled: assertTrue() 24593.2 iterations per millisecond assertions disabled: if-statement 25118.5 iterations per millisecond assertions disabled: assert 25912.2 iterations per millisecond assertions disabled: assertTrue() 24825.6 iterations per millisecond assertions enabled: if-statement 25835.9 iterations per millisecond assertions enabled: assert 25127.6 iterations per millisecond assertions enabled: assertTrue() 25572.9 iterations per millisecond assertions disabled: if-statement 25469.6 iterations per millisecond assertions disabled: assert 25448.3 iterations per millisecond assertions disabled: assertTrue() 25415.7 iterations per millisecond assertions enabled: if-statement 25838.6 iterations per millisecond assertions enabled: assert 25158.9 iterations per millisecond assertions enabled: assertTrue() 25541.7 iterations per millisecond assertions disabled: if-statement 25373.6 iterations per millisecond assertions disabled: assert 25402.5 iterations per millisecond assertions disabled: assertTrue() 25370.9 iterations per millisecond The first run shows quite different results from the next two runs, so it is best disregarded.\nFor the Benchmark class, see my previous post: Benchmarking code written in Java or C# (or any GCed, JITted, VM-based language)\n","date":"2014-07-18T18:45:13.534Z","permalink":"https://blog.michael.gr/post/2014-07-benchmarking-java-8-lambdas/","title":"Benchmarking Java 8 lambdas"},{"content":"NOTE: This paper contains various inaccuracies.\nSometimes we need to measure the time it takes for various pieces of code to execute in order to determine whether a certain construct takes significantly less time to execute than another. It sounds like a pretty simple task, but anyone who has ever attempted to do it knows that simplistic approaches are highly inaccurate, and achieving any accuracy at all is not trivial.\nBack in the days of C and MS-DOS things were pretty straightforward: you would read the value of the system clock, run your code, read the value of the clock again, subtract the two, and that was how much time it took to run your code. The rather coarse resolution of the system clock would skew things a bit, so one trick you would at the very least employ was to loop waiting for the value of the system clock to change, then start running your code, and stop running at another transition of the value of the system clock. Another popular hack was to run benchmarks with interrupts disabled. Yes, back in those days the entire machine was yours, so you could actually do such a thing.\nNowadays, things are far more complicated. For one thing, the entire machine tends to never be yours, so you cannot disable interrupts. Other threads will pre-empt your thread, and there is nothing you can do about it, you just have to accept some inaccuracy from it. Luckily, with modern multi-core CPUs this is not so much an issue as it used to be, but in modern VM-based languages like Java and C# we have additional and far more severe inaccuracies introduced by the garbage collection and the jitting. Luckily, their impact can be reduced.\nIn order to avoid inaccuracies due to jitting, we always perform one run of the code under measurement before the measurements begin. This gives the JIT compiler a chance to do its job, so it will not be getting in the way later, during the actual benchmark.\nIn order to avoid inaccuracies due to garbage collection, we always perform one full garbage collection before starting the benchmark, and we try to keep the benchmark short, so as to reduce the chances of another garbage collection happening before it completes. The garbage collection APIs of most VMs tend to be somewhat snobbish, and they do not really guarantee that a full garbage collection will actually take place when requested, so we need an additional trick: we allocate an object keeping only a weak reference to it, then we keep calling the VM to garbage collect and run finalizers until that object disappears. This still does not guarantee that a full garbage collection will take place, but it gives us the closest we can have to a guarantee by using only conventional means.\nSo, here is the class that I use for benchmarking, employing all of the above tricks:\npackage saganaki; import java.lang.ref.WeakReference; /** * Measures the time it takes to run a piece of code. * * @author Michael Belivanakis (michael.gr) */ public class Benchmark { private static final long NANOSECONDS_PER_MILLISECOND = 1000_000L; private final long durationInMilliseconds; /** * Initializes a new instance of {@link Benchmark}. * * @param durationInMilliseconds for how long to run the benchmark. */ public Benchmark( long durationInMilliseconds ) { this.durationInMilliseconds = durationInMilliseconds; } /** * Runs the benchmark, printing the results to {@link System#out} * * @param prefix text to print before the results. * @param runnable the code to benchmark. */ public void runAndPrint( String prefix, Runnable runnable ) { double iterationsPerMillisecond = run( runnable ); iterationsPerMillisecond = roundToSignificantFigures( iterationsPerMillisecond, 6 ); System.out.println( prefix + \u0026#34; \u0026#34; + iterationsPerMillisecond + \u0026#34; iterations per millisecond\u0026#34; ); } /** * Runs the benchmark * * @param runnable the code to benchmark. * * @return number of iterations per millisecond. */ public double run( Runnable runnable ) { //run the benchmarked code once, so that it gets JITted runnable.run(); //perform a full garbage collection to bring the VM to an as clean as possible state runGarbageCollection(); //wait for a system clock transition long currentNanos = System.nanoTime(); long startNanos = currentNanos; while( currentNanos == startNanos ) currentNanos = System.nanoTime(); startNanos = currentNanos; //run the benchmarked code for the given number of milliseconds long endNanos = startNanos + (durationInMilliseconds * NANOSECONDS_PER_MILLISECOND); long iterations; for( iterations = 0; currentNanos \u0026lt; endNanos; iterations++ ) { runnable.run(); currentNanos = System.nanoTime(); } //calculate and return number of iterations per millisecond. return iterations / ((double)(currentNanos - startNanos) / NANOSECONDS_PER_MILLISECOND); } /** * Runs a full garbage collection. * * See Stack Overflow: Forcing Garbage Collection in Java? */ private static void runGarbageCollection() { WeakReference\u0026lt;Object\u0026gt; ref = new WeakReference\u0026lt;\u0026gt;( new Object() ); for(; ; ) { System.gc(); Runtime.getRuntime().runFinalization(); if( ref.get() == null ) break; Thread.yield(); } } /** * Rounds a number to a given number of significant digits. * * See Stack Overflow: rounding to an arbitrary number of significant digits * * @param number the number to round * @param digits the number of significant digits to round to. * * @return the number rounded to the given number of significant digits. */ private static double roundToSignificantFigures( double number, int digits ) { if( number == 0 ) return 0; @SuppressWarnings( \u0026#34;NonReproducibleMathCall\u0026#34; ) final double d = Math.ceil( Math.log10( number \u0026lt; 0 ? -number : number ) ); @SuppressWarnings( \u0026#34;NumericCastThatLosesPrecision\u0026#34; ) final int power = digits - (int)d; @SuppressWarnings( \u0026#34;NonReproducibleMathCall\u0026#34; ) final double magnitude = Math.pow( 10, power ); final long shifted = Math.round( number * magnitude ); return shifted / magnitude; } } For an application of the above class, see my next post: Benchmarking Java 8 lambdas\n","date":"2014-07-18T15:45:12.883Z","permalink":"https://blog.michael.gr/post/2014-07-benchmarking-code-written-in-java-or-c/","title":"Benchmarking code written in Java or C# (or any GCed, JITted, VM-based language)"},{"content":"In my many years of experience in programming I have noticed that there are some programmers who refuse to use a debugger, or try to use the debugger as little as possible, as in, only when they run out of alternative options. They tend to rely solely on the diagnostic log to troubleshoot problems in their code, so their code tends to spew thousands of lines of log entries per second, and they keep trying to divine the causes of exceptions by looking at post-mortem stack traces.\nQuite often these people do not understand what usefulness others find in debuggers. I once requested the lead developer of a certain shop (Powernet, Athens, Greece, circa 2000) to enable debugging for me on their development web server so that I can run my debugger on the web site that I was developing in that shop, and she asked me \u0026quot;what do you need a debugger for?\u0026quot; Luckily, she proceeded to fulfil my request after a couple of long seconds of me staring blankly at her.\nListen folks, if you want to be called a \u0026quot;programmer\u0026quot; and if you want to be worth the cost of the keyboard you are pounding on, the debugger needs to be your absolute first tool of choice at the slightest need for troubleshooting, not your last tool of choice, not even your second tool of choice. Companies that develop IDEs go through huge pains to provide us with nice sleek and powerful debuggers so that we can do our job better, don't you dare let their efforts go to waste.\nA call stack trace in the diagnostic log of your program will tell you which function was called by which function, and that's all. This is enough in many simple cases, but when things get just slightly complicated, (and they usually do,) it is not enough. Lacking any additional information, what you end up doing is theorizing about what might have happened instead of looking and seeing what happened.\nIn addition to which function was called by which function, the debugger will also show you the values of the parameters to each call, and the values of the local variables within each call. For any variable which is an object, the debugger will show you the contents of that object, so in essence you have access to the state of the entire machine at the moment that the exception was thrown. When you have all that information at your disposal, then you can say that you are solving a problem. Anything less than that, and what you are actually doing is monkeying with the problem.\nSimilarly, in my career I have noticed lots of people who, when they want to perform a test run of the program that they are developing, always hit the \u0026quot;Run\u0026quot; button of their IDE instead of the \u0026quot;Debug\u0026quot; button. Listen folks, if you want to be called a \u0026quot;programmer\u0026quot; then the button you should be pressing is the \u0026quot;Debug\u0026quot; button. You should forget that the \u0026quot;Run\u0026quot; button exists. \u0026quot;Run\u0026quot; is for users. Programmers use \u0026quot;Debug\u0026quot;. Always \u0026quot;Debug\u0026quot;. Only \u0026quot;Debug\u0026quot;.\nStarting your program in debug mode does not mean that you are necessarily going to be doing any debugging; it just means that if some issue pops up during the test run, then you will be able to debug your program on the spot, rather than having to rely on the diagnostic log, or having to re-run the program in debug mode hoping that the issue will be reproducible. Modern development environments even support program modification while debugging, (they call it \u0026quot;edit-and-continue\u0026quot; in the Microsoft world, \u0026quot;hot swap\u0026quot; in the Java world,) so if a small problem pops up you might even be able to fix it on the fly and continue running.\nIf you don't want to take my word for it, you could take a hint from the key bindings of your IDE. In Visual Studio the description of the F5 key is \u0026quot;Run the application\u0026quot; while the description of Ctrl+F5 is \u0026quot;Run the code without invoking the debugger (Start without Debugging)\u0026quot;. As you can see, \u0026quot;Run the application\u0026quot; in Microsoft parlance means \u0026quot;Debug the application\u0026quot;, and the key combination for debugging is the simple one, while the key combination for running without debugging is the more complicated one. Similarly, in Eclipse, F11 is \u0026quot;Debug\u0026quot;, Ctrl+F11 is \u0026quot;Run\u0026quot;. In PyDev, the same. Obviously, the creators of these IDEs expect you to be running your program with debugging far more often than without. For me, it is 99.99% of the time with debugging, 0.01% without.\nSome people complain that application start up is slower when debugging than without; I have not noticed such a thing, but what I have noticed is that sometimes one might be making use of some exotic feature of the debugger without realizing it, (for example having forgotten a so-called \u0026quot;function breakpoint\u0026quot; or a \u0026quot;memory changed breakpoint\u0026quot; active) and that is slowing things down. Just make sure you do not have any fancy debugger features enabled unless you actually need them, and running your program with debugging should be about as fast as running it without debugging.\nRelated article: DotNet code running faster under the profiler?\n","date":"2014-07-14T06:28:52.744Z","permalink":"https://blog.michael.gr/post/2014-07-what-do-you-need-debugger-for/","title":"What do you need a debugger for?"},{"content":"\rA memo to developers all over the world about the pronunciation of the word \u0026quot;apache\u0026quot;.\nFolks, just so that you know, the world famous Apache Software Foundation which lends its name to the world famous Apache Web Server is not pronounced uh-pach; it is pronounced uh-pach-ee. The final letter is not a silent \u0026quot;e\u0026quot;, it is a loudly and clearly pronounced \u0026quot;e\u0026quot;.\nThere exist two words in English which are spelled \u0026quot;Apache\u0026quot;; one is of French origin, and according to dictionary.com it means \u0026quot;a Parisian gangster, rowdy, or ruffian\u0026quot;. This one does end in a silent \u0026quot;e\u0026quot;, but it is not the one that the Apache Software Foundation was named after.\nThe other word is of Mexican-Spanish origin, it means \u0026quot;a member of an Athabaskan people of the southwestern U.S.\u0026quot;, it ends in a definitely non-silent \u0026quot;e\u0026quot;, and it is the word you are looking for.\nHead over to dictionary.com to check out these two words and click on the little speaker icons to hear their pronunciation: http://dictionary.reference.com/browse/apache\nAlso, in the Wikipedia article about the Apache Software Foundation (http://en.wikipedia.org/wiki/Apache_Software_Foundation) we read:\nThe name 'Apache' was chosen from respect for the Native American Apache Nation, well known for their superior skills in warfare strategy and their inexhaustible endurance. It also makes a pun on \u0026quot;a patchy web server\u0026quot; (a server made from a series of patches) but this was not its origin.\nAnd as a side note to fellow USAians: The same applies to the world famous Porsche brand of cars: the final \u0026quot;e\u0026quot; is not silent. Please quit saying porsh; it is por-sheh. See: http://youtu.be/4OuPY-1snyw\n","date":"2014-06-04T15:29:12.571Z","permalink":"https://blog.michael.gr/post/2014-06-pronouncing-name-of-your-web-server/","title":"Pronouncing the name of your web server"},{"content":"\rNowadays the interwebz abounds with beautiful images of our Earth from orbit. Lately I have picked up the habit of trying to figure out what part of our world is visible when I see such an image. It is usually quite a puzzle, since the scale of the picture is not always obvious, parts of it are always obscured by clouds, the North can really be anywhere, and worst of all, countries are not painted with different colors! (Duh!) I am usually successful in this, but today I had a real tough one.\nA couple of seconds into Cosmos: S01E07, there is a picture of Earth from orbit. Click on the picture above and see if you can identify the visible land before reading further down.\nYou might think that it is really obvious, but then try to verify your hypothesis by comparing the picture above against google earth, and whoops, you see that you were wrong.\nSo, what's going on?\nReady for the solution?\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nWell, it is simple: it is, indeed, Gibraltar, but the image displayed in Cosmos has been mirrored. So, even though the Atlantic Ocean appears to the left, and the Mediterranean Sea to the right, Morocco is up, and Spain is down, instead of the other way around.\nWhy, oh why?\n","date":"2014-05-13T20:08:18.451Z","permalink":"https://blog.michael.gr/post/2014-05-picture-of-earth-from-orbit-in-cosmos/","title":"Picture of Earth from Orbit in Cosmos S01E07"},{"content":"If you live outside of Europe you might be lucky enough to have no idea what this is all about, but if you live in Europe you are probably sick and tired by now of this message popping up every time you first visit a site:\nThis site uses cookies to help deliver services. By using this site, you agree to the use of cookies. {Learn more} {Got it}\nThe creators of these sites are not to blame for these messages; they are being forced to display them against their will, (and waste money and resources in doing so,) in order to comply with EU regulations. These messages are mandated by law.\nI mean, really, how about this:\nThis site uses the Helvetica font to help deliver services. By using this site, you agree to the use of Helvetica. {Learn more} {Got it}\nOr this:\nThis site uses TCP/IP to help deliver services. By using this site, you agree to the use of TCP/IP. {Learn more} {Got it}\nAll these statements make precisely the same amount of sense: none.\nThe legislators who came up with the one about cookies are a bunch of technically illiterate ignoramuses who, in a fashion typical of politicians full of shit, have the audacity to be legislating on things they have absolutely no clue about. They should be removed from office and prohibited from ever holding any job other than milking goats.\n","date":"2014-05-02T00:19:21.873Z","permalink":"https://blog.michael.gr/post/2014-05-by-using-this-site-you-agree-to-use-of/","title":"\"By using this site, you agree to the use of cookies\""},{"content":"This question was sighted on stackoverflow.com on Thursday, April 30, 2013. It was deleted within 2 minutes from being posted, but not before I managed to take a screenshot of the summary.\nIt is funny when you can tell what's wrong with the code by just looking at the summary!\n","date":"2014-04-23T19:00:05.731Z","permalink":"https://blog.michael.gr/post/2014-04-stackoverflowcom-question-deleted/","title":"Stackoverflow-com question deleted within 2 minutes"},{"content":"A few days ago one of the svchost.exe processes on my machine (Win7 64) started exhibiting this annoying behavior: it will start with about 30 to 40 megabytes of memory, which stays roughly constant for a while, but then later it begins bloating, slowly but surely, possibly at a slightly exponential rate, until a few hours later it is taking up so many gigabytes that I cannot work on my computer anymore. So, I have to stop what I am doing, save everything, and restart the computer, only to have to go through the same ordeal a few hours later.\nOn at least two occasions I have witnessed this happening along with unreasonably high CPU utilization, up to a full CPU core.\nObviously, this started happening after I installed or tweaked something, but I did not notice the precise point in time that it started happening, and my machine is a busy machine, so I had no suspects to name.\nI looked around the interwebz for a solution, but to no avail. People give some good troubleshooting hints, but nobody seems to have an actual solution.\nThe svchost.exe process which causes the problem contains the following services:\nApplication Information (AppInfo) Background Intelligent Transfer Service (BITS) Certificate Propagation (CertPropSvc) Computer Browser (Browser) Multimedia Class Scheduler (MMCSS) Remote Desktop Configuration (SessionEnv) Shell Hardware Detection (ShellHWDetection) System Event Notification Service (SENS) Server (LanmanServer) Task Scheduler (Schedule) Themes (Themes) User Profile Service (ProfSvc) Windows Update (wuauserv) Windows Management Instrumentation (Winmgmt) I decided to give the process of elimination a try. I was able to terminate most of those services without seeing any difference in behavior. The one service which absolutely refuses to terminate is the Task Scheduler: Apparently someone at Microsoft has decided that scheduled tasks are such an awesome thing to have, that nobody in their right minds should ever want to try running Windows without them.\nSo, the culprit appears to be the Task Scheduler, but what can be done about it? How can we make it stop hoarding our precious RAM?\nI think I have found a solution.\nNow, this is a complex problem which could have different causes on different systems out there, so this may or may not solve your particular problem; however, it has solved my problem, and that's why I am saying I have found a solution, not the solution. You can read on for the possibility that your problem is the same as mine, or just for the possibility that my troubleshooting approach helps you find a solution to your particular problem.\nOpen up the Control Panel, open up Administrative Tools, and launch the Task Scheduler console. There are many, I mean many tasks in there, but luckily the one we are interested in is easy to find: From the tree on the left side, select Task Scheduler Library, and in the middle pane you will see a bunch of tasks scheduled for execution. Keep hitting F5 to refresh, and in the Last Run Time column you will see which one of those tasks keeps repeatedly firing: it is the one whose last run time keeps changing with every refresh. In my case the offending task was the Adobe Flash Player Updater. So, right click on that task, and select Disable. Voil?, CPU consumption goes to zero, and memory consumption goes down to the normal 40 megabytes or so. More importantly, they both stay there.\nSo, after all, in my case the culprit was not really the Task Scheduler, it was the Adobe Flash Player Updater. Thank you, Adobe! You rock!\nNot.\nCheers!\nOld comments\nAnonymous 2015-05-09 19:24:28 UTC\nOnly 9 comments so far? I guess everybody must be too lazy to say thank you, because I assume the number of people helped with this piece of information is higher by some orders of magnitude. While it may not have precisely fixed what I was looking for (or it indeed may, I'll see in the coming days), but in any case, just learning about the existence of this scheduler and how to modify it, is very valuable for all of us non-IT-professionals.\nUnspecified 2016-06-11 16:00:02 UTC\nHelp me please i'm dying right now, i've follow all the things you said on top, and i've read all the comments i was so happy that i found google updater BUT it wasnt the real problem, everytime i refresh it nothing changes. Please somebody get me out of this hell\nAnonymous 2016-05-05 15:02:06 UTC\nThank you so much! much appreciated\nCatherineAdams 2019-10-03 13:52:22 UTC\nThanks, worked for me also.\nAnonymous 2015-11-12 18:37:50 UTC\nTHANK YOU ! Worked perfectly. For me it was Google Update ..\nAnonymous 2014-12-06 16:53:20 UTC\nThank you - Google Updater was killing my system:)\nAnonymous 2015-07-08 19:55:37 UTC\nI'm having something very similar except it's on a Windows 2008 R2 server and the utilization spikes happen intermittently. There are no tasks in the Task Library, but there are a bunch of sub-folders with task inside. I have checked all of them and cannot find a start or stop time that coincides with my CPU spikes.\nMark 2014-02-14 08:32:20 UTC\nBrilliant ....I don't have a problem at the moment with this machine but that information is gold....I'd be willing to bet that's the problem with my old laptop...and this one has stinking Adobe running every hour. If Adobe spent as much time being useful as it does updating it'd be one amazing chunk of software.\npeter 2013-10-08 21:18:55 UTC\nThanks. You saved my sanity\nTommy Boy 2016-12-03 00:02:35 UTC\nFantastic information - now running good as new.\nUnspecified 2016-01-01 14:01:44 UTC\ni can't thank you enough mate, my problem was adobe fp updater too!! have a good year mate!! YOU ROCK!!\nAnonymous 2015-05-22 22:48:44 UTC\nThank you very much. Windows 7 32bit user with the same, totally frustrating problem. Fix Seems to be working.\nAnonymous 2016-05-10 04:23:15 UTC\nThank you!\nmichael.gr 2013-10-08 21:44:15 UTC\nGlad to be of help, Peter! C-:=\nAnonymous 2015-12-02 12:56:30 UTC\nUnfortunately, today my CPU is back to very high again! Something more sinister is going on by the looks of it, but I wouldn't have a clue - not exactly the computer wiz, me. :) Oh well... Disabling GoogleUpdate didn't have any effect btw.\nAnonymous 2015-06-01 02:53:21 UTC\nWorked for me, thanks! Turned out to be a web crawler that wasn't showing up in malware scans.\nAnonymous 2013-12-08 00:08:37 UTC\nLet me just say....if i could pay you for this amazing amount of help, I would go to the bank right now...THANK YOU GOD!!!\nAnonymous 2016-03-31 18:47:07 UTC\nSo after about 2 hours of scanning the living heck out of a client machine I found tons of junk but couldn't figure out what the cause. I saw that scheduler was in the list of processes running in the pid but I didn't think to look in it. This ended up being my problem with adobe flash running 2 tasks along with got to meeting running 2 as well. I did have to restart windows update for the changes to take effect but so far its holding. Thanks for the insight ever after nearly 3 years of your post.\nAnonymous 2016-11-22 12:49:10 UTC\nDidnt really work out for me, but as few ppl here mentioned it set me on right track with win update(wuauserv service)...thanks a lot\nAnonymous 2017-08-08 03:57:31 UTC\nHi Michael, Thanks a lot for your great post !\nit solved my CPU at 25% constant issue (now it's 0 ~ 2 % again as it must be when it's at idle ) :)\nIn my case the problem file was:\nWindows\\ServiceProfiles\\LocalService\\AppData\\Local\\FontCache4.0.0.0.dat\nI also found that's safe to erase all *.dat files in this folder to solve this problem. Hope this helps somebody else to !\nBest regards\n-Luis\nAnonymous 2015-08-04 21:50:09 UTC\nSaved my bacon! Thank you!\nAnonymous 2016-03-09 17:05:30 UTC\nOk so, Everything in that middle fame was disabled for me from the beginning, the newest last time run of 'em all is for about 1 week ago so i don't think it is related to this section, Anyways i still face the high cpu usage... I have no idea what should i do.\nAnonymous 2015-12-01 23:54:30 UTC\nBrilliant! Thank you! I can't find a Last Run Time column, but just disabled the Flash Player Updater - with immediate effect! I don't think I can resist the GoogleUpdateTaskMachineUA, so I'm going for that too!\nmichael.gr 2015-12-02 00:43:48 UTC\nLOL!\nverge 2015-05-20 09:11:27 UTC\nThank you this helped me as well\nmichael.gr 2015-05-20 09:34:34 UTC\nGlad to be of help! C-:=\nAnonymous 2015-06-29 04:18:08 UTC\nOMG - my laptop had become unusable due to svchost netsvcs using all my memory. I would never have thought to look at GoogleUpdate tasks as the culprit. But there it is, disabling those, I have a laptop which is no longer burning my lap, unable to open a new window.\nThank you for solving this and posting the solution!\nAnonymous 2014-12-17 23:42:22 UTC\nI observe a very similar problem a few mins after startup of my win7 64 bit machine and the two scheduled tasks which were running at that moment: GoogleUpdateTaskMachineCore GoogleUpdateTaskMachineUA Both to be run daily. I have deactivated both, will check if it fixed it. Adobe Flash Player Updater is the next suspect.\nThank you Michael for posting this!\nAnonymous 2016-04-13 07:39:24 UTC\nThank You, I have been struggling with this for at least a year (probably two) \u0026amp; I got sick of killing the svchost -process, especially as doing so also kills Aero, for me it's either Google or Adobe flash updater (I had to kill the process to open task scheduler, it hasn't restarted after I disabled adobe flash updater \u0026amp; tweaked the google tasks to not run when computer is being used)\nAnonymous 2016-04-13 08:32:49 UTC\nnope, disabled all tasks in task scheduler library, svchost -process eating 25% of CPU\nOutcastT2 2015-08-28 10:43:18 UTC\nThank you so much! It was Acrobat Reader Updater for me. I appreciate this. You saved me hours of annoyance, I'm sure.\nSting2001 2017-01-19 19:29:14 UTC\nIn my case it was the Windows Update service. Thank you!\nAnonymous 2016-04-13 08:43:35 UTC\nlooks like it might be Windows Update wasting my CPU while doing something, come to think of it, it's been over a month since I was last notified of new updates, I guess windows update is trying to do just that but is failing \u0026amp; somehow managing to hog 25% of CPU while doing the failing, any suggestions?\nmichael.gr 2016-04-18 08:54:33 UTC\nSorry Toni, I don't have any concrete suggestion to make for your scenario. Windows Update problems are a separate ocean of pain.\nUnspecified 2016-02-04 10:28:16 UTC\nHOLY SHIT! Thank you! You are a messiah and the liberator. My problem wasn't any of those pesky tasks, but windows updater. Apparently I hadn't been updating as much as Microsoft would like, so they decided to wreck my Windows' performance. Well, now the whole update-reminder crap is off and I intend to find a way to keep it that way.\nmichael.gr 2016-02-04 13:53:05 UTC\nI am glad to be useful, Paavo! C-:=\nmichael.gr 2014-05-09 17:07:40 UTC\nC-:= glad to be of help!\nAnonymous 2015-10-07 20:23:43 UTC\nThanks!\nSubin 2016-07-07 17:02:26 UTC\nThis Works Perfectly. Thanks a lot dude.\nUnspecified 2016-03-31 21:14:36 UTC\nMichael, you have just solved my problem, thanks.\nmichael.gr 2016-03-31 21:38:17 UTC\nI am very glad to help, Paulo! C-:=\nAnonymous 2015-05-29 00:06:09 UTC\nMy tasks are so screwed they won't even display for me to disable. It's just in a constant state of refresh. If I figure it out I'll let everyone know.\nFaraz Ali 2016-10-08 20:22:50 UTC\nThanks for the pro tip - I have been plagued by the svchost.exe hogging CPU cycles for over a year. I wasn't able to identify the culprit but disabled all Google update related tasks, Adobe Flash update and Dropbox update. I also uninstalled GoToMeeting. Let's see what happens after reboot\nAnonymous 2014-05-09 13:31:43 UTC\nTHANK YOU!!!!!!!!!!!!! This worked perfectly. For me it was Google Updater!\nSinac 2016-04-22 05:55:12 UTC\nthank you, it's work flash updater, like you\nUnspecified 2016-04-01 17:17:36 UTC\nTHANK YOU, Michael.gr!!!! AGAZILLIONBIGHUGS!\nmichael.gr 2016-04-03 01:34:10 UTC\nGlad to be of help, Beverly! These comments make it worthwhile for me! C-:=\nAleck 2016-07-06 16:51:07 UTC\nThank you!\n","date":"2013-07-07T22:06:25.193Z","permalink":"https://blog.michael.gr/post/2013-07-07-solved-svchostexe-high-cpu-and-memory/","title":"Solved: svchost-exe high CPU and memory"},{"content":"\rI had this happening on my development machine, (I use C# on VS2010) so I went over to The Code Project and asked if anyone else could reproduce it, and sure enough, it has been confirmed.\nSteps to reproduce:\nCreate a new project, make it a C# console application, use all default settings. Open the generated Program.cs file and replace its contents with the following:\nnamespace ConsoleApplication1 { public class ParameterAttribute: System.Attribute { } class Program { [Parameter( name:\u0026#34;foo\u0026#34; )] int Field1; static void Main( string[] args ) { } } } I know, there is an error in the code; please bear with me:\nMove the mouse pointer over the word \u0026quot;name\u0026quot; in the code, and click. In other words, place the caret inside \u0026quot;name\u0026quot;.\nI hope you have saved everything, because once you do that, Visual Studio 2010 disappears without even saying good bye. It panic-quits!\nMore information: I am on Win7 x64 SP1, and this is what I get from Visual Studio / Help / About / Copy Info:\nMicrosoft Visual Studio 2010 Version 10.0.40219.1 SP1Rel Microsoft .NET Framework Version 4.5.50709 SP1Rel Installed Version: Ultimate Microsoft Visual C# 2010 01019-532-2002102-70075 Microsoft Visual C# 2010 Microsoft Visual C++ 2010 01019-532-2002102-70075 Microsoft Visual C++ 2010 Microsoft Visual F# 2010 01019-532-2002102-70075 Microsoft Visual F# 2010 Microsoft Visual Studio 2010 Architecture and Modeling Tools 01019-532-2002102-70075 Microsoft Visual Studio 2010 Architecture and Modeling Tools UML® and Unified Modeling Language™ are trademarks or registered trademarks of the Object Management Group, Inc. in the United States and other countries. Microsoft Visual Studio 2010 Code Analysis Spell Checker 01019-532-2002102-70075 Microsoft Visual Studio 2010 Code Analysis Spell Checker Portions of International CorrectSpell™ spelling correction system ® 1993 by Lernout \u0026amp; Hauspie Speech Products N.V. All rights reserved. The American Heritage® Dictionary of the English Language, Third Edition Copyright © 1992 Houghton Mifflin Company. Electronic version licensed from Lernout \u0026amp; Hauspie Speech Products N.V. All rights reserved. Microsoft Visual Studio 2010 Team Explorer 01019-532-2002102-70075 Microsoft Visual Studio 2010 Team Explorer Microsoft Visual Web Developer 2010 01019-532-2002102-70075 Microsoft Visual Web Developer 2010 Crystal Reports Templates for Microsoft Visual Studio 2010 Crystal Reports Templates for Microsoft Visual Studio 2010 Microsoft Visual Studio 2010 SharePoint Developer Tools 10.0.40219 Microsoft Visual Studio 2010 SharePoint Developer Tools Microsoft Visual Studio 2010 Ultimate - ENU Service Pack 1 (KB983509) KB983509 This service pack is for Microsoft Visual Studio 2010 Ultimate - ENU. If you later install a more recent service pack, this service pack will be uninstalled automatically. For more information, visit http://support.microsoft.com/kb/983509. The related discussion at The Code Project can be found here: The Code Project: The Lounge: VS2010 C# intellisense crash - reproduce and confirm\n","date":"2013-07-07T20:30:51.11Z","permalink":"https://blog.michael.gr/post/2013-07-07-a-monstrous-visual-studio-2010/","title":"A monstrous Visual Studio 2010 intellisense bug"},{"content":"Techniques demonstrated: Ultra-fast font rendering Ultra-fast bitmap scaling Ultra-fast bitmap rotation Projection of 3D structures to 2D for rendering Invisible surface detection Ultra-fast rendering of lines and polygons directly into video RAM Gouraud shading Ultra-fast dithering A voxel rendering experiment A tiled floor rendering experiment Summary (just give me the TL;DR) A collection of very short YouTube videos of graphics demos that I did all by myself at home for fun back in the mid-nineties (when I was in my twenties) rendering pixels directly into the video RAM of the 320x200-pixel, 256-color palette VGA without the use of any libraries. Everything is in C or C++ with the crucial routines written in 80386 assembly, in some cases generating machine code on the fly.\nA longer description for those who like reading It was the end of 1993. The peak time of the 320x200-pixel, 256-color VGA. Around that time the world saw some legendary game releases such as Doom and Prince of Persia 2. The year before that, we had seen Wolfenstein 3D and Indiana Jones and the Fate of Atlantis. Wing Commander and Wing Commander II had already been released as early as 1990 and 1991 respectively. Most of those games were raster-graphics-based. Wolfenstein 3D was a weird thing that we did not quite know whether to call it raster-based or vector-based; Doom looked rather vector-based, so it was beginning to seem that vector graphics might be the way of the future; but when Lucas Arts released Star Wars: X-Wing, even the slightest doubt went away.\nMy available time for experimentation with vector graphics was limited, since I was attending classes at the University and holding a full time job, but I managed to put a few demos together. In order to run these demos today one would need a DOS emulator, but I will spare the reader from that hassle, by providing short videos of the demos instead. (Each video is only a few seconds long.)\nIt is worth noting that all these demos are DOS applications and they are not making use of any ready-made graphics libraries. A direct BIOS call puts the graphics adapter in 320x200x256 mode, (regular scan lines instead of the exotic Mode X,) and from that moment on it is just routines entirely written by me in 80386 assembly and C++ directly accessing the video RAM. Rendering is done top to bottom, left to right, which is inadvisable because it increases contention with video readout by the VGA hardware, but I did not care about that yet, I wanted to first get everything to work correctly before tweaking things to squeeze the maximum performance out of it.\nDistortions, jaggedness, and / or fuzziness that you might observe in the following videos are mostly due to the imprecise nature of interpolation via integer arithmetic. The skips that you will notice in the movement are to an extent due to the emulation of a DOS-era system under modern Windows, and mostly, due to the video capturing process. Natively, the demos used to run as smoothly as silk.\nI found a way to do interpolation by addition and extraction of the high-order word, (division by 65536,) which is essentially fixed point real number arithmetic. This has the advantage of not requiring a comparison and a conditional jump within the interpolation loop: the only jump is the loop jump. This would be even more advantageous on modern hardware, (if we were still doing vector graphics on the CPU, which we don't,) since it would not cause branch prediction to fail once every few pixels. Still, even back then, the fewer instructions made a considerable difference.\nText Scrolling and bitmap scaling This is the very first demo that I ever made. It might seem childish nowadays, but believe me, back then, to get something like this to run so fast, it was something.\nBitmap Rotation and Scaling The following demo shows rotation and scaling of a bitmap. The underlying algorithm performs line interpolations across the source bitmap to visit the pixels that it copies to the video RAM.\n\u0026quot;Ark\u0026quot; This was going to be a 3-D arkanoid, never made it though. I do not remember now how I produced the sounds, it is very likely that I was controlling the PC speaker. (Remember that?)\nAnimated 3-D objects - Wireframe The next demo is all about vector graphics. You will see various 3-D objects being animated against a procedurally generated backdrop which is made so as to give the illusion of a 3-dimensional room.\nOn the first run of the demo, the objects are rendered as wire frames. Edges that belong to surfaces that face towards (are visible by) the viewer are drawn in color, while edges that belong to surfaces facing away from (are not visible by) the viewer are drawn in black. Also, black edges are drawn before colored edges, so the former never cross over the latter. Double buffering is utilized to prevent flicker.\nAnimated 3-D objects - Solid On the 2nd run of the demo, the objects are rendered as solids. Without reading it anywhere, I discovered that if I was careful to define the points of each surface in a clockwise fashion when the surface is visible, then after rotation and projection the points would remain clockwise if the surface was still visible, but they would turn out counter-clockwise if the surface had become invisible. So, detection of \u0026quot;clockwiseness\u0026quot; is the simplistic mechanism that I am employing to detect and refrain from drawing invisible surfaces in this demo.\nAnimated 3-D objects - Shaded That was all very nice, but then in the summer of 1994 Lucas Arts released Star Wars: Tie Fighter, which I immediately bought, and I was astonished to find out that surfaces in that game were shaded in a way which created a very convincing illusion of curvature. Some sort of never seen before random dithering effect gave them a very computationally expensive look. I thought \u0026quot;this is impossible!\u0026quot;\nAt that point I decided that I could not keep reinventing everything by myself, and I had to hit the books. Luckily, in the options of the game there was a hint: the check box for enabling or disabling the shading read \u0026quot;Enable Gouraud shading\u0026quot;. So, I went to the University library to look up the term, and very quickly I had a pretty clear idea of how to do it. Unfortunately, another problem still remained: with the 256-color palette which was the best that you could hope for on a PC at that time, a simple interpolation across brightness values would yield awfully ugly results. (Banding.) I needed to somehow add dithering without spending more than a couple of clock cycles per pixel. Achieving such a feat seemed inconceivable, but obviously TIE Fighter was doing it, (the computationally expensive look,) so there had to be a way.\nThe solution came in the form of a nifty assembly language hack: I loaded a 16-bit register with a random pattern of zeros and ones, and every time I had a pixel value in the AL register and I needed to add noise to it, I would execute a circular rotation instruction on the random-pattern register, which would essentially yield a random bit in the carry flag. Then, I would execute a special instruction which adds the carry flag to the AL register, thus incrementing or not incrementing the AL register at random, in effect selecting or not selecting the next (brighter) palette entry at random. The result was just awesome.\nDespite what the demo says, I did not implement proper Gouraud shading, I just wrote a routine which draws surfaces of variable brightness, which is the basis of Gouraud shading, and I made this routine use dithering so as to overcome the limited palette constraint of the 256-color display. In order to do it properly I would have had to have a proper light source, and to compute the normal at each vertex, and then interpolate brightness values along each edge. Instead, I just used the Y coordinate of each scan line as a relative brightness value. The result is pretty cool, though it is a pity that I did not have the time to take it one step further and make it proper.\nHere is a screen capture showing the shaded sphere up close and personal, magnified from 320x200 to 1280x800 using pixel-resize to avoid blur, allowing detailed inspection of the quality of the rendered artifact. A careful look reveals that the randomness repeats periodically, and the period is exactly 16 pixels, that is, the width of the register which is being rotated.\nThe shaded sphere frozen in time and magnified for inspection. Click to enlarge. Flying in a voxel world At some point voxel based games came out, and they too looked very interesting. I tried to figure out how they work, I came up with a rough idea, and I tried to implement it. Please excuse the crudeness of the following demo, I did it in a weekend:\nTextured floor rendering Just some doodling around with floor rendering.\n","date":"2013-06-07T17:58:01.605Z","permalink":"https://blog.michael.gr/post/2013-06-07-vintage-stuff-from-my-days-of-320x200/","title":"Vintage stuff from my days of 320x200-pixel 256-color VGA"},{"content":"The development of a game like Darkfall Online in a country like Greece was a rather unlikely thing to happen, so I consider myself very lucky that it happened and that I was part of it. Here is the wikipedia page for the game: https://en.wikipedia.org/wiki/Darkfall\nThe funding came from a Greek-Libyan family which has made enough money out of oil-related construction to be able to afford the luxury of investing on something fun, rather than on something with a high ROI, or even a guaranteed ROI. Thus Razorwax, a promising team of Norwegians some of whom were already published in the gaming industry, was brought to Greece to build Darkfall. I was the first Greek programmer hired, and I worked primarily on the GUI of the game. I stayed with the company for about a year after the game was released.\nMe at Aventurine, in 2004. What follows is a rough diagram of the design of the GUI that I built for Darkfall: The graphics engine (written by a colleague in C++) provided me with just two asynchronous primitives, one for drawing textures, and one for drawing text. The layer we called middleware (written by another colleague, also in C++) provided me with the functionality of the \u0026quot;GetTextExtents\u0026quot; Win32 GDI function, and with an interface to the browser window (encapsulated instances of Microsoft Internet Explorer.)\nA rough diagram of the design of the Darkfall GUI. I abstracted these very few and very basic primitives inside a simple Engine Interface Layer, on top of which I built the GUI Framework Layer, which is basically a full-blown general-purpose graphical user interface written in Java, with fixed and resizable windows maintained in a z-order, adorned with borders and captions, supporting both modal and modeless dialogs having controls inside them, offering drag-and-drop functionality, etc.\nI also had to build all the controls that I needed from scratch: push/check/radio buttons, a text edit control, a drop-down list control, a tabbed panel control, a viewport control (adds scrollbars to any other control,) a slider, a progress bar, and even a fully functional tree control, complete with expand/collapse buttons, all of them highly customizable so as to support all of the requirements of Darkfall without actually being Darkfall-specific. The framework layer also interfaced with the input system, which was primarily written by me, partly in Java and partly in C++, interfacing with DirectInput.\nThen, on top of the framework layer I built the Darkfall layer, whose windows and controls extend those of the framework to offer the functionality, the look, and the feel required by Darkfall. This layer is quite extensive, since we are talking about an MMORPG, and it has a massive asynchronous interface with the gamelogic module, written by colleagues in Java.\nIn the following screenshot you can see the most frequently used windows of the Darkfall GUI: starting from the top-right corner and going clockwise: the minimap window, 3 party windows, two chat windows, the backpack, and the hotbar. The two small windows near the center of the screen are the target info window on the left, and the vital stats window on the right. The crosshair and the free-floating text above the vital stats window are both parts of the transparent HUD component which is child of the desktop.\nFull-screen view of the game showing a fully cluttered GUI. Click to enlarge. When I started working for Darkfall my experience with MMORPGs was very limited, but after a little bit of research I quickly learned one thing: the GUI quite often gets in your way as you are trying to play the game. This was not so much true with Everquest II, but it was quite evident in Horizons. In Darkfall we decided that we should have two modes of play, among which the user should be able to switch by pressing the right mouse button:\nGame mode, for interacting with the world, where: moving the mouse causes the camera to rotate and the crosshairs to point at things in the world; the left mouse button shoots the weapon you are holding; GUI mode, for interacting with the GUI, where: moving the mouse causes an arrow pointer to move about the screen; the left mouse button performs a standard GUI click. Therefore, I reasoned, when in Game mode we should automatically get rid of windows that are unnecessary for interacting with the world, and keep on the screen only what's necessary. When the user switches to GUI mode, then the extra GUI windows should automatically appear. When the user switches back to game mode again, those extra windows should automatically disappear.\nThus, in Darkfall it is possible to play the game like this:\nFull-screen view of the game showing an almost completely uncluttered GUI. Click to enlarge. I called this feature cluttering and uncluttering the GUI, (please excuse my non-native English,) and Claus Grovdal (our lead designer) liked it very much and said that it was a unique feature not quite seen done precisely in this way in any other game.\nI abstracted the mechanism that provides cluttering and uncluttering, and I provided two implementations, one using fading, (the extra windows fade in when going into gui mode and fade out when going to game mode,) and another using animation (the extra windows slide from outside the screen radially into their positions and out again.) The animating clutterizer was eventually dropped because of frame-rate concerns: it was thought that it would betray a low frame rate much more dramatically than the fading clutterizer would.\nMost windows in the Darkfall GUI have a little button on their border called the sticky button, which selects whether the window will be visible while in game mode or not. There are some exceptions to this, for example the options window never makes sense to show while in game mode, so it does not have a sticky button and it always gets uncluttered.\nWhat follows is a brief description of just a few of the windows that can be seen in Darkfall.\nThe chat window is one of the most frequently used windows of the game, especially if we take into account the fact that MMORPGs are considered by many to be nothing more than \u0026quot;chat applications with fancy graphics\u0026quot;. A player can have an arbitrary number of chat windows open, each one of which may have an arbitrary number of chat tabs in it. The buttons on the top-right corner of the window border are, from left to right: a drop-down slider for selecting the transparency of the window, a settings button which opens up a configuration dialog for that chat window, the sticky check-box, and a close button. I built the first version of the chat control, and later a colleague extended it to build the final version which features coloring, droppable item icons, and tooltips.\nTwo chat windows. Click to enlarge. The backpack window contains an instance of the Darkfall Inventory Control. It displays the items that the player is carrying with him. It may contain other containers, (i.e. a bag) which may in turn contain other containers ad infinitum. The percent-bar at the bottom shows how full the backpack is.\nThe backpack window. Other instances of the inventory control can be found in the bank window, the clan vault window, the vendor trade window, and each one of the two panels of the player trade window. Also, when you double-click on a container like a bag inside your backpack, what opens up is, of course, another inventory window.\nEvery item in the Darkfall GUI has an icon, and hovering the mouse over it shows a tooltip window with information about the item. Naturally you can drag-and-drop items between inventory windows, but you can also drop an item into the chat box. When you do that, other players can see the icon of the item, and its corresponding tooltip.\nA chat window with an item icon (offered by Corporal Buddy Silvercloud) and its tooltip. Click to enlarge. One window which you will not see in Darkfall is the GUI Debug window, because it is only available in the debug build of the game. It contains a tree control which displays the entire window hierarchy of the GUI, from the desktop component (the root) all the way down to the individual texture components (the leaf nodes) of which everything is comprised. A blinking frame is displayed around the original (unclipped) rectangle of the selected component, (in this case, one of the textures that make up the minimap,) and a tooltip shows extensive information about it. It is very useful for troubleshooting all kinds of issues that may pop up in the GUI during the course of development.\nThe GUI Debug window. Click to enlarge. The spells and skills windows also make use of the same tree component, and the fact that they look vastly different is testament to how customizable this component is.\nThe following two pictures show the hotbar window. It is a matrix of 10 by 10 placeholders where you can put all your weapons and spells for quick access, and where your currently selected weapon and currently selected spell are displayed. There is a set of key bindings for selecting which one of the 10 columns will be visible, and another set of key bindings for activating one of the 10 items in the currently visible column. In the first picture we can also see a progress overlay which shows that a certain spell is recharging, giving us a visual indication of how long it will take before we can use it again. The second picture shows the fully unfolded view of the hotbar, which is primarily used for dragging and dropping items from other inventory windows and from the spells window.\nIf you have lots of screen real estate to spare, you can actually play the game with the hotbar unfolded like this, but most people prefer it folded.\nThe component shown below is the minimap. It displays a small fraction of the world around the vicinity of the user, along with points of interest. You can zoom all the way in to 300% or out to 60%.\nThe minimap showing a coastline and some points of interest. The visible seams between the tiles are caused by imprecision due to scaling, and they are a defect that the game was initially shipped with.\nThe minimap with a tooltip showing all items found in the vicinity of the mouse cursor. Another frequently used window of Darkfall is the so-called Paperdoll Window. It has slots where the player can drop wearable items to wear them.\nThe Paperdoll window (1 of 2) The Paperdoll window (2 of 2) Another very important component of the Darkfall GUI is the HUD component. It contains the crosshair texture and it shows information about the item or player that is at any given moment pointed at. Darkfall is unique among MMORPGs in that it takes a first-person-shooter approach, and therefore the crosshair is the player's center of attention most of the time. For this reason, while playing the game I used to keep my targeting component (showing even more information about the targeted item) and my vital stats component (showing my health, my stamina, etc.) very close to the crosshair so as to be able to check their contents without much eye movement.\nIn the following picture I have my crosshairs on a rock outcrop; the HUD informs me that the pickaxe can be used on that item, and the target info window gives me more information about it.\nIn the following picture I have my crosshairs on another player, and I can see their rank, the clan they belong to, their name, and their health. The target info window tells me that the other player is an Alfar Male, in case I had not noticed.\nThis is an underwater view (in a swamp, hence the slimy green color,) where a breath bar is also visible, showing me how much time I have before I must resurface to breathe. The \u0026quot;Description of r_Alfar\u0026quot; text in the targeting component shows how unpolished the game was at the time of its release.\nThe party windows show other members of your party, (if you happen to belong to one,) and their health.\nWhat is shown above is the first version of the party window, which was kind of wasteful in terms of screen real estate. At some point we sent out a patch which, among other things, made party windows look like this:\nOf course this was a description of only a few of the windows found in Darkfall. I also made a multitude of other windows for the game, each with its own unique features and challenges. Those that I can remember right off the top of my head were:\nThe Worldmap window The Compass window The Skills window The Spells window The Effective Spells window (showing spells that are currently affecting the player) Various other inventory windows such as: The Bank window The Clan Vault window The Player Loot window The Monster Loot window etc The Vendor Interaction window The Crafting window The Enchanting window The Player Trade window The City Building (and Upgrading) window The Main Menu window The Options window The Video Settings window The Audio Settings window The Key Bindings window The Browser window (containing an encapsulated instance of Microsoft Internet Explorer) The following is a screenshot of the Character Creator, a module built by a colleague on top of my framework to allow players to customize their characters when first starting the game. This particular screenshot is taken from a development environment where some of the textures happened to be kind of messed up, so it is unique in a special way.\nThe Character Creator with a few textures messed up. Click to enlarge. Finally, a picture of (almost) the entire Darkfall crew, taken in Athens on October of 2008, about 5 months before the game was released. I am the guy holding his very surprised baby son in his arms. (By that time I had started trimming my hair very short.) I think only about 4 people are missing from this photo, unfortunately two of them being the only two women in the crew at that time.\nThe Darkfall crew, Oct 2008. Click to enlarge. Old comments\nAnonymous 2014-11-24 02:04:09 UTC\nThis GUI rocked.... DF1 best game ever just needed a skillcap and no grind.. then add onto it.. not 2.0\nAnonymous 2015-02-17 03:29:44 UTC\nThe gui was awesome good job, We dearly miss DF1.\nUnspecified 2016-12-17 16:45:02 UTC\nhttps://darkfallriseofagon.com/\nIt's back and better than ever. BPG is gonna take it to the top!\nAnonymous 2014-02-23 19:41:43 UTC\nImpressive how much time and effort was put into the game, you guys did a amazing job and i am glad i had the chance to fully support DFO.\nUnfortunately AV killed it.\nCheers\nAnonymous 2014-10-18 03:10:26 UTC\nDarkfall Online was the greatest game ever. It just needed a better economy (i.e. remove npc vendors) and more incentives for the \u0026quot;sheep\u0026quot; of the game to play so the \u0026quot;wolves\u0026quot; would have someone else to interact with.\nNinogan 2023-07-05 10:11:30 UTC\nWas cleaning through my bookmarks and found this article. Huge nostalgia and very informative. I haven't thought about or played any of the new Darkfall iterations like RoA in years and years but it was a good time back then!\n-Ninogan Swiftstep\nUnspecified 2021-07-25 00:06:37 UTC\nDo you have anymore Darkfall shirts? I've been searching all over for one!!!\nAnonymous 2014-02-24 00:21:11 UTC\nI prefer the the GUI in Darkfall Online over the GUI in Darkfall Unholy Wars!\nFnights 2015-05-16 20:30:36 UTC\nMaybe Darkfall 1 is coming back, with your gargeous GUI. https://forums.darkfallonline.com/showthread.php?409055-Announcement-regarding-DFO-we-are-still-investigating-technical-issues Cross fingers.\nAnonymous 2015-04-02 13:52:35 UTC\nI'm a huge fan of the UI you built. It was very hard to play the console-game crap built by your successor in DF:UW after 5 years of DFO.\nCeliah Ailey Ps: Lol, so your secret non-dev game character was Mega Hurts? Damn.. You were always running around naked in Tolenque at the time of the Yssam Rebellion. Hahaha. Good times! ^.^ Anonymous 2014-08-15 23:16:19 UTC\nAV killed the entie vision of the game with Unholy Wars..I feel sad for that the norweigans who originally came up with the idea of Darkfall now have to witness the trainwreck of a sellout Unholy Wars is.\nThe Darkfall vision is dead.\nRIP Darkfall Online.\nRIP Darkfall.\nmichael.gr 2015-04-02 17:15:03 UTC\nLOL! - The legendary Celiah Ailey! - Nice to hear from you again!\nYes, I was that guy. I played Darkfall for several months, my time as the resident pest of Tolenque was just a fraction of that time. But I remember that's where we met.\nWe had a blast, didn't we? Ah, the good days. C-:=\n","date":"2013-06-03T19:14:11.412Z","permalink":"https://blog.michael.gr/post/2013-06-03-a-few-samples-of-my-work-on-darkfall/","title":"A few samples of my work on Darkfall Online"},{"content":"This post shows some screen captures demonstrating work that I did on a Computer-Aided Civil Engineering application called \u0026quot;FESPA for Windows\u0026quot; back in 1996-1998 while working at SENA Ltd. on behalf of \u0026quot;LH Logismiki\u0026quot;.\n\u0026quot;FESPA for Windows\u0026quot; is now called \u0026quot;Master\u0026quot;. SENA Ltd. does not exist anymore, but LH Logismiki does, and their website can be found here: http://www.lhlogismiki.gr/. I took these screen captures from their web site, and apparently the modules that I built for them back in 1998 have not changed much, 15 years later. Even most of the icons that I designed for them are still the same.\n3DV on black background #1. Click to enlarge. The above is a screen capture of the \u0026quot;3DV\u0026quot; module, designed and written entirely by me. It is from the early black-background days, before we were told to switch to white background to \u0026quot;make it look more like Microsoft Word\u0026quot;. 3DV can pan the structure up, down, right and left, zoom it in and out, and rotate it in any way the user likes. Using data produced by the finite-element analysis module, it can also show the distortions that the building undergoes due to static loads, or in the event of an earthquake. It can even animate the distortion of the whole building. (Pretty fancy stuff!)\n3DV on black background #2 . Click to enlarge. The wireframe representation of the building is done using plain GDI calls to draw lines \u0026amp; text, and BitBlt to draw the nodes. Flicker is eliminated by means of simple double buffering: the entire image is drawn on an off-screen bitmap, which is then copied to the screen with BitBlt.\n3DV on a white background panel #1 . Click to enlarge. Here is the final \u0026quot;ms-wordesque\u0026quot; white background look with which the application was released to the market: the left and top-right panels contain modules written by colleagues, showing different kinds of ground plans. The bottom-right panel contains an instance of the 3DV module. Here 3DV displays the building without perspective; I was told that Civil Engineers tend to prefer this kind of view.\n3DV on a white background panel #2 . Click to enlarge. In the above screen capture 3DV is on the top-left panel, with perspective again switched off. Along each member it displays a torque diagram, using data obtained from the finite-element analysis module. On the right side two other modules display different kinds of 3D views of the building. Back in 1998 these modules had not been implemented yet.\nTables on two panels, 3DV on another, something else in the center. . Click to enlarge. On the left side and on the top-right side we can see instances of the \u0026quot;Tables\u0026quot; module, also designed and written entirely by me. It looks and feels like a spreadsheet, allowing the user to navigate through large amounts of mostly numeric data and edit single cells or multiple cells at once, or apply bulk operations on selected cells. On the middle panel there is a ground plan module (written by a colleague) and on the bottom-right panel is another instance of 3DV.\nMaximized 3DV with member properties and node properties. . Click to enlarge. Here we have a maximized view of the 3DV module, allowing us to see some detail. Most, if not all of those icons on the toolbar were made by me. I was actually surprised to find these screenshots on the web site of LH Logismiki in May of 2013 because they show that this module has not had to be altered by much over a period of 15 years . On the left side we can see a selected vertical member (thick red) and a modeless dialog allowing us to view and edit its properties, while on the right side there is a selected node and its properties.\nMaximized 3DV with force diagrams #1 . Click to enlarge. In the above picture we can see 3DV maximized, showing various kinds of force diagrams along horizontal and vertical members of a building rendered with perspective.\nMaximized 3DV with force diagrams #2 . Click to enlarge. All kinds of force diagrams here. As you can see, I made 3DV capable of displaying quite a lot of information.\nMaximized 3DV with force diagrams #3 . Click to enlarge. Another view with diagrams, this time along with the modeless dialog box through which you can select what kinds of forces you want to see on the diagrams and how you want to see them. The sliders are for magnification.\n3DV animating a building under stress. . Click to enlarge. A screenshot of 3DV animating the distortions of a building. A standing-still light gray wire frame shows the building at rest while the colored wire frame twists, shakes and rocks. The modeless dialog contains the controls for starting and stopping the animation, the slider for selecting the speed of the animation, and the sliders for magnifying the distortion along each axis so as to make the effect more visible (that is, more dramatic!)\n","date":"2013-05-30T14:02:25.662Z","permalink":"https://blog.michael.gr/post/2013-05-30-vintage-stuff-from-my-days-of-visual-c/","title":"Vintage: my work on FESPA for Windows (Visual C++ and MFC)"},{"content":"The TL;DR version of this post:\nDepress as simultaneously as possible, and keep holding down for about a second, both the home button and the power button.\nThe order in which the buttons are depressed does not matter, but simultaneity matters: from the moment that one of the buttons has been depressed, there is an extremely small window of time, perhaps as small as one tenth of a second, within which the other button must also be depressed, or else no scweesho fo joo!\nYou will know that you have managed to take a screenshot when your phone will emit an oh-so-vintage camera shutter sound, a bright white frame will momentarily appear along the borders of the screen, and a \u0026quot;picture\u0026quot; icon will take seat in the notification area.\nThe oh-how-much-I-love-writing version of the post:\nUnfortunately, Android does not offer an intuitive, and more importantly discoverable, way to take a screen capture. As a result, the information describing how to achieve this task is quite elusive, and obtaining it is a hard quest in and of itself. Not only that, but even if you find the information, it turns out that the task involves a cumbersome key combination and it requires an amazing skill level at nimbleness of the fingers, so quite often people cannot make the magic happen even if they are told how to do it; subsequently, they consider the information wrong and continue on their quest for the right magic words. The whole process is shrouded with rumors. It is embroidered with fables. It is the stuff that legends are made of. Things are only made worse by the fact that most people out there need to make use of this feature rarely, so even if they learn how to do it once, by the time they need it again, they have usually forgotten how it is done.\nYes, people need this feature rarely, but then again considering that the number of people using Android all over the planet at the time of writing this is almost 1 billion and rising, I estimate that at any given moment there must be hundreds of people out there trying to find instructions on how to take a screenshot with android, or fumbling with the buttons trying to make the instructions happen.\nSo, when you need this piece of information what you usually do is that you hit the interwebz, but it is funny how there appears to be virtually nobody out there who will give a clear and unambiguous description of precisely how to achieve this task. That's why I decided to write this post.\nNow, a couple of suggestions to the Android development team should any one of them ever happen to read this blog post of mine:\nMake the screen capture feature discoverable. Among the options offered by the \u0026quot;Device Options\u0026quot; dialog that pops up when you long-press the power button, there could easily be an option called \u0026quot;Screen Capture\u0026quot;. Selecting this option would of course dismiss the \u0026quot;Device Options\u0026quot; dialog before taking the screenshot, and afterwards a message could pop up informing the user that next time they need to take a screenshot they can try their luck with such and such tricky and cumbersome key combination, check here to never show this again, thank you.\nMake the screen capture feature less cumbersome. This means not only easier to perform, but also easier to explain. The requirement that both buttons must be pressed within a 100 millisecond or so window is senile, because it is precisely those two things: hard to perform, and hard to even explain. Users should be required to hold down one of the two buttons first, and then they should be allowed all the time in the world (or at least an entire second if eternity is not an option) to find and press the other button for the screen capture to happen. The way it is now it is broken; fix it.\n","date":"2013-05-27T18:34:02.795Z","permalink":"https://blog.michael.gr/post/2013-05-27-screen-capture-screenshot-under-android/","title":"Screen Capture (Screenshot) under Android 4"},{"content":"If I knew there are no macros in VS2012 I would have saved myself the trouble of upgrading from Windows XP to Windows 7 and reinstalling all my stuff from scratch. The all caps menu is fixable; the awful colors are fixable; the dreadful icons I can live with; the rest of the god-awful ugliness I can live with; but the macros? I cannot work without macros! So, I am reverting to VS2010 and sticking to it for now.\nAside from that, the operating system upgrade means that I am now 64-bit! Going up in life! C-:=\n","date":"2013-05-07T21:52:44.066Z","permalink":"https://blog.michael.gr/post/2013-05-07-visual-studio-2010-up-until-update-2-is/","title":"Microsoft Visual Studio 2012 (up until update 2) is lame!"},{"content":"This is so funny I had to share it.\nSomewhere in some troubleshooting forum (it does not matter where) a certain technical issue is being discussed (it does not matter what) and user 'tattoodavie' leaves the following comment:\ni have the same problem but i have windows 7 ultimate 7600 installed on an asus X83Vb-X2 notebook, but i also have another problem... i cant get into the bios and set the clock, change the settings... nothing...its password protected. it was my ex-girlfriends, she bought the laptop, but gave it to me for me for my birthday, turns out it came from a pawn shop, which one i will never know, seeing as how we cant talk to eachother any more (court order) i have no clue how i can clear the password. I have had it for some time now, and have grown too attached to it to. i just installed windows 7 and the new NVIDIA 182 driver for its GeForce 9300m GS 512mb graphix card. and now every time i restart this thing it sets the time and date back to 12/05/2008. I assume thats the date it was built. any ideas what the he** is going on?\nOkay, it is from here: social.technet.microsoft.comWindows 7 64 bit and Asus P5Q BIOS issue\n","date":"2013-05-04T10:43:16.656Z","permalink":"https://blog.michael.gr/post/2013-05-04-tattoodavies-woes/","title":"tattoodavie's woes"},{"content":"![[media/spring-in-action-cover.png]]My notes on the \u0026quot;Spring in Action\u0026quot; book by Craig Walls and Ryan Breidenbach from Manning Publications Co.\nPage 45\nThe author suggests that there is an ongoing debate on the issue of constructor injection vs. setter injection, and provides arguments for each one of them. Unfortunately, in the arguments that he provides for constructor injection he paints a straw-man, while his arguments for setter injection are all non-issues. It is no wonder then, that at the end of the discussion his personal preference turns out to be setter injection.\nArguments for constructor injection:\nOf course, this assumes that the bean's constructor has all of the bean?s dependencies in its parameter list.\nNo, constructor injection does not assume that; instead, it requires it, because that's precisely the point of constructor injection. It is quite unfair to take the central premise of constructor injection and reword it as a drawback. Perhaps the author meant to say instead that constructor injection assumes that all of the dependencies are available at the time of the construction of the bean. Well, the only case in which a dependency might be unavailable at construction time is if the design of our system has a circular dependency built into it. In which case the flaw is with our design, not with the concept of constructor injection! Correspondingly, I favor the use of constructor injection everywhere, and in the rare and unfortunate case when a circular dependency exists and is not easy to factor out, the exceptional use of setter injection as a self-documenting means of temporarily circumventing the problem.\nBy only allowing properties to be set through the constructor, you are, effectively, making those properties immutable, preventing accidental change in the course of application flow.\nThis is taking a very important benefit of constructor injection and wording it as if it was a minor advantage. The issue is not just the possibility of accidental change; the issue is the fundamental object-oriented concept of encapsulation, which mandates that a class should not expose its internal members to the outside world. The dependencies of a class are, by definition, its own private business. By making every single dependency of a class available for anyone out there to modify via a setter method, encapsulation goes out the window. The dependencies might as well be declared as public non-final members, it would make no difference.\nAlso, the issue of immutability goes beyond just the members of the bean; if all members of the bean are immutable, then the bean itself becomes immutable. The benefits of immutability are widely known, and I should not need to list them here, but let me briefly just mention thread-safety-for-free, reduction of overall program complexity as a direct result of statelessness, and the ability to use an immutable class as a key in a map.\nArguments for setter injection:\nIf a bean has several dependencies, the constructor's parameter list can be quite lengthy.\nSo what? By the same token, if setter injection is used, then the list of setters will be quite lengthy. This is not an argument.\nIf there are several ways to construct a valid object, it can be hard to come up with unique constructors, since constructor signatures vary only by the number and type of parameters.\nOh, puh-leez! if there are several ways to construct the bean, (a small possibility,) and if it is hard to come up with unique constructors, (a minutely small possibility,) then someone, somewhere, will have to squeeze their brain a little bit more to come up with unique constructors. If one does not want to have to think at work every once in a while, maybe they should not have become a programmer. Also: with constructors, there are only so many ways to construct a bean as there are constructors for it. With setters, there exist something of the order of number-of-setters-factorial number of ways to construct it. What do you prefer?\nIf a constructor takes two or more parameters of the same type, it may be difficult to determine what each parameter's purpose is.\nYeah, sure. Let me help: the 1st parameter will have purpose A, and the 2nd parameter will have purpose B. And if you ever find yourself in doubt, hover your mouse over the invocation, and your IDE will be very happy to clarify it for you. (Or use a better language, like C#, which allows you to supply parameter names in invocations.)\nConstructor injection does not lend itself readily to inheritance. A bean's constructor will have to pass parameters to super() in order to set private properties in the parent object.\nWha-what? Constructor injection does not lend itself readily to inheritance? Is the author ignorant about the concept of constructors or the concept of inheritance? Of course a bean's constructor will have to pass parameters to super, and it will have to pass exactly those parameters that the super requires, and that's precisely what guarantees proper instantiation of the super, and that's the whole point with using constructors, and a very important premise of inheritance. Again, a feature presented as a drawback. It is not a sign of a wise man to make the same mistake twice.\nIn short, I do not believe that there really is a debate; it is just that eternal difference in world views between the real engineers, who are compelled to build compiler-enforced robustness into their software, and those who are programmers by accident, and prefer a certain cozy amount of sloppiness sprinkled everywhere for the sake of some ill-perceived ease in coding, testing, or refactoring. It really is the same short-sighted mentality that says \u0026quot;I make all members public because this way I can access them easier.\u0026quot; Part of the problem might also be some ignorance-induced superstition, on behalf of the second group of people, against the use of constructors. I have seen it again and again: many people simply do not get constructors. And people tend to fear the unknown.\nPage 62\n\u0026quot;Furthermore, if a constructor has multiple constructors, {...}\u0026quot; should read \u0026quot;Furthermore, if a bean has multiple constructors, {...}\u0026quot;.\nPage 80\n\u0026quot;It just so happens that the Harry, {...}\u0026quot; should read \u0026quot;It just so happens that Harry, {...}\u0026quot;.\nPage 85\n\u0026quot;Even though it's prototype scoped, the guitar method would only be injected {...}\u0026quot; should read \u0026quot;Even though it's prototype scoped, the guitar bean would only be injected {...}\u0026quot;\nPage 141\n\u0026quot;Spring's basic autoproxy facility is fine for working with simple advice or when in a pre-Java 5 environment. But if you're targeting Java 5, you may want to consider Spring's support for AspectJ's annotation-based aspects. Let's see how to create aspects in Spring that are annotation based.\u0026quot;\nWell, I am kind of disappointed by the fact that I had to read 24 pages of \u0026quot;Spring's basic autoproxy facility\u0026quot;, which is another way of saying \u0026quot;advising the hard way\u0026quot;, in order to finally reach section 4.3.2 \u0026quot;Autoproxying @AspectJ aspects\u0026quot; where advising begins to make sense. Couldn't we have just skipped most of that stuff?\nPage 173\nIt's like the Pareto principle2 flipped on its head; 20 percent of the code is needed to actually query a row while 80 percent is just boilerplate code.\n--excellent observation!\n\u0026quot;This is all the more reason to let a framework deal with the boilerplate code so that we know that it written once and written right.\u0026quot; should read \u0026quot;This is all the more reason to let a framework deal with the boilerplate code so that we know that it is written once and written right.\u0026quot;\nPage 211\n\u0026quot;it's better to keep thing separate\u0026quot; should read \u0026quot;it's better to keep things separate\u0026quot;\nPage 239\nThe word \u0026quot;suspect\u0026quot; should be \u0026quot;suspend\u0026quot;.\n","date":"2013-03-27T16:22:49.59Z","permalink":"https://blog.michael.gr/post/2014-09-my-notes-on-spring-in-action-manning/","title":"My notes on \"Spring in Action\" (Manning)"},{"content":"These are my notes on the book \u0026quot;Clean Code\u0026quot; by Robert C. Martin.\nI am in agreement with almost everything that this book says; the following notes are either quotes from the book which I found especially interesting and/or especially well written, or they point out issues that I disagree with, or errata which objectively need correction. If some point from the book is not mentioned below, then I agree with it.\nPage xxii (Foreword):\nBack in my days working in the Bell Labs Software Production Research organization (Production, indeed!) we had some back-of-the-envelope findings that suggested that consistent indentation style was one of the most statistically significant indicators of low bug density. We want it to be that architecture or programming language or some other high notion should be the cause of quality; as people whose supposed professionalism owes to the mastery of tools and lofty design methods, we feel insulted by the value that those factory floor machines, the coders, add through the simple consistent application of an indentation style. To quote my own book of 17 years ago, such style distinguishes excellence from mere competence.\nPage 6:\n[?] what if you were a doctor and had a patient who demanded that you stop all the silly hand-washing in preparation for surgery because it was taking too much time? Clearly the patient is the boss; and yet the doctor should absolutely refuse to comply. Why? Because the doctor knows more than the patient about the risks of disease and infection. It would be unprofessional (never mind criminal) for the doctor to comply with the patient. So too it is unprofessional for programmers to bend to the will of managers who don?t understand the risks of making messes.\nPage 13:\nWe are willing to claim that if you follow these teachings, you will enjoy the benefits that we have enjoyed, and you will learn to write code that is clean and professional. But don?t make the mistake of thinking that we are somehow ?right? in any absolute sense. There are other schools and other masters that have just as much claim to professionalism as we. It would behoove you to learn from them as well. Indeed, many of the recommendations in this book are controversial. You will probably not agree with all of them. You might violently disagree with some of them. That?s fine. We can?t claim final authority. On the other hand, the recommendations in this book are things that we have thought long and hard about. We have learned them through decades of experience and repeated trial and error. So whether you agree or disagree, it would be a shame if you did not see, and respect, our point of view.\nPage 14:\nIndeed, the ratio of time spent reading vs. writing is well over 10:1. We are constantly reading old code as part of the effort to write new code.\nPage 32\nwww.fitnese.org should read www.fitnesse.org\nPage 35\nI find all this \u0026quot;very small methods\u0026quot; discussion rather misdirected. If you do this you end up with a multitude of tiny methods that have very long and hard to read and understand names (necessary to capture their painstakingly narrow specificity) and whose existence is atrociously artificial: they serve no purpose other than to pre-chew a perfectly sized meal into tiny pill-sized chunks that might be swallowed without chewing. I think it is not pragmatic, that is, not feasible in real-world scenarios. From the narrative my guess is that the author has never really practiced it in reality: he only briefly saw it in a few screens of source code from a colleague's project.\nThe absurdity continues with the classification of methods into four categories, depending on whether they accept zero, one, two, or the incredibly high number of three arguments. (Four arguments being, presumably, unthinkable.) Puh-leez, gimme a break! As Albert Einstein said, things should be as simple as possible, but not simpler. Unless it is violating the \u0026quot;Single Responsibility Principle\u0026quot; (SRP), a method needs to accept as many arguments as necessary to do its job. There are two ways to reduce the number of arguments to a method which is not violating the SRP: one is to introduce more member variables to the class which contains the method, (which is evil, because they are state,) and the other is to introduce new classes, the sole purpose of which is to group values together so as to give the appearance of a single parameter. But introducing more (artificial) entities in a system is far worse than having a few methods with plenty of arguments. The presence of lots of methods with lots of arguments might be taken as a hint that perhaps we are missing an entity that our system really needs, and nothing more. If our system does not really need a new entity, then methods with plenty of arguments is the way to go, really.\nLuckily, on page 176, Chapter 12: Emergence, in the section titled Minimal Classes and Methods the book provides somewhat of a correction, by stating:\nIn an effort to make our classes and methods small, we might create too many tiny classes and methods. So this rule suggests that we also keep our function and class counts low\nand\nHigh class and method counts are sometimes the result of pointless dogmatism.\nPage 46\nThe \u0026quot;real solution\u0026quot; offered seems wrong to me.\nPage 48\nSome programmers follow Edsger Dijkstra's rules of structured programming. Dijkstra said that every function, and every block within a function, should have one entry and one exit. Following these rules means that there should only be one return statement in a function, no break or continue statements in a loop, and never, ever, any goto statements.\nOkay, no problem with the \u0026quot;never, ever, any goto statements\u0026quot;. Actually, I am glad the author begins by saying \u0026quot;Some programmers follow...\u0026quot;, indicating that he included this piece of nonsense in the book only because it is popular enough to be perhaps worth mentioning, not because he really adheres to it, nor to actively suggest that the reader should adhere to it. I think the author should have either skipped such a lame \u0026quot;argument from authority\u0026quot;, or he should have provided justification for it, so as to make it an argument by reasoning rather than an argument from authority. But I am afraid that in this case ol' Edsger is not backed by any valid reasoning.\nPage 49\nIn the end, I wind up with functions that follow the rules I've laid down in this chapter. I don't write them that way to start. I don't think anyone could.\nOkay, I am glad we are in agreement on this one.\nPage 53\nChapter 4: Comments : The author is entitled to have his own opinion about comments. I have my own opinions, which are very different from his.\nPage 100 - \u0026quot;Data Transfer Objects\u0026quot;\nThe quintessential form of a data structure is a class with public variables and no functions. This is sometimes called a data transfer object, or DTO. DTOs are very useful structures, especially when communicating with databases or parsing messages from sockets, and so on. They often become the first in a series of translation stages that convert raw data in a database into objects in the application code.\nSomewhat more common is the \u0026quot;bean\u0026quot; form shown in Listing 6-7. Beans have private variables manipulated by getters and setters. The quasi-encapsulation of beans seems to make some OO purists feel better but usually provides no other benefit.\n(emphasis mine.)\nPage 176\nConsider, for example, a coding standard that insists on creating an interface for each and every class. Or consider developers who insist that fields and behavior must always be separated into data classes and behavior classes. Such dogma should be resisted and a more pragmatic approach adopted.\nThis brings to mind certain funny ways of doing things at my current workplace.\nPage 185\nRecommendation: Learn these basic algorithms and understand their solutions.\nUh, esqueez me, algorithms don't have solutions. Problems have solutions. Luckily, what was described was not algorithms, it was problems.\nPage 187\n\u0026quot;Attempts to repeat the systems can be frustratingly\u0026quot; should read \u0026quot;Attempts to repeat the failures can be frustrating\u0026quot;.\n\u0026quot;trial an error\u0026quot; should read \u0026quot;trial and error\u0026quot;.\nPage 272\nWhere it says \u0026quot;any function that used to take an int for a month, now takes a Month enumerator\u0026quot; the word enum should have been used instead of enumerator.\nPage 276\nI also deleted all the final keywords in arguments and variable declarations. As far as I could tell, they added no real value but did add to the clutter. Eliminating final flies in the face of some conventional wisdom. For example, Robert Simmons strongly recommends us to \u0026quot;. . . spread final all over your code.\u0026quot; Clearly I disagree. I think that there are a few good uses for final, such as the occasional final constant, but otherwise the keyword adds little value and creates a lot of clutter. Perhaps I feel this way because the kinds of errors that final might catch are already caught by the unit tests I write.\nAnd clearly, I agree with Robert Simmons. First of all, the 'final' keyword is compiler - enforced documentation, which is the best kind of documentation: when I see an entity marked as final, I know right away that I do not have to worry about the possibility that it may change as I read the rest of the code. Thus, the 'clutter' argument is blown out of the water. Secondly, there is never a valid reason to let a run time test catch an error that could have been caught at compile time: it is a well documented fact in the industry that the sooner you catch an error, the less it costs to fix it. And last but not least, a non-final entity is immensely more complex than a final entity. Therefore, the more entities are final in the code, the smaller the overall complexity of the code. And we strive for less complex code, don't we?\nPage 282\n\u0026quot;So I pushed it up\u0026quot; should probably read \u0026quot;So I pushed it down\u0026quot;, since judging by many other sentences, the author considers \u0026quot;down\u0026quot; to refer to descendants and \u0026quot;up\u0026quot; to refer to ancestors. (See page 279, \u0026quot;So I pushed it down\u0026quot;, and page 282, \u0026quot;the getDayOfWeek method is another one that should be pulled up from SpreadSheetDate\u0026quot; and \u0026quot;I pulled the implementation up into DayDate\u0026quot; and \u0026quot;So I pulled them all up from SpreadsheetDate\u0026quot; etc.)\nPage 283\n\u0026quot;making the all three methods much clearer\u0026quot; should read \u0026quot;making all three methods much clearer\u0026quot;.\nPage 300\nAnd in the FEET_PER_MILE case, the number 5280 is so very well known and so unique a constant that readers would recognize it even if it stood alone on a page with no context surrounding it.\nThe world is not the USA. Most of the world uses the metric system, and knows very little about miles and feet. When given a number in \u0026quot;United States customary units\u0026quot;, many people outside the USA know how to convert it to metric, but it is rare to meet someone who is proficient in converting from metric to US units, and even more rare to meet someone who has the slightest clue, or the slightest interest, in converting between US units. Therefore, the number 5280 means absolutely nothing to the vast majority of the population of the planet. As a matter of fact, there are so many non-USAians working as software engineers in the USA, that even in the USA it would be a bad idea to assume that anyone who sees the number 5280 in a piece of code will know what it stands for.\nPage 301\nIn \u0026quot;Declaring a variable to be an ArrayList when a List will due is overly constraining\u0026quot;, the word \u0026quot;due\u0026quot; should be \u0026quot;do\u0026quot; instead.\nPage 302\n\u0026quot;G30: Functions Should Do One Thing\u0026quot; -- no, functions should do as many things as necessary to accomplish their job with no duplication of code. Splitting functions into much smaller functions looks good as an exercise on paper, but in real life scenarios things are not so beautiful. A clean cut purpose will have to be devised for each one of the smaller functions, and a nice descriptive name will have to be invented for it. Lots of local variables may have to be passed as parameters to it, and more than one of them may need to be modified, which is impossible in Java. (That's one of the reasons why I love C#.) And at the end of all this, the reader will still be left wondering why it is a separate function when it is only used from one and only one place in the code. These problems could be alleviated by introducing more classes into the system, but then again I do not think that a design with one class more than necessary is better than a design with one method longer than necessary. Of course my way of doing things requires comments: there where each piece of code would be documented with a self-explanatory method name, I would preface that same piece of code with a descriptive comment within a long method. That's why I also disagree with the author on the issue of comments.\nPage 302\n\u0026quot;You must saturate the gradient before you can reticulate the splines, and only then can you dive for the moog.\u0026quot; -- wow, sounds very cool, but it will be confusing to non-native speakers of English, who are bound to try to look for meaning in the sentence, whereas, I suppose, there is none. This is really a minor issue, I am not really complaining about it. I have to admit that it adds some fun to the reading even for a non-native speaker such as me. On the other hand I am afraid that this book will also be read by plenty of people whose English is just barely up to task, and who will, undoubtedly, hit their dictionaries in an effort to discover the meaning of these words, and failing to make sense after examining all synonyms in their language, they will probably call a friend who knows better English, only to puzzle them too with a sentence that was not meant to make any sense in the first place.\nPage 302\nYou might complain that this increases the complexity of the functions, and you'd be right. But that extra syntactic complexity exposes the true temporal complexity of the situation.\nVery well said. Einstein's famous quote is also about the exact same thing: \u0026quot;Things should be as simple as possible. But not more simple.\u0026quot;\nPage 311\nIn the sentence ?choose names the reflect the level of abstraction of the class or function you are working in? the first \u0026quot;the\u0026quot; should be \u0026quot;that\u0026quot;.\nPage 312\nWhere it says \u0026quot;This is emphasized by the fact that there is a function named renamePage inside the function named doRename!\u0026quot; it should say \u0026quot;This is aggravated by the fact that there is a call to a function named renamePage inside the function named doRename!\u0026quot;\n","date":"2013-03-20T11:53:45.464Z","permalink":"https://blog.michael.gr/post/2013-03-20-my-notes-on-clean-code/","title":"My Notes On 'Clean Code' By Robert C. Martin"},{"content":"Before reading any further, please read the disclaimer in the C# Bloopers post.\nAs it turns out, an explicit interface method implementation in C# must be tied to the base-most interface to which it belongs; it cannot be tied to a descendant interface.\nnamespace Test14 { class Test { interface A { void F(); } interface B: A { } class C: B { void A.F() //OK { } void B.F() //error CS0539: \u0026#39;B.F\u0026#39; in explicit interface declaration is not a member of interface { } } } } Well, sorry, but... it is.\n","date":"2013-01-04T11:51:30.698Z","permalink":"https://blog.michael.gr/post/2013-01-04-csharp-blooper-14-weird-annoying-interface/","title":"C# Blooper №14: Weird / annoying interface method visibility rules"},{"content":"Before reading any further, please read the disclaimer in the C# Bloopers post.\nThis is a blooper of the Common Language Runtime (CLR), not of the language itself: Stack\u0026lt;T\u0026gt; and Queue\u0026lt;T\u0026gt; derive from ICollection, but not from ICollection\u0026lt;T\u0026gt;, so they do not support the Remove( T ) method! Why, oh why?\n","date":"2013-01-04T11:44:32.906Z","permalink":"https://blog.michael.gr/post/2013-01-04-csharp-blooper-13-stack-and-queue-do-not/","title":"C# Blooper №13: 'Stack' and 'Queue' do not implement 'ICollection'"},{"content":"Before reading any further, please read the disclaimer in the C# Bloopers post.\nWhen writing generic methods in C#, it is possible to use the 'where' keyword to specify constraints to the types that the generic parameters can take. Unfortunately, these constraints cannot be used for resolving overloaded methods. Case in point:\nnamespace Test12 { class Test { public static bool Equals\u0026lt;T\u0026gt;( T a, T b ) where T: class { return object.Equals( a, b ); } public static bool Equals\u0026lt;T\u0026gt;( T a, T b ) where T: struct //error CS0111: Type \u0026#39;Test\u0026#39; already defines a member called \u0026#39;Equals\u0026#39; with the same parameter types { return a.Equals( b ); } } } ","date":"2013-01-04T11:31:10.587Z","permalink":"https://blog.michael.gr/post/2013-01-04-csharp-blooper-12-where-constraints-not/","title":"C# Blooper №12: 'Where' constraints not included in method signatures"},{"content":"Before reading any further, please read the disclaimer in the C# Bloopers post.\nWhen you assign an enum to int, you have to cast it. That's good. When you assign an int to enum, you also have to cast it. That's also good. But if you assign zero to an enum, you don't have to cast it! Go figure.\nnamespace Test11 { class Test { public enum myenum { a, b, c } void test_myenum( myenum f, int i ) { i = (int)myenum.a; //need to cast; that\u0026#39;s good. f = (myenum)5; //need to cast; that\u0026#39;s still good. f = 0; //wtf? no need to cast? } } } ","date":"2013-01-04T11:27:27.155Z","permalink":"https://blog.michael.gr/post/2013-01-04-csharp-blooper-11-zero-to-enum-conversion/","title":"C# Blooper №11: Zero to Enum conversion weirdness"},{"content":"Before reading any further, please read the disclaimer in the C# Bloopers post.\nOverall, C# takes an approach which is far more friendly to novice programmers than its predecessors, C and C++ were. For example, in the case of switch statements, C# does not allow the old, error-prone style of C and C++ where you could simply fall through from one case statement to the following one; instead, at the end of each case statement C# requires either a break statement, or a goto statement to explicitly jump to another label. That's all very nice and dandy, except for one thing: C# requires a break or goto even at the last case statement of a switch statement!\nnamespace Test9 { class Test { void statement() { } void wtf_is_it_with_falling_through_the_last_case_label( int a ) { switch( a ) { case 42: statement(); break; default: statement(); break; //Need this \u0026#39;break\u0026#39; or else: CS0163: Control cannot fall through from one case label (\u0026#39;default:\u0026#39;) to another } } } } I mean, seriously, WTF?\n","date":"2013-01-04T11:16:41.747Z","permalink":"https://blog.michael.gr/post/2013-01-04-csharp-blooper-9-annoying-case-statement/","title":"C# Blooper №9: Annoying case statement fall-through rules"},{"content":"Before reading any further, please read the disclaimer in the C# Bloopers post.\nThis is rather a Microsoft Visual Studio blooper than a Microsoft C# blooper: When formatting source code, Visual Studio offers an \u0026quot;indent case contents\u0026quot; option, but you will only find it useful if you happen to have a crooked notion as to how switch statements should be formatted. The one and only normal form of formatting switch statements is not supported.\nnamespace Test10 { class Test { void statement() { } void test( int a ) { /* with \u0026#34;indent case contents\u0026#34; option selected: */ switch( a ) { case 42: /* this is not properly indented */ { statement(); break; } default: /* this is properly indented */ statement(); break; } /* with \u0026#34;indent case contents\u0026#34; option deselected: */ switch( a ) { case 42: /* this is properly indented */ { statement(); break; } default: /* this is not properly indented */ statement(); break; } /* the normal way of indenting cannot be achieved: */ switch( a ) { case 42: { statement(); break; } default: statement(); break; } } } } I know, you might disagree that my way of formatting switch statements is in any way 'normal'. So, in your case, let us agree on this: my way of formatting switch statements, whether you like it or not, is in perfect accordance to the way I format the rest of my code; and since Visual Studio allows me to precisely describe my coding style, it should also allow for a switch statement style that matches the rest of my code.\n","date":"2013-01-04T11:06:17.702Z","permalink":"https://blog.michael.gr/post/2013-01-04-csharp-blooper-10-switch-statements-are-not/","title":"C# Blooper №10: Switch statements are not properly formatted"},{"content":"Before reading any further, please read the disclaimer in the C# Bloopers post.\nThe Microsoft C# compiler does not issue 'condition is always true' and 'condition is always false' warnings. Perhaps these warnings are not particularly meaningful in Java, which lacks conditional compilation directives, and therefore if( true ) and if( false ) are the only means of achieving conditional compilation; but in C#, which has special conditional compilation directives, conditions which are always true or always false are invariably so by mistake; therefore, these warnings would indeed be meaningful, and useful.\nnamespace Test8 { class Test { void statement() { } void test() { if( true ) //no warning about \u0026#39;condition is always true\u0026#39;. statement(); while( false ) //no warning about \u0026#39;condition is always false\u0026#39;, even though the compiler obviously knows what\u0026#39;s going on, since the following warning is issued: statement(); //warning CS0162: Unreachable code detected } } } For more information see Why the Microsoft C# compiler lacks many useful warnings.\n","date":"2013-01-04T10:51:13.725Z","permalink":"https://blog.michael.gr/post/2013-01-04-csharp-blooper-8-no-warnings-for-conditions/","title":"C# Blooper №8: No warnings for conditions that are always true/false"},{"content":"As my C# Bloopers series of articles shows, the Microsoft C# compiler fails to issue many useful warnings which one would reasonably expect from a decent compiler of any language, and which are in fact readily and lavishly issued by Java compilers.\nAfter all these years that the Microsoft C# compiler has been maturing, one cannot help but postulate that there are ulterior motives behind this continued state of misery with respect to warnings. For lo and behold, it just so happens that the \u0026quot;Ultimate\u0026quot; (most feature-packed and most outrageously expensive) edition of Microsoft Visual Studio contains a \u0026quot;Code Analysis\u0026quot; feature, which is capable of issuing hundreds of different types of warnings, ranging from the pedantic to the arcane, and including most, if not all, of the missing warnings that I am discussing here.\nNow, besides the fact that the Code Analysis feature comes at a considerable additional cost, it is also very cumbersome to use on a frequent basis, since it has been built as a separate product feature, instead of having been integrated into the compiler. For one thing, it is very slow. Another thing is that it is very spammy: we are talking about multiple warnings for every single line of code here, most of which are useless, and the first thing you need to do about them is to turn them off. And there are several dozen warnings to turn off.\nThis cumbersomeness makes the Code Analysis feature unsuitable for use in the instant builds which developers tend to perform every few minutes or so, and more suitable to use as a separate code-quality-assurance step to be performed once or twice during the entire development process of a product, or at best on nightly builds. Unfortunately, it is precisely on instant builds that the warnings I am talking about in these articles are most useful. Better yet, most of these warnings are useful in real-time, while typing the code, in the form of yellow curly underlines. For example, as a developer, I want to know that a parameter to a method I have just written goes unused before I even proceed to start coding the next method, because it most probably means that I forgot something or I did something wrong. This information is useful not tomorrow, not in a couple of months, but right now.\nSo, what has probably happened here is that the Microsoft C# compiler team was told by Microsoft's marketing department to intentionally cripple the C# compiler and make it less useful to all of us, by moving some of the functionality which rightfully belongs to it into some other module, so that their premium \u0026quot;Ultimate\u0026quot; product can have a raison d'être.\nI bet you that Balmer is behind this.\n","date":"2012-11-13T10:46:49.809Z","permalink":"https://blog.michael.gr/post/2012-11-13-why-microsoft-c-compiler-lacks-many/","title":"Why the Microsoft C# compiler lacks many useful warnings"},{"content":"Before reading any further, please read the disclaimer in the C# Bloopers post.\nIf you have a private method which is never used, you will not be given a warning about it by the Microsoft C# compiler. This means that your app will likely get shipped with unnecessary chunks of code in it, some of them possibly containing string literals that were never meant to make it out of the development environment, or even requiring libraries to be linked which were never meant to be required on the field.\nnamespace Test7 { class Test { private void unused() //no warning about unused private method. { } } } For more information see Why the Microsoft C# compiler lacks many useful warnings.\n","date":"2012-11-10T07:29:56.763Z","permalink":"https://blog.michael.gr/post/2012-11-10-csharp-blooper-7-no-warnings-about-unused/","title":"C# Blooper №7: No warnings about unused private methods"},{"content":"Once upon a time I was dissatisfied with my salary at my workplace, and I let it show. The boss, fearing that he was about to lose me, placed an ad in the newspaper for my exact job description. Since I was looking for a job, I saw the ad in the newspaper. What I did was to reply to that ad, sending my boss my resume, which of course included precisely those qualifications that the job required. The boss got the message.\n","date":"2012-11-09T16:19:29.453Z","permalink":"https://blog.michael.gr/post/2012-11-09-how-to-get-raise/","title":"How to get a raise"},{"content":"Before reading any further, please read the disclaimer in the C# Bloopers post.\nA variable identifier is, of course, only visible within the scope in which it is declared. This includes nested (child) scopes, but it does not include enclosing (parent) scopes. In C# however, once a variable identifier has been used in a scope, its name is \u0026quot;poisoned\u0026quot;, so it cannot be used in enclosing scopes. Take this example:\nnamespace Test4 { class Test { void test() { if( this != null ) { object o; o = null; if( o == this ) return; } object o; //error CS0136: A local variable named \u0026#39;o\u0026#39; cannot be declared in this scope because it would give a different meaning to \u0026#39;o\u0026#39;, which is already used in a \u0026#39;child\u0026#39; scope to denote something else } } } Well, I am sorry, but in the above case the new variable named 'o' would most definitely *not* give a different meaning to the 'o' which was used in the child scope. It would, if it had been declared before the 'if' statement, but it wasn't. Luckily, if this \u0026quot;feature\u0026quot; was to be removed from the language, it would not break any existing code. So, can we please have this fixed? Pretty please?\nSee also: C# Blooper №5: Lame/annoying variable scoping rules, Part 2\n","date":"2012-11-09T15:04:33.651Z","permalink":"https://blog.michael.gr/post/2012-11-09-csharp-blooper-4-lameannoying-variable/","title":"C# Blooper №4: Lame/annoying variable scoping rules, Part 1"},{"content":"Before reading any further, please read the disclaimer in the C# Bloopers post.\nOne common mistake that programmers make is to forget to make use of a parameter to a method. This can lead to quite subtle bugs that are hard to track down and correct.\nNow, other language compilers are kind enough to warn the programmer that a parameter is unused, and they also allow temporary suppression of the warning for the rare case when such lack of use is legitimate. But not so in Visual C#. If you forget to use a parameter in Visual C#, you will not know unless you run the \u0026quot;Code Analysis\u0026quot; tool on it.\nnamespace Test6 { class Test { void moo( int a ) //no warning about unused parameter \u0026#39;a\u0026#39;. { } } } For more information see Why the Microsoft C# compiler lacks many useful warnings.\nOld comments\nmichael.gr 2013-03-25 15:37:52 UTC\nThis blooper is about unused parameters. Yes, I would like a warning every single time a parameter goes unused.\nVigilanti 2013-03-25 03:12:35 UTC\ndo you want it to warn you, every time you write a unused function...\nAnonymous 2014-02-17 11:30:59 UTC\nO F C O U R S E !!!!!\n","date":"2012-11-09T14:29:46.38Z","permalink":"https://blog.michael.gr/post/2012-11-09-csharp-blooper-6-no-warnings-about-unused/","title":"C# Blooper №6: No warnings about unused parameters"},{"content":"Before reading any further, please read the disclaimer in the C# Bloopers post.\nIn light of C# Blooper №4: Lame/annoying variable scoping rules, Part 1, this one is more of a confusing error message than an actual new blooper. What I am doing below is that I am declaring a field within a class, I am accessing that field from within a method, and further down within the same method I am declaring a new local variable with the same name as the field. Now, C# will not allow me to declare that local variable because it has the same name as the field, but that's not where I am receiving the error. Instead, the error is given when accessing the field. If you only read the first sentence of the error message, it does not make any sense at all. If you bother also reading the second sentence, it gives you a hint as to the real problem. Now, that's not very cool.\nnamespace Test5 { public class Test { public int a; void foo() { for( int i = 0; i \u0026lt; 10; i++ ) { a = 10; //error CS0844: Cannot use local variable \u0026#39;a\u0026#39; before it is declared. The declaration of the local variable hides the field \u0026#39;Test5.Test.a\u0026#39;. } string a = \u0026#34;\u0026#34;; Console.WriteLine( a ); } } } See also: C# Blooper №4: Lame/annoying variable scoping rules, Part 1\n","date":"2012-11-09T14:16:36.668Z","permalink":"https://blog.michael.gr/post/2012-11-09-csharp-blooper-5-lameannoying-variable/","title":"C# Blooper №5: Lame/annoying variable scoping rules, Part 2"},{"content":"\rI wanted to copy data from the Outlook contacts list to the Lync contacts list. I managed to do it. Here is how.\nIt is kind of amazing how crippled the co-operation is between Microsoft Outlook and Microsoft Lync. (Note: I am talking about Microsoft Office Professional Plus 2010, with Outlook version 14.0 32-bit and Lync 2010.) Never mind the fact that these two programs should be sharing the exact same contact list and I should not have to do anything of that sort; it sounds like a simple task, right? Copy contacts from one program to another. These two programs belong to the same Office suite and are supposedly seamlessly integrated with each other. Well, seamlessly my @$$. You can copy single contacts from Lync to Outlook. But if you want to copy a contact from Outlook to Lync, or copy multiple contacts, then you are completely out of luck. No-can-do, apparently.\nWell, I found a trick to do it. Here is how:\nSelect all the contacts within Outlook that you want to copy to Lync.\nHit \u0026quot;Send email\u0026quot;, and a new email will be prepared, with all the selected contacts in the \u0026quot;To:\u0026quot; field.\nSelect all the contacts in the \u0026quot;To:\u0026quot; field.\nDrag and drop the contacts from the \u0026quot;To:\u0026quot; field into a group (not contact) in Lync.\nDiscard the new email.\nVoila!\nIf you know of a simpler way, please let me know.\nAlso, if you know of any way to do perform the incredibly advanced operation of... (drum roll please) PRINT YOUR OUTLOOK CONTACTS WITH PICTURES, (duh!) please let me know.\nCheers!\nOld comments\nAnonymous 2013-04-10 14:17:37 UTC\nDoes not work on my pc. have Lync 2013, outlook 2010, Win7 any other methods?\nAnonymous 2013-03-22 15:31:47 UTC\nI do this but the phone numbers don't come across. I just see the contacts email address added to Lync. Is this what you're seeing?\nAnonymous 2013-06-26 19:23:33 UTC\nAwesome dude\nAnonymous 2014-10-08 10:28:59 UTC\nAnyone had any luck with this running in an Office 2013 environment? Cheers.\nAnonymous 2013-07-23 07:25:35 UTC\nThanks, works but had some issues as above where the number's didn't come across. Strange thing for me is that I got mixed results depending on the contact although they have the same fields populated.\nMaverick 2013-12-20 15:02:12 UTC\nIt worked. Great job Mike. Awesome.\nmichael.gr 2013-03-22 19:16:08 UTC\nAnonymous, I won't be able to check it so as to answer your question until Tuesday.\nAnonymous 2014-10-26 20:13:51 UTC\nDoesn't work with Lync 2013. Thanks anyway for your help! Also, how stupid is this Microsoft? Two communication programs that are part of the same software suite that can't share contacts!!\nGunner_leo14 2013-03-14 16:10:52 UTC\nThank you! you are a genius!\nMike Ullrich 2014-03-21 16:05:15 UTC\nMichael, as for Outlook 2013 and Lync2013 it doesn't work. I tried. Does anyone have any ideas?\nAnonymous 2013-07-16 04:49:01 UTC\nFirst week in a new company and you made my day. Thanks.\nAnonymous 2014-08-19 19:57:55 UTC\nJust what I needed...\nAnonymous 2013-08-15 07:30:54 UTC\nThank you!!! That's AWESOME!\nAnonymous 2015-02-25 16:59:22 UTC\nHi had problems to do that way with contacts from Outlook 2007 to Lync client 2010. Know anyone of you that there is a known issue with that task and that different product versions? Anyway I think a Outlook contact is not a real Lync contact because it is not a lync contact or lync user behind and therefore also no sip address. So in my point of view it is expected that a normal outlook contact will not display correctly on an lync client if there is no other information about that outlook contact into the lync system. But maybe there is another better reason for that. I am not 100% sure. :)\nmichael.gr 2013-12-20 15:18:01 UTC\nC-:= I am glad to be of help!\nAnonymous 2013-03-26 02:08:09 UTC\nThanks Michael. Works a treat. Steve\nmichael.gr 2013-04-10 14:41:49 UTC\n@Anonymous of March 22: (Sorry for the delay!) My contacts were transferred with phone numbers, so I do not know what is wrong in your case.\n@Anonymous of April 10: I would bet that your problem is due to the different versions of Lync and Outlook. Sorry, I do not have any other ideas.\nAnonymous 2013-07-07 19:28:01 UTC\nanyone know how to do the same with Office 2013?\nAnonymous 2014-02-11 15:13:29 UTC\nThanks much Michael, I appreciate your sharing!\n","date":"2012-10-25T13:41:51.22Z","permalink":"https://blog.michael.gr/post/2012-10-25-how-to-copy-multiple-contacts-from/","title":"How to copy multiple contacts from Outlook to Lync"},{"content":"I was looking around for advice on what settings are best for scanning printed photos and I was amazed by the number of answers floating around on the great interwebz which are misguided, or are technically correct but miss the point. So, here is my advice.\nFirst of all, let us define the goal: For a home user to scan printed photos so as to retain as much as possible of the visual information contained in the print, within reasonable limits, and without wasting too much space.\nThe answer, in a nutshell: Scan at 600dpi, save as 24-bit-color compressed PNG or compressed TIFF. Do not use any of the fancy options for noise reduction, color correction, contrast enhancement, etc that might be provided by your scanner. If you need to improve something, edit the scanned picture later using your favorite image processing software, but never touch the original scanned files: always work on a copy of the original.\nIf the nutshell is good enough for you, then off you go, and happy scanning.\nIf you are interested to know why, read on.\nPlease note that the way the goal was expressed, we do not have to take into consideration what we are planning to do with the pictures later. When you scan, scan for posterity. Scan so that you can later crop, rotate, retouch, print. etc. If the photos need to be communicated through a low-resolution medium such as the web, then you can later make scaled-down copies for that purpose. But at the time of scanning, the point is to not limit yourself on what you will be able to do later with the scanned photos.\nThe resolution of photo prints ranges between 150 and 300 dpi. When we scan at 600 dpi, we are doing twice the resolution of a photo print, (or better,) which is the theoretical maximum required so as to capture all the information that there is on the photo print. Every bit of it. Not an iota of information left out. Sure, some color fidelity will inevitably get lost, but that's a different ball game altogether. As far as resolution is concerned, anything above 600 dpi is a pure waste of space.\n24 bits per pixel is also perfectly adequate to fully match the capacity of film to convey color. There will be some reduction in color fidelity stemming from the fact that the sRGB color space into which you will most likely be scanning will not necessarily (and in all likelihood never) exactly match the color space of the photo, but the loss in fidelity will be imperceptible, and the measures required to correct this problem are disproportionately painstaking, and outside the \u0026quot;within reasonable limits\u0026quot; requirement stated as the goal in the beginning of this article. Furthermore, when you are working with differences in quality that are not perceptible, it is very easy to make a mistake which reduces, rather than enhances, the fidelity of the digitization, even by an imperceptible amount, and you will never know. So, color is best left at 24 bits per pixel sRGB and never messed with.\nUse compressed PNG or TIFF because these file formats offer LOSSLESS compression. Lossless compression means that 100% of the information in the image is retained. Not very close to 100%, not imperceptibly different from 100% but precisely 100%. Time and again I hear about people who save their pictures in uncompressed TIFF format; obviously, they do not understand squat about compression. It really is not rocket science: there are two kinds of compression, lossy and lossless. Lossy compression (JPEG) achieves huge savings, at a slight (usually imperceptible) expense to quality. Lossless compression (PNG, TIFF) achieves great, but not huge savings, at NO expense to quality. Using no compression serves no purpose, and is just plain stupid. It just spreads your picture over more sectors on your hard drive, increasing the chance that it will one day be lost due to a sector going bad.\nSince lossy compression usually represents an imperceptible loss of quality, we could be scanning into JPEG, but the problem here is that if we ever retouch the picture, and then save it again as JPEG, the lossy compression will be re-applied, thus compounding the loss of quality. Keep repeating this cycle, and at some point the deterioration will start becoming perceptible. That's why we always use lossless compression on originals and on working copies, and lossy compression when publishing.\n","date":"2012-10-07T09:13:54.371Z","permalink":"https://blog.michael.gr/post/2012-10-07-scanning-printed-photos/","title":"Scanning printed photos"},{"content":"\rMy Canon Pixma MX700 printer died the other day as it was printing. It just went completely dead, as if the power cord was unplugged. I tried a few things and I did in fact manage to bring it to life, but only for a couple of days. Then, it died for good.\nHere is what happened.\nFirst, I tried a different power outlet. That was not it. Then, I checked the power cord. No problem there. Then, I took out the power supply box and disconnected it from the printer, then I reconnected it and put it back in. No improvement. Then I opened up the power supply box and examined the circuit board in it.\nI am not terribly familiar with hardware, so I could not tell if the smell was of burned electronic components or if it was just the regular smell of electronics that have been sitting inside a closed box for a few years. Nothing looked burned though, and all the electrolytic capacitors seemed intact. I located 3 fuses on the board, and I checked each one of them for continuity. They were all fine.\nSo, I decided to give up, and I started putting things back together. I placed the circuit board back in the power supply box, I closed the box, and then for some reason I did something backwards: first I connected the power chord to the power supply box, and then I connected the power supply box to the printer. As I was attaching the connector, I noticed that one of the pins was momentarily making a little spark. And then lo and behold, the printer came back to life! The printer had fixed itself!\nAnd that, ladies and gentlemen, is why I hate hardware.\nUPDATE 2012/07/13: nope, the resurrection was only temporary. The printer is dead. Dead as a doornail. And that, ladies and gentlemen, is why I hate hardware even more.\nOld comments\nGreg Jenkins 2014-01-23 18:31:28 UTC\nFalse alarm. Now it's displaying this alert; \u0026quot;UO52 The type of print head is incorrect. Please install the correct print head.\u0026quot; It's print head is the only one it has ever had on it. Grrr Cullman Alabama\nAnonymous 2013-03-19 12:56:38 UTC\nThank you, Michael! MX700 has been a steady workhorse of a printer until it was completely dead this morning. Unplugged \u0026amp; left off for over 5 mins; replugged to printer \u0026amp; outlet. Poof! It worked. Saved me $ $ $ \u0026amp; lots of frustration. Thank you for using your misfortune to help the rest of us!!\nmichael.gr 2014-02-14 09:49:54 UTC\nLOL, Mark! C-:=\nDon 2014-03-16 17:26:54 UTC\nI also have the problem of the printer dying. so I tried your cure and I did get the power light back on. but nothing is working and now I can't turn the power off. The light stays on but nothing is working. So I turned it off to write this and I will check later. I hope it will start up, I bought a Epson 2540 and it works ok but is very noisy.\nAnonymous 2013-11-02 23:22:44 UTC\n^ Thank you for the reply, I actually hooked up the power supply with the power cable to the outlet, and got +, and - 7.5 V on the middle and 2 leads, so looks like that unit was working just fine. I spent the entire day today dissembling the printer down to its electronic components. Checked all connections, and tested the wiring....all looks good..but the printer just doe snot turn on at all. One thing though the middle choker emitted a reasonable loud buzzing noise. Then again they look to be low quality chokers...which tend to do so (know it form graphic card's experience).....Frankly I am about to give up....2.5 days sitting on this...and nothing...not even a clue whats wrong.... I guess I can blame the solar flares ....which actually happened the same day the printer went down...and the day I tried to send a fax...it was turned on, i left the room and it was dead when I got back.....or it was just some darn power spike, since the printer is the only equipment that I never secured with a power strip. with a voltage regulator :(\nGreg Jenkins 2014-01-22 23:36:31 UTC\nWow. I was skeptical because my problems seem to never just fix themselves. But I took your advice, left the printer unplugged from the wall outlet while I ran errands and whenever I plugged it back in, it went right to work. Many thanks to all of you for sharing.\nCullman Alabama\nAnonymous 2013-08-30 23:18:56 UTC\nThank You!!\nMy MX700 died right before one of my grad school classes and I needed to print papers.\nJust popped out the power box and re-attacked the corner and it booted up fine. Thanks, I couldn't afford to buy a new printer on my college student budget.\nAnonymous 2013-11-13 01:37:59 UTC\nNew Power Supply didn't fix problem Have power supply for sale print on way to dump\nAnonymous 2013-04-11 05:39:09 UTC\nThis approach just worked for me on a PIXMA MX882. We too had had a power interruption, and the printer was totally dead after it. I tried unplugging and replugging without any improvement. Then I found this blog, took the power cord out of the plug and out of the printer, put it back into the plug and then back into the printer, and wala! (April, 2013)\nUnspecified 2015-07-29 03:48:40 UTC\nI definitely tried the reverse plug in trick and it did nothing...I guess i'm the 1 out of 10 that doesn't work. smh\nMark 2014-02-14 08:19:34 UTC\nwell that was an hour of my life I'm never getting back ...my MP610 died mid print, I removed the power supply, cursed at the tabs that are damn near impossible to press at the magic angle in order to remove the PSU cover (the trick is to angle the screwdriver down into the box not straight into the hole ...it's a clip that needs to be pressed in to release the tab that's locked into it....and make sure you curse a lot...that seems to help too) ...but I digress...I bridged red and yellow and presto 24v and 32v so I guess the PSU is ok ...but the printer is still as dead as my faith in Canon. And that, ladies and gentlemen, is why I hate hardware. (I'm also not too fond of the evil masterminds that came up with that clip on lid thingy ...those guys are just a Siamese cat and a monocle away from being super villains)\nAnonymous 2013-12-27 13:19:33 UTC\nMy MX700 has \u0026quot;died\u0026quot; on me twice due to blips in house power due to outages in area. I have found that just unplugging the printer from the wall and leaving it unplugged for 5-10 minutes \u0026quot;resets\u0026quot; the printer somehow. It powers right back up. If I unplug it and plug it right back in, it remains \u0026quot;dead\u0026quot;. Needs several minutes unplugged.\nmichael.gr 2013-06-03 22:47:10 UTC\nThanks for the comment, Mike. I have heard that the cost of a new power supply for this printer is quite close to the price of the printer itself, but I hope that you prove me wrong. If possible, please post a comment with your findings, along with your country of origin. Cheers!\nVerlyn 2013-06-21 00:20:57 UTC\nI called Canon support. They are shipping a new AC Adapter to me in North Carolina for $35. I was unable to find any other source on the web. (I think the part number is K30290.) I hope this fixes my dead MX700.\nAnonymous 2014-03-16 14:38:45 UTC\nI have ALL power it will copy from above but it will NOT print anything from monitor onto printer I bought new ink unplugged downloaded and more still no printing paper just slide right out if I demanded nothing with a blank sheet????\nAnonymous 2014-07-31 16:47:57 UTC\nMX700....just died. Printed something perfectly, and died. It's been unplugged for....days. Replugged it in.....nothing. Nothing.\nAnonymous 2013-04-11 05:36:45 UTC\nThis approach just worked for me on a PIXMA MX882. We too had had a power interruption, and the printer was totally dead after it. I tried unplugging and replugging without any improvement. Then I found this blog, took the power cord our of the plug and out of the printer, put it back into the plug and then back into the printer, and wala! (April, 2013)\nProvaPersonaleProva 2015-07-30 17:21:46 UTC\nI wanted to tell you that the problem of the MX700 are two chips uquali marked SN0507087 which contains 4 motor drivers and 2 dc-dc converter having them mounted without heat sink leads them to burn even considering how the printer gets old engines make more effort to turn and require more current. riquarda as the spark that you said and 'normal because there was still tension in the power supply capacitors, and as you may have noticed the power supply produces a two voltages 24v and the other a bit' more low,\nmichael.gr 2015-07-31 06:19:26 UTC\nThanks for the insight, Sergio.\nIf that is true, then I think that this is a product with a defective design, which Canon should have recalled and fixed free of charge for everyone.\nS/V Waypoint 2014-01-15 19:41:11 UTC\nAdd one more!\nThanks\nUnspecified 2013-10-13 04:54:15 UTC\nHey, this fix worked for me. Printer was dead as a doornail. I disconnected the power cord, removed the power supply and disconnected it from the printer. Then I reconnected the power cord, reconnected the power supply to the printer, it powered up, and I replaced it in the printer. When I placed paper in it, the darn thing printed a document that I sent to it days ago. Go figure!\nAnonymous 2015-03-28 17:30:09 UTC\nThanks Michael for the post. It didn't work for me, but I did get to earn some brownie points with the missus for my diligence:)\nUnspecified 2013-06-03 18:47:12 UTC\nI just had a quick power outage and besides the clocks all being reset the only other victim was the printer. I found this site and followed the instruction and sure enough it worked. Dont know for how long however knowing that it is only a power supply problem means that I can just google for another power supply for a whole lot less that buying a new printer....I hope!\nmichael.gr 2014-01-23 21:56:50 UTC\nWeird.\nAnonymous 2013-11-02 04:32:38 UTC\nHow did you open up the power adapter case Sir? I am having a hard time opening it.\nDoug 2014-06-10 19:26:59 UTC\nWOW. As with many others, I found this thread on Google because I had the same thing...unplugged from the printer, plugged immediately back in and nothing. Checked voltage at the printer plug, and had 119v. I waited 5 minutes after reading the tips here, plugged it back in, and voila! Hope it stays that way - it's been a great MFP.\nAnonymous 2014-01-08 21:05:07 UTC\nWow. I can't believe that worked. I purposely plugged the connector in a couple times until I saw the spark. Then it worked.\nEmilia 2012-08-12 17:09:30 UTC\nAugust 12, 2012. I can't believe this!!!! Had the excact same problem today. Tried all the usual stuff, dead.. . and then I decided to Google it and found your posts. Did the same thing and it worked.... totally cool and you saved me $ $ $ $! Hope it stays! Thanks again!!\nmichael.gr 2013-10-13 09:45:38 UTC\nI am so happy that this post is helping some people! Isn't the internet awesome? C-:=\nUnspecified 2013-10-13 04:53:39 UTC\nFound this post. Did the same thing. Pulled out the PS, disconnected it, reconnected it, put it back in and it powered up just like new. Thanks!!!!\nAnonymous 2013-11-05 02:34:33 UTC\n^ Thanks for an update, curious how that will go. Odd though you had 6.3V....looks like we might have different issues...\nBokonon 2015-09-07 22:44:06 UTC\nLast week my MX700 went dead after years of stable use. I popped out the PSU... and then remembered I'd done this about 3 yrs ago. (The PSU case looked like rats had gone at the edges). This time, PSU still appears OK (R-Y bridge -\u0026gt; 24/32V)... but repeating trick now just leaves power LED on immediately at AC connect, all else dark. Ugh; I guess one resurrection is all it gets!\nmichael.gr 2015-09-07 23:09:05 UTC\nThanks for the update, Bokonon!\nAnonymous 2014-01-12 18:04:21 UTC\nWorked for me, too!! Thanks so much, Michael, you're great. (Heike from Germany)\nmichael.gr 2012-12-07 15:57:59 UTC\nThanks for the comment, Chris. Actually, thank you all for your comments. Well, if this article proves to be of any help to anyone, I am glad, despite the fact that it won't raise my MX700 back from the dead. C-:=\nAnonymous 2013-05-18 01:59:18 UTC\nI don't want to be the know-it-all here, but when this happens, just unplug the power chord from the outlet and let the capacitors in the printer discharge all their power. 5-10 minutes should do it. Then plug back in. Works for me.\nUnspecified 2015-12-05 14:26:38 UTC\nThank you! so far this has helped!\nAnonymous 2013-07-18 15:05:29 UTC\nMy MX700 happened to have the same problem 2 days ago. I tried as you suggested : unplugged from the wall for an hour, plugged it back in with no luck. Can't wait to get back home to try removing the PS in hopes that it will resurrect. - Ray\nmichael.gr 2013-07-18 19:26:01 UTC\nGood luck, Ray!\nPatrick 2013-02-12 00:19:46 UTC\nMany thanks. Just tried these steps on my own MX700, and it worked! (for now)\nmichael.gr 2013-07-07 08:01:22 UTC\nNote: the fuses that I am talking about in the article are soldered on the board, they are not the good old kind of fuses that you can snap out and replace. So if you are hoping to open the case to replace the fuses, be prepared for a lot of hassle.\nGary 2013-07-07 10:43:46 UTC\nHi Michael Thank you for the information. I will check with Canon on the cost of a replacement power box and start pricing out a new unit. Too bad...really liked this one and only had it for 3-4 years. Expected it to last longer given limited use.\nRegards\nGary\nAnonymous 2012-12-07 02:00:04 UTC\nMichael:\nDon't know if you get any satisfaction reading all these success stories based on your temporary success, but sure enough the magic continues...\nMy MX-700 (which is the best multifunction device I've ever had) looked to be DOA after a recent power loss and recovery surge. Wasn't the plug, wasn't the breaker, wasn't the outlet. Removed the power supply, disconnected it from the unit (didn't get it open because I didn't know which way to pry the tabs to pop the lid), then reinstalled.... Magic! It came back to life. Hope all this good karma gets you a bullet-proof replacement device. Thanks for the post. -Chris\nAnonymous 2012-07-19 01:32:26 UTC\nI found your post because I just had the exact same thing happen. Our power has been flickering a lot lately. My husband was trying to get the power supply cover off when I read him your post, so he plugged it back in (power supply first, wall second) and it worked. I hope ours doesn't die again in a few days!\nmichael.gr 2013-11-02 10:34:14 UTC\nOh, it was very difficult. I do not think it was made to ever be opened. I had to pry it open with a screw driver by sticking the screw driver into it at many points along the seam of the case and twisting the screw driver to force the case to open. As a result, the case sustained visible damage. My suggestion: do not open it, there is nothing in there to fix anyway.\nUnspecified 2015-07-29 03:47:11 UTC\nI definitely tried the reverse plug in trick and it did nothing...I guess i'm the 1 out of 10 that doesn't work. smh\nAnonymous 2013-08-06 05:11:17 UTC\nMy MX700 also dropped dead today. The unplug-plug trick did not work for me. I investigated further and concluded that the power supply (K30290) was still ok. red-black: 24V, 7.5V in standby blue-black: 32V, 10V in standby (Connect red and yellow to get the unit out of standby.)\n-Stefan\nAnonymous 2013-12-22 01:47:12 UTC\nThe latter, bridged. Somewhere out there on the internet I found a hint that this power supply goes into a standby mode with 1/3 the voltage. I was wondering if this is true and how to activate the normal operation mode. The printer was dead, so I experimented - and voila - connecting the yellow wire to the red powered up both 24 and 32V supplies...\nAnonymous 2014-03-06 21:58:49 UTC\nAnother good thing is to measure both outputs with an oscilloscope. My printer's PSU seems to work great. Now hoping that is a fault of the power on/off switch, or worse, the logic board itself. Also, check this: \u0026quot;http://elektrotanya.com/canon_pixma_mp610.pdf/download.html\u0026quot;\nAnonymous 2013-11-14 17:59:30 UTC\nHi Stefan, did you connect red and yellow wires, or the wires were plugged into the power supply, but just bridged (like some people bridge PC power supplies to use the power supplies with out a plugged in motherboard? Thank you.\nmichael.gr 2014-10-23 19:24:21 UTC\nPam, I do not know why you want to go deeper than just pulling out the power supply, but if the problem is further inside the printer, then yes, it would require unscrewing the printer box. But I think only an authorized Canon Service Center would be able to do anything about it.\neljay 2013-12-26 19:24:48 UTC\ncould you explain that in more detail for those of us who are learning? i'm not sure what you mean by bridge and i want to try everything - don't have the $ to get a new printer.\nAnonymous 2014-10-23 17:25:15 UTC\nStephan or Michael, sharing your grief... had massive noreaster last night and power surge. Printer died. Tried all Michael's suggestions... to no success. How do I access the red/yellow wires aside from just pulling out power supply? Do I need to take printer apart and access circuit board?\nthx -Pam\nAnonymous 2013-11-06 22:44:41 UTC\n^ just to get this right....so the middle and 2 side mid pins should read 24V? is that right? in that case i guess the power supply is the issue, could someone please confirm the power output that should be coming gout of those pins please (of a fully working model)? Thank you.\nPegeen Lanahan 2015-03-10 00:23:59 UTC\nWorked like a charm. You're a genius, Michael Gr, even though you're a hardware hater ;-)\nAnonymous 2015-05-30 21:43:52 UTC\nLOL! Great quote - though perhaps these days the real world changes so fast that software can't keep up? Our multifunction Pixma MG5350 died instantly half-way through a page, as most all above, a few months ago. This is the first useful page I've found on the web for it! It's a bit worrying that the solution is ritual rather than engineering (discuss sometime...) The ritual produced signs of life for a second or so. Eventually it switched on long enought to perform its start-up ink-tank-flushing ritual; but did not complete - came up with (red border, yellow lightning flash lit) \u0026quot; U052 The type of print head is incorrect. Install the correct print head.\u0026quot; This rings a faint bell, haven't remembered where it was... Meanwhile had a look at the print assembly and saw aok except the Magenta cartridge was flashing slowly. Tried a new one, but still stuck at U052 until inspiration comes.... [Our previous m/f Canon Pixma - after warranty expiry - developed the habit of shutting down randomly -- wiggling the mains input lead produced sparking noises from the psu ! So I think we're reluctntly abandoning Canon, brilliant though their devices are when in working order. For now, have just got an EPSON XP-720 - a bit weird, but it does the basics we need (including printing labels on CDs). Obv would be nice if the Canon can be resurrected without huge expenditure, as a standby at least...]\nThanks for taking the trouble to blog all this! Theo B\nmichael.gr 2015-03-10 07:43:59 UTC\nI am glad it worked! I am a hardware hater probably because I am a software engineer by profession. Just yesterday I posted this status on facebook: A wise person once said, \u0026quot;Hardware eventually fails. Software eventually works.\u0026quot; (tweet by some \u0026quot;Mitch Kapor\u0026quot;)\nAnonymous 2015-05-30 22:36:33 UTC\nO Woe - 90 pages of tirades from users on the US Canon user site, thread topic: \u0026quot;Call to Arms for the dreaded U052 Wrong Printhead Error \u0026quot; Looks like this is the end of line for me... Good luck to you all - Theo B\nellenhof 2015-09-08 18:46:27 UTC\nAmazing and simple! Thank you! We had a power surge from a lightening strike and I was sure I would have to replace the printer. But unplugging waiting then plugging it back in worked!\nAnonymous 2017-05-01 03:30:54 UTC\ni have a cannon 5220 no power took out psu check voltage tried all threads found but still not working need help\nDM in AZ 2014-08-03 02:58:09 UTC\nLooking at a friends cannon pixma iP3500 - similar problem = died, no pwr up. the connector to the k30290 'ac adapter' inside the printer (Very short, couple turns through a ferrite donut) has all red wires.\nI haven't tried too hard on the clips (did get their special screw out). Considering trying pwr up outside of printer \u0026amp; see what I get- would guess black on outside of 5 in a row pins \u0026amp; yellow in center? Nope - referencing the pin nearest the end of the box (furthest from ac in end) I get readings of (ref), 0, 7.9, 0, 9.8 = sounds like 'standby' ? I hesitate to do pin to pin jumping with pwr on... has been unplugged for probably ah hour min since tried in printer earlier. I replaced original nasty screw with similar phillips ;-)\nAnonymous 2013-04-05 05:19:33 UTC\nOMG- the freaking thing JUST as described happened to us- so we unplugged the PS and plugged it back in- and it fired up. Up until then it had been a workhorse and stopped dead right in the middle of printing- no warning either! So we're fingers crossed- hoping we can print out out taxes and a few scholarship applications before it dies again!\nAnonymous 2013-11-11 01:47:55 UTC\n...anyone?\nmichael.gr 2013-07-07 07:59:42 UTC\nHello, Gary! Yes, unfortunately the power supply case has a cover which snaps in with plastic hooks, and it gave me the impression that it was built to never be opened once closed. The only advice I can give you is to try to pry it open with a flat head screwdriver just a little bit at a time from every side possible. I was able to open it without destroying it, but I certainly left marks on it.\nThen again, if you can get a replacement for $35 as Verlyn above did, then it is probably not worth the trouble.\nThat is, of course, if the power supply is your problem.\nAnonymous 2014-11-28 22:08:11 UTC\nTried unplugging and plugging the connector, then plugging the power cord back in with no luck. Had I tried the sequence that you had described in the order you mentioned, it may have worked. While attempting to pry open the designed-not-to-be-opened power supply box, I found when I finally got it open that I had broken one of the printed circuit lans, beyond solderable repair. Sadly, off to the recycler.\nGary 2013-07-07 00:15:44 UTC\nHi Mike\nMy MX700 just did the same thing- stopped dead in the middle of a print. I tried unplugging and re-plugging- did not work. Then tried removing the power unit and unplugging the connector- re-connected and re-plugged...still no joy. How did you get the power supply unit open to check the fuses? That is what I would like to do next. Can anyone on this thread advise how to get this open (without totally destroying it!)\nRegards\nGary\nAnonymous 2013-12-01 07:29:00 UTC\nremove securing screw (special type) and use a small screwdriver to unclip the tags 3/4 in all and the circuit board pops out.\nYelloDevil 2015-08-09 22:58:04 UTC\nThe 1150 doesn't seem to have a screw on the power board cover, so I just broke that bitch. That's how I roll.\nAnonymous 2013-11-04 12:06:14 UTC\nThursday Oct 31st Same problem whilst scanning printer died. unplugged and remove power supply refitted power supply finished scanning. Friday no response from printer. removed power supply and tested voltage 32 volts Ok 24 volts about 6.3v in both states. contacted Canon Australia gave me email address sales @partsovernight.com.au and ordered new power supply $60. Now to wait about 5 days for replacement will publish results later.\nAnonymous 2014-08-28 13:59:17 UTC\nMy Canon MX300 printer won't turn on but was giving out a small popping sound after pressing the on/off button. This blog post as well as comments above gave me ideas to try fix it! It's now fixed! Attempt 1. : Turn power off at wall after printer stopped working, unplug power cord from AC adapter, disconnect AC adapter from wires connecting printer, replug power cord, turn power on at wall, reconnect printer wires to AC adapter (creating sparks). Nothing (except small popping sound). Attempt 2: Tried disconnecting and reconnecting printer wires a few more times to adapter creating (more sparks). Nothing. Attempt 3: Unplugged power cord and waited for a day to turn back on. Nothing. Just when I was about to give up, thought I'd try one more thing. Attempt 4: Performed actions in \u0026quot;Attempt 1\u0026quot; again, after no response I tried disconnecting and connecting the power cable from the printer (while power is still on) quickly in succession. After 3 or 4 times of pulling out and pushing cord in, suddenly printer resurrected from the dead!!! Success!! Thank you so much everyone. :D\nAnonymous 2014-05-28 01:04:39 UTC\nThanks for posting this , as it worked for me also. Just did an internet search for the Ac adaptor and part number to see what the price was and clicked on this thread, thanks again.\nProvaPersonaleProva 2015-07-30 17:20:52 UTC\nVolevo segnalarvi che il problema della MX700 sono due chip uquali marcati SN0507087 che contengono 4 motor driver e 2 dc-dc converter il fatto di averli montati senza dissipatore di calore li porta a bruciarli anche considerando che come la stampante invecchia i motori fanno piu fatica a girare e richiedono piu corrente . per quanto riquarda la scintilla che dicevi e' normale perche c'era ancora tensione nei condensatori dell'alimentatore e come avrai notato l'alimentatore produce due tensioni una di 24v e l'altra un po' piu bassa,\nAnonymous 2013-11-02 03:05:07 UTC\nJust did as others said, disconnected the power supply and put it back and it is working again...we'll see for how long.\nAnonymous 2015-01-12 02:43:43 UTC\nYet another success! My MX700 powered off and wouldn't power on. I found this post, tried it, and it worked! Surprising. And I'm a hardware guy. I'd like to see the schematics. I get frustrated with software. Especially Windows. You saved a lot of people some hassle and expense. Thank you, Michael. Jeff\nAnonymous 2014-06-25 18:18:47 UTC\n+1\nAnonymous 2014-06-27 13:21:25 UTC\nI have Canon MX700 and I had a power outage around 5 days ago. the power light keeps flashing on and off on the on/off button, but won't boot up. How does one just pop out a power box as one person on here stated?\nbenarkwiz 2016-06-05 19:20:10 UTC\nworked for me\n","date":"2012-07-05T21:16:46.337Z","permalink":"https://blog.michael.gr/post/2012-07-05-canon-pixma-mx700-printer-dead-and/","title":"Canon Pixma MX700 printer dead and resurrected (Not!)"},{"content":"I experienced this problem today, drive C: of my local computer \u0026quot;Pegasus\u0026quot; was not appearing on the remote computer as \u0026quot;C on PEGASUS\u0026quot; when I connected to the remote computer via Remote Desktop (Terminal Services.) All other drives of Pegasus were showing fine on the remote computer, but the one I actually needed (C:) was not. The drive did not even appear under \\\\tsclient in \u0026quot;Network Places\u0026quot;.\nJudging by the problems reported by people from all over the world who have this problem and are searching for solutions on the interwebz, it may happen with any local resource, like printers, the clipboard, etc.\nLuckily, I found a solution to the problem:\nTerminate the RDP session not by closing the RDP window, but by actually logging off. Then, start a new RDP session, and the problem will have most likely gone away.\nIt is unclear why local resources sometimes fail to show on the remote computer during an RDP session; it is one of those things that \u0026quot;just happen\u0026quot;, and that tend to go away if you just \u0026quot;close and reopen it\u0026quot;. (Or get out of the car and get back in again, as the joke goes.) The reason for the frustration with this particular problem is that more often than not we do not really \u0026quot;close and reopen it\u0026quot;, because we tend to just close the RDP window, which does not terminate our logon session with the server. By logging off and connecting again, the logon session gets restarted, and that's the \u0026quot;close and reopen\u0026quot; needed to fix the problem.\nOld comments\nmoshe 2013-07-18 16:11:06 UTC\namazing, thanks for this tip - it worked!\nmichael.gr 2013-07-18 19:26:42 UTC\nGlad to be of help! C-:=\n","date":"2012-05-02T09:47:10.402Z","permalink":"https://blog.michael.gr/post/2012-05-02-local-resources-unavailable-on-remote/","title":"Solved: Local resources unavailable on remote desktop"},{"content":"So, today it occurred to me that the C# application that I am developing is a bit too slow on startup, and I decided to throw the visual studio profiler at it to see if I have goofed up somewhere. To my astonishment, under the profiler my app ran 10 times faster. The slowness I wanted to troubleshoot was nowhere to be found.\nI also tried running the release version, and as I expected it performed better than the debug version under the profiler, so the universe was still in its place, but still, I would very much like to know what the profiler did that made the debug version of my app run so much faster. For one thing, it would be a great convenience to be able to enjoy this speedup while developing; waiting for 2 instead of 20 seconds for my app to start every time I want to check something would be very good for productivity.\nI tried my luck with various google searches, and I found a couple of articles on StackOverflow, but none pointed at the exact cause of the problem.\nLuckily, after quite a bit of hard thinking, troubleshooting, and browsing through the myriad of potentially relevant settings in Visual Studio, I found the answer:\nIt is the \u0026quot;Enable unmanaged code debugging\u0026quot; feature.\nIn Visual Studio this feature is not under \u0026quot;Tools / Options / Debugging\u0026quot;, (because that would make too much sense,) it is under \u0026quot;Project / Properties / Debug\u0026quot;. Enabling that feature makes everything slow as molasses. The profiler disables the debugger, and that feature with it, so the application appears to run lightning fast.\nHere is a StackOverflow question to which I added my newly acquired wisdom:\nstackoverflow.com: Launching VS Profiler boosts Application Performance x20?\n","date":"2012-04-27T08:55:01.062Z","permalink":"https://blog.michael.gr/post/2012-04-27-net-code-running-faster-under-profiler/","title":"DotNet code running faster under the profiler?"},{"content":"I had been thinking about posting this for quite some time now, and all by coincidence I happened to get a chance to mention it just the other day in an answer that I wrote to a question on Programmers-StackExchange. So, here it is in a more formal way:\nIf class M stores or manipulates or in any other way works with instances of destructible (disposable) class D, it may not assume the responsibility to destruct these instances, unless it is explicitly told that ownership of these instances is transferred to it. Therefore, class M must accept a boolean called 'handoff' as a construction-time parameter, stating whether instances of D are being handed off to it, and it can therefore destruct them when it is done with them.\nExample:\n//Note: the IReader interface extends IDisposable IReader reader = new BinaryStreamReader( ... ); reader = new BufferedStreamReader( reader, handoff:true ); try { /* use the reader interface */ } finally { reader.Dispose(); //this destructs the buffered stream reader, and //destruction cascades to the binary stream //reader because handoff was specified. } Example:\nvar collection = new CollectionOfDestructibles( handoff:true ); collection.Add( new Destructible( 1 ) ); collection.Add( new Destructible( 2 ) ); collection.Add( new Destructible( 3 ) ); collection.Dispose(); //this destructs the collection and every single //one of its contents, since handoff was specified. In languages which support optional parameters, the 'handoff' parameter should default to false.\n","date":"2012-01-13T15:27:33.249Z","permalink":"https://blog.michael.gr/post/2012-01-13-the-handoff-pattern/","title":"The \"Handoff\" Pattern"},{"content":"Before reading any further, please read the disclaimer in the C# Bloopers post.\nWhen you declare a member variable and you pre-initialize it at the same time, and then you try to re-initialize it within the constructor without ever making use of its original pre-initialized value, you receive no warning about the field having already been initialized.\nnamespace Test3 { public class Test { public readonly string m = \u0026#34;m\u0026#34;; public string n = \u0026#34;n\u0026#34;; private string o = \u0026#34;o\u0026#34;; protected readonly string p = \u0026#34;p\u0026#34;; protected string q = \u0026#34;p\u0026#34;; private string r = \u0026#34;r\u0026#34;; Test() { m = \u0026#34;m2\u0026#34;; //Blooper: no warning about field having already been initialized. n = \u0026#34;n2\u0026#34;; //Blooper: no warning about field having already been initialized. o = \u0026#34;o2\u0026#34;; //Blooper: no warning about field having already been initialized. p = \u0026#34;p2\u0026#34;; //Blooper: no warning about field having already been initialized. q = \u0026#34;q2\u0026#34;; //Blooper: no warning about field having already been initialized. r = \u0026#34;r2\u0026#34;; //Blooper: no warning about field having already been initialized. o.ToLower(); //to prevent Warning CS0414: The field is assigned but its value is never used. r.ToLower(); //to prevent Warning CS0414: The field is assigned but its value is never used. } } } This means that you may accidentally invoke complex initialization logic twice, unnecessarily wasting memory and clock cycles, and it may also lead to logic errors, if by any chance that initialization logic has side effects which are only meant to occur once. It may also confuse someone reading your code, (or even yourself looking at your code months later,) trying to figure out what's the purpose behind the seemingly repeated initialization, before the realization sinks in that it is simply redundant. Furthermore, if the re-initialization happens to differ from the pre-initialization, a good question arises, asking which one of the two was meant to be the correct one.\nIt is a pity, because the compiler could warn the programmer against this pitfall.\nAlso see related post: C# Blooper №2: No warnings about accessing uninitialized members.\n","date":"2012-01-06T13:51:54.447Z","permalink":"https://blog.michael.gr/post/2012-01-06-csharp-blooper-3-no-warnings-about-fields/","title":"C# Blooper №3: No warnings about fields having already been initialized"},{"content":"Before reading any further, please read the disclaimer in the C# Bloopers post.\nWhen you declare a member variable, and then you try to read it from within the constructor without having first initialized it, you receive no warning about accessing an uninitialized member. This happens even if the member is declared as readonly.\nnamespace Test2 { public class Test { public readonly string m; public string n; protected readonly string o; protected string p; private readonly string q; private string r; Test() { m.ToUpper(); //Blooper: no warning about accessing uninitialized member. n.ToUpper(); //Blooper: no warning about accessing uninitialized member. o.ToUpper(); //Blooper: no warning about accessing uninitialized member. p.ToUpper(); //Blooper: no warning about accessing uninitialized member. q.ToUpper(); //Blooper: no warning about accessing uninitialized member. r.ToUpper(); //Blooper: no warning about accessing uninitialized member. q = \u0026#34;q\u0026#34;; //to prevent Warning CS0649: Field is never assigned to, and will always have its default value null r = \u0026#34;r\u0026#34;; //to prevent Warning CS0649: Field is never assigned to, and will always have its default value null } } } Someone might argue that this is behavior is fine because the member in question is guaranteed to contain its default value. First of all, a readonly member containing its default value is completely useless. (See C# Blooper №1: No warnings about uninitialized readonly members) Secondly, if the compiler is to help the developer catch potential errors and write better code, this is not a valid excuse: a different strategy is necessary.\nIf the programmer intends the member to contain its default value, then the programmer ought to explicitly state so. Failing to do so ought to imply intention to initialize the member later on, and certainly before any attempt is made to read the member. This way, the programmer can have it both ways: they can have members pre-initialized to their default values, and they can receive warnings when they fail to initialize members.\nAlso please note that the compiler is capable of detecting that the value with which a member is being explicitly initialized is the default value for the type of the member, and so it can refrain from emitting any additional code for the assignment; thus, there is no performance issue.\nAlso see related post: C# Blooper №3: No warnings about fields having already been initialized.\n","date":"2012-01-06T12:33:53.384Z","permalink":"https://blog.michael.gr/post/2012-01-06-csharp-blooper-2-no-warnings-about/","title":"C# Blooper №2: No warnings about accessing uninitialized members"},{"content":"\rPlease do not get me wrong; C# is awesome. It is the language of my choice, even though I am pretty well versed in C++ and Java. That having been said, it cannot be denied that C# has its share of flaws, too. In this series of posts I am documenting some of them, in no particular order.\nAlso please note that some of the issues described here are Visual Studio bloopers, or DotNet bloopers, and not C#-in-general bloopers.\nC# Blooper №1: No warnings about uninitialized readonly members\nC# Blooper №2: No warnings about accessing uninitialized members\nC# Blooper №3: No warnings about fields having already been initialized\nC# Blooper №4: Lame/annoying variable scoping rules, Part 1\nC# Blooper №5: Lame/annoying variable scoping rules, Part 2\nC# Blooper №6: No warnings about unused parameters\nC# Blooper №7: No warnings about unused private methods\nC# Blooper №8: No warnings for conditions that are always true/false\nC# Blooper №9: Annoying case statement fall-through rules\nC# Blooper №10: Switch statements are not properly formatted\nC# Blooper №11: Zero to Enum conversion weirdness\nC# Blooper №12: 'Where' constraints not included in method signatures\nC# Blooper №13: 'Stack' and 'Queue' do not implement 'ICollection'\nC# Blooper №14: Weird / annoying interface method visibility rules\nStay tuned, there is more to come.\n","date":"2012-01-05T12:21:02.778Z","permalink":"https://blog.michael.gr/post/2012-01-05-csharp-bloopers/","title":"C# Bloopers"},{"content":"\rIntroduction Hail Mary Initialization is the cargo cult programming practice of pre-initializing a local variable with some default value, \u0026quot;just in case\u0026quot;, even though that value will subsequently be overwritten in all code paths before it will ever be read. It is commonly done under the impression that it reduces the chances of error, but in reality it achieves the exact opposite: it increases the chances of error.\n(Useful pre-reading: About these papers)\nWhat it is Again and again I see programmers writing code like the following:\nint a = 0; if( x ) a = y; else a = z; f( a ); In this example, variable a is declared and initialized at line 1, but then it receives another value either at line 3 or line 5, before it is read for the first time at line 6. So, the initialization at line 1 was entirely superfluous.\nA surprisingly large number of programmers are under the impression that a plain local variable declaration like int a; is somehow incomplete. They have trained themselves to see such declarations as somehow missing something, without which bad things are bound to happen. As a result, they believe that when a local is declared it must always be pre-initialized with some value, even when a meaningful value is not yet available.\nThe belief is so popular, that it enjoys alleged \u0026quot;best practice\u0026quot; status, even \u0026quot;common knowledge\u0026quot; status, despite it being dead wrong.\nHow did it start The practice of indiscriminately pre-initializing all variables was not always wrong. It started back in the dark ages of the first Fortran and C compilers, when it was kind of necessary. Compilers back then had a combination of unfortunate characteristics:\nThey required all local variables within a function to be declared up-front. They were not smart enough to detect an attempt to read an uninitialized variable. Back in those days, accidental reading of uninitialized variables was a very common mistake, leading to many a monstrous bug. (See The Mother of All Bugs.) After having to troubleshoot and fix a few bugs of this kind, every new programmer would quickly learn to always pre-initialize every local variable without asking why.\nThe practice of blindly pre-initializing everything continued well into the 1990s, even though by that time compilers were fully capable of issuing warnings about accessing uninitialized variables. The practice continued because programmers were refusing to believe that they could be out-smarted by a compiler, so they were either not enabling, or deliberately disabling the relevant warnings.\nAfter decades of blindly pre-initializing everything, it became a cargo cult habit, so programmers keep doing this today, also in modern languages like Java and C#, without really knowing why they are doing it, nor asking themselves if there are any downsides to this practice.\nAnd as it turns out, there are plenty.\nWhat is wrong with it A number of things:\nIt violates the Principle of Least Astonishment1 When I see that a variable is initialized to a certain value, I am tempted to assume, based on the type of the variable and the initial value, that it has a certain role to play in the algorithm which follows. For example, seeing an integer initialized with zero prepares me to see it being used as a counter, or as an accumulating sum; then, it is rather disappointing to look further down only to discover that none of that happens, and the variable is overwritten with something entirely different before it is ever used.\nHowever, that's just an annoyance. There is worse, keep reading.\nIt confuses syntax highlighting When a variable receives a value only once, it is an effectively immutable variable. However, when a variable receives a value twice, then it is by definition mutable. If you have any self-esteem whatsoever, you are using a modern Integrated Development Environment (IDE) and you have configured it to syntax-color mutable variables differently from immutable ones. Unfortunately, Hail-Mary initialization will cause many of your local variables to be colored as mutable, even though they were never meant to be mutable. This is a misleading signal, and coping with it causes cognitive overhead.\nHowever, that's just an annoyance too. Stay with me, there is worse.\nIt leads to misuse of the type system Some data types do not have default values that you can pre-initialize a variable with, so the desire to always pre-initialize everything sometimes leads programmers to use slightly different types instead, which essentially constitutes a misuse of the type system. For example, some languages (e.g. C#) support explicit nullability of reference types. This means that you cannot pre-initialize a non-nullable reference variable with null. If, in your desire to pre-initialize everything, you decide to turn a non-nullable reference into a nullable reference, then you have just committed an act of sabotage against yourself, and anyone else who will ever look at that code, by making the code considerably more complicated than it needed to be. The same applies to enums: people often add special \u0026quot;invalid\u0026quot; or \u0026quot;unknown\u0026quot; values to their enums, for no reason other than to accommodate their craving for Hail-Mary Initialization. Such counterfeit values add needless complexity to everything.\nIt prevents the compiler from issuing useful warnings. Modern compilers of most mainstream programming languages do extensive data flow analysis and are fully capable of pointing out any situation where a variable is used without first having been initialized. Thus, accidental use of uninitialized variables is never a problem today.\nIf you say \u0026quot;but I do not see any such warnings\u0026quot; then you are trying to write code without first having figured out how to enable all warnings that your compiler can issue. Stop whatever it is that you are doing, figure out how to enable all warnings, enable them, and only then continue coding. If you say \u0026quot;but my compiler does not support issuing such warnings\u0026quot; then you are using the wrong compiler. Stop using that compiler, and start using a different one. If you say \u0026quot;but there is no such compiler for the language I use\u0026quot; then throw away everything and start from scratch with a different language. I do not care what it takes; in the 3rd millennium you cannot be programming without flow analysis warnings. Once you have warnings about uninitialized variables, the superfluous initialization of a variable becomes bad practice, because it circumvents other checks that the compiler does for you, and opens up the possibility of error:\nIf you begin by pre-initializing a variable with a value which is by definition meaningless, (since a meaningful value is not yet known at that time, otherwise you would have just used that meaningful value and you would be done,) then as far as the compiler can tell, the variable has been initialized. The compiler does not know that the initial value is meaningless. Thus, if you forget further down to assign an actual meaningful value to that variable, the compiler will not be able to warn you. So, you have deliberately sent yourself back in time, to the dark ages of the first compilers, where warnings for uninitialized variables had not been invented yet. Essentially, you have circumvented the safety checks of the compiler and you have achieved the exact opposite of what you were trying to accomplish: instead of decreasing the chances of error, you have increased the chances of error.\nFortunately, modern compilers are not only capable of issuing a warning if you attempt to use an uninitialized variable; they are also capable of issuing a warning when you unnecessarily initialize a variable. Unfortunately, programmers that keep making these mistakes tend to have both of those warnings disabled.\nConclusion Never initialize any variable before you have a meaningful value to assign to it. Make sure you have all warnings enabled, so you do not even have to remember to do this; the compiler will remind you.\n(This post has evolved from an original answer of mine on CodeReview.StackExchange.com.)\nOld comments\nNeolisk 2012-01-04 20:34:48 UTC\nYour point would be 200% right if only compilers always worked right. For example, when you have an if statement checking AAA Is Nothing, VS warns you that AAA might be Nothing during this call, so be careful... I guess I have to thank Microsoft for that, but the point stands. :)\nSee Principle of Least Astonishment\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2012-01-03T10:11:32.748Z","permalink":"https://blog.michael.gr/post/2012-01-03-hail-mary-initialization/","title":"Hail-Mary Initialization"},{"content":"(When the class is public and the member is public, protected or protected internal.)\nBefore reading any further, please read the disclaimer in the C# Bloopers post.\nThe C# compiler is kind enough to give you a \u0026quot;field is never assigned to\u0026quot; warning if you forget to initialize a readonly member which is private or internal, or if the class in which it is being declared is internal. But if the class is public, and the readonly member is public, protected or protected internal, then no warning for you! Why, oh why?\nnamespace Test1 { class Test1 { #if TRY_IT public readonly int m; //OK: warning CS0649: Field is never assigned to, and will always have its default value 0 protected readonly int n; //OK: warning CS0649: Field is never assigned to, and will always have its default value 0 internal readonly int o; //OK: warning CS0649: Field is never assigned to, and will always have its default value 0 private readonly int p; //OK: warning CS0649: Field is never assigned to, and will always have its default value 0 protected internal readonly int q; //OK: warning CS0649: Field is never assigned to, and will always have its default value 0 Test1() { if( p != 0 ) //To avoid warning \u0026#39;The field is never used\u0026#39; return; } #endif } public class Test2 { #if TRY_IT private readonly int m; //OK: warning CS0649: Field is never assigned to, and will always have its default value 0 internal readonly int n; //OK: warning CS0649: Field is never assigned to, and will always have its default value 0 Test2() { if( m != 0 ) //To avoid warning \u0026#39;The field is never used\u0026#39; return; } #endif public readonly int o; //Blooper: no warning about field never assigned to. protected readonly int p; //Blooper: no warning about field never assigned to. protected internal readonly int q; //Blooper: no warning about field never assigned to. } public sealed class Test3 { public readonly int m; //Blooper: no warning about field never assigned to. } } For a moment you might think \u0026quot;well, a descendant might initialize that member\u0026quot;, but that theory does not hold any water, for a number of reasons:\nInternal classes may also be subclassed, but the compiler does not fail to issue the warning in their case. Sealed classes may not be subclassed, but the compiler fails to issue the warning in their case, as Test3 in the sample code demonstrates. The warning makes sense for the sake of the integrity of the base class regardless of what a derived class may or may not do. Lastly but most importantly, the C# specification expressly prohibits a derived class from initializing a readonly member of a base class. You get Error CS0191: A readonly field cannot be assigned to (except in a constructor or a variable initializer) which, incidentally, is a little bit misleading, because you may be trying to assign the field from within a constructor, only it is the constructor of the wrong class. According to MSDN Documentation about this warning, the exhibited behavior is to be expected:\nCompiler Warning (level 4) CS0649: Field 'field' is never assigned to, and will always have its default value 'value' The compiler detected an uninitialized private or internal field declaration that is never assigned a value.\nThe question is: why?\nUPDATE:\nI posted this question on StackOverflow, and Eric Lippert himself answered it. The short answer is that it is an oversight of the compiler, but the long answer is also quite interesting and worth reading.\n","date":"2011-12-31T09:26:01.408Z","permalink":"https://blog.michael.gr/post/2011-12-31-csharp-blooper-1-no-warnings-about/","title":"C# Blooper №1: No warnings about uninitialized readonly members"},{"content":"Note: This post has been superseded by a new post in 2022. See Intertwine.\nThis is a C# project that I did back in 2011. It consists of a (rather informal) white paper which describes the project, and a zip file containing the source code in the form of a Microsoft Visual Studio solution.\nHere is the abstract:\nA mechanism is proposed for converting (entwining) method call invocations of any interface to a general purpose single-method normal form, and converting back (untwining) from the normal form to interface invocations, so that operations can be performed on the normal form in a way agnostic to the interface being invoked. The normal form is a delegate in C# or a functional interface in Java, realized as object AnyCall( int selector, object[] parameters ). A DotNet implementation is provided in C#, though the discussion also applies to Java.\nAnd here is the table of contents:\nAbstract (page 1) The Problem (page 1) Why messages are bad (page 2) What is missing (page 2) The Solution (page 2) A hand-crafted solution (page 3) Automating with Intertwine (page 6) Appendix 1: A note about Dynamic Proxies (page 6) Appendix 2: An example: Interface multicasts (events) (page 7) Appendix 3: Things to fix (page 8) Download the white paper: Intertwine v2.1.pdf Download the source code: Intertwine v2.0.zip\n","date":"2011-10-16T18:42:25.526Z","permalink":"https://blog.michael.gr/post/2011-10-16-intertwine-normalizing-interface/","title":"Intertwine: Normalizing Interface Invocations"},{"content":"If you are a developer with any real-world experience to speak of, you have undoubtedly come across the following situation: you made a change which was not meant to be committed, (for instance, some debug statement or some mock-up of functionality meant to be filled-in later,) and then you forgot about it and went ahead and committed all of your code. This mishap can be a cause of severe frustration for your fellow co-workers, and the source for memorable \u0026quot;WTF moments\u0026quot; for the QA department.\nNow, if you are like me, you like to automate things. Why should I have to remember to do something on my computer, when my computer can be tasked with reminding me to do it? Is a computer the ultimate automation tool or not?\nThe interwebz abound with questions on precisely how to achieve this bit of automation:\nstackoverflow: Subversion: prevent local modifications to one file from being committed?\nstackoverflow: SVN Pre-commit hook for temporarly commented out code (in java)?\nstackoverflow: SVN: Is there a way to mark a file as “do not commit”?\nIn short, if you are using SVN, here is how to do it:\nHave a pre-commit hook in the repository which checks to see whether any file contains the string NOCOMMIT, and if so, it fails the commit.\nSo, when I alter a source file in a way which is not meant to be committed, I append a //NOCOMMIT comment right next to each change, and I do not have to worry about it anymore. If I do accidentally attempt to commit it, the pre-commit hook of the repository will block my commit and let me know which files contain the NOCOMMIT keyword, so I can go into each one of those files and fix it.\nI find this feature so useful that I honestly even use it when programming at home, where obviously, I am the only programmer in the team.\nIf you are using SVN on windows, you can paste the following into a file called pre-commit.bat in the hooks folder of your SVN repository:\n:: Stops commits that contain the NOCOMMIT keyword. setlocal set REPOS=%1 set TXN=%2 SVNLook diff %REPOS% -t %TXN% | findstr /I /M /L NOCOMMIT \u0026gt; nul if %errorlevel% gtr 0 ( exit 0 ) else ( echo Your commit has been blocked because it contains the keyword NOCOMMIT. 1\u0026gt;\u0026amp;2 exit 1 ) With SVN on Unix systems, something like the following will do the trick, though please note that I have not tested it. (Note: suggestions from a comment by Georgi have been applied.)\n#!/bin/sh REPOS=\u0026#34;$1\u0026#34; TXN=\u0026#34;$2\u0026#34; /usr/local/bin/svnlook diff -t \u0026#34;$TXN\u0026#34; \u0026#34;$REPOS\u0026#34; | grep -i \u0026#34;NOCOMMIT\u0026#34; \u0026gt; /dev/null \u0026amp;\u0026amp; { echo \u0026#34;Your commit has been blocked because it contains the keyword NOCOMMIT.\u0026#34; 1\u0026gt;\u0026amp;2; exit 1; } Old comments\nGeorgi 2014-05-13 17:00:18 UTC\nHi Michael,\nThank you for the great article That's what I needed. Just a few notes on the linux script:\nsvnlook command should be \u0026quot;diff\u0026quot;, not \u0026quot;log\u0026quot;. There should be opening bracket before \u0026quot;echo\u0026quot;. I only got it to work when I added \u0026quot;exit 0;\u0026quot; at the end of the file. So my working script looks like:\n#!/bin/sh REPOS=\u0026#34;$1\u0026#34; TXN=\u0026#34;$2\u0026#34; SVNLOOK=/usr/bin/svnlook $SVNLOOK diff -t \u0026#34;$TXN\u0026#34; \u0026#34;$REPOS\u0026#34; | grep -i \u0026#34;NOCOMMIT\u0026#34; \u0026amp;gt; /dev/null \u0026amp;\u0026amp; { echo \u0026#34;Your commit has been blocked because it contains the keyword NOCOMMIT.\u0026#34; 1\u0026amp;gt;\u0026amp;2; exit 1; } exit 0; michael.gr 2014-05-13 17:22:40 UTC\nThank you for contributing, Georgi!\nBen 2015-04-17 19:17:29 UTC\nGood call Michael: http://stackoverflow.com/questions/29707649/svn-pre-commit-hook-how-to-block-a-keyword-in-certain-file-types\nBen 2015-04-17 16:47:08 UTC\nThis is soooo close to what I need, but not quite there. For some reason, I've been unable to do what I need, which is check for existence of a keyword in only certain filetypes. For instance, I do not want to allow .java files to have \u0026quot;http://\u0026quot; or \u0026quot;https://\u0026quot; in them. I've tried using --include=*.java but it seems to get ignored in my pre-commit file (works fine on the command line). Thoughts? Thanks in advance!\nmichael.gr 2015-04-17 18:38:00 UTC\nI do not know, Ben. But it sounds like a good question for stackoverflow.com\n","date":"2011-06-11T18:11:46.729Z","permalink":"https://blog.michael.gr/post/2011-06-11-do-not-commit-that/","title":"Preventing a file from being committed to SVN if it contains a certain keyword"},{"content":"Solon's original phrase was \u0026quot;Gerasko D' Aei Polla Didaskomenos\u0026quot;, but here in Greece we use the shorter form, \u0026quot;Gerasko Aei Didaskomenos\u0026quot;. I grow older forever learning new things.\nIt is never too late to learn something new. For example, just a few months ago I learned of the existence of the \u0026quot;??\u0026quot; operator in C#, after several years of using the language. Of course I had come across that operator in the reference, but it had not occurred to me how useful it would be in real-life scenarios. Then, one day I chanced upon someone else's code making use of it. There was a big 'aha!'.\nSo, today I learned one of the most useful things I know about Visual Studio.\nLet us suppose for a moment that you are a real programmer, not a button-clicker that Microsoft thinks you are. So, when you double-click on a file in the Solution Explorer, what you want to see is the code, naturally. And this is what usually happens. But then one day you start programming Windows Forms, and suddenly double-clicking on some (but not all) of the files in your project does not show the code. Instead, Microsoft believes that what you must want to see in these cases, is, undoubtedly, their spiffy Windows Forms Designer. Because obviously, they think you must be a mindless button-clicker.\nSo, every time you double-click on a .cs file that happens to contain a form class, you are presented with a great big huge gotcha! as the designer appears before you instead of the code editor, and you are reminded, for the thousandth time, that double-clicking on a .cs file will do vastly different things, depending on what the class inside that .cs file derives from.\nSo, is there a way to convince Visual Studio to always show you the code instead of the designer when you double click on an item in solution explorer? In other words, is there any way to tell Visual Studio that you are a real programmer and not the button-clicker that Microsoft thinks you are?\nIt turns out that there is. I found it out today thanks to StackOverflow, (see here and here,) and it really made my day. In short, here it is:\nThe next time you want to open a .cs file, right-click on it and select \u0026quot;Open With...\u0026quot;; on the dialog that opens, select \u0026quot;CSharp Editor\u0026quot;, and then click on the button which says \u0026quot;Set as Default\u0026quot;. That's it, it will work forever after for all .cs files.\nThe only problem now is, that after a couple of years of Windows Forms Programming while being conditioned to always be careful to right-click on forms .cs files and select 'View Code', I have to first unlearn that before I can start double-clicking freely again.\nCheers!\n","date":"2011-06-09T09:05:07.22Z","permalink":"https://blog.michael.gr/post/2011-06-09-gerasko-aei-didaskomenos/","title":"Gerasko Aei Didaskomenos"},{"content":"So, for some time now, whenever I try to 'batch build' from within Microsoft Visual Studio 2010, I get the following error:\nC:\\WINDOWS\\Microsoft.NET\\Framework\\v4.0.30319\\Microsoft.Common.targets(2868,9): error MSB3021: Unable to copy file \u0026quot;x\\Intermediate\\y.dll\u0026quot; to \u0026quot;x\\y.dll\u0026quot;. Could not find file 'x\\Intermediate\\y.dll'.\nIndeed, there is no file 'x\\Intermediate\\y.dll'. But when I switch configurations and try to build the regular (non-batch) way, it builds fine.\nAmused, and since I can live without the batch-build functionality, I have just let it be all this time, writing it off as one more of those weird wonders of Microsoft.\nThen today I figured out what the problem was. I am to blame, because I brought this upon myself. And I brought it upon myself while trying to circumvent some other, even more wonderful, weird wonder of Microsoft.\nMy MSBuild woes begun years ago, on my very first day of C# programming, when I saw those bin and obj directories in my project folder, and my first reaction was, of course, to want to make them go away.\nThere is an option in project properties to set the output folder, but not the intermediate folder. This takes care of the bin folder, but the stupid tool keeps creating an obj folder under the solution folder and puts all intermediate files in there, no matter what.\nAs you might understand, I decided that I just would not have any of that. I was determined to never write a single line of C# if I did not first find a way to make that annoying obj directory go away.\nNeedless to say, on my first day of C# programming I did not write any C# at all. I just banged my head against the keyboard triyng to find a way to get that folder to go away.\nAfter a lot of googling around, I discovered allegations about the existence of some \u0026lt;IntermediateOutputPath\u0026gt; tag that goes inside a project file. So, I edited the .csproj file with my text editor and after each \u0026lt;OutputPath\u0026gt; tag I inserted an \u0026lt;IntermediateOutputPath\u0026gt; tag as follows:\n\u0026lt;IntermediateOutputPath\u0026gt;x\\Intermediate\\\u0026lt;/IntermediateOutputPath\u0026gt;\n(where x is my output path.)\nNow, MSBuild would put the intermediate files in the folder that I specified, but it kept ALSO putting them in obj under my solution folder. That was just mind-blowing.\nSo, I took a drastic measure: I deleted the obj folder and I created an empty text file called obj in its place, which I marked as read-only and hidden, and I added it in the ignored files in SVN.\nStrangely enough, this awful hack worked. Or at least it appeared to work. Just today I realized that it worked for the regular build, but it has been the reason why my batch-build does not work.\nSo, it appears that the only way to have batch-build functionality in Visual Studio is to resign to having a stupid obj folder under your project folder. Oh, well!\n","date":"2011-05-29T08:40:41.518Z","permalink":"https://blog.michael.gr/post/2011-05-29-wonderful-msbuild/","title":"Wonderful MSBuild"},{"content":"Java 7 introduced the AutoCloseable interface, which is roughly equivalent to the IDisposable interface of C#, to be used in synergy with the new try-with-resources statement, which is equivalent to the using-disposable construct of C#.\nThe problem with Java's AutoCloseable interface is that its close() method is declared to throw a checked exception:\nvoid close() throws Exception This is a problem if you are one of the many programmers who prefer unchecked exceptions over checked ones, because it forces you to deal with checked exceptions every time you write a try-with-resources statement, despite the fact that none of your classes ever throw any checked exceptions on close(). Simply declaring that your class implements AutoCloseable forces checked exceptions upon you.\nLuckily, there is a fix for this. Here it is:\npublic interface AutoCloseable2 extends AutoCloseable { @Override void close(); } There, I fixed it for you.\nBy declaring a new interface which redefines the close() method as not throwing any checked exceptions, the problem goes away.\nP.S.\nI just looked at the Oracle documentation for the AutoCloseable interface and found out that this had already been anticipated:\n[...] subclasses of the AutoCloseable interface can override this behavior of the close method to throw specialized exceptions, such as IOException, or no exception at all.\n","date":"2007-11-17T09:30:50.652Z","permalink":"https://blog.michael.gr/post/2007-11-fixing-autocloseable-interface-of-java/","title":"Fixing the AutoCloseable interface of Java"},{"content":"\rIn the movie \u0026quot;Sunshine\u0026quot; by Danny Boyle (IMDB) there is a scene which is impossible to comprehend unless you have some knowledge of physics. The protagonist Robert Capa (Cillian Murphy) finds himself trapped in an airlock from which he must escape at any cost. The airlock contains a space suit, and has one outer hatch towards space, and one inner hatch towards the interior of the spaceship, which is locked.\nCapa first uses a blowtorch to bore a tiny hole on the inner hatch; then, he wears the spacesuit, fastens himself securely so that he won't be blown away, and presses the button which opens the outer hatch. The airlock decompresses violently to the outer void, and for a split second nothing else seems to happen, but immediately afterwards the inner hatch gets mysteriously blown away too, and the atmosphere of the ship escapes, (together with many objects from its interior,) through the airlock, to space.\nAfter a few moments the flow of debris abates, and Capa is able to enter the ship. (Do not try this at home.) The movie does not spend any time explaining the cause of the explosion of the inner hatch; for those who got it, fine; for the rest, it is a mystery.\nWhat blew up the inner hatch was the tiny hole that Capa bore on it before decompressing the airlock. It works exactly the same way a pin prick causes a balloon to explode. You can read all about it in Wikipedia: Balloon popping but I think I can do a better job at explaining it:\nEvery point on the balloon's surface experiences an outward force due to the pressure differential between the inside and the outside of the balloon, but it is kept in place due to its bonds with the material surrounding it. Thus, under normal conditions, the forces balance out, and the balloon is in equilibrium.\nHowever, if a tiny hole is made on the balloon's surface, the bonds are disturbed: the points around the hole do not have enough bonds with surrounding material anymore to keep them in place. As a result, they start pulling away, making the hole slightly bigger; the bigger hole allows more air to escape, thus, increasing the imbalance, and causing the points around the hole to pull away even more. This is a reinforcing feedback loop, which results in catastrophic escalation of the rate at which the hole expands; in other words, the balloon explodes.\n","date":"2007-09-23T01:16:34.213Z","permalink":"https://blog.michael.gr/post/2007-09-23-exploding-hatch-in-sunshine/","title":"The exploding hatch in the movie Sunshine (2007)"},{"content":"\r]\nYesterday I did maintenance on my web site and I re-organized lots of things. I also decided to post a recent picture of me, which happens to be pretty good; I know I still owe it to everyone to post pictures from past vacations, and from my recent wedding, but that will take some time to put together, while this picture can be posted now, so, here you go.\nNote in 2025:\nThis post was retrieved from my old blog via The Wayback Machine (archive.org). It used to link to the picture in higher resolution, but archive.org did not index the higher resolution image.\n","date":"2005-11-21T00:00:00Z","permalink":"https://blog.michael.gr/post/2005-11-21-recent-picture/","title":"General web site maintenance, plus a recent picture of me"},{"content":"\rNote in 2025:\nThis content was retrieved from my old blog via The Wayback Machine (archive.org). Unfortunately, archive.org did not store some of the pictures, and any of the ZIP files.\nA bunch of graphics demos that I created during the first half of the nineties, along with quite a bit of rambling about the techniques that I used, my thoughts on how things have changed since then, references to the original games which influenced the creation of the demos, etc.\nIntroduction These are a bunch of little applications that I created back in the dark ages of computing, when filenames were eight plus three in caps, screens were 80 by 25 in text, game graphics were 320 by 200 in 256 colors at best, the Internet barely existed, the memory was divided in 16-byte segments, and even the slightest program crash meant that you had to reboot your computer. I am talking about the first half of the nineties.\nBeing more than ten years old, these demos certainly qualify as vintage stuff by software aging standards. Each one of them is a programming testbed, or a proof of concept at best, and not really a demo that you can fire up and sit back and relax watching as the demos of the famous demo scene are. None of them is anything great compared to the awesome graphics that we are used to today, but back then each one of them did represent a feat, and a bunch of them together in a floppy disk along with my Résumé did in fact result in job interviews with Southern Californian gaming companies such as Activision, Blizzard, and CyberDreams. CyberDreams did actually extend a job offer to me, but I did not accept it because I wanted more money. These demos were all written in Borland C++ 3.5 and Borland Turbo Assembler. (I used to be a big Borland fan back then, when it made sense.) The target architecture was 80386 16-bit real mode, and the target operating system was MS-DOS.\nThe demos ARK ]\nThe ARK demo shows the basic technique of back-buffered sprites. It was created with the aspiration to one day become a 3D Arkanoid game. You are looking at a room, there is a red ball bouncing off the walls making some oh-so-eighties sounds from the PC speaker, and there is a racquet which you control with the mouse. That's all. The ball always bounces off the screen, there no check to determine whether it hit the racquet or not.\nThis demo is so simplistic that it does not even utilize pixmap scaling. As the ball moves close to or away from the viewer, the different ball size is achieved by selecting a larger or smaller sprite to display. The only cool feature in this demo is the minimal front-buffer update, which basically means that instead of copying the entire back-buffer into the video RAM on every frame, it keeps track of and updates only those rectangular portions that have changed since the last frame.\nDownload ark.zip\nAvailable controls: SPACE Begin the demo Mouse Move the racquet around ESC Terminate the demo SCROLL Two things that the ARK demo was obviously missing was text display and pixmap scaling. When I developed these routines a short time thereafter, I created this little demo to show them off. SCROLL.EXE just displays an image of an eagle which zooms in and out of the center of the screen, while a scrolling marquee of text zips across the screen. The pixmap scaling routine contained in this demo represents my very first endeavors with the concept of interpolation, as well as one of my first attempts at live code generation: when you call this routine to scale a pixmap from a certain size to another size, it generates a piece of 80386 machine code which will scale one raster line from the source image into the destination image as efficiently as possible, and then it invokes this code while interpolating over raster lines.\nThe text rendering routine also makes use of live code generation: instead of having a single character drawing routine which reads information from some font data structure and writes it to the video buffer, there is one character drawing routine per character in the font. Each one of these character drawing routines consists of a series of machine instructions that directly store the pixels that make up the character at the raster lines pointed by the DI register. This is about as fast as it gets.\nThese live-code-generation and loop unwinding techniques are good reminders of how different the coding paradigm was back then: the hardware and the firmware did practically nothing for you, so you had to do everything by yourself, and the CPUs were so wimpy that many times you had to work hard and come up with miracles in order to achieve the best possible performance.\nBack then unwinding a tight loop into a long sequence of code was considered a good idea, because the decrement-and-jump instruction took up a non-negligible number of clock cycles, as well as the precious CX register. Besides, the prefetch queue of the 386 was only long enough for a couple of instructions, meaning that no matter how small your loop was, it would never enjoy the privilege of being fully cached in the CPU anyway, so short code offered no advantage over long code as far as caching was concerned. Mind you, it would take a couple of more years before the Microsoft Visual C++ Compiler would start offering \u0026quot;function inlining\u0026quot; and \u0026quot;loop unwinding\u0026quot; optimizations.\nToday these tricks have either fallen out of grace, or are inapplicable to begin with. Inapplicable, because the hardware and the firmware nowadays handle all the low level stuff, and even when they do not, we have so much CPU power that we never really need to bother with assembly language tricks anymore. Out of grace, because tight code is preferred nowadays over long code in order to best utilize the caching that modern hardware architectures are invariably equipped with, and also because modern CPUs will generally execute the loop-counter-decrement-and-jump instruction in parallel with other instructions in their pipeline, possibly predicting its outcome, and completely invalidating our intuitive notion of what is efficient and what is not, by yielding a total execution time of nearly zero clock cycles for that instruction.\nI used to program in Assembly language with the reference book next to me, not because I needed to look up the instructions, (of course I knew them all by heart,) but because I had to always look up the number of clock cycles that each individual instruction and each individual addressing mode would require, so as to always be sure that for any given block of code I had chosen the most optimal sequence of instructions. I look back at those times now, and I think to myself \u0026quot;what a waste of time!\u0026quot;.\nDownload scroll.zip\nAvailable controls: Any key Terminates the demo BMPROT BMPROT demonstrates the use of a routine written in 80386 Assembly which copies a convex polygonal area from one pixmap to another, applying rotation and scaling on the way. Its creation was inspired by just one simple little thing: the nicely rotated images of galaxies that were decorating the background of spaceship dogfights and serving as points of reference in Wing Commander by Origin. When you run the demo you see the image of an eagle rotating around the center of the screen. You can use the plus and minus keys on the numeric keypad to increase or decrease the scale factor of the rotating picture. I think that the visual result achieved by my routine is better than that of Origin, but then again I suppose that is only because Origin choose to take shortcuts in order to save clock cycles.\nOn the top-left corner of the screen you can see the number of frames per second, along with the current settings for vertical flyback and buffering. You can try hitting 'V' to disable vsync and see the rotation go ballistic.\nYou might notice that the image is kind of jerky as it rotates, and this is especially visible at high zoom levels, when the image is covering the entire screen. This is partly due to the complex scheme employed for achieving proper clipping for the transformed polygon both in the source and in the destination pixmap, and partly due to reasons that... are currently unknown and under research. (Hey, nobody's perfect.)\nDownload bmprot.zip\nAvailable controls: V Toggle waiting for vertical flyback B Toggle clearing the backbuffer Numeric Keypad Plus ('+') Enlarge the rotating bitmap Numeric Keypad Minus ('-') Shrink the rotating bitmap ESC Terminate the demo VECTOR This is a vector graphics demo which displays three-dimensional solid objects rotating, moving around, and bouncing against the walls of a room, using a few different drawing modes.\nThe original version of this demo had just two drawing modes: Wireframe and Solid Polygons, while Gouraud Shading was added later. It was inspired by X-Wing by Lucas Arts and MechWarrior II by Activision, which made use of vector graphics, as opposed to the bit-mapped graphics of games such as Wing Commander by Origin, which had been pretty standard until those days.\nUnfortunately, as I was working on graphics alone on my spare time, I was constantly re-inventing wheels because I did not have the necessary resources to learn the theory behind things, I had no guidance, no previous work to build on, and also because I was arrogant enough to believe that I could figure out everything by myself in less time than it would take me to find the relevant literature and read it. So, I was still trying to figure out how on earth X-Wing and MechWarrior managed to create their marvelous sensation of realism, (it seemed to have something to do with the way they illuminated the polygons that made up the objects, but back then I did not even know it was called Flat Shading,) when Lucas Arts released that awesome sequel to X-Wing called TIE Fighter, and I still remember how my jaw dropped when I first saw TIE Fighter's much more complex form of polygon shading. Luckily, in the options screen of TIE Fighter I discovered that this technique had a name, and the name actually sounded like something researchable: \u0026quot;Gouraud Shading\u0026quot;. That was when I finally realized that I was going nowhere by trying to figure things out by myself, and I went straight to the library to learn who that Gouraud guy was and exactly what he had become famous for. A few sleepless days later, the \u0026quot;Gouraud Shaded Polygons\u0026quot; drawing mode was added to the demo.\nThe major challenge here was that on a 256-color palette you cannot just interpolate from one intensity value to another and expect the result to look good, because the palette generally contains few colors between any desired pair of intensity values, so a hideous banding phenomenon occurs. In order to avoid banding, it is necessary to add some kind of progressive noise. The prospect of coming up with an algorithm which performs real-time dithering on every single face of a 3-D animated solid object sounded like plain impossible, but luckily TIE Fighter stood as proof that it could be done. I did not consult any books for that one, because I knew that the solution would have to be very dirty in order to be feasible. My dithering algorithm (or should I say hoax) loads a pseudorandom bit pattern in a 32-bit register, and every time it needs to render a pixel it performs one circular rotation on that register, which leaves a pseudorandom bit in the carry flag. Then, an \u0026quot;add zero with carry\u0026quot; operation is performed on the accumulator, (which already contains the color index of the pixel to render,) and this has the effect of pseudorandomly incrementing or omitting to increment the color index, essentially adding noise. The algorithm makes use of 16 different 32-bit pseudorandom bit patterns ranging from almost all bits cleared to almost all bits set, and at any given time it selects one of the patterns depending on the desired intensity, so a total of 16 X 16 = 256 different shades of noise are possible out of only 16 different palette entries.\nPlease note that the mathematics in this demo are not sound, and so the shading does not look as cool as it would with proper math. When I wrote the Gouraud Shader I wanted to see the results on my screen ASAP, without having to mess with --what appeared to me back then as-- hairy trigonometry, so what I did in this demo is that I emulated a light source directly above the object (at 12 o'clock) by calculating the intensity value of each vertex based on nothing but its Y coordinate (its distance from the light source) instead of properly calculating the relative angle between the light source and the normal to the surface. Not correct, but good enough to show the beauty of Gouraud Shading with progressive noise.\nAutomatic object movement, rotation, and selection are all enabled by default for the purpose of the demo, but you can disable them if you want to take control of the objects by hitting Alt+M, Alt+S, Alt+R, and Alt+D. You can hit F1 to see the help screen which explains some of the keys, but the easiest thing to do is to print the \u0026quot;available controls\u0026quot; section at the end of this article.\nThe number of frames per second is displayed on the top-right corner of the screen. Since waiting for vertical flyback is enabled by default, this number should reflect the refresh rate that the VGA virtualization module of your Virtual DOS Machine wants my demo to see, which is usually around 65, regardless of what your video card feeds your monitor with. If you disable waiting for vertical flyback by hitting Alt+V, the number of frames per second should skyrocket. (It reads 300 on my 1.7 GHz P4.) Unfortunately, the code which handles the movement of objects in space is a simplistic quantum-of-spatial-advancement-per-frame type of thing, (instead of a proper velocity vector approach,) so as the number of frames per second increases, the movement of the objects becomes frantic. I know that this not the right way to do things, but for the purposes of this demo it works just fine.\nVECTOR.EXE was made out of a multitude of .CPP and .ASM files from my graphics library, and one and only one application-specific file. This file, VECTOR.CPP, is included with the demo as sample source code. Please note that this code has not been written to impress; this is how I used to write code for myself ten years ago. Also please note that this code was NOT made having performance as an absolute priority; when writing it I was more interested in proving the concepts that I had in my mind, rather than proving that I can write fast and dirty spaghetti code.\nVECTOR.EXE obtains the descriptions of the objects that it renders from a text file called VECTOR.BDL, which contains statements of a home-brewed 'Body Definition Language', which is read using a simple hard-case parser. (XML had not been invented yet back then.) The invisible surface elimination algorithm is very simple: all faces are required to be specified in BDL in a clockwise fashion, so, if after projection a face is still clockwise, then it is visible; if it has become counter-clockwise, it is facing away from the viewer, so it is invisible.\nDownload vector.zip\nAvailable controls: F1 Display the help screen Alt+M Toggle automatic object movement Alt+S Toggle automatic object selection Alt+R Toggle automatic object rotation Alt+D Toggle automatic drawing mode selection Alt+V Toggle waiting for vertical flyback Numeric Keypad Up Arrow Move the object up Numeric Keypad Down Arrow Move the object down Numeric Keypad Left Arrow Move the object to the left Numeric Keypad Right Arrow Move the object to the right Numeric Keypad '+' Move the object closer Numeric Keypad '-' Move the object away Numeric Keypad '5' Re-center the object, bring it close to the camera, and undo any rotation. Cursor Keypad Up Arrow Rotate the object clockwise about the X axis Cursor Keypad Down Arrow Rotate the object counter-clockwise about the X axis Cursor Keypad Left Arrow Rotate the object clockwise about the Y axis Cursor Keypad Right Arrow Rotate the object counter-clockwise about the Y axis '\u0026lt;' Rotate the object clockwise about the Z axis '\u0026gt;' Rotate the object counter-clockwise about the Z axis A Display the cube B Display the checkered sphere C Display the pyramid D Display the cone E Display the cylinder F Display the dodecahedron G Display the decatetrahedron H Display the crystal with the pentagonal corss-section I Display the crystal with the octagonal cross-section J Display the plain Sphere K Display the \u0026quot;Weird #1\u0026quot; object L Display the \u0026quot;Weird #2\u0026quot; object M Display the \u0026quot;Weird #3\u0026quot; object ESC Terminate the demo TESTSHAD This is a testbed for my implementation of the Gouraud shading algorithm, which is good to have, because the calculations of the intensity values in the VECTOR demo are so mickey-moused that someone might conceivably doubt that I actually developed a real Gouraud shader. So, this testbed gives you the ability to test the functionality of the Gouraud shader to make sure it works.\nAn object with three differently colored faces is displayed. The three faces have one common vertex at the center, in which the intensity is set to maximum, while the other three peripheral vertices have intensities which you can vary individually to see the Gouraud shader at work. The coordinates of each vertex are picked at random, but if you do not like them you can hit SPACE to have the program select a different set of coordinates. At any given time you can see the intensity value of one of the peripheral vertices, and by pressing TAB you can cycle through all three of them. While the intensity value of a peripheral vertex is displayed, you can use the '+' and '-' keys on the numeric keypad to increase or decrease the intensity value at that vertex, so that you can watch the Gouraud shader at work.\nIf you think that the colors do not look very nice, remember that this is good old video mode 13, meaning that the maximum number of individual colors on the entire screen is 256, which makes color arithmetic quite hard. In this particular palette I have allocated 16 shades of red, 16 shades of green, and 16 shades of blue.\nDownload testshad.zip\nAvailable controls: SPACE select a different set of random vertex coordinates TAB cycle through the peripheral vertices Numeric Keypad Plus ('+') increase the intensity at the current vertex Numeric Keypad Minus ('-') decrease the intensity at the current vertex ESC terminate the demo TEX The novelty of the Gouraud-Shaded polygons of TIE Fighter did not last long; as the average home PC was becoming faster and faster, some pretty amazing games started showing up, including Quake I by Id Software and Descent by Parallax Software, both of them featuring fully three-dimensional, fully textured worlds for the first time on a PC. Back then, that did not look just advanced, it looked like magic. (Need I quote Arthur C. Clarke's 3rd Law?) Still not having learned my lesson, I tried to write a texturing routine for arbitrarily oriented polygons all by myself without first looking at any previous research whatsoever, and I failed miserably. Then I set my goal to something more reasonable, that is, texturing of surfaces that are either horizontally or vertically oriented. That one worked, and the result can be seen in this demo.\nYou are hovering above an endless surface covered with a hexagonal tile. You can move forward and backward, you can increase or decrease your altitude above the surface, and you can turn to your left or to your right. That's all.\nUnfortunately, the keyboard sensing logic of this demo is so simplistic that it is dependent on the typematic autorepeat of the system on which it runs, so navigation is a little bit cumbersome. My apologies for this.\nDownload tex.zip\nAvailable controls: Left Arrow Turn Left Right Arrow Turn Right Up Arrow Move Forward Down Arrow Move Backwards Page Up Increase Altitude Page Down Decrease Altitude Numeric Keypad '5' Reset Altitude ESC terminate the demo FLIGHT In 1994 I went to a job interview with CyberDreams, during which the gentleman interviewing me allowed me to have a glimpse of the their upcoming CyberRace title. In this game you control a craft which hovers above a beautifully voxel-rendered terrain similar to that seen in Comanche, only better looking and faster. The terrain he chose for that demonstration was Olympus Mons on Mars, constructed from data they had obtained from NASA. Flying around the terrain was nice, but the mind-blowing part came when he flew over the opening of a deep crater and the craft took a slight dive into the void, immediately regaining altitude to clear the other side of the rim: the sensation was reminiscent of a similar dive of the Millennium Falcon in the asteroid chase scene on Star Wars I, with the same roller-coaster effect in the heart and in the stomach and in other areas of the body which need not be mentioned here. When I went back home after the interview I had this uncontrollable urge to do the same thing, and I just could not find rest unless I accomplished it.\nThis demo shows the result of my efforts. It is not nearly as good as Comanche or CyberRace, yet it does count as a crude proof of concept, and it is certainly improvable. Most importantly, it was made in a couple of days by a twenty-something year old \u0026quot;foreign student\u0026quot; working alone, having nothing to aid him other than the knowledge that it can be done after having seen it done.\nPlease note that this demo is made in Mode 13, not Mode X, so the raster lines are horizontal, not vertical, even though vertical raster lines would have helped a lot with the ray-tracing.\nDownload flight.zip\nAvailable controls: Numeric Keypad Left Arrow Turn Left Numeric Keypad Right Arrow Turn Right Numeric Keypad Up Arrow Look Down Numeric Keypad Down Arrow Look Up Numeric Keypad Page Down Do not press this key! Numeric Keypad End Do not press this key! Numeric Keypad '+' Increase Speed Numeric Keypad '-' Decrease Speed Numeric Keypad '5' Level Camera Cursor Keypad Left Arrow Move North Cursor Keypad Right Arrow Move South Cursor Keypad Up Arrow Increase Altitude Cursor Keypad Down Arrow Decrease Altitude ESC terminate the demo Cover image: An actual photo of my computer monitor showing one of the demos\n","date":"2005-11-20T00:00:00Z","permalink":"https://blog.michael.gr/post/2005-11-20-vintage-demos/","title":"Vintage Demos"},{"content":"\rNote in 2025:\nThis post was retrieved from my old blog via The Wayback Machine (archive.org). It is ancient. Do not be surprised if external links do not work anymore.\nA short video clip (14 MB) which gives a nice indication of our work in progress has been circulating in gaming web sites lately. Since it leaked sometime during the summer vacation it spread like wildfire, and now that everyone in the MMORPG gaming community has seen it, I guess I can go ahead and mention it. I assure you that everything in this video clip is actual game functionality, nothing has been faked or scripted especially for the purpose of filming it.\nA player carrying a staff enters the human capital through the main city gate and sees a number of other players who are already in the city, playing around and duelling each other. One of them is seen casting an \u0026quot;insect swarm\u0026quot; spell. (Everyone's health has been set to some extremely high value for testing purposes.) As soon as he enters through the gate, the player anounces his presence by shooting a huge fireball, (again, the effect is tremendously exaggerated just for the fun of it,) and when the other players see this, they all turn against him and start slashing him with their swords, pushing him back out of the city.\nIn the 3D world you can see some really nice graphics and features, like flags waving in the breeze, the detail of the ballustrades and chains on either side of the drawbridge, the smoke coming out of the smokestacks, the very finely detailed trees, etc. You can hardly tell due to the low quality of the screencap, but the banners hanging from the towers on either side of the main entrance are also slowly waving, the clouds in the sky are moving, etc. On the 2D GUI, (which is what I have been mostly working on,) you can see the hotbar on the left, the chat window along the bottom, the health and stamina box in the bottom-right corner, and the minimap in the top-right corner. These are all going to undergo changes before the final release of the game, especially the minimap. Of course since this is an MMORPG there is a lot more GUI which pops up depending on the circumstances.\nHere is the discussion thread about the video clip at the official Darkfall Online forums.\nI am not hosting the clip on my web site, but here is a number of sites from which it can be downloaded (use the one which yields the fastest download for you:)\nfrom the-combine.net (direct download: here)\nfrom guildvsguild.com (direct download: here)\nfrom bloodofkings.net (direct download: here)\n","date":"2005-08-18T00:00:00Z","permalink":"https://blog.michael.gr/post/2005-08-18-darkfall-video-released/","title":"Darkfall video released"},{"content":" Note in 2025:\nThis post was retrieved from my old blog via The Wayback Machine (archive.org).\nCoffee is a very important pleasure of life for me. I started drinking instant coffee when I was 15 years old, and when I reached 30 I switched to filter. My day has not begun unless I have had a mugful of my favorite blend at home, and I usually have at least one more mug later on during the day. (These numbers represent a significant reduction compared to the four or five mugs that I used to drink every day until recently.)\nMy favorite blend consists of one part vanilla-hazelnut flavored filter coffee, for the taste, and one part espresso, for the punch. Yes, I put espresso in the filter coffee maker, the result is just filter coffee with a slight espresso taste, you should try it one day.\nOn this page I describe my environmentally friendly procedure for making coffee. By environmentally friendly I mean that it significantly cuts down on filter paper consumption, which means fewer wood cut to make the paper, less chlorine used to bleach it, and less waste in the landfills. Of course, the most environmentally friendly way to make coffee would be to make no coffee at all, which would mean less virgin forest cut to make plantations, less energy consumed to process and transport the coffee, less precious resources waster to market coffee to consumers, and less energy consumed to make the coffee in every home, but I guess that would be a bit too much \u0026quot;stepping lightly on the earth\u0026quot; for us.\nThe Procedure Naturally, if you want to cut down on paper coffee filter consumption, the thing to do is to use a metallic coffee filter which you can wash and reuse every day for years. The following picture shows your regular super-market variety metallic coffee filter filled with one spoonful of filter coffee.\nUnfortunately, the problem with the metallic filters, and the reason why they have not caught on, is that they do let fine coffee grounds to pass through. (This is especially true if your blend consists of half espresso.) So, the solution is to use a paper filter and a metallic filter together. The metallic filter holds the bulk of the coffee grounds, while the paper filter collects the fine silt. The metallic filter gets removed and washed every time you make coffee, while the paper filter stays in the coffee maker for many days, accumulating silt. The following picture shows my hand placing the metallic filter inside an (already used) paper filter in the coffee maker.\nAs it turns out, the silt does not have any adverse effect on the taste of the coffee; on the contrary, by impeding the flow of coffee it prolongs the percolation process, thus yielding stronger coffee. After one or two weeks (depending on use) the paper filter collects so much silt that when you try to remove the metallic filter it sticks to it. That's when you know it is time to replace the paper filter. The following picture shows how the paper filter looks inside the coffee maker once you have removed the metallic filter to wash it in preparation for making another fresh pot.\nIf you try this procedure, I would be interested to know what you thought of it.\n","date":"2005-06-09T00:00:00Z","permalink":"https://blog.michael.gr/post/2005-06-09-my-way-of-making-coffee/","title":"My way of making coffee"},{"content":" Note in 2025:\nThis post was retrieved from my old blog via The Wayback Machine (archive.org).\nBecause the email address that I gave them a couple of years ago when I registered with them (enfocus@michael.gr) has started receiving spam, which means that it ended up in the hands of spammers.\n","date":"2004-06-05T00:00:00Z","permalink":"https://blog.michael.gr/post/2004-06-05-enfocus-software-sucks/","title":"Enfocus Software Sucks"},{"content":" Note in 2025:\nThis post was retrieved from my old blog via archive.org. These short stories are extremely old. They are like from a previous life of mine.\nI am not under the impression that I am gifted in writing fictional stories, let alone writing them in English, which is not my mother tongue; I am only posting these short stories here because I think that they are not so bad as to deserve to be kept forever hidden.\nAcross the Border\nThe Death of the Old Man\nThe First Day of Spring\nThe Man On The Moon\nThe Rendezvous\nThe Retreat\nThe Superintendent\nThe Birds\nChina 101 (a poem)\n","date":"2004-04-17T00:00:00Z","permalink":"https://blog.michael.gr/post/2004-04-17-short-stories/","title":"Short Stories"},{"content":"\rNote in 2025:\nThis post was retrieved from my old blog via The Wayback Machine (archive.org). Do not be surprised if the link does not work anymore, or if it directs to some unrelated place.\nIn case you have been wondering what I have been up to recently, go to http://www.darkfallonline.com and check out the project that I am working on!\nThis is the web site of a Massively Multiplayer On-line Role Playing Game (MMORPG) which is being developed by Razorwax \u0026amp; Aventurine (Norwegian and Greek companies respectively) and which is much anticipated by the international gaming community.\n","date":"2004-03-29T00:00:00Z","permalink":"https://blog.michael.gr/post/2004-03-29-darkfall-online-website/","title":"New Darkfall Online web site released today"},{"content":"\rNote in 2025:\nThis post was retrieved from my old blog via The Wayback Machine (archive.org). It is ancient. Do not be surprised if any links in it do not work anymore.\nGeorge W Bush has been Google-bombed!\nIf you visit Google nowadays, and try searching for the words \u0026quot;miserable failure\u0026quot;, the first entry that will come up is the biography of the president on the website of the White House!\nThe trick is possible because Google searches more than just the contents of web pages - it also counts how often a site is linked to, and with what words.\nThus, members of an online community can affect the results of Google searches - called \u0026quot;Google bombing\u0026quot; - by linking their sites to a chosen one.\nYou can find the entire BBC article here: http://news.bbc.co.uk/2/hi/americas/3298443.stm\nSo, here, I will be a responsible citizen of the Net and I will do my part:\nmiserable failure Cover image: Emperor Bush - Photo: Associated Press - taken from BBC.com\n","date":"2004-01-09T00:00:00Z","permalink":"https://blog.michael.gr/post/2004-01-09-g-w-bush-miserable-failure/","title":"'Miserable failure' links to Bush"},{"content":"The bad news is that while pursuing my own business ideas, small contracts were hard to come by, so eventually I ran out of money and I had to find a regular job.\nThe good news is that this is an absolutely unique job, which enables me to work on fun stuff within a very cool international team, and even the pay is halfway decent (well, by greek standards that is...) so I am quite happy.\nIf you would like to know what my new job is about, check out this address: www.razorwax.com. It is quite a small web site, so you can view every single web page before following one of the links to other related sites to find more information.\nSo yes, that's it for me, I am taking a break from freelancing and from trying to convince people about my revolutionary business ideas, at least for as long as this company keeps making the payroll.\n","date":"2003-12-09T00:00:00Z","permalink":"https://blog.michael.gr/post/2003-12-09-new-job/","title":"I got a new job!"},{"content":" Note in 2025:\nInformation on this page was retrieved from my old blog via The Wayback Machine (archive.org). The file extensions have been removed to avoid attracting people looking to download music, because there is nothing to download here.\nMy Greek music collection\n2002GR - Αύριο 2002GR - Αχ, Καημένη 2002GR - Δεν είσαι Έρωτας Εσύ 2002GR - Εσύ Μού \u0026#39;πες πως μ\u0026#39; Αγαπάς 2002GR - Μαγική Αυλή 2002GR - Μόνος 2002GR - Ο Σιδερένιος Άνθρωπος (112 kbps) (vinyl rip) 2002GR - Ο Σιδερένιος Άνθρωπος (bad quality) (vinyl rip) 2002GR - Ο Σιδερένιος Άνθρωπος (vinyl rip) 2002GR - Τι να Σου Προσφέρω Αθηναϊκή Κομπανία - Το Μινόρε Της Αυγής Αλεξοπούλου Μαίρη - Η Μπάμπολα Αλεξοπούλου Μαίρη - Οι κούκλες Αντωνίου Νίκος - Το Καλοκαίρι Εκείνο Απώντες - Ζωή Αρβανιτάκη Ελευθερία - Δυνατά, Δυνατά (live) Αρλέτα - Ερωτικό (incomplete) Αρλέτα - Η Σερενάτα Αρλέτα - Μια φορά θυμάμαι Αρλέτα - Μιά φορά θυμάμαι (160 kbps) Αρλέτα - Σάββατο Απόγευμα Αρλέτα - Τα μικρά παιδιά (νέο κύμα) Αρλέτα - Τώρα θ\u0026#39; ανοίξω τα φτερά Αρνάκια - Η Πτώση (cut down) Αρχέλων - Σποτάκι Αστεριάδη Πόπη - Μια Αγάπη Για Το Καλοκαίρι Αστεριάδη Πόπη - Σκληρό μου αγόρι Αστεριάδη Πόπη - Φεγγαράκι Άγνωστοι - Καλοκαίρι Στην Καρδιά (from Mungo Jerry) Άγνωστοι (60\u0026#39;s) - Λόγια Της Αγάπης Άγνωστοι (60\u0026#39;s) - Σαν Το Καράβι Άγνωστος - Ο Χορός του Ζορμπά Άσημος Νικόλας - Μπαγάσα Βαμβακάρης Μάρκος - Φραγκοσυριανή Βανδή Δέσποινα - Υποφέρω Βάνου Τζένη - Σ\u0026#39;αγαπώ Βέττα Καλλιόπη - Spirits of the World 4 Βουγιουκλάκη Αλίκη - Σήκω Χόρεψε Συρτάκι Βουτσάς Κώστας - Αψού, Γείτσες Βουτσάς Κώστας - Φφφσστ Μπόινγκ Βόρειοι - Σαλούνα Βόσσου Σοφία - Το Φιλαράκι Γαβαλάς Π. Γκίκα Β. - Οι Γλάροι Γαλαξίες - Γιάνγκα για μικρά παιδιά (too quiet) Γαλάνη Δήμητρα - Μαρία με τα κίτρινα Γερμανός Βαγγέλης - Γιατί σε θέλω (vinyl rip) (skip at 3\u0026#39;00\u0026#39;\u0026#39;) Γερμανός Βαγγέλης - Είσαι Ένας Διάβολος Γερμανός Βαγγέλης - Κρουαζιέρα Γερμανός Βαγγέλης - Ο Απόκληρος Γερμανός Βαγγέλης - Ο Κηπουρός Γερμανός Βαγγέλης - Σημαδούρα Γερμανός Βαγγέλης - Στη Μπανιέρα Γιοκαρίνης - Κιθαρίστας Ή Ντράμερ (ends with major scratch) Γιοκαρίνης Γιάννης - Δεν Είμαστε Καλά Γιοκαρίνης Γιάννης - Νοσταλγός του Rock\u0026#39;n\u0026#39;Roll Γιοκαρίνης Γιάννης (\u0026amp; Κούτρας) - Τσικαμπούμ Γκαιφυλιάς Θανάσης - Ωτοστόπ Γλέζου Δέσποινα - Αποσμητικά (live) Γρηγοριάδης - Ψέμα Δεληβοριάς Φοίβος - Εκείνη Δεληβοριάς Φοίβος - Θέλω να σε ξεπεράσω Δενάρδου Κλειώ - Το καλοκαίρι εκείνο Δημητριάδη Μαρία - Ένα Πρωϊνό Δημητριάδης Γιώργος \u0026amp; Οι Μικροί Ήρωες - Σαν Να Μην Πέρασε Μια Μέρα Διαμαντή Λίτσα - Συννεφιές Διάφανα Κρίνα - Ανταρκτική Διάφανα Κρίνα - Αυτό το τραγούδι δεν είναι για σένα Διάφανα Κρίνα - Για όλα αυτά που δεν θα δω Διάφανα Κρίνα - Κυριακή Διάφανα Κρίνα - Τελευταία Μέρα Διάφανα Κρίνα - Χειμώνας Διονυσίου Στράτος - Βρέχει Φωτιά Στην Στράτα Μου Εκείνος \u0026amp; εκείνος - Χρώματα Ενδελέχεια - Βουτιά Από Ψηλά (spike at 0\u0026#39;20\u0026#39;\u0026#39;) Εξαδάκτυλος - Ο ανεπρόκοπος Ερασιτέχνες Εραστές - Στον Δρόμο Ευσταθίου - Το Βαπόρι απ\u0026#39;την Περσία Έλενα - Σε Περιμένω Το Άλλο Καλοκαίρι Ζιγκ Ζαγκ - Να Μου Γράφεις Ζιώγαλας Νίκος - Βέροια Θεσσαλονίκη Αθήνα Ζιώγαλας Νίκος - Θα Σου Φανερωθώ Ζιώγαλας Νίκος - Να η οθόνη Ζιώγαλας Νίκος - Σαν Σταρ του Σινεμά Ζιώγαλας Νίκος - Το Τζάμπο Ζωντανοί Νεκροί - Μετά από τη ληστεία Ζώρας Σταύρος - Τώρα θα μείνεις μοναχή (too quiet) Θεοδωράκης Μίκης - Δόξα τω θεώ (instrumental) (96 kbps) Θεοδωράκης Μίκης - Ένα Το Χελιδόνι Θεοδωράκης Μίκης - Η Μαργαρίτα η Μαργαρώ Θεοδωράκης Μίκης - Στρώσε το Στρώμα Σου Γιά Δυό (instrumental) (112 kbps) Θεοδωράκης Μίκης - Zorba the Greek Θεοδωράκης Μίκης \u0026amp; Φαραντούρη - Το Γελαστό Παιδί Ιωαννίδης Α. - Δεν μπορώ Καζαντζίδης Στέλιος, Μαρινέλλα - Βράχο Βράχο τον Καημό Μου Καζούλης Βασίλης - Αν Ήσουν Άγγελος Καζούλης Βασίλης - Βορεινά Λιμάνια Καζούλης Βασίλης - Φανή Καλατζής Γιάννης \u0026amp; Διαμαντή Λίτσα - Δελφίνι Δελφινάκι Καλδάρας Απόστολος (\u0026amp; Μπιθικώτσης) - Μου Σπάσανε το Μπαγλαμά Κανελλίδου Αλέκα - Crazy girl (112 kbps) Κανελλίδου Αλέκα - Shy Boy Καρβέλας Νίκος - Ας Πεθάνω Πιό Νωρίς (Δε Γαμιέται) Καρβέλας Νίκος - Ένα Χρόνο Το Περισσότερο (\u0026amp; Βίσση Άννα) Καρβέλας Νίκος - Καλοκαιρινές Διακοπές Καρβέλας Νίκος - Τα Εσώρουχά σου Καρβέλας Νίκος - Όλα Είναι Εντάξει Κατσιμίχα Αφοί - Αϊ της αγάπης μαχαιριά Κατσιμίχα Αφοί - Ανόητες Αγάπες (\u0026amp; Στόκας) Κατσιμίχα Αφοί - Γέλα Πουλί μου (live) Κατσιμίχα Αφοί - Γέλα Πουλί μου Κατσιμίχα Αφοί - Γυρίζω τις Πλάτες μου στο Μέλλον Κατσιμίχα Αφοί - Δεν Φταίς Εσύ Κατσιμίχα Αφοί - Η Μοναξιά Κατσιμίχα Αφοί - Η Μπαλάντα του Φάνη Κατσιμίχα Αφοί - Θεσαλονίκη Κατσιμίχα Αφοί - Καλό Ταξίδι Κατσιμίχα Αφοί - Μη Γυρίσεις Κατσιμίχα Αφοί - Μιά Βραδιά στο Λούκι Κατσιμίχα Αφοί - Νύχτωσε Νύχτα Κατσιμίχα Αφοί - Παίξε βραχνή μου φυσαρμόνικα Κατσιμίχα Αφοί - Παλιάτσος και Ληστής Κατσιμίχα Αφοί - Ρίτα Ριτάκι Κατσιμίχα Αφοί - Σχήμα Λόγου Κατσιμίχα Αφοί - Το Καλοκαιράκι Κατσιμίχα Αφοί - Ψέματα Κατσιμίχας Χάρης \u0026amp; Μικρούτσικος Θάνος - Θεσσαλονίκη Κορκολής Στέφανος - Στους Πέντε Ανέμους Κουρούκλη Ζωίτσα - Η Τραμοντάνα Κουρούκλη Ζωίτσα - Oldies But Goodies Κούτρας - Kuro Siwo Κόκοτας Σταμάτης - Στου Όθωνα τα Χρόνια Κόκοτας Σταμάτης - Όνειρο Απατηλό Κότσιρας Γιάννης - Πώς μπορώ Κότσιρας Γιάννης - Το Τσιγάρο Κυριαζής Χρήστος - Έλα Μωράκι Μου Λαθρεπιβάτες - Απόψε λεω να μην κοιμηθούμε Λαθρεπιβάτες - Δε φταίω εγώ που μεγαλώνω Λέανδρος Βίκυ - Μόνο Εσύ Λέανδρος Βίκυ - Πυρετός του έρωτα (incomplete) Λέανδρος Βίκυ - Σ\u0026#39;αγαπώ Λέανδρος Βίκυ - Στο Παλιό Ξενοδοχείο (1989) Λέανδρος Βίκυ - Το Μυστικό Σου Λέανδρος Βίκυ - Χαμένη Αγάπη (remix \u0026#39;93) Λέανδρος Βίκυ - Χαμένη Αγάπη Λήδα - Όταν Θα Γεννηθεί Ο Γιός Σου Λήδα \u0026amp; Σπύρος - Έλα να δεις τον τόπο μου Λίντα Μαίρη - Περασμένες μου αγάπες Λογαρίδης Σταύρος - Close the door Λοϊζος Μάνος - Δέκα Παλικάρια (Νταλάρας;) Λοϊζος Μάνος - Καλημέρα Ήλιε (orchestral) Λοϊζος Μάνος - Ο Δρόμος Λοϊζος Μάνος - Σ\u0026#39; ακολουθώ Λοϊζος Μάνος - Το ζεϊμπέκικο της Ευδοκίας (incomplete) Μαβίλη Αλέκα - Αυτό τ\u0026#39; Αγόρι Μαρινέλλα - Άνοιξε Πέτρα Μαρινέλλα - Να Παίζει Το Τρανζίστορ Μαρίνα - Αν τύχει να περάσετε από \u0026#39;κει Μαρίνα - Να ήμουν αετός Μαρίνα - Ο κούκλος Μαχαιρίτσας Λαυρέντης - Βασίλισσα της σιωπής (\u0026amp; Τσακνής) (live) Μαχαιρίτσας Λαυρέντης - Δανεική Μαχαιρίτσας Λαυρέντης - Διδυμότειχο Blues (\u0026amp; Νταλάρας) (live) (6\u0026#39;09\u0026#39;\u0026#39;) Μαχαιρίτσας Λαυρέντης - Διδυμότειχο Blues (\u0026amp; Τσακνής) (live) Μαχαιρίτσας Λαυρέντης - Διδυμότειχο Blues (live) (7\u0026#39;08\u0026#39;\u0026#39;) (112 kbps) Μαχαιρίτσας Λαυρέντης - Ένας τούρκος στο παρίσι Μαχαιρίτσας Λαυρέντης - Και τι ζητάω (\u0026amp; Σαββόπουλος) Μαχαιρίτσας Λαυρέντης - Μια μέρα δανεική (incomplete) Μαχαιρίτσας Λαυρέντης - Να δεις τι σου\u0026#39;χω για μετά Μαχαιρίτσας Λαυρέντης - Νότος Μαχαιρίτσας Λαυρέντης - Ρίξε κόκκινο στη νύχτα Μάνου Αφροδίτη - Αριστερά στην Εδέμ (incomplete) Μάνου Αφροδίτη - Ένα Καλοκαίρι Μάνου Αφροδίτη - Μία μέρα μίας Μαίρης Μάνου Αφροδίτη \u0026amp; Φέρτης Γιάννης - Σαν με κοιτάς (160 kbps) Μάνου Αφροδίτη \u0026amp; Φέρτης Γιάννης - Σαν με κοιτάς (live, awful) Μάνου Αφροδίτη \u0026amp; Φέρτης Γιάννης - Σαν με κοιτάς Μηλιώκας Γιάννης - Για το Καλό Μου Μηλιώκας Γιάννης - Θεσσαλονίκη Μηλιώκας Γιάννης - Κακοσάλεσι Μηλιώκας Γιάννης - Ποιμενικό Rock Μητροπάνος - Σε αναζητώ στη Σαλονίκη Μητσιάς Μανώλης - Στην Ελευσίνα Μια Φορά Μικρές Περιπλανήσεις - Μικρές Περιπλανήσεις Μικρούτσικος \u0026amp; Λαζόπουλος - Ψάξε Στ\u0026#39; Όνειρό μας Μοσχολιού Βίκυ - Δεν Είναι Όνειρο Η Ζωή Μοσχολιού Βίκυ - Πέρα Από Τη Θάλασσα Μοσχολιού Βίκυ - Τα Τραίνα Που Φύγαν Μπιγάλης Κώστας - Απάνω Στα Βινύλια (incomplete) Μπιγάλης Κώστας - Μπάμ Μπιγάλης Κώστας - Του Αιγαίου Τα Blues Μπιθικώτσης Γρηγόρης - Άσπρη Μέρα και για Μας Μπιθικώτσης Γρηγόρης - Μάτια Βουρκωμένα Μπιθικώτσης Γρηγόρης - Ο Καϊμός (instrumental) Μπιθικώτσης Γρηγόρης - Ο Καϊμός Μπιθικώτσης Γρηγόρης - Που\u0026#39;ναι τα Χρόνια Μπιθικώτσης Γρηγόρης - Στο Περιγιάλι το Κρυφό Μπιθικώτσης Γρηγόρης - Όνειρο Δεμένο (instrumental) Μπιθικώτσης Γρηγόρης - Όταν Σφίγγουν το Χέρι Μπιθικώτσης Γρηγόρης \u0026amp; Γιουλάκης Νίκος - Στον Πειραιά Συννέφιασε Μπλέ - Ενοχές Μπλέ - Σε Αγαπώ Μπλέ - Φοβάμαι Μπλέ - Χωρίς Τίτλο Μπουρπούλια - Απογοήτευση Νοστράδαμος - Δωσ\u0026#39;μου το χέρι σου Νοστράδαμος - Τα Παραμύθια της Γιαγιάς Νταλάρας Γιώργος - Ταξίδι Στα Κύθηρα Ντουνιάς Γιάννης - Του άντρα του πολλά βαρύ (scratch at 1\u0026#39;21\u0026#39;\u0026#39;) Ξαρχάκος - Μάτια Βουρκωμένα (instrumental) (Μπιθικώτσης;) Ξενάκη Πένυ - Μιά Φορά Θυμάμαι Ξεφτίλας - Το Bad Block Στο Δίσκο Μου Ξυλούρης Νίκος - Αυτόν τον Κόσμο τον Καλό Ξυλούρης Νίκος - Ζαβαρακατρανέμια Ξυλούρης Νίκος - Ντιρλαντά Ξυλούρης Νίκος - Το Καφενείο Η Ελλάς Ξυλούρης Νίκος - Χίλια Μύρια Κύματα Ξύλινα Σπαθιά - Αδρεναλίνη Ξύλινα Σπαθιά - Αλλάζει πρόσωπα η θλίψη Ξύλινα Σπαθιά - Ατλαντίς Ξύλινα Σπαθιά - Λιωμένο Παγωτό Ξύλινα Σπαθιά - Ξεσαλονίκη Ξύλινα Σπαθιά - Ο Βασιλιάς της Σκόνης Ξύλινα Σπαθιά - Το Νερό που Κυλάει Ξύλινα Σπαθιά - Φωτιά στο Λιμάνι Ξύλινα Σπαθιά - Robot Οναρ - Με Τρομάζεις Ονειροπαγίδα - Κάθε Πρωί Ονειροπαγίδα - Ξέρεις (bad rip) Παλόμα Μποκίου - Γαρύφαλλε Πανούσης Τζίμης - (Βίβερε Περικολοσαμέντε - 01) - Αναφορά Πανούσης Τζίμης - (Βίβερε Περικολοσαμέντε - 02) - Νεοέλληνας Πανούσης Τζίμης - (Βίβερε Περικολοσαμέντε - 08) - Βίβερε Περικολοσαμέντε Πανούσης Τζίμης - (Βίβερε Περικολοσαμέντε - 09) - Ισοπαλία Πανούσης Τζίμης - (Βίβερε Περικολοσαμέντε - 10) - Σαν το Σαμουήλ στο Κούγκι Πανούσης Τζίμης - (Βίβερε Περικολοσαμέντε - 11) - Φάτε τους! Πανούσης Τζίμης - (Βίβερε Περικολοσαμέντε - 12) - Happy End Πανούσης Τζίμης - (Βίβερε Περικολοσαμέντε) - Ράπισμα Πανούσης Τζίμης - (Βίβερε Περικολοσαμέντε) - Το Μουνί και το Δελφίνι Πανούσης Τζίμης - (Δουλείες Του Κεφαλιού - 13) - Usa For Marketing (live) Πανούσης Τζίμης - (Hard Core - 01) - Part 1 Πανούσης Τζίμης - (Hard Core - 02) - Part 2 Πανούσης Τζίμης - 10.000 Watt Πανούσης Τζίμης - 10000 Watt (bad recording) Πανούσης Τζίμης - 15 - Ποιήματα (incomplete) Πανούσης Τζίμης - Αέρα Πανούσης Τζίμης - Αλέκα (live) (excerpt) Πανούσης Τζίμης - Αλέκα (live) Πανούσης Τζίμης - Ανακωχή Πανούσης Τζίμης - Αποκάλυψη Τώρα Πανούσης Τζίμης - Αχ, Ευρώπη (live) (4\u0026#39;05\u0026#39;\u0026#39;) Πανούσης Τζίμης - Αχ, Ευρώπη (live) (5\u0026#39;11\u0026#39;\u0026#39;) Πανούσης Τζίμης - Αχ, Ευρώπη (live) (6\u0026#39;13\u0026#39;\u0026#39;) (ends with spike) Πανούσης Τζίμης - Γαμάτε Γιατί Χανόμαστε (Λογοκριμένο) Πανούσης Τζίμης - Γαμάτε Γιατί Χανόμαστε (live) Πανούσης Τζίμης - Για Μια Χούφτα Δολλάρια Πανούσης Τζίμης - Γιαγιά Πατίνι (στα αγγλικά) Πανούσης Τζίμης - Γιαγιά Πατίνι (live) (starts with spike) Πανούσης Τζίμης - Δεύτερη Προβολή Πανούσης Τζίμης - Δουλειές Του Κεφαλιού Πανούσης Τζίμης - Είμαι Γυφτάκι Πανούσης Τζίμης - Είμαστε Υπερδυνάμεις Πανούσης Τζίμης - Ελένη Όπως Ελλάδα Πανούσης Τζίμης - Εργατική Υποχώρηση Πανούσης Τζίμης - Ερωτικό Πανούσης Τζίμης - Ένα Το Χελιδόνι (live excerpt) Πανούσης Τζίμης - Ένα Τραγούδι για το Χειμώνα Πανούσης Τζίμης - Ζόμπι, το Ξύπνημα των Νεκρών Πανούσης Τζίμης - Η Αϊσέ στο βεσέ.C Πανούσης Τζίμης - Η Αυθεντική Λαμπάντα (live) Πανούσης Τζίμης - Η Αυτοκρατορία των Αισθήσεων Πανούσης Τζίμης - Η Ελλάδα Στα Κάρβουνα Πανούσης Τζίμης - Θα Σου Πάρω Λαμπατέρ (live) Πανούσης Τζίμης - Θα Σου Πώ Πανούσης Τζίμης - Θα σου γαμηθώ (excerpt) Πανούσης Τζίμης - Ισοπαλία (skip at 1\u0026#39;38\u0026#39;\u0026#39;, too long) Πανούσης Τζίμης - Ιστορία για Αγρίους Πανούσης Τζίμης - Καβουράκια (56 kbps 22 khz) (live excerpt) Πανούσης Τζίμης - Καπρί Σε Φινί Πανούσης Τζίμης - Κάγκελα Παντού (live) Πανούσης Τζίμης - Κάγκελα Παντού Πανούσης Τζίμης - Κάθε Εμπόριο για Καλό Πανούσης Τζίμης - Λογοδιάρροια Πανούσης Τζίμης - Μάγισσα Μανούλα Πανούσης Τζίμης - Με λένε Πόπη (live excerpt, cut down) Πανούσης Τζίμης - Μεταμοντέρνο Πανούσης Τζίμης - Μια Τρύπα Στη Σημαία (has skips, too long) Πανούσης Τζίμης - Μιά Ασπρόμαυρη Μικρή Φωτογραφία Πανούσης Τζίμης - Μουσικές Ταξιαρχίες (112 kbps) Πανούσης Τζίμης - Μουσικές Ταξιαρχίες Πανούσης Τζίμης - Ναγκασάκι Πανούσης Τζίμης - Ντίσκο (σ\u0026#39;ότι μας λένε θα λέμε όχι) Πανούσης Τζίμης - Ο Κάϊν Ζεί Πανούσης Τζίμης - Ο Λάκος Με Τ\u0026#39; Αστεία Πανούσης Τζίμης - Ο Τζιμάκος και το Τέρας Πανούσης Τζίμης - Οι Εκλογές Πανούσης Τζίμης - Οικογενειακή Συνωμοσία (Ερωτικό) Πανούσης Τζίμης - Ουγκέτσου Μονογκατάρι Πανούσης Τζίμης - Ούζο Power Πανούσης Τζίμης - Π.Χ Πανούσης Τζίμης - Παπουτσωμένος Λόγος Πανούσης Τζίμης - Πάρε το Χαπάκι Σου Πανούσης Τζίμης - Ποτ Πουρί Πανούσης Τζίμης - Πυγολαμπίδα Πανούσης Τζίμης - Σαν Το Σαμουήλ Στο Κούγγι (live) Πανούσης Τζίμης - Σαν Φεγγαράκι (+Κονιτοπούλου) Πανούσης Τζίμης - Σατανικά (live excerpt) Πανούσης Τζίμης - Σουζάνα (live) Πανούσης Τζίμης - Σουλιωτοπούλα + Γαμάτε Πανούσης Τζίμης - Στενές Επαφές Τρίτου Τύπου (Disco Tsoutsouni) Πανούσης Τζίμης - Τα Προβατάκια Πανούσης Τζίμης - Τεκές Πανούσης Τζίμης - Την Εικόνα σου Πανούσης Τζίμης - Τι Έχεις Και Κλαίς Πανούσης Τζίμης - Τι Πάθος Ατελείωτο (live) (σκέτο) Πανούσης Τζίμης - Τι Πάθος Ατελείωτο Πανούσης Τζίμης - Το Μουνί και το Δελφίνι Πανούσης Τζίμης - Το Παιδί Του Σωλήνα Πανούσης Τζίμης - Το Παρδαλό Τζιτζίκι Πανούσης Τζίμης - Το Ρετιρέ της Ανωμαλίας Πανούσης Τζίμης - Το Τραύμα (skip at 1\u0026#39;04\u0026#39;\u0026#39;) Πανούσης Τζίμης - Το Τραύμα Πανούσης Τζίμης - Τουμπερλέκ (Ρούμς του Λετ) Πανούσης Τζίμης - Τραίνο Πανούσης Τζίμης - Όσα Παίρνει Ο Άνεμος (one song for the calavrita place) Πανούσης Τζίμης - Όχι Άλλο Νταλάρα (bad recording, cut down) Πανούσης Τζίμης - Φασμπίντερ και Ξερό Ψωμί Πανούσης Τζίμης - Φάτε Μάτια Ψάρια Πανούσης Τζίμης - Φάτε Τους! Πανούσης Τζίμης - Φυσική Ιστορία + Όχι Άλλο Νταλάρα (excerpt) Πανούσης Τζίμης - Φυσική Ιστορία Πανούσης Τζίμης - Χόρεψε Μανόλη (cut down) Πανούσης Τζίμης - Χτυπάω Κάρτα Στην Αγκαλιά Σου Πανούσης Τζίμης - Ψόφια Γλώσσα Πανούσης Τζίμης - Disco Τσουτσούνι Πανούσης Τζίμης - Do It Πανούσης Τζίμης - One song for the calavrita place (112 kbps) Πανούσης Τζίμης - SOS Πεντάγωνο Καλεί Μόσχα Πανούσης Τζίμης - USA for Marketing (112 kbps) Παπαδόπουλος Λάκης - Κάποιες παλιές αμαρτίες (\u0026amp; Φίλιππος Πλιάτσικας) Παπαδόπουλος Λάκης - Μαργαρίτα Μαργαρώ (instrumental) Παπαδόπουλος Λάκης (Με τα Ψηλά Ρεβέρ) - Αγώνας Ταχύτητας (Σε Ζητάω) Παπαδόπουλος Λάκης (Με τα Ψηλά Ρεβέρ) - Γυριστρούλα Παπαδόπουλος Λάκης (Με τα Ψηλά Ρεβέρ) - Δικαίωμα Στο Όνειρο Παπαδόπουλος Λάκης (Με τα Ψηλά Ρεβέρ) - Πράγματα που δεν Έκανες Παπαδόπουλος Λάκης (Με τα ψηλά ρεβέρ) - Ιωάννα και Μαριέττα Παπαδόπουλος Λάκης (Με τα ψηλά ρεβέρ) - Με τίποτα Παπαδόπουλος Λάκης (Με τα ψηλά ρεβέρ) - Τα μπλέ παπούτσια Παπαδόπουλος Λάκης (Με τα ψηλά ρεβέρ) - Φάλτσα μενεξεδιά Παπακωνσταντίνου Βασίλης - Αγαπάω και Αδιαφορώ Παπακωνσταντίνου Βασίλης - Άνοιξέ μου να κρυφτώ Παπακωνσταντίνου Βασίλης - Άσε Mε Να Κάνω Λάθος Παπακωνσταντίνου Βασίλης - Βικτώρια Παπακωνσταντίνου Βασίλης - Βράδυ Σαββάτου Παπακωνσταντίνου Βασίλης - Για Σένα Παπακωνσταντίνου Βασίλης - Δε Σηκώνει Παπακωνσταντίνου Βασίλης - Δεν Υπάρχω Παπακωνσταντίνου Βασίλης - Εγώ θα είμαι εδώ Παπακωνσταντίνου Βασίλης - Ευτυχώς Παπακωνσταντίνου Βασίλης - Έλα να με βρείς Παπακωνσταντίνου Βασίλης - Ένα Μπλούζ Παπακωνσταντίνου Βασίλης - Θεσσαλονίκη (live) Παπακωνσταντίνου Βασίλης - Ιδιαζόντως Παπακωνσταντίνου Βασίλης - Καταρρέω Παπακωνσταντίνου Βασίλης - Καυσαεριώδεις Θυμιάσεις Παπακωνσταντίνου Βασίλης - Κι αν είμαι Ροκ Παπακωνσταντίνου Βασίλης - Κρύψου Παπακωνσταντίνου Βασίλης - Λεγεωνάριος Παπακωνσταντίνου Βασίλης - Μπαλάντα για τον Γιάννη Κ Παπακωνσταντίνου Βασίλης - Μπρός Γκρεμός και πίσω Γκόμενες Παπακωνσταντίνου Βασίλης - Ο Γουίλυ ο Μαύρος Θερμαστής Παπακωνσταντίνου Βασίλης - Ο Μαύρος Γάτος Παπακωνσταντίνου Βασίλης - Ο Στρατιώτης Παπακωνσταντίνου Βασίλης - Πρέβεζα Παπακωνσταντίνου Βασίλης - Πρίν το Τέλος Παπακωνσταντίνου Βασίλης - Πρώτη Μαϊου Παπακωνσταντίνου Βασίλης - Σ\u0026#39; Ακολουθώ Παπακωνσταντίνου Βασίλης - Σαν των Ματιών σου τις Βαφές Παπακωνσταντίνου Βασίλης - Σεμπάστιαν Παπακωνσταντίνου Βασίλης - Στέλλα Παπακωνσταντίνου Βασίλης - Σφεντόνα Παπακωνσταντίνου Βασίλης - Το Μαχαίρι (live) Παπακωνσταντίνου Βασίλης - Τρίτος Παγκόσμιος Παπακωνσταντίνου Βασίλης - Τσέ (live) Παπακωνσταντίνου Βασίλης - Φεύγουν Καράβια Παπακωνσταντίνου Βασίλης - Φοβάμαι Παπακωνσταντίνου Βασίλης - Χαιρετίσματα Παπακωνσταντίνου Βασίλης - Federico Garcia Lorca Παπακωνσταντίνου Βασίλης - O Κουρσάρος Παπαμιχαήλ Δημήτρης, Βουγιουκλάκη Αλίκη - Η αγάπη θέλει δύο Παπαμιχαήλ, Βουγιουκλάκη - Η Αγάπη Θέλει Δύο Παπάζογλου Νίκος - Αύγουστος Παπάζογλου Νίκος - Στη ρωγμή του χρόνου (spike at end) Παπάζογλου Νίκος - Υδροχόος Παπάς Λάκης - Κι ύστερα μου μιλάς Παππάς - Κι ύστερα μου μιλάς (νέο κύμα) Πασχάλης - Εσένα Που Σε Ξέρω Τόσο Λίγο (skip at 1\u0026#39;49\u0026#39;\u0026#39;) Πασχάλης - Λόλα (too quiet) Πασχάλης - Μάθημα Σολφέζ (\u0026amp; Μπέσσυ Αργυράκη) Πελόμα Μποκίου - Άν ήξερα Περίδης Ορφέας - Ένα Δικό Σου Χάδι 2 Περίδης Ορφέας - Ένα δικό σου χάδι Περίδης Ορφέας - Κάτι Μου Κρύβεις Περίδης Ορφέας - Ο Ρομπέν των Καμένων Δασών (live) Πορτοκάλογλου Νίκος - (Βαλκανιζατέρ) Χωρίς Αμορτισέρ Πορτοκάλογλου Νίκος - Που Ήσουνα Φως Μου Πορτοκάλογλου Νίκος - Ταξίδι Πορτοκάλογλου Νίκος - Ότι δέν σε Σκοτώνει Πορτοκάλογλου Νίκος \u0026amp; Δεληβοριάς Φοίβος - Υπάρχει Λόγος Σοβαρός Πορτοκάλογλου Νίκος \u0026amp; Κανά Μελίνα - Κλείσε Τα Μάτια Σου Πουλικάκος - Σκόνη Πέτρες Λάσπη Πουλόπουλος - Καμαρούλα Μιά Σταλιά Πουλόπουλος - Τ\u0026#39;άσπρα πουλιά (νέο κύμα) Πουλόπουλος Γιάννης - Το Άγαλμα (112 kbps) Πράσσειν Άλογα - Βασίσου πάνω σου (Cut Down) Πυξ Λαξ - (Ζόρικοι Καιροί - 07) - Έλα Πυξ Λαξ - (Ζόρικοι Καροί - 04) - Πυξ Λαξ Πυξ Λαξ - (Ο Μπαμπούλας Τραγουδάει Μόνο Τις Νύχτες - 01) - Οι Παλιές Αγάπες Πάνε Στον Παράδεισο Πυξ Λαξ - (Ο Μπαμπούλας Τραγουδάει Μόνο Τις Νύχτες - 04) - Ποδήλατα Δίχως Φρένα Πυξ Λαξ - (Ο Μπαμπούλας Τραγουδάει Μόνο Τις Νύχτες - 12) - Στα Καμίνια Πυξ Λαξ - (Ο Μπαμπούλας Τραγουδάει Μόνο Τις Νύχτες - 13) - Να\u0026#39;ρθεις όπως θα\u0026#39;ρθουν τα Χελιδόνια Πυξ Λαξ - (Ο Μπαμπούλας Τραγουδάει Μόνο Τις Νύχτες - 14) - Τα Όνειρα της Μαίρης Πυξ Λαξ - (Ο Μπαμπούλας Τραγουδάει Μόνο Τις Νύχτες - 15) - Hey Man Κοίτα Μπροστά Πυξ Λαξ - (Ο Μπαμπούλας Τραγουδάει Μόνο Τις Νύχτες - 16) - Όλο μ\u0026#39;αφήνεις να σ\u0026#39;αφήνω Πυξ Λαξ - (Παίξε Παλιάτσο Τα Τραγούδια Σου Τελειώνουν - 11) - Έλα (live) Πυξ Λαξ - (Παίξε Παλιάτσο τα Τραγούδια σου Τελειώνουν - 02) - Να Χαθώ στα Βήματά Σου Πυξ Λαξ - (Παίξε Παλιάτσο τα Τραγούδια σου Τελειώνουν - 03) - Πούλα με Πυξ Λαξ - (Παίξε Παλιάτσο τα Τραγούδια σου Τελειώνουν - 04) - Παίξε Παλιάτσο τα Τραγούδια σου Τελειώνουν Πυξ Λαξ - (Παίξε Παλιάτσο τα Τραγούδια σου Τελειώνουν - 05) - Σαν το Σοφό που Γερνάει Πυξ Λαξ - (Παίξε Παλιάτσο τα Τραγούδια σου Τελειώνουν - 06) - Ακόμα Προσπαθώ Πυξ Λαξ - Ακόμα Τίποτα δεν Είδες Πυξ Λαξ - Απογοήτευση του Περαστικού Πυξ Λαξ - Εσυ Εκεί Πυξ Λαξ - Έπαψες Αγάπη να Θυμίζεις Πυξ Λαξ - Η θανάσιμη μοναξιά του Αλέξη Ασλάνη Πυξ Λαξ - Ίριδα Πυξ Λαξ - Λάμια Πυξ Λαξ - Μια Συνουσία Μυστική Πυξ Λαξ - Μοναξιά μου Όλα Πυξ Λαξ - Ο Έρωτας Κοιμήθηκε Νωρίς Πυξ Λαξ - Οι παλιές αγάπες Πυξ Λαξ - Πούλα με Πυξ Λαξ - Σ\u0026#39;Αγαπώ Πυξ Λαξ - Στα Βαθιά σου Νερά Πυξ Λαξ - Υδροχόος Πυξ Λαξ - Όλο μ\u0026#39;αφήνεις να σ\u0026#39;αφήνω Πυξ Λαξ - Stilvh (check title) Ρακιντζής - Μωρό Μου Φάλτσο Ροζάκης - Έτσι Είναι Η Ζωή (incomplete) Ροζάκης \u0026amp; The Playboys - Μιά Μέρα Θα \u0026#39;ρθεις Σαββόπουλος Διονύσης - Θαλασσογραφία (live) (112 kbps) Σαββόπουλος Διονύσης - Θαλασσογραφία Σαββόπουλος Διονύσης - Κολοέλληνες (incomplete) Σαββόπουλος Διονύσης - Μη μιλάς άλλο γι αγάπη (νέο κύμα) Σαββόπουλος Διονύσης - Ντιρλαντά Σαββόπουλος Διονύσης - Συννεφούλα Σιδηρόπουλος Παύλος - Κάποτε θα\u0026#39;ρθουν Σιδηρόπουλος Παύλος - Μού\u0026#39;πες θα Φύγω Σιδηρόπουλος Παύλος - Να μ\u0026#39;αγαπάς Σιδηρόπουλος Παύλος - Ο Μπάμπης ο Φλού Σιδηρόπουλος Παύλος - Στην Κ. Σιδηρόπουλος Παύλος - Της Εθνικής Συμφιλίωσης Σιδηρόπουλος Παύλος - Το 69 Σιδηρόπουλος Παύλος - Χωρίς Αιτία Σιδηρόπουλος Παύλος - R\u0026#39;n\u0026#39;R Στο Κρεβάτι (live) Σπανουδάκης Σταμάτης - Άντε Γειά Σπανουδάκης Σταμάτης - Θάλασσα Σπανουδάκης Σταμάτης - Κύματα Σπανουδάκης Σταμάτης - Ξαφνικός Έρωτας Σπανουδάκης Σταμάτης - Ξημέρωμα Στα Τείχη Σπανουδάκης Σταμάτης - Πέρασαν (live) Σπανουδάκης Σταμάτης - Φθινόπωρο Σπανουδάκης Σταμάτης - The lost moment which past us by Σπυριδούλα - Νάϋλον Ντέφια και Ψόφια Κέφια Συνήθεις Ύποπτοι - Περικοπές Ενός Απόκρυφου Ευαγγέλιου Σχοινοβάτες - Αποσύνθεση Τάμμυ - Αγόρι Μου (112 kbps) Τάμμυ - Αγόρι Μου (spike at 2\u0026#39;09\u0026#39;\u0026#39;) Τερμίτες - Μηχανικά Τερμίτες - Πάρε με από εδώ Τερμίτες - Πόσο Σε Θέλω Τερμίτες \u0026amp; Νταλάρας - Σκόνη (Live) Τζορντανέλλι Λάκης - Αγοράζω παλιά Τζορντανέλλι Λάκης - Το Κορίτσι Του Φίλου Μου Τουρνάς Κώστας - Άνθρωπε Αγάπα (\u0026amp; Χατζής, Κατσιμίχα, Αλεξία, Δάντης) Τουρνάς Κώστας - Άννα Τουρνάς Κώστας - Η μηχανή του χρόνου Τουρνάς Κώστας - Ήλιε Μου (live, unplugged) Τουρνάς Κώστας - Ήλιε μου (live) (\u0026amp; Δάντης - Πάει η αγάπη μου) Τουρνάς Κώστας - Κυρίες Και Κύριοι (\u0026amp; Κατσιμίχα Αφοί) Τουρνάς Κώστας - Κυρίες Και Κύριοι Τουρνάς Κώστας - Ο Αχιλέας από το Κάϊρο Τουρνάς Κώστας - Ο Πέτρος Τουρνάς Κώστας - Ο χρόνος ο καλύτερος Τουρνάς Κώστας - Σαν γελάς (live) Τουρνάς Κώστας - Τίποτα δεν μας φτάνει Τουρνάς Κώστας - Τον Σεπτέμβρη Τουρνάς Κώστας - Υποκριτής Τουρνάς Κώστας - Όμορφη (192 kbps) Τουρνάς Κώστας - Όμορφη Τουρνάς Κώστας - Όπου φυσάει ο άνεμος (\u0026amp; Βόσσου Σοφία) Τουρνάς Κώστας - Φωνή Τρύπες - Δε Χωράς Πουθενά Τρύπες - Ένα Πληρωμένο Τραγούδι Τρύπες - Η Μάσκα Που Κρύβεις Τρύπες - Τα Κανονικά Παιδιά Τρύπες - Ταξιδιάρα Ψυχή Τρύπες - Τσακισμένη Χαρά (Καινούργια Ζάλη) Τρύπες - Τσιφτετέλι Τρύπες - Όλες οι απαντήσεις (incomplete) Τρύπες - Χάρτινο Τσίρκο Τσακνής Διονύσης - Δημόσιος Φορέας Τσακνής Διονύσης - Φτιάξε Καρδιά Μου Το Δικό Σου Παραμύθι Τσακνής Διονύσης \u0026amp; Μαχαιρίτσας - Φλασάκι (live) Τσαλιγοπούλου Ελένη - Πιάσε Με Τσιτσάνης Β. \u0026amp; Λαμπράκη Χ. - Απόψε Στις Ακρογιαλιές Τσιτσάνης Βασίλης - Ακρογιαλιές Δειλινά Τσιτσάνης Βασίλης - Συννεφιασμένη Κυριακή Υπόγεια Ρεύματα - Αγέρας Δραπέτης Υπόγεια Ρεύματα - Ανδρείκελα Υπόγεια Ρεύματα - Βροχή Υπόγεια Ρεύματα - Κοιτάς Μακριά Υπόγεια Ρεύματα - Μ\u0026#39;αρέσει να μη λέω πολλά Υπόγεια Ρεύματα - Στο Ύψος των Ματιών μου Υπόγεια Ρεύματα - Χάρτινες Λέξεις Όναρ - Χαρτινα φανάρια Φαραντούρη (Θεοδωράκης) - Άσμα Ασμάτων Φαραντούρη (Θεοδωράκης) - Το Γελαστό Παιδί Φατμέ - Πες Το Και Έγινε Φατμέ - Ταξίδι Φατμέ - Το Καλοκαιράκι Φατμέ - Ψέματα Χατζηδάκης Μάνος - Οδός Ονείρων Χατζηδάκης Μάνος - Τα Τραγούδια Της Αμαρτίας - 1 - Εισαγωγή Χατζηδάκης Μάνος - Τα Τραγούδια Της Αμαρτίας - 2 - Τύψεις Χατζηδάκης Μάνος - Τα Τραγούδια Της Αμαρτίας - 3 - Ενοχή Χατζηδάκης Μάνος - Τα Τραγούδια Της Αμαρτίας - 4 - Έρωτας Χατζηδάκης Μάνος - Χορός Με Τη Σκιά Μου - Έλα Σε Μένα Χατζηδάκης Μάνος - Χορός Με Τη Σκιά Μου - Η Εποχή Της Αγάπης Χατζηδάκης Μάνος - Χορός Με Τη Σκιά Μου - Η Σκάλα Του Ουρανού Χατζηδάκης Μάνος - Χορός Με Τη Σκιά Μου - Μην Τον Ρωτάς Τον Ουρανό - All alone am I Χατζηδάκης Μάνος - Χορός Με Τη Σκιά Μου - Οριζόντια Εικόνα Χατζηδάκης Μάνος - Χορός Με Τη Σκιά Μου - Πέρα Στο Θολό Ποτάμι Χατζηδάκης Μάνος - Χορός Με Τη Σκιά Μου - Σ\u0026#39; Αγαπώ Χατζηδάκης Μάνος - Χορός Με Τη Σκιά Μου - Τα Λιανοτράγουδα Χατζηδάκης Μάνος - Χορός Με Τη Σκιά Μου - Χασάπικο \u0026#39;40 Χατζηδάκης Μάνος - Χορός Με Τη Σκιά Μου - Χορός Με Τη Σκιά Μου Χατζηδάκης Μάνος - Χορός Με Τη Σκιά Μου - Dedication Χατζηδάκης Μάνος - Χορός Με Τη Σκιά Μου - Kelomai se goggyla Χατζηδάκης Μάνος - Χορός Με Τη Σκιά Μου - Love her Χατζηδάκης Μάνος - Best Of - Ένας Μύθος Χατζηδάκης Μάνος - Best Of - Η Κυρά Χατζηδάκης Μάνος - Best Of - Η Πίκρα Σήμερα Χατζηδάκης Μάνος - Best Of - Θαλασσοπούλια μου Χατζηδάκης Μάνος - Best Of - Μιά Παναγιά Χατζηδάκης Μάνος - Best Of - Μίλησέ Μου Χατζηδάκης Μάνος - Best Of - Ο Αμαξάς Χατζηδάκης Μάνος - Best Of - Οδός Ονείρων Χατζηδάκης Μάνος - Best Of - Σ\u0026#39; Αγαπώ Χατζηδάκης Μάνος - Best Of - Φέρτε Μου Ένα Μαντολίνο Χατζηδάκης Μάνος - Best Of - Φιλντησένιο Καραβάκι Χατζηδάκης Μάνος - Best Of - Χασάπικο Σαράντα Χατζηδάκης Μάνος - Fifteen Vespers - Ευριδίκη Χατζηδάκης Μάνος - Fifteen Vespers - Έγινε Παρεξήγηση Χατζηδάκης Μάνος - Fifteen Vespers - Η Πέτρα Χατζηδάκης Μάνος - Fifteen Vespers - Η Τιμωρία Χατζηδάκης Μάνος - Fifteen Vespers - Κάθε Κήπος Χατζηδάκης Μάνος - Fifteen Vespers - Κυρ Αντώνης Χατζηδάκης Μάνος - Fifteen Vespers - Κυρ Μιχάλης Χατζηδάκης Μάνος - Fifteen Vespers - Νυχτερινός Περίπατος Χατζηδάκης Μάνος - Fifteen Vespers - Ο Ταχυδρόμος Πέθανε Χατζηδάκης Μάνος - Fifteen Vespers - Τα Παιδιά Του Πειραιά Χατζηδάκης Μάνος - Fifteen Vespers - Το Πάρτυ Χατζηδάκης Μάνος - Fifteen Vespers - Το Τριαντάφυλλο Χατζηδάκης Μάνος - Fifteen Vespers - Το Φεγγάρι Είναι Κόκκινο Χατζηδάκης Μάνος - Fifteen Vespers - Υμητός Χατζηδάκης Μάνος - Fifteen Vespers - Φέρτε Μου Ένα Μαντολίνο Χατζηδάκης Μάνος (Το χαμόγελο της Τζοκόντας - 01) Όταν έρχονται τα σύννεφα (starts with spike) Χατζηδάκης Μάνος (Το χαμόγελο της Τζοκόντας - 04) Βροχή Χατζηδάκης Μάνος (Το χαμόγελο της Τζοκόντας - 06) the concerto Χατζηδάκης Μάνος (Το χαμόγελο της Τζοκόντας - 07) mr Noll Χατζηδάκης Μάνος (Το χαμόγελο της Τζοκόντας - 08) Οι Δολοφόνοι Χατζηδάκης Μάνος (Το χαμόγελο της Τζοκόντας - 08) the assassins Χατζηδάκης Μάνος (Το χαμόγελο της Τζοκόντας - 10) Χορός Με Την Σκιά Μου Χατζηδάκης Μάνος (Το χαμόγελο της Τζοκόντας) - Orchestral Χατζηδάκης Μάνος (Gioconda\u0026#39;s Smile - 02) Countess Esterhazy (160 kbps) Χάρυ Κλύν - Ελληνικό Τοπίο Χάρυ Κλύν - Εν Ώρα Παράδοσης (excerpt) Χάρυ Κλύν - ΕΣΥ (vinyl rip) (incomplete) Χάρυ Κλύν - Ισπανικό (excerpt) Χάρυ Κλύν - Ο Νονός Χάρυ Κλύν - Ο Τελευταίος Καταληψίας (excerpt) Χάρυ Κλύν - Ρόδες Ανάποδα Χάρυ Κλύν - Ταξί Χάρυ Κλύν - Τι Έχετε Να Πείτε (excerpt) Χάρυ Κλύν - Όπως Οι Αρχαίοι Πρόγονοι (excerpt) Χάρυ Κλύν - Φεμινισμός (excerpt) Χάρυ Κλύν - Computer Love Χάρυ Κλύν \u0026amp; Διονύσης Σαββόπουλος - Χαρτόγραφο Χάρυ Κλύν \u0026amp; Σαββόπουλος - Ελλάδα Η Χώρα Χρυσός Τέρης - Τάκα Τάκα Χωματά - Τα λυπημένα δειλινά (νέο κύμα) Χωματά Καίτη - Και Αν Σε Αγαπώ Δεν Σε Ορίζω Χωματά Καίτη - Μιά Αγάπη Για Το Καλοκαίρι Χωματά Καίτη - Να Διώξω τα Σύννεφα (νέο κύμα) Χωματά Καίτη - Το Χριστινάκι (νέο κύμα) (112 kbps) Ψόφιοι Κοριοί - Επιστροφή στη Φύση Ψόφιοι Κοριοί - Όμορφη Πόλη A.M.A.N. - A.M.A.N A.M.A.N. - Interview with Clinton Alba Aris - Στη μαμά μου θα το πω Ariones - Τρέμει Η Καρδιά Μου Blue Birds - Προσευχή Blue Birds - Julie Charms - Γλυκειά αγαπημένη (2\u0026#39;33\u0026#39;\u0026#39;) Charms - Γλυκειά αγαπημένη (2\u0026#39;43\u0026#39;\u0026#39;) Charms - Έλα πάλι έλα Charms - Έξω απ\u0026#39;τον κόσμο Charms - Νοσταλγία Charms - Τρελοκόριτσο (160 kbps) Charms - Τρελοκόριτσο Charms - I\u0026#39;m Coming Back Charms - Mr. Goose Cinqueti - Υπερήφανη μαίρη Cousins - Fovamai Daltons - Γιεζαέλ Dragons - Acropolis Yanka Forminx - Jenka Beat Forminx - Jeronimo Yanka Gelsomina - Τι Μου Συμβαίνει Λοιπόν Harry Klyn - Ο Τελευταίος Καταληψίας Harry Klyn - Πάνα Φόν Τελεστέτ Horror Vacui - Το Κύκνειο Άσμα Idols - Ξαφνικά μ\u0026#39; αγαπάς Idols - Τρικυμία στην καρδιά μου Jeronymo - Jeronymo Rough Mix M.G.C - Miss Marple\u0026#39;s Theme Magic De Spell - Εμένα οι φίλοι μου Magic De Spell - Ήρωες Olympians - Αναμνήσεις Olympians - Άν μιά μέρα σε χάσω Olympians - Είσαι αυτή που αγαπώ Olympians - Ιστορία Olympians - Μείνε κοντά μου Olympians - Μικρή ζωγραφιά Olympians - Ο Αλέξης Olympians - Ο Τρόπος Olympians - Συγγνώμη Olympians - Σχολείο Olympians - Το Κορίτσι Του Μάη Olympians - Τώρα Που Έφυγε Ο Αλέξης (incomplete) Olympians - Τώρα που έφυγε ο Αλέξης (spike at 2\u0026#39;47\u0026#39;\u0026#39;) Olympians - Ψεύτικη αλήθεια Olympians - Hopeless Endless Ways Olympians - Mamy Blue Peppino Di Capri - Melagholia (live) Persons - Είσαι το κορίτσι που αγαπώ Poll - Άνθρωπε Αγάπα Poll - Άνθρωπε αγάπα (live) (1) Poll - Άνθρωπε αγάπα (live) (2) Poll - Δεν θα\u0026#39;σαι μάρτυς Poll - Ειρήνη (live) Poll - Έλα Ήλιε Μου (1) Poll - Έλα Ήλιε μου (2) Poll - Έλα Ήλιε μου (live) Poll - Η γενιά μας (live) (160 kbps) Poll - Η γενιά μας Poll - Κι αυτό το καλοκαίρι (live) Poll - Ξημερώνει (live) Poll - Ο γέρος + Η γενιά μας (live) Poll - Ο γέρος Poll - Σαν γελάς Poll - Στην πηγή (live) Poll - Στιγμές (live) Poll - Όμορφη (live) Poll - Όσες Φορές (live) Poll - Όσες Φορές Poll - Φοίνικες (live) (160 kbps) Poll - Φοίνικες (live) Poll - Φυλακή (live) (major skip at 2\u0026#39;14\u0026#39;\u0026#39;) Poll - Ψάχνω να Βρώ το Φίλο μου (live) Robert Williams - Μη Μου Λες Γειά Χαρά Socrates - Κλείς\u0026#39;τα μάτια σου και άκου Socrates - Socrates Sophie \u0026amp; the New Hopes - Μια αγάπη έσβησε Sounds - Έτος 2525 (In the Year 2525) Sounds - Το μάθημα Sounds - Τραγουδώντας στη βροχή (Singing in the rain) Sovereign Group - Αγόρι Μου Γλυκό (Έλενα \u0026amp;) Sovereign Group - Το δικό σου μυστικό Stereo Nova - Πάζλ στον Αέρα Stereo Nova - Six pm Strangers - Το Ραντεβού Sylva Grissi - Γιατί (too quiet) Terror-X-Crue - Η γεύση του μένους Tony \u0026amp; Vana Pinelli - Στον κήπο της αγάπης Tsopana Rave - Δώσε Ό,τι Έχεις Tsopana Rave - Είμαι Καλά Tsopana Rave - Μου Το\u0026#39;χα Πεί Vicky Leandros - Apres Toi Vicky Leandros - Lay down (candles in the rain) Vicky Leandros - Mamy blue (1971) Vikings - Catherine Vikings - Francoise Xarry Klynn - Κύρκος, Κανάλια (bad rip) Xarry Klynn - Μία μέρα στην Αθήνα Xarry Klynn - Τι έχετε να πείτε Xarry Klynn - Το τέρας του Λοχ Νες Xarry Klynn - Όπως οι αρχαίοι ημών πρόγονοι ","date":"2002-10-03T00:00:00Z","permalink":"https://blog.michael.gr/post/2002-10-03-my-greek-music-collection/","title":"My Greek music collection"},{"content":" Note in 2025:\nInformation on this page was retrieved from my old blog via The Wayback Machine (archive.org). The file extensions have been removed to avoid attracting people looking to download music, because there is nothing to download here.\nMy international (non-Greek) music collection\nA A\\A-Ha - Take On Me A\\Abba - Dancing Queen A\\Abel - Onderweg (Dutch music) A\\ACDC - Back In Black A\\ACDC - For Those About To Rock A\\ACDC - Giving The Dog A Bone A\\ACDC - Have A Drink On Me A\\ACDC - Hells Bells A\\ACDC - Highway to Hell A\\ACDC - Let Me Put My Love Into You A\\ACDC - Nervous Shakedown A\\ACDC - Rock And Roll Ain\u0026#39;t Noise Pollution A\\ACDC - Shake A Leg A\\ACDC - Shoot To Thrill A\\ACDC - What Do You Do For Money Honey A\\ACDC - You Shook Me All Night Long A\\Accept - Winter Dreams A\\Ace of Base - All That She Wants A\\Adiemus - Adiemus A\\Aerosmith - Come Together A\\Aerosmith - Dream On A\\Aerosmith - Sweet Emotion A\\Al Stewart - On the Border A\\Al Stewart - Time Passages A\\Al Stewart - Year of the Cat A\\Alan Parsons - A Dream Within A Dream A\\Alan Parsons - Eye in the Sky A\\Alan Parsons - Games People Play A\\Alan Parsons - Lucifer A\\Alan Parsons - Mamma Gamma A\\Alan Parsons - Sirius Intro A\\Alanis Morisette - (Jagged Little Pill - __).jpg A\\Alanis Morisette - (Jagged Little Pill - 01) - All I Really Want A\\Alanis Morisette - (Jagged Little Pill - 02) - You Oughta Know A\\Alanis Morisette - (Jagged Little Pill - 03) - Perfect A\\Alanis Morisette - (Jagged Little Pill - 04) - Hand In My Pocket A\\Alanis Morisette - (Jagged Little Pill - 05) - Right Through You A\\Alanis Morisette - (Jagged Little Pill - 06) - Forgiven A\\Alanis Morisette - (Jagged Little Pill - 07) - You Learn A\\Alanis Morisette - (Jagged Little Pill - 08) - Head Over Feet A\\Alanis Morisette - (Jagged Little Pill - 09) - Mary Jane A\\Alanis Morisette - (Jagged Little Pill - 10) - Ironic A\\Alanis Morisette - (Jagged Little Pill - 11) - Not The Doctor A\\Alanis Morisette - (Jagged Little Pill - 12) - Wake Up A\\Alannah Myles - Black Velvet (4\u0026#39;25\u0026#39;\u0026#39;) A\\Alannah Myles - Black Velvet (4\u0026#39;48\u0026#39;\u0026#39;) A\\Albinoni - Adagio (excerpt) A\\Alex Reece - Feel The Sunshine A\\Alice Cooper - Billion Dollar Babies A\\Alice Cooper - I\u0026#39;m Eighteen A\\Alice Cooper - No More Mr. Nice Guy A\\Alice Cooper - Poison A\\Alice Cooper - School\u0026#39;s out A\\Alice Deejay - Better off alone A\\Alice In Chains - Down in a Hole A\\Alice In Chains - Got Me Wrong A\\Alice In Chains - Grind A\\Alice In Chains - Heaven Beside You A\\Alice In Chains - Man In The Box A\\Alice In Chains - Rooster A\\Alice In Chains - Rotten Apple A\\Alice In Chains - Would (live, unplugged) A\\Alice In Chains - Would A\\All Saints - Lady Marmelade A\\Alphaville - Big in Japan A\\Alphaville - Sounds Like A Melody A\\Amanda Lear - Enigma A\\Andreas Johnson - Glorious A\\Andres Segovia - Asturias Leyenda A\\Ani DiFranco - Wishin\u0026#39; and Hopin\u0026#39; A\\Animals - Don\u0026#39;t Let Me Be Misunderstood (112 kbps) A\\Animals - Good Times A\\Animals - It\u0026#39;s My Life A\\Animals - See See Rider A\\Animals - The House of the Rising Sun A\\Animals - The Night (112 kbps) A\\Animals - We Gotta Get Out Of This Place A\\Animals - When I Was Young A\\Animotion - Obsession A\\Anne Clark - Poem Without Words I - The Third Meeting A\\Anne Clark - Poem Without Words II - Journey By Night A\\Aphrodite\u0026#39;s Child - Break A\\Aphrodite\u0026#39;s Child - It\u0026#39;s Five O\u0026#39;Clock A\\Aphrodite\u0026#39;s Child - Rain And Tears A\\Aphrodite\u0026#39;s Child - Spring, Summer, Winter \u0026amp; Fall A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 01) The System A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 02) Babylon A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 03) Loud Loud Loud A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 04) The Four Horsemen A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 05) The Lamb A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 06) The Seventh Seal A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 07) Aegian Sea A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 08) Seven Bowls A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 09) The Wakening Beast A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 10) Lament A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 11) The Marching Beast A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 12) The Battle of The Locust A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 13) Do It A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 14) Tribulation A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 15) The Beast A\\Aphrodite\u0026#39;s Child (666 Disc 1 - 16) Ofis (96 kbps) A\\Aphrodite\u0026#39;s Child (666 Disc 2 - 01) Seven Trumpets A\\Aphrodite\u0026#39;s Child (666 Disc 2 - 02) Altamont A\\Aphrodite\u0026#39;s Child (666 Disc 2 - 03) The Wedding Of The Lamb A\\Aphrodite\u0026#39;s Child (666 Disc 2 - 04) Capture Of The Beast A\\Aphrodite\u0026#39;s Child (666 Disc 2 - 05) Infinity A\\Aphrodite\u0026#39;s Child (666 Disc 2 - 06) Hic Et Nunc (bleep at 1\u0026#39;33\u0026#39;\u0026#39;) A\\Aphrodite\u0026#39;s Child (666 Disc 2 - 07) All The Seats Where Occupied A\\Apocalyptica - Enter Sandman A\\Apocalyptica - Nothing Else Matters A\\Apocalyptica - The Unforgiven A\\Apollo 440 - Can\u0026#39;t Stop the Rock A\\Apollo 440 - Don\u0026#39;t Fear The Reaper A\\Aqua - Barbie Girl A\\Aretha Franklin - Respect A\\Argent - Hold Your Head Up A\\Armand van Helden - Funk Phenomenon A\\Art of Noise - Moments In Love A\\Arthur Brown - Fire A\\Asia - Heat of the Moment A\\Asia - Only Time Will Tell A\\Asia - Sole Survivor A\\Association - Six Man Band A\\Average White Band - Pick Up The Pieces (Swingers Soundtrack) A\\Ayla - Brainchild B B\\B-52s - Love Shack (4\u0026#39;19\u0026#39;\u0026#39;) B\\B-52s - Love Shack (5\u0026#39;21\u0026#39;\u0026#39;) B\\B-52s - Rock Lobster (4\u0026#39;54\u0026#39;\u0026#39;) B\\B-52s - Rock Lobster (6\u0026#39;51\u0026#39;\u0026#39;) B\\Babe Ruth - For A Few Dollars More B\\Babe Ruth - The Mexican B\\Babys - Head First B\\Bach - Toccata and Fugue B\\Bachman Turner Overdrive - Taking Care Of Buisness B\\Bachman Turner Overdrive - You Ain\u0026#39;t Seen Nothing Yet (bleeps in the beginning) B\\Backstreet Boys - Larger Than Life B\\Bad Company - Bad Company B\\Bad Company - Can\u0026#39;t Get Enough B\\Bad Company - Don\u0026#39;t Let Me Down B\\Bad Company - Electric Land B\\Bad Company - Feel Like Making Love B\\Bad Company - If You Need Somebody B\\Bad Company - Movin\u0026#39; On B\\Bad Company - Ready For Love B\\Bad Company - Rock \u0026#39;n\u0026#39; Roll Fantasy B\\Bad Company - Rock Steady B\\Bad Company - Run With The Pack B\\Bad Company - Seagull B\\Bad Company - Shooting Star B\\Bad Company - The Way I Choose B\\Bad English - When I See You Smile B\\Balanescu Quartet - Model (instrumental adaptation) B\\Baltimora - Tarzanboy B\\Bananarama - Cruel Summer B\\Bananarama - Venus (adaptation) B\\Band - The Weight B\\Bangles - Am I Only Dreaming B\\Bangles - Eternal Flame B\\Bangles - Hazy Shade of Winter B\\Bangles - Manic Monday B\\Bangles - Walk Like An Egyptian B\\Barclay James Harvest - Child Of The Universe (crappy studio version) B\\Barclay James Harvest - Hard Hearted Woman B\\Barclay James Harvest - Hymn B\\Barclay James Harvest - Love On The Line B\\Barclay James Harvest - Poor Man\u0026#39;s Moody Blues B\\Barclay James Harvest - Suicide (crappy Octoberon version) B\\Barry McGuire - Eve Of Destruction B\\Bassheads - Fade to Grey (7= Edit) B\\BB King - Call it Stormy Monday B\\BB King and Tracy Chapman - The Thrill Is Gone B\\BB King and U2 - When Love Comes to Town B\\Beach Boys - Wipeout (check artist) B\\Beastie Boys - Fight For Your Right To Party B\\Beastie Boys - Sabotage B\\Beatles - (1968 - White Album - Disc 1 - 01) - Back In The USSR B\\Beatles - (1968 - White Album - Disc 1 - 02) - Dear Prudence B\\Beatles - (1968 - White Album - Disc 1 - 03) - Glass Onion B\\Beatles - (1968 - White Album - Disc 1 - 04) - Ob-La-Di-Ob-La-Da B\\Beatles - (1968 - White Album - Disc 1 - 05) - Wild Honey Pie B\\Beatles - (1968 - White Album - Disc 1 - 06) - Continuing Story of Bungalow Bill B\\Beatles - (1968 - White Album - Disc 1 - 07) - While My Guitar Gently Weeps B\\Beatles - (1968 - White Album - Disc 1 - 08) - Happiness Is A Warm Gun B\\Beatles - (1968 - White Album - Disc 1 - 09) - Martha My Dear B\\Beatles - (1968 - White Album - Disc 1 - 10) - I\u0026#39;m So Tired B\\Beatles - (1968 - White Album - Disc 1 - 11) - Blackbird B\\Beatles - (1968 - White Album - Disc 1 - 12) - Piggies B\\Beatles - (1968 - White Album - Disc 1 - 13) - Rockie Racoon B\\Beatles - (1968 - White Album - Disc 1 - 14) - Don\u0026#39;t Pass Me By B\\Beatles - (1968 - White Album - Disc 1 - 15) - Why Don\u0026#39;t We Do It In The Road B\\Beatles - (1968 - White Album - Disc 1 - 16) - I Will B\\Beatles - (1968 - White Album - Disc 1 - 17) - Julia B\\Beatles - (1968 - White Album - Disc 2 - 01) - Birthday B\\Beatles - (1968 - White Album - Disc 2 - 02) - Yer Blues B\\Beatles - (1968 - White Album - Disc 2 - 03) - Mother Nature\u0026#39;s Son B\\Beatles - (1968 - White Album - Disc 2 - 04) - Everybody\u0026#39;s Got Something To Hide B\\Beatles - (1968 - White Album - Disc 2 - 05) - Sexy Sadie B\\Beatles - (1968 - White Album - Disc 2 - 06) - Helter Skelter B\\Beatles - (1968 - White Album - Disc 2 - 07) - Long, Long, Long B\\Beatles - (1968 - White Album - Disc 2 - 08) - Revolution 1 B\\Beatles - (1968 - White Album - Disc 2 - 09) - Honey Pie B\\Beatles - (1968 - White Album - Disc 2 - 10) - Savory Truffle B\\Beatles - (1968 - White Album - Disc 2 - 11) - Cry Baby Cry B\\Beatles - (1968 - White Album - Disc 2 - 12) - Revolution 9 B\\Beatles - (1968 - White Album - Disc 2 - 13) - Good Night B\\Beatles - Across the Universe B\\Beatles - Can\u0026#39;t Buy Me Love B\\Beatles - Hello Goodbye B\\Beatles - Lucy in the Sky with Diamonds B\\Beatles - Strawberry Fields Forever (+weird 10 secs) (orchestra) B\\Beau Brummels - Just A Little B\\Beck - Loser B\\Bee Gees - How Deep Is Your Love B\\Bee Gees - Staying Alive B\\Belinda Carlisle - Heaven is a Place on Earth B\\Ben E. King - Stand By Me B\\Benny Hill - Yakety Sax B\\Berlin - Take My Breath Away B\\Better than Ezra - Good B\\Betweenzone - Underground B\\Big Mountain - Baby I Love Your Way B\\Biily Joel - Piano man B\\Bill Conti - (Rocky Soundtrack) - Gonna Fly Now (2\u0026#39;48\u0026#39;\u0026#39;) B\\Bill Conti - (Rocky Soundtrack) - Gonna Fly Now (4\u0026#39;49\u0026#39;\u0026#39;) B\\Bill Haley \u0026amp; The Comets - Rock Around the Clock (live) B\\Bill Haley \u0026amp; The Comets - Shake Rattle And Roll B\\Bill Withers - Ain\u0026#39;t No Sunshine (192 kbps) B\\Billy Idol - Cradle of Love B\\Billy Idol - Eyes Without A Face B\\Billy Idol - Mony Mony B\\Billy Idol - Rebel Yell B\\Billy Idol - White Wedding B\\Billy Joel - River of Dreams B\\Billy Joel - The Lion Sleeps Tonight B\\Billy Ocean - Loverboy B\\Billy Squire - My Kinda Lover (192 kbps) B\\Bjork - Bachelorette B\\Bjork - Big Time Sensuality B\\Bjork - Come To Me B\\Bjork - Human Behavior B\\Bjork - Violently Happy B\\Black Blood - AIE A\u0026#39;Mwana B\\Black Crowes - Hard To Handle B\\Black Crowes - Remedy B\\Black Crowes - She Talks to Angels B\\Black Sabbath - Die Young B\\Black Sabbath - Heaven And Hell B\\Black Sabbath - Iron Man (common version) B\\Black Sabbath - Iron Man (uncommon version) B\\Black Sabbath - NIB B\\Black Sabbath - Paranoid B\\Black Sabbath - War Pigs B\\Blind Melon - No Rain B\\Blondie - Call Me (Theme From American Gigolo) B\\Blondie - Heart of Glass B\\Blondie - Nothing is Real But the Girl B\\Blondie - One Way or Another (check artist) B\\Blondie - Rapture B\\Blondie - The Tide Is High B\\Bloodhound Gang - Bad Touch B\\Blow Monkeys - You Don\u0026#39;t Own Me (adaptation) B\\Blue Oyster Cult - Astronomy B\\Blue Oyster Cult - Ballroom Blitz (adaptation) B\\Blue Oyster Cult - Burnin\u0026#39; For You B\\Blue Oyster Cult - Don\u0026#39;t Fear The Reaper B\\Blue Oyster Cult - Joan Crawford (Live) B\\Blue Oyster Cult - Joan Crawford B\\Blues Brothers - New Orleans (live) B\\Blues Brothers - Peter Gunn Theme B\\Blues Brothers - Soul Man (live) B\\Blues Brothers - Sweet Home Chicago B\\Blur - Coffee And TV B\\Blur - Girls And Boys B\\Bob Dylan - Everybody Must Get Stoned B\\Bob Dylan - Hurricane B\\Bob Dylan - Subterranean Homesick Blues B\\Bob Marley - (Legend - 01) - It Is Love B\\Bob Marley - (Legend - 02) - No Woman No Cry (live) B\\Bob Marley - (Legend - 03) - Could You Be Loved B\\Bob Marley - (Legend - 04) - Three Little Birds B\\Bob Marley - (Legend - 05) - Buffalo Soldier B\\Bob Marley - (Legend - 06) - Get Up Stand Up B\\Bob Marley - (Legend - 07) - Stir It Up B\\Bob Marley - (Legend - 08) - One Love People Get Ready B\\Bob Marley - (Legend - 09) - I Shot The Sherrif B\\Bob Marley - (Legend - 10) - Waiting In Vain B\\Bob Marley - (Legend - 11) - Redemption Song B\\Bob Marley - (Legend - 12) - Satisfy My Soul B\\Bob Marley - (Legend - 13) - Exodus B\\Bob Marley - (Legend - 14) - Jamming B\\Bob Marley - Coming In From The Cold B\\Bob Marley - Is This Love (112 kbps) B\\Bob Marley - No Woman No Cry (112 kbps) B\\Bob Marley - Pimpers Paradise B\\Bob Seger - Against the Wind B\\Bob Seger - Down On Main Street B\\Bob Seger - Kathmandu B\\Bob Seger - Night Moves B\\Bob Seger - Old Time Rock and Roll B\\Bob Seger - Turn the Page (live) B\\Bobby Fuller Four - I Fought the Law B\\Bobby McFerrin - Don\u0026#39;t Worry Be Happy B\\Bobby Pickett - Monster Mash B\\Bobby Vinton - Sealed With A Kiss (adaptation) B\\Bodycount - Copkiller B\\Bokomolech - Sin River B\\Bolland - You\u0026#39;re In The Army Now (vbr - high) B\\Boney M - Belfast (2\u0026#39;23\u0026#39;\u0026#39;) B\\Boney M - Belfast (3\u0026#39;28\u0026#39;\u0026#39;) B\\Boney M - Daddy Cool B\\Boney M - Rasputin B\\Boney M feat. Mobi T. - Daddy Cool \u0026#39;99 (check artist) B\\Bonnie Tyler - I Need A Hero B\\Bonnie Tyler - Total Eclipse of the Heart B\\Bonnie Tyler (\u0026amp; Pat Benatar) - Total Eclipse Of The Heart B\\Boo Radleys - There She Goes B\\Booker T \u0026amp; The MG\u0026#39;s - Green Onions B\\Boppers - Rama Lama Ding Dong B\\Boston - Can\u0026#39;t Fight This Feeling Anymore B\\Boston - More Than a Feeling B\\Boston Pops - Indiana Jones Theme B\\Box Tops - The Letter B\\BrainBug - Benedictus B\\BrainBug - Rain B\\Bread - Mother Freedom B\\Breeders - Cannonball (gap at 2\u0026#39;05\u0026#39;\u0026#39;) B\\Brian Eno \u0026amp; Jah Wobble - (Spinner - 01) - Where We Lived B\\Brian Eno \u0026amp; Jah Wobble - (Spinner - 02) - Like Organza B\\Brian Eno \u0026amp; Jah Wobble - (Spinner - 03) - Steam B\\Brian Eno \u0026amp; Jah Wobble - (Spinner - 04) - Garden Recalled B\\Brian Eno \u0026amp; Jah Wobble - (Spinner - 05) - Marine Radio B\\Brian Eno \u0026amp; Jah Wobble - (Spinner - 06) - Unusual Balance B\\Brian Eno \u0026amp; Jah Wobble - (Spinner - 07) - Space Diary 1 B\\Brian Eno \u0026amp; Jah Wobble - (Spinner - 08) - Spinner B\\Brian Eno \u0026amp; Jah Wobble - (Spinner - 09) - Transmitter And Trumpet B\\Brian Eno \u0026amp; Jah Wobble - (Spinner - 10) - Left Where It Fell (+one more track) B\\Brian Hyland - Sealed With a Kiss B\\Britney Spears - Hit Me Baby One More Time B\\Britney Spears - Oops! I Did I Again B\\Britney Spears - The Beat Goes On B\\Britney Spears Parody - Oops! I Farted Again B\\Britney Spears Parody - Oops! I\u0026#39;m Pregnant Again B\\Bronski Beat - Small Town Boy (4\u0026#39;07\u0026#39;\u0026#39;) B\\Bronski Beat - Small Town Boy (5\u0026#39;03\u0026#39;\u0026#39;) B\\Bruce Hornsby and the Range - That\u0026#39;s Just The Way It Is B\\Bruce Springsteen - 57 Channels B\\Bruce Springsteen - Because The Night (from Live-1975-85) (live) B\\Bruce Springsteen - Because The Night (live) B\\Bruce Springsteen - Hungry Heart B\\Bruce Springsteen - Pink Cadillac B\\Bryan Adams - (Everything I Do) I Do It For You B\\Bryan Adams - Can\u0026#39;t Stop This Thing We Started B\\Bryan Adams - Run To You B\\Bryan Adams - Somebody B\\Bryan Adams - Summer of \u0026#39;69 B\\Buffalo Springfield - Cinnamon Girl B\\Buffalo Springfield - For What It\u0026#39;s Worth B\\Bush - Comedown B\\Bush - Glycerine B\\Bush - Machine Head B\\Bush - The Chemicals Between Us B\\But I\u0026#39;m a Cheerleader - 01 - April March - Chick Habit B\\Byrds - My Back Pages B\\Byrds - Turn! Turn! Turn! (To Everyting There Is A Season) C C\\C \u0026amp; C Music Factory - Everybody Dance Now C\\C-real - Stop Killing Time C\\C-real - With Or Without You (Album Version) C\\C. J. Lewis - Sweets for My Sweet C\\Cake - I Will Survive (adaptation) C\\Camper Van Beethoven - Pictures of Matchstick Men (adaptation) C\\Camper Van Beethoven - Take the Skinheads Bowling C\\Can - Vitamine C C\\Cantaloop - Flip Fantasia C\\Captain Sensible - Wot C\\Cardigans - Carnival C\\Cardigans - Erase and Rewind C\\Cardigans - Lovefool (from Romeo+Juliet soundtrack) C\\Cardigans - My Favourite Game C\\Carl Orff - Carmina Burana - O Fortuna (2\u0026#39;29\u0026#39;\u0026#39;) C\\Carl Orff - Carmina Burana - O Fortuna (3\u0026#39;00\u0026#39;\u0026#39;) C\\Cars - Drive C\\Cars - Good Times Roll C\\Cars - Just What I Needed C\\Cars - Let\u0026#39;s Go C\\Cars - My Best Friend\u0026#39;s Girl C\\Cars - Shake It Up C\\Cars - What I Like About You C\\Cars - You Might Think C\\Cat Stevens - Lady D\u0026#39;Arbanville (minor spike at 0\u0026#39;24\u0026#39;\u0026#39;) C\\Cat Stevens - Wild World C\\Chaka Demus \u0026amp; Pliers - Twist and Shout (adaptation) C\\Champs - Tequila C\\Charles \u0026amp; Eddie - Would I Lie To You (gap at 2\u0026#39;29\u0026#39;\u0026#39;) C\\Cheap Trick - Ain\u0026#39;t That A Shame (live) C\\Cheap Trick - I Want You To Want Me (live) C\\Cheap Trick - Surrender C\\Cheech \u0026amp; Chong - Gonorrhea C\\Cher - If I Could Turn Back Time C\\Cheryl Crow - All I Want To Do C\\Chicago - I Don\u0026#39;t Want To Live Without Your Love (1999) C\\Chicago - Stay The Night C\\Chordettes - Lollipop C\\Chris de Burgh - Don\u0026#39;t Pay The Ferryman (live) C\\Chris de Burgh - Revolution, Liberty (spike at 1\u0026#39;55\u0026#39;\u0026#39;) C\\Chris Isaak - Blue hotel C\\Christina Aguilera, Mya, Pink, Lil Kim - Lady Marmalade C\\Christopher Cross - Ride Like the Wind C\\Chubby Checker - Let\u0026#39;s Twist Again C\\Chubby Checker - Twist And Shout C\\Chuck Berry - (Get Your Kicks On) Route 66 C\\Chuck Berry - Johnny B. Goode C\\Chuck Berry - You Never Can Tell C\\Chumbawamba - Tub Thumping C\\Church - Under The Milky Way C\\Cinderella - Coming Home C\\Cinderella - Don\u0026#39;t Know What You Got Til It\u0026#39;s Gone C\\Cindi Lauper - Girls Just Wanna Have Fun C\\Cindi Lauper - Time After Time C\\City - Am Fenster (Crappy English Version) C\\City - Am Fenster (Edit Version) C\\City - Am Fenster (Original Version) C\\City - Am Fenster (Special Version) C\\Clash - I Fought The Law C\\Clash - London Calling C\\Clash - Rock the Casbah C\\Clash - Should I Stay or Should I Go C\\Claude Chall \u0026amp; Carlos Campos - Flying Carpet (Ambient Mix) (vbr-high) C\\Claude Francois - Donna donna C\\Cockney Rebel - Here Comes The Sun (adaptation) C\\Cockney Rebel - Mr Soft C\\Cockney Rebel - Sebastian C\\Coldplay - Don\u0026#39;t Panic C\\Collective Soul - December C\\Collective Soul - Listen C\\Collective Soul - Shine (live, acoustic) C\\Collective Soul - Shine C\\Collective Soul - The World That I Know C\\Concrete Blonde - Joey C\\Concrete Blonde - Mexican Moon C\\Cornelius Brothers \u0026amp; Sister Rose - Treat Her Like A Lady (192 kbps) C\\Count Five - Psychotic Reaction C\\Counting Crows - Mr. Jones C\\Cowboy Junkies - Common Disaster C\\Cowboy Junkies - Sweet Jane C\\Cracker - Low C\\Cramps - Human Fly C\\Cranberries - Electric Blue C\\Cranberries - Salvation C\\Cranberries - Zombie C\\Crash Test Dummies - Mmm Mmm Mmmm C\\Cream - Crossroads (live) C\\Cream - Strange Brew C\\Cream - Sunshine Of Your Love C\\Cream - Tales Of Brave Ulysees C\\Cream - White Room C\\Creedence Clearwater Revival - Bad Moon Rising (112 kbps) C\\Creedence Clearwater Revival - Born On The Bayou C\\Creedence Clearwater Revival - Down On The Corner C\\Creedence Clearwater Revival - Fortunate Son C\\Creedence Clearwater Revival - Have You Ever Seen the Rain C\\Creedence Clearwater Revival - I Heard it Through the Grapevine C\\Creedence Clearwater Revival - I Put A Spell On You C\\Creedence Clearwater Revival - Rollin On A River C\\Creedence Clearwater Revival - Susie Q C\\Creedence Clearwater Revival - Who\u0026#39;ll Stop The Rain C\\Crosby, Stills, Nash \u0026amp; Young - Find The Cost Of Freedom C\\Crosby, Stills, Nash \u0026amp; Young - Helpless C\\Crosby, Stills, Nash \u0026amp; Young - Ohio C\\Crowded House - Don\u0026#39;t Dream It\u0026#39;s Over C\\Crowded House - Something so Strong C\\Crystals - Da Doo Ron Ron C\\Cult - Edie (Ciao Baby) C\\Cult - Fire Woman C\\Cult - Rain C\\Cult - She Sells Sanctuary C\\Cult - Sweet Soul Sister C\\Cult - Wild Flower C\\Culture Club - Do You Really Want To Hurt Me C\\Cure - A Forest - (live 1984) C\\Cure - A Forest (4\u0026#39;54\u0026#39;\u0026#39;) C\\Cure - A Forest (crappy mixed up version) C\\Cure - Boys Don\u0026#39;t Cry C\\Cure - Fascination Street C\\Cure - Killing in an Arab C\\Cure - Lovecats (1983) C\\Cure - Lullaby C\\Cure - Why Can\u0026#39;t I Be You C\\Cutting Crew - I Just Died In Your Arms Tonight C\\Cypress Hill - Insane In The Brain D D\\Dalida - Mamy blue (italian) D\\Damn Yankees - Coming of Age D\\Damn Yankees - High Enough D\\Damned - Alone Again Or (adaptation) D\\Dana Carvey - 100 Year Old Man D\\Dana Carvey - Chopping Broccoli D\\Dana Carvey - OJ D\\Dana Carvey - Prostitution and George Bush D\\Danny Elfman \u0026amp; Lalo Schifrin - Mission Impossible Main Theme (check artist) D\\Daryl Hall \u0026amp; John Oates - Maneater D\\Dave Brubeck - Everybody\u0026#39;s Jumpin\u0026#39; D\\Dave Brubeck - My Favourite Things D\\Dave Brubeck - Pick Up Sticks D\\Dave Brubeck - Strange Meadow Lark D\\Dave Brubeck - Take Five D\\Dave Brubeck - Three To Get Ready D\\Dave Brubeck Quartet, Paul Desmond - A Foggy Day D\\Dave Clark Five - Glad All Over D\\Dave Pike - Mathar D\\David Bowie - 1984 D\\David Bowie - Absolute Beginners D\\David Bowie - Ashes To Ashes D\\David Bowie - Blue Jeans D\\David Bowie - Cat People (crappy version) D\\David Bowie - Cat People D\\David Bowie - Changes D\\David Bowie - China Girl D\\David Bowie - Fame D\\David Bowie - Fashion D\\David Bowie - Golden Years D\\David Bowie - Heroes D\\David Bowie - Lady Grinning Soul D\\David Bowie - Let\u0026#39;s Dance D\\David Bowie - Modern Love D\\David Bowie - Panic in Detroit D\\David Bowie - Rebel Rebel D\\David Bowie - Space Oddity D\\David Bowie - Starman D\\David Bowie - The Man Who Sold The World D\\David Bowie - This Is Not America D\\David Bowie - Ziggy Stardust D\\David Bowie \u0026amp; Lou Reed - Dirty Boulevard D\\David Bowie \u0026amp; Mick Jagger - Dancing In The Streets (live) D\\David Essex - Rock On D\\David Stewart \u0026amp; Candy Dulfer - Lilly Was Here (3\u0026#39;56\u0026#39;\u0026#39;) D\\David Stewart \u0026amp; Candy Dulfer - Lilly Was Here (4\u0026#39;18\u0026#39;\u0026#39;) D\\Dawn Penn - You Don\u0026#39;t Love Me (No No No) D\\Dead Can Dance - Ariadne D\\Dead Can Dance - Ascension D\\Dead Can Dance - Dawn of the Iconoclast D\\Dead Can Dance - Devorzhum D\\Dead Can Dance - Musica Eternal D\\Dead Can Dance - Summoning of the Muse D\\Dead Can Dance - The Fatal Impact D\\Dead Can Dance - The Garden Of Zephirus D\\Dead Can Dance - The Host of Seraphim D\\Dead Can Dance - The Snake And The Moon D\\Dead Can Dance - The Song of the Sybil D\\Dead Can Dance - The Tomb of Seraphim D\\Dead Can Dance - The Writing on My Fathers Hand D\\Dead Can Dance - Threshold D\\Dead Can Dance - Xavier D\\Dead Can Dance - Yulunga (Spirit Dance) D\\Dead Can Dance \u0026amp; Loreena McKennit - Saltarello D\\Dead Kennedys - Holiday In Cambodia D\\Dead Kennedys - Kill the Poor D\\Dead Kennedys - Too Drunk To Fuck D\\Dead Kennedys - Viva Las Vegas (112 kbps) D\\Dead Or Alive - You Spin Me Right Round D\\Dee C Lee - See The Day D\\Deelite - Groove Is In The Heart D\\Deep Blue Something - Breakfast At Tiffany\u0026#39;s D\\Deep Forest - Anasthasia D\\Deep Forest - The First Twilight D\\Deep Purple - Black Night (112 kbps, bad recording) D\\Deep Purple - Black Night (bad quality) D\\Deep Purple - Black Night (live) D\\Deep Purple - Burn D\\Deep Purple - Child In Time (bleep at 9\u0026#39;25\u0026#39;\u0026#39;) D\\Deep Purple - Highway Star D\\Deep Purple - Hush D\\Deep Purple - Mistreated D\\Deep Purple - Perfect Strangers D\\Deep Purple - Sail Away D\\Deep Purple - Smoke On The Water (3\u0026#39;49\u0026#39;\u0026#39;) D\\Deep Purple - Smoke On The Water (5\u0026#39;41\u0026#39;\u0026#39;) D\\Deep Purple - Soldier Of Fortune D\\Deep Purple - Space Truckin (vinyl rip) D\\Deep Purple - When A Blind Man Cries D\\Deep Purple - Woman From Tokyo D\\Def Leppard - Love Bites D\\Deftones - Change (In the House of Flies) D\\Del Shannon - Runaway (2\u0026#39;17\u0026#39;\u0026#39;) (vinyl rip) D\\Del Shannon - Runaway (2\u0026#39;34\u0026#39;\u0026#39;) D\\Demis Roussos - Forever and Ever D\\Demis Roussos - Mamy Blue (english) (adaptation) D\\Demis Roussos - When A Man Loves A Woman (adaptation) D\\Denis Leary - Asshole D\\Depeche Mode - Enjoy The Silence D\\Depeche Mode - Eveything counts D\\Depeche Mode - Its No Good (192 kbps) D\\Depeche Mode - Just Can\u0026#39;t Get Enough (192 kbps) D\\Depeche Mode - Photographic D\\Depeche Mode - Strangelove (live) D\\Depeche Mode - Tainted Love D\\Derek And the Dominos - Bell Bottom Blues D\\Derek And the Dominos - Layla D\\Deus - Roses D\\Dexy\u0026#39;s Midnight Runners - Come On Eileen D\\Diana Ross - Upside Down D\\Diana Ross \u0026amp; The Supremes - Baby Love D\\Diana Ross \u0026amp; the Supremes - I Will Survive D\\Dick Dale - Misirlou (Pulp Fiction Movie Theme) D\\Dick Dale - Misirlou D\\Dick Dale \u0026amp; his Deltones - Pipeline (48 khz) D\\Dido - Here With Me D\\Digable Planets - Flip Fantasia (check artist) D\\Dio - Don\u0026#39;t Talk To Strangers D\\Dio - Egypt (The Chains Are On) D\\Dio - Holy Diver D\\Dio - Rainbow in the Dark D\\Dio - The Last in Line D\\Dio and Yngwie Malmsteen - Dream On (Aerosmith Tribute) D\\Dio, Steve Lukather, Bob Kulick, Phil Soussan, Randy Castill - Welcome to My Nightmare D\\Dion \u0026amp; The Belmonts - Prima Donna D\\Dion \u0026amp; The Belmonts - The Wanderer D\\Dionne Warwick - Walk On By D\\Dire Straits - Down To The Waterline D\\Dire Straits - Money For Nothing D\\Dire Straits - So Far Away D\\Dire Straits - Sultans of Swing D\\Dire Straits - The Man\u0026#39;s Too Strong D\\Dire Straits - Twisting By The Pool D\\Dire Straits - Walk Of Life D\\Dirty Dancing - I\u0026#39;ve Had The Time Of My Life D\\Divinyls - I Touch Myself D\\DJ Miko - What\u0026#39;s Up D\\Don Henley - All She Wants To Do Is Dance D\\Don Henley - Dirty Laundry D\\Don Henry - Come Rain Or Come Shine (live) D\\Don McLean - American Pie D\\Donna Summer - Hot Stuff D\\Donna Summer - She Works Hard For Her Money D\\Donovan - Celia Of The Seals D\\Donovan - Hurdy Gurdy Man D\\Donovan - Season Of The Witch D\\Donovan - Sunshine Superman D\\Doobie Brothers - Listen To The Music D\\Doobie Brothers - Long Train Running D\\Doop - Doop D\\Doors - Alabama Song D\\Doors - Back Door Man D\\Doors - Break On Through D\\Doors - End Of The Night D\\Doors - Hello, I Love You D\\Doors - L.A. Woman D\\Doors - Light My Fire D\\Doors - Love Her Madly D\\Doors - Love Me Two Times D\\Doors - People Are Strange D\\Doors - Riders On The Storm D\\Doors - Roadhouse Blues (LIVE) D\\Doors - Spanish Caravan D\\Doors - Summer\u0026#39;s Almost Gone D\\Doors - Take it As it Comes D\\Doors - The Changeling D\\Doors - The End (192 kbps) D\\Doors - Twentieth Century Fox D\\Doors - When The Music\u0026#39;s Over D\\Doris Day - Whatever Will Be Will Be (Que Sera Sera) D\\Double - The Captain Of Her Heart D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Douglas Adams) 1 of 8 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Douglas Adams) 2 of 8 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Douglas Adams) 3 of 8 (1sec short) D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Douglas Adams) 4 of 8 (1sec short) D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Douglas Adams) 5 of 8 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Douglas Adams) 6 of 8 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Douglas Adams) 7 of 8 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Douglas Adams) 8 of 8 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Peter Jones) 01 of 12 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Peter Jones) 02 of 12 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Peter Jones) 03 of 12 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Peter Jones) 04 of 12 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Peter Jones) 05 of 12 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Peter Jones) 06 of 12 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Peter Jones) 07 of 12 (cut down) D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Peter Jones) 08 of 12 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Peter Jones) 09 of 12 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Peter Jones) 10 of 12 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Peter Jones) 11 of 12 D\\Douglas Adams - Hitch Hiker\u0026#39;s Guide to the Galaxy (Peter Jones) 12 of 12 D\\Douglas Adams - Mostly Harmless - (Douglas Adams) - 1 of 8 D\\Douglas Adams - Mostly Harmless - (Douglas Adams) - 2 of 8 D\\Douglas Adams - Mostly Harmless - (Douglas Adams) - 3 of 8 D\\Douglas Adams - Mostly Harmless - (Douglas Adams) - 4 of 8 D\\Douglas Adams - Mostly Harmless - (Douglas Adams) - 5 of 8 D\\Douglas Adams - Mostly Harmless - (Douglas Adams) - 6 of 8 D\\Douglas Adams - Mostly Harmless - (Douglas Adams) - 7 of 8 D\\Douglas Adams - Mostly Harmless - (Douglas Adams) - 8 of 8 D\\Drifters - Under the Boardwalk D\\Drill - What You Are D\\Duane Eddy - Peter Gunn Theme D\\Duran Duran - A View to a Kill D\\Duran Duran - Come Undone D\\Duran Duran - Girls on Film D\\Duran Duran - Hungry Like The Wolf (3\u0026#39;27\u0026#39;\u0026#39;) D\\Duran Duran - Hungry Like The Wolf (3\u0026#39;41\u0026#39;\u0026#39;) D\\Duran Duran - Is There Something I Should Know D\\Duran Duran - Ordinary World (4\u0026#39;38\u0026#39;\u0026#39;) D\\Duran Duran - Ordinary World (5\u0026#39;39\u0026#39;\u0026#39;) D\\Duran Duran - Rio D\\Duran Duran - Save a Prayer Til the Morning After D\\Duran Duran - The Reflex D\\Duran Duran - Union of The Snake D\\Duran Duran - Wild Boys D\\Dusty Springfield - Summer is Over D\\Dusty Springfield - Wishin\u0026#39; And Hopin\u0026#39; E E\\Eagle Eye Cherry - Save Tonight E\\Eagles - Hotel California E\\Eagles - New kid in town E\\Eagles - One Of These Nights E\\Eagles - Witchy Woman E\\Eartha Kitt - Where Is My Man (dance mix) E\\Eartha Kitt - Where Is My Man (original 12\u0026#39;\u0026#39;) E\\East 17 - It\u0026#39;s Alright E\\Easybeats - Friday On My Mind E\\Echo \u0026amp; The Bunnymen - The Killing Time E\\Eddie Cochran - Summertime Blues E\\Eddie Grant - Electric Avenue E\\Eddie Grant - I Dont Wanna Dance E\\Eddie Money - Baby Hold On E\\Eddie Money - Take Me Home Tonight E\\Eddie Money - Two Tickets To Paradise E\\Edgar Broughton Band - Evening Over Rooftops E\\Edgar Winter Group - Frankenstein E\\Edgar Winter Group - Free Ride E\\Edie Brickell \u0026amp; New Bohemians - What I Am E\\Edwyn Collins - A Girl Like You E\\Edwyn Collins - The Magic Piper (Of Love) E\\Elastica - Connection E\\Elastica - Stutter E\\Electric Light Orchestra - Don\u0026#39;t Bring Me Down E\\Electric Light Orchestra - Evil Woman E\\Electric Light Orchestra - Last Train To London E\\Electric Light Orchestra - Midnight Blue E\\Electric Light Orchestra - Roll Over Beethoven E\\Electric Light Orchestra - Telephone Line E\\Electric Prunes - I Had Too Much to Dream (Last Night) E\\Ella Fitzgerald - Putting on the Ritz E\\Elton John - Don\u0026#39;t Let the Sun Go Down on Me E\\Elton John - Rocket Man (112 kbps) E\\Elvis Presely - Rock A Hula Baby E\\EMF - Unbelievable E\\Emerson Lake And Palmer - From The Beginning E\\Encore - Le Disc Jockey (check artist, title) E\\Enigma - MCMXC A.D. - 01 - The Voice Of Enigma 2 E\\Enigma - MCMXC A.D. - 02 - Principles Of Lust - I - Sadeness E\\Enigma - MCMXC A.D. - 03 - Principles Of Lust - II - Find Love E\\Enigma - MCMXC A.D. - 04 - Principles Of Lust - III - Sadeness (II) E\\Enigma - MCMXC A.D. - 05 - Callas Went Away E\\Enigma - MCMXC A.D. - 06 - Mea Culpa E\\Enigma - MCMXC A.D. - 07 - The Voice \u0026amp; the Snake E\\Enigma - MCMXC A.D. - 08 - Knocking On Forbidden Doors E\\Enigma - MCMXC A.D. - 09 - Back to the Rivers of Belief E\\Enigma - MCMXC A.D. (limited) - 01 - The Voice Of Enigma E\\Enigma - MCMXC A.D. (limited) - 02 - Principles Of Lust a) Sadeness E\\Enigma - MCMXC A.D. (limited) - 03 - Find Love E\\Enigma - MCMXC A.D. (limited) - 04 - Principles Of Lust c) Sadeness (Reprise) E\\Enigma - MCMXC A.D. (limited) - 05 - Callas Went Away E\\Enigma - MCMXC A.D. (limited) - 06 - Mea Culpa E\\Enigma - MCMXC A.D. (limited) - 07 - The Voice \u0026amp; The Snake E\\Enigma - MCMXC A.D. (limited) - 08 - Knocking On Forbidden Doors E\\Enigma - MCMXC A.D. (limited) - 09 - Back To The Rivers Of Belief a) Way To Eternity E\\Enigma - MCMXC A.D. (limited) - 10 - Back To The Rivers Of Belief b) Hallelujah E\\Enigma - MCMXC A.D. (limited) - 11 - Back To The Rivers Of Belief c) The Rivers Of Belief E\\Enigma - MCMXC A.D. (limited) - 12 - Sadeness (Meditation) E\\Enigma - MCMXC A.D. (limited) - 13 - Mea Culpa (Fading Shades) E\\Enigma - MCMXC A.D. (limited) - 14 - Principles Of Lust (Everlasting Lust) E\\Enigma - MCMXC A.D. (limited) - 15 - The Rivers Of Belief (The Returning Silence) E\\Ennio Morricone - Chi Mai E\\Enya - (1987 - The Celts - __).jpg E\\Enya - (1987 - The Celts - 01) - The Celts (96 kbps) E\\Enya - (1987 - The Celts - 02) - Aldebaran (96 kbps) E\\Enya - (1987 - The Celts - 03) - I Want Tomorrow (96 kbps) E\\Enya - (1987 - The Celts - 04) - March of the Celts (96 kbps) E\\Enya - (1987 - The Celts - 05) - Deireadh An Tuath (96 kbps) E\\Enya - (1987 - The Celts - 06) - The Sun in the Stream (96 kbps) E\\Enya - (1987 - The Celts - 07) - To Go Beyond [1] (96 kbps) E\\Enya - (1987 - The Celts - 08) - Fairytale (96 kbps) E\\Enya - (1987 - The Celts - 09) - Epona (96 kbps) E\\Enya - (1987 - The Celts - 10) - Triad St Patrick cu Chulainn Oisin (96 kbps) E\\Enya - (1987 - The Celts - 11) - Portrait (Out of the Blue) (96 kbps) E\\Enya - (1987 - The Celts - 12) - Boadicea (96 kbps) E\\Enya - (1987 - The Celts - 13) - Bard Dance (96 kbps) E\\Enya - (1987 - The Celts - 14) - Dan y Dwr (96 kbps) E\\Enya - (1987 - The Celts - 15) - To Go Beyond [2] (96 kbps) E\\Enya - (1988 - Watermark - __).jpg E\\Enya - (1988 - Watermark - 01) - Watermark E\\Enya - (1988 - Watermark - 02) - Cursum Perficio E\\Enya - (1988 - Watermark - 03) - On Your Shore E\\Enya - (1988 - Watermark - 04) - Storms in Africa E\\Enya - (1988 - Watermark - 05) - Exile E\\Enya - (1988 - Watermark - 06) - Miss Claire Remembers E\\Enya - (1988 - Watermark - 07) - Orinoco Flow (4\u0026#39;26\u0026#39;\u0026#39;) E\\Enya - (1988 - Watermark - 08) - Evening Falls E\\Enya - (1988 - Watermark - 09) - River E\\Enya - (1988 - Watermark - 10) - The Longships E\\Enya - (1988 - Watermark - 11) - Na Laetha Gael M\u0026#39;Oige E\\Enya - (1988 - Watermark - 12) - Storms in Africa (Part II) E\\Enya - (1991 - Shepherd Moons - __).jpg E\\Enya - (1991 - Shepherd Moons - 01) - Shepherd Moons E\\Enya - (1991 - Shepherd Moons - 02) - Caribbean Blue E\\Enya - (1991 - Shepherd Moons - 03) - How Can I Keep From Singing E\\Enya - (1991 - Shepherd Moons - 04) - Ebudoe E\\Enya - (1991 - Shepherd Moons - 05) - Angeles E\\Enya - (1991 - Shepherd Moons - 06) - No Holly for Miss Quinn E\\Enya - (1991 - Shepherd Moons - 07) - Book Of Days E\\Enya - (1991 - Shepherd Moons - 08) - Evacuee E\\Enya - (1991 - Shepherd Moons - 09) - Lothlorien E\\Enya - (1991 - Shepherd Moons - 10) - Marble Halls E\\Enya - (1991 - Shepherd Moons - 11) - Afer Ventus E\\Enya - (1991 - Shepherd Moons - 12) - Smaointe E\\Enya - (1995 - The Memory Of Trees - __).jpg E\\Enya - (1995 - The Memory of Trees - 01) - The Memory of Trees E\\Enya - (1995 - The Memory of Trees - 02) - Anywhere Is E\\Enya - (1995 - The Memory of Trees - 03) - Pax Deorum E\\Enya - (1995 - The Memory of Trees - 04) - Athair AR Neamh E\\Enya - (1995 - The Memory of Trees - 05) - From Where I Am E\\Enya - (1995 - The Memory of Trees - 06) - China Roses E\\Enya - (1995 - The Memory of Trees - 07) - Hope Has a Place E\\Enya - (1995 - The Memory of Trees - 08) - Tea-House Moon E\\Enya - (1995 - The Memory of Trees - 09) - Once You Had Gold E\\Enya - (1995 - The Memory of Trees - 10) - La Sonadora E\\Enya - (1995 - The Memory of Trees - 11) - On My Way Home E\\Erasure - Voulez Vous E\\Eric Carmen - All By Myself E\\Eric Clapton - After Midnight E\\Eric Clapton - Classical Gas (The Story Of Us) E\\Eric Clapton - Cocaine (Instrumental) E\\Eric Clapton - Cocaine E\\Eric Clapton - I Can\u0026#39;t Stand it (low volume) E\\Eric Clapton - Instrumental Blues Jams - B Minor Jam E\\Eric Clapton - Instrumental Blues Jams - Blues In A (abrupt end) E\\Eric Clapton - Instrumental Blues Jams - Eric After Hours Blues E\\Eric Clapton - Instrumental Blues Jams E\\Eric Clapton - Wonderful Tonight E\\Eric Johnson - Cliffs of Dover E\\Eric Johnson - Righteous E\\Eric Johnson - Trademark E\\Eric Serra - (Le Grand Bleu - 01) - The Big Blue Ouverture E\\Eric Serra - (Le Grand Bleu - 02) - Deep Blue Dream E\\Eric Serra - (Le Grand Bleu - 03) - Sailing to Death E\\Eric Serra - (Le Grand Bleu - 05) - La Raya E\\Eric Serra - (Le Grand Bleu - 18) - Leaving The World Behind E\\Ernest Ranglin - Surfin\u0026#39; E\\Escape Club - Wild Wild West (4\u0026#39;07\u0026#39;\u0026#39;) E\\Escape Club - Wild Wild West (5\u0026#39;43\u0026#39;\u0026#39;) (112 kbps) E\\Europe - The Final Countdown E\\Eurythmics - Here Comes the Rain Again E\\Eurythmics - Sweet Dreams (3\u0026#39;36\u0026#39;\u0026#39;) E\\Eurythmics - Sweet Dreams (4\u0026#39;52\u0026#39;\u0026#39;) E\\Eurythmics - Who\u0026#39;s That Girl (112 kbps) E\\Everclear - Breakfast at Tiffanys E\\Everlast - Ends E\\Everything But The Girl - Miss You (4\u0026#39;20\u0026#39;\u0026#39;) E\\Everything But The Girl - Miss You (4\u0026#39;55\u0026#39;\u0026#39;) E\\Exciters - Do-Wah-Diddy-Diddy E\\Extreme - More Than Words (4\u0026#39;10\u0026#39;\u0026#39;) (check artist) E\\Extreme - More Than Words (5\u0026#39;37\u0026#39;\u0026#39;) (acoustic) F F\\Faith No More - Be Agressive F\\Faith No More - Epic F\\Faith No More - Falling To Pieces F\\Faith No More - War Pigs (adaptation) F\\Faith No More - We Care A Lot F\\Faith No More - Woodpecker From Mars F\\Falco - Der Kommisar F\\Falco - Jeanny (stoopid mix for the USA) F\\Falco - Jeanny F\\Falco - Rock Me Amadeus F\\Fast Ball - The Way F\\Fat Boy Slim - Rockafeller Skank (3\u0026#39;59\u0026#39;\u0026#39;) (from the single) F\\Fat Boy Slim - Rockafeller Skank (6\u0026#39;50\u0026#39;\u0026#39;) F\\Fates Warning - In Trance (adaptation of Scorpions) F\\Fats Domino - Ain\u0026#39;t it a Shame F\\Ferris Buehler\u0026#39;s Day Off - Oh Yeah! F\\Fever Tree - San Francisco Girls (3\u0026#39;34\u0026#39;\u0026#39;) (scratch at 0\u0026#39;04\u0026#39;\u0026#39;) F\\Fever Tree - San Francisco Girls (4\u0026#39;03\u0026#39;\u0026#39;) (Return Of The Native) F\\Fifth Dimension - Aquarius - Let The Sunshine In F\\Fine Young Cannibals - Good Thing F\\Fine Young Cannibals - She Drives Me Crazy F\\Fireballs - Bottle Of Wine F\\Firehouse - Don\u0026#39;t Treat Me Bad F\\Five Man Electrical Band - Signs F\\Fixx - One Thing Leads To Another F\\Fixx - Red Skies At Night F\\Fleetwood Mac - Big Love F\\Fleetwood Mac - Don\u0026#39;t Stop F\\Fleetwood Mac - Dreams (live) F\\Fleetwood Mac - Dreams F\\Fleetwood Mac - Go Your Own Way F\\Fleetwood Mac - Hold Me F\\Fleetwood Mac - Little Lies F\\Fleetwood Mac - Over My Head F\\Fleetwood Mac - Rhiannon F\\Fleetwood Mac - Second Hand News F\\Fleetwood Mac - The Chain F\\Fleetwood Mac - You Can Go Your Own Way F\\Fleetwood Mac - You Make Loving Fun F\\Flock Of Seagulls - I Ran F\\Flock Of Seagulls - Wishing (4\u0026#39;08\u0026#39;\u0026#39;) F\\Flock Of Seagulls - Wishing (5\u0026#39;30\u0026#39;\u0026#39;) F\\Focus - Hocus Pocus (3\u0026#39;20\u0026#39;\u0026#39;) F\\Focus - Hocus Pocus (6\u0026#39;41\u0026#39;\u0026#39;) (192 kbps) F\\Foo Fighters - Big Me F\\Foo Fighters - Hey Johnny Park F\\Foo Fighters - This is a Call F\\Fool\u0026#39;s Garden - Lemon Tree F\\Foreigner - Cold As Ice F\\Foreigner - Dirty White Boy F\\Foreigner - Double Vision F\\Foreigner - Feels Like the First Time F\\Foreigner - Hot Blooded F\\Foreigner - I Want To Know What Love Is F\\Foreigner - Juke Box Hero F\\Foreigner - Urgent F\\Foreigner - Waiting For A Girl Like You (4\u0026#39;35\u0026#39;\u0026#39;) F\\Foreigner - Waiting For a Girl Like You (4\u0026#39;48\u0026#39;\u0026#39;) F\\Foundations - Build Me Up Buttercup F\\Four Non Blondes - What\u0026#39;s Up F\\Four Seasons - Can\u0026#39;t Take My Eyes Off You F\\Four Tops - Reach Out I\u0026#39;ll Be There F\\Francis Lai Orchestra - Theme from Love Story F\\Frank Sinatra - The Best Is Yet To Come F\\Frank Zappa - Bobby Brown F\\Frankie Goes To Hollywood - Relax F\\Frankie Valli - Grease (Grease Soundtrack - 01) F\\Fred Astaire - Puttin\u0026#39; On The Ritz F\\Freddie King - Get Out of My Life, Woman F\\Freddie King - Reconsider Baby F\\Fredie Mercury - Living On My Own F\\Free - All Right Now F\\Fresh Prince - Boom! Shake The Room!! F\\Frida - I Know There\u0026#39;s Something Going On G G\\Garbage - (Version 2.0 - __).jpg G\\Garbage - (Version 2.0 - 01) - Temptation Waits G\\Garbage - (Version 2.0 - 02) - I Think I\u0026#39;m Paranoid G\\Garbage - (Version 2.0 - 03) - When I Grow Up G\\Garbage - (Version 2.0 - 04) - Medication G\\Garbage - (Version 2.0 - 05) - Special G\\Garbage - (Version 2.0 - 06) - Hammering In My Head G\\Garbage - (Version 2.0 - 07) - Push It G\\Garbage - (Version 2.0 - 08) - The Trick Is To Keep Breathing G\\Garbage - (Version 2.0 - 09) - Dumb G\\Garbage - (Version 2.0 - 10) - Sleep Together G\\Garbage - (Version 2.0 - 11) - Wicked Ways G\\Garbage - (Version 2.0 - 12) - You Look So Fine G\\Garbage - #1 Crush (from Romeo + Juliet soundtrack) G\\Garbage - #1 Crush (studio version) G\\Garbage - A Stroke of Luck G\\Garbage - Dog New Tricks G\\Garbage - Milk (Massive Attack Trance Mix) G\\Garbage - Milk G\\Garbage - Only Happy When it Rains G\\Garbage - Queer G\\Garbage - Stupid Girl (dance mix) G\\Garbage - Stupid Girl G\\Garbage - Vow G\\Gary Hoey - Wipeout G\\Gary Lewis \u0026amp; the Playboys - Sealed With a Kiss (adaptation) G\\Gary Moore - Midnight Blues G\\Gary Moore - Parisienne Walkways (live) G\\Gary Moore - Still got the blues G\\Gary Wright - Dream Weaver G\\Gavin Friday - You, Me And World War III G\\Gazebo - I Like Chopin (dance mix) G\\Gazebo - I Like Chopin (vbr - high) G\\Gazebo - Lunatic G\\Gene Vincent - Be-Bop A Lula G\\Genghis Khan - Dshinghis Khan G\\Genghis Khan - Moscow G\\George Carlin - 7 Deadly Words G\\George Formby - When I\u0026#39;m Cleaning Windows G\\George Harrison - I\u0026#39;ve Got My Mind Set on You G\\George Harrison - My Sweet Lord G\\George Harrison \u0026amp; Eric Clapton - While my Guitar Gently Weeps (live) G\\George Michael - Careless Whisper (4\u0026#39;59\u0026#39;\u0026#39;) G\\George Michael - I Want Your Sex G\\George Thoroughgood - Bad To The Bone G\\George Thoroughgood - Get a Haircut G\\George Thoroughgood - I Drink Alone G\\Georgia Satellites - Hippy Hippy Shake (adaptation) G\\Georgia Satellites - Keep Your Hands To Yourself G\\Gerry Rafferty - Baker Street (4\u0026#39;07\u0026#39;\u0026#39;) G\\Gerry Rafferty - Baker Street (5\u0026#39;59\u0026#39;\u0026#39;) G\\Gerry Rafferty - Baker Street (6\u0026#39;05\u0026#39;\u0026#39;) (160 kbps) G\\Gerry Rafferty - Baker Street (6\u0026#39;28\u0026#39;\u0026#39;) G\\Ghost In The Machine - A Time Long Forgotten G\\Gin Blossoms - Found Out About You G\\Giorgio Moroder - The Chase (Midnight Express Theme) G\\Giorgio Moroder - Theme From Midnight Express G\\Gloria Gaynor - I Will Survive (3\u0026#39;14\u0026#39;\u0026#39;) G\\Gloria Gaynor - I Will Survive (3\u0026#39;43\u0026#39;\u0026#39;) G\\Golden Earring - Radar Love G\\Golden Earring - Twilight Zone G\\Goo Goo Dolls - All Eyes On Me G\\Goo Goo Dolls - Dizzy G\\Goo Goo Dolls - Iris G\\Goran Bregovic - Lullaby G\\Grand Funk Railroad - It\u0026#39;s a Man\u0026#39;s World G\\Grand Funk Railroad - Mississippi Queen G\\Grand Funk Railroad - The Loco-motion G\\Grand Funk Railroad - We\u0026#39;re An American Band G\\Grass Roots - Let\u0026#39;s Live for Today G\\Grateful Dead - Touch Of Grey G\\Grateful Dead - Truckin\u0026#39; (96 kbps) G\\Grease Soundtrack - __.jpg G\\Grease Soundtrack - Grease G\\Great White - Once Bitten Twice Shy G\\Great White - Rock Me G\\Green Day - Brain Stew (Godzilla Remix) (From Godzilla - The Album - 10) G\\Green Day - When I Come Around (112 kbps) G\\Greg Kihn Band - Jeopardy G\\Grid - Swamp Thing G\\Guess Who - American Woman G\\Guns \u0026#39;N Roses - (Appetite For Destruction - __).jpg G\\Guns \u0026#39;N Roses - (Appetite For Destruction - 01) - Welcome To The Jungle G\\Guns \u0026#39;N Roses - (Appetite For Destruction - 02) - It\u0026#39;s So Easy G\\Guns \u0026#39;N Roses - (Appetite For Destruction - 03) - Nightrain G\\Guns \u0026#39;N Roses - (Appetite For Destruction - 04) - Out Ta Get Me G\\Guns \u0026#39;N Roses - (Appetite For Destruction - 05) - Mr Brownstone G\\Guns \u0026#39;N Roses - (Appetite For Destruction - 06) - Paradise City G\\Guns \u0026#39;N Roses - (Appetite For Destruction - 07) - My Michelle G\\Guns \u0026#39;N Roses - (Appetite For Destruction - 08) - Think About You G\\Guns \u0026#39;N Roses - (Appetite For Destruction - 09) - Sweet Child O\u0026#39; Mine G\\Guns \u0026#39;N Roses - (Appetite For Destruction - 10) - You\u0026#39;re Crazy G\\Guns \u0026#39;N Roses - (Appetite For Destruction - 11) - Anything Goes G\\Guns \u0026#39;N Roses - (Appetite For Destruction - 12) - Rocket Queen G\\Guns \u0026#39;N Roses - (Use Your Illusion II - __).jpg G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 01) - Civil War G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 02) - 14 Years G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 03) - Yesterdays G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 04) - Knockin\u0026#39; On Heaven\u0026#39;s Door G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 05) - Get In The Ring G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 06) - Shotgun Blues G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 07) - Breakdown G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 08) - Pretty Tied Up G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 09) - Locomotive G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 10) - So Fine G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 11) - Estranged G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 12) - You Could Be Mine G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 13) - Don\u0026#39;t Cry [Alternate Lyrics] G\\Guns \u0026#39;N Roses - (Use Your Illusion II - 14) - My World G\\Guns \u0026#39;N\u0026#39; Roses - Cats In The Cradle G\\Gypsy Kings - Bamboleo H H\\Handel - Messiah, Hallellujah H\\Harajuku - Phantom Of The Opera (6\u0026#39;29\u0026#39;\u0026#39;) H\\Harold Faltameyer - Axel F (Beverly Hills Cop Theme) (check artist) H\\Harry Belafonte - Banana Boat Song (Day-O) (scratch at 2\u0026#39;36\u0026#39;\u0026#39;) (1) H\\Harry Belafonte - Banana Boat Song (Day-O) (scratch at 2\u0026#39;36\u0026#39;\u0026#39;) (192 kbps) H\\Harry Belafonte - Banana Boat Song (Day-O) (scratch at 2\u0026#39;36\u0026#39;\u0026#39;) (2) H\\Harry Belafonte - Banana Boat Song (Day-O) (scratch at 2\u0026#39;36\u0026#39;\u0026#39;) (3) H\\Harry Belafonte - Banana Boat Song (Day-O) (scratch at 2\u0026#39;36\u0026#39;\u0026#39;) H\\Harry Belafonte - Banana Boat Song (Day-O) (skip at 1\u0026#39;11\u0026#39;\u0026#39;, scratch at 1\u0026#39;57\u0026#39;\u0026#39;, 2\u0026#39;36\u0026#39;\u0026#39;) H\\Hawkwind - Hassan I Sahba H\\Head East - Never Been Any Reason H\\Heart - Alone H\\Heart - Crazy On You H\\Heart - Magic Man H\\Heart - These Dreams H\\Henry Mancini - Bring in the Clowns (Love Theme from Romeo \u0026amp; Juliet) H\\Henry Mancini - Love Story Theme H\\Henry Mancini - Pink Panther Theme (192 kbps) H\\Henry Mancini - Pink Panther Theme H\\Henry Mancini - The Entertainer H\\Henry Mancini - The Godfather Theme H\\Herb Alpert - Never On Sunday H\\Herb Alpert \u0026amp; The Tijuana Brass - Tequilla H\\Herbie Hancock - Axel F (Beverly Hills Cop Theme) (check artist) H\\Herbie Hancock - Cantalope Island H\\Herbie Hancock - Rockit H\\Herman\u0026#39;s Hermits - No milk Today H\\HIM - Join Me In Death H\\Hole - Doll Parts H\\Hole - Violet H\\Hollies - Long Cool Woman H\\Hollies - The Air That I Breathe (adaptation) H\\Hooters - Johnny B H\\House Of Pain - Jump Around H\\House Of Pain - Shamrocks \u0026amp; Shenanigans (Boom Shalock Lock Boom) (Butch Vig mix) H\\Huey Lewis \u0026amp; the News - I Want A New Drug H\\Huey Lewis \u0026amp; the News - The Heart Of Rock N Roll H\\Huey Lewis \u0026amp; the News - The Power Of Love H\\Hugo Montenegro - The Good The Bad \u0026amp; The Ugly H\\Human League - Don\u0026#39;t You Want Me H\\Human League - Fascination I I\\Ian Dury \u0026amp; The Blockheads - Sex \u0026amp; Drugs \u0026amp; Rock \u0026amp; Roll I\\Iggy Pop - Cry For Love I\\Iggy Pop - I Wanna Be Your Dog I\\Iggy Pop - I\u0026#39;m Bored I\\Iggy Pop - In the Death Car I\\Iggy Pop - The Passenger (scratch at 3\u0026#39;48\u0026#39;\u0026#39;) (1) I\\Iggy Pop - The Passenger (scratch at 3\u0026#39;48\u0026#39;\u0026#39;) (2) I\\Iggy Pop - The Passenger (scratch at 3\u0026#39;48\u0026#39;\u0026#39;) (3) I\\Iggy Pop - The Passenger (scratch at 3\u0026#39;48\u0026#39;\u0026#39;) (4) I\\Iggy Pop - The Passenger (scratch at 3\u0026#39;48\u0026#39;\u0026#39;) (5) I\\Iggy Pop - The Passenger (scratch at 3\u0026#39;48\u0026#39;\u0026#39;) (6) I\\Imagination - Just An Illusion (2\u0026#39;20\u0026#39;\u0026#39;) I\\Imagination - Just An Illusion (3\u0026#39;56\u0026#39;\u0026#39;) I\\INXS - By My Side I\\INXS - Devil Inside I\\INXS - Disappear I\\INXS - Listen Like Thieves I\\INXS - Mystify I\\INXS - Need You Tonight I\\INXS - Never Tear Us Apart I\\INXS - New Sensation I\\INXS - Original Sin I\\INXS - Suicide Blonde I\\Indeep - Last Night A DJ Saved My Life I\\India - Joi (vbr-high) I\\Information Society - Pure Energy (I Wanna Know) I\\Information Society - Walking Away I\\Irene Cara - Fame I\\Irene Cara - Flashdance (What A Feeling) (vbr-high) I\\Iron Butterfly - In A Gadda Da Vida (112 kbps) I\\Iron Maiden - (1980 - Iron Maiden - __).jpg I\\Iron Maiden - (1980 - Iron Maiden - 01) - The Prowler I\\Iron Maiden - (1980 - Iron Maiden - 02) - Remember Tomorrow I\\Iron Maiden - (1980 - Iron Maiden - 03) - Running Free I\\Iron Maiden - (1980 - Iron Maiden - 04) - The Phantom Of The Opera I\\Iron Maiden - (1980 - Iron Maiden - 05) - Transylvania I\\Iron Maiden - (1980 - Iron Maiden - 06) - Strange World I\\Iron Maiden - (1980 - Iron Maiden - 07) - Sanctuary I\\Iron Maiden - (1980 - Iron Maiden - 08) - Charlotte The Harlot I\\Iron Maiden - (1980 - Iron Maiden - 09) - Iron Maiden I\\Iron Maiden - (1981 - Killers - 07) - Killers I\\Iron Maiden - (1982 - The Number Of The Beast - __).jpg I\\Iron Maiden - (1982 - The Number Of The Beast - 02) - Children Of The Damned I\\Iron Maiden - (1982 - The Number Of The Beast - 04) - 22 Acacia Avenue I\\Iron Maiden - (1982 - The Number Of The Beast - 05) - The Number Of The Beast I\\Iron Maiden - (1982 - The Number Of The Beast - 08) - Hallowed Be Thy Name I\\Iron Maiden - (1983 - Piece Of Mind - __).jpg I\\Iron Maiden - (1983 - Piece Of Mind - 03) - Flight of Icarus I\\Iron Maiden - (1983 - Piece Of Mind - 04) - Die With Your Boots On (clear sound) I\\Iron Maiden - (1983 - Piece Of Mind - 04) - Die With Your Boots On (full sound) I\\Iron Maiden - (1983 - Piece Of Mind - 05) - The Trooper I\\Iron Maiden - Doctor Doctor (adaptation of UFO) I\\Isley Brothers - Let\u0026#39;s Twist Again I\\Isley Brothers - Shout I\\Isley Brothers - Twist and Shout (live) J J\\J. Geils Band - Centerfold J\\J. Geils Band - Monkey Island J\\J. J. Cale - After Midnight (112 kbps) J\\J. J. Cale - Cajun Moon (112 kbps) J\\J. J. Cale - Call Me The Breeze (112 kbps) J\\J. J. Cale - Carry On (112 kbps) J\\J. J. Cale - Cocaine J\\J. J. Cale - Devil In Disguise (112 kbps) J\\J. J. Cale - Don\u0026#39;t Cry Sister (112 kbps) J\\J. J. Cale - Hey baby J\\J. J. Cale - Sensitive Kind (112 kbps) J\\J. J. Cale - Traces J\\Jackson Browne - Running on Empty (live) J\\Jah Wobble - Visions Of You (feat. Sinead O\u0026#39;Connor) J\\James Gang - Funk #49 J\\James Gang - Walk Away J\\James Taylor Quartet - Austin\u0026#39;s Theme J\\James Taylor Quartet - Indian Summer J\\Jan Hammer - Crockett\u0026#39;s Theme J\\Jane Child - Don\u0026#39;t Wanna Fall In Love J\\Janes Addiction - Jane Says J\\Janis Joplin - Cry Baby J\\Janis Joplin - Down On Me J\\Janis Joplin - Me and Bobby McGee J\\Janis Joplin - Mercedes Benz (live) J\\Janis Joplin - Piece of My Heart J\\Janis Joplin - Summertime J\\Janis Joplin - Try Just a Little Bit Harder J\\Jason Donovan - Sealed With a Kiss (adaptation) J\\Jean Francois Maurice - Monaco J\\Jean Michel Jarre - (1978 - Equinoxe) Part 1 (gap at 0\u0026#39;54\u0026#39;\u0026#39;) J\\Jean Michel Jarre - (1978 - Equinoxe) Part 1 J\\Jean Michel Jarre - (1978 - Equinoxe) Part 2 (abrupt end) J\\Jean Michel Jarre - (1978 - Equinoxe) Part 2 (gap at 0\u0026#39;40\u0026#39;\u0026#39;) J\\Jean Michel Jarre - (1978 - Equinoxe) Part 3 J\\Jean Michel Jarre - (1978 - Equinoxe) Part 4 (check version) J\\Jean Michel Jarre - (1978 - Equinoxe) Part 4 (gap at 0\u0026#39;09\u0026#39;\u0026#39;) J\\Jean Michel Jarre - (1978 - Equinoxe) Part 5 J\\Jean Michel Jarre - (1978 - Equinoxe) Part 6 J\\Jean Michel Jarre - (1978 - Equinoxe) Part 7 J\\Jean Michel Jarre - (1978 - Equinoxe) Part 8 J\\Jean Michel Jarre - Chronologie Part 1 (incomplete) J\\Jean Michel Jarre - Chronologie Part 2 (incomplete) J\\Jean Michel Jarre - Chronologie Part 3 J\\Jean Michel Jarre - Chronologie Part 4 J\\Jean Michel Jarre - Chronologie Part 5 J\\Jean Michel Jarre - Chronologie Part 6 J\\Jean Michel Jarre - Chronologie Part 7 J\\Jean Michel Jarre - Chronologie Part 8 J\\Jean Michel Jarre - Oxygene 1-6 - 01 - Part 1 (spike at 2\u0026#39;38\u0026#39;\u0026#39;) J\\Jean Michel Jarre - Oxygene 1-6 - 02 - Part 2 J\\Jean Michel Jarre - Oxygene 1-6 - 03 - Part 3 J\\Jean Michel Jarre - Oxygene 1-6 - 04 - Part 4 J\\Jean Michel Jarre - Oxygene 1-6 - 05 - Part 5 J\\Jean Michel Jarre - Oxygene 1-6 - 06 - Part 6 J\\Jean Michel Jarre - Oxygene 7-13 - 01 - Oxygene 7 J\\Jean Michel Jarre - Oxygene 7-13 - 02 - Oxygene 8 J\\Jean Michel Jarre - Oxygene 7-13 - 03 - Oxygene 9 (cut down) J\\Jean Michel Jarre - Oxygene 7-13 - 04 - Oxygene 10 J\\Jean Michel Jarre - Oxygene 7-13 - 05 - Oxygene 11 J\\Jean Michel Jarre - Oxygene 7-13 - 06 - Oxygene 12 J\\Jean Michel Jarre - Oxygene 7-13 - 07 - Oxygene 13 J\\Jefferson Airplane - It\u0026#39;s No Secret J\\Jefferson Airplane - Somebody To Love J\\Jefferson Airplane - White Rabbit J\\Jefferson Starship - Nothing\u0026#39;s Gonna Stop Us Now J\\Jennifer Paige - Crush (Dance Mix) J\\Jennifer Paige - Crush J\\Jennifer Rush - The Power Of Love J\\Jerry Lee Lewis - Great Balls of Fire J\\Jesus \u0026amp; Mary Chain - Head On J\\Jesus \u0026amp; Mary Chain - Snakedriver J\\Jesus Jones - Right Here Right Now (2\u0026#39;32\u0026#39;\u0026#39;) J\\Jethro Tull - Aqualung J\\Jethro Tull - Bouree J\\Jethro Tull - Locomotive Breath (192 kbps) J\\Jimmy Buffett - Margaritaville J\\Joan Baez - Donna Donna J\\Joan Jett - I Hate Myself For Loving You J\\Joan Jett - I Love Rock \u0026#39;n\u0026#39; Roll (early version with the Sex Pistols) J\\Joan Jett - I Love Rock \u0026#39;n\u0026#39; Roll J\\Joan Osborne - What If God Was One Of Us J\\Joe Cocker - Unchain My Heart J\\Joe Cocker - When a Man Loves a Woman (skip at 1\u0026#39;19\u0026#39;\u0026#39;) J\\Joe Cocker - With A Little Help From My Friends J\\Joe Cocker - You Can Leave Your Hat On J\\Joe Satriani - (Surfing With The Alien - __).jpg J\\Joe Satriani - (Surfing With The Alien - 01) - Surfing With The Alien J\\Joe Satriani - (Surfing With The Alien - 02) - Ice Nine J\\Joe Satriani - (Surfing With The Alien - 03) - Crushing Day J\\Joe Satriani - (Surfing With The Alien - 05) - Satch Boogie J\\Joe Satriani - (Surfing With The Alien - 07) - Circles J\\Joe Satriani - (Surfing With The Alien - 08) - Lords Of Karma J\\Joe Satriani - (Surfing With The Alien - 10) - Echo J\\Joe Satriani - Summer Song J\\Joe Satriani - The Forgotten (Part2) J\\Joe Walsh - Life\u0026#39;s Been Good J\\Joey DeLuxe - Undercover (From Godzilla - The Album - 13) J\\John Cougar Mellancamp - Authority Song J\\John Cougar Mellancamp - Jack And Diane J\\John Cougar Mellancamp - Pink Houses J\\John Cougar Mellencamp - R.O.C.K J\\John Lennon - Imagine J\\John Lennon - Woman J\\John Travolta - Greased Lightning (Grease Soundtrack - 08) J\\John Travolta \u0026amp; Olivia Newton John - Summer Nights (Grease Soundtrack - 02) J\\John Travolta \u0026amp; Olivia Newton John - You\u0026#39;re The One That I Want (Grease Soundtrack - 04) J\\Johnny \u0026amp; Edgar Winter - Rock and Roll Hoochie Koo J\\Johnny Hates Jazz - Shattered Dreams J\\Johnny Kidd and the Pirates - Shakin\u0026#39; All Over J\\Johnny Nash - I Can See Clearly Now J\\Journey - Any Way You Want It J\\Journey - Don\u0026#39;t Stop Believing J\\Journey - Faithfully J\\Journey - Lovin\u0026#39;, Touchin\u0026#39;, Squeezin\u0026#39; J\\Journey - Separate Ways (Worlds Apart) J\\Journey - Wheel In The Sky J\\Journey - Who\u0026#39;s Crying Now J\\Joy Division - Love Will Tear Us Apart J\\Judas Priest - Better By You, Better Than Me J\\Judas Priest - Breaking The Law J\\Judas Priest - Living After Midnight J\\Judas Priest - You Got Another Thing Coming J\\Julee Cruise - Artificial World J\\Julee Cruise - Falling J\\Julee Cruise - I Float Alone J\\Julee Cruise - Into The Night J\\Julee Cruise - Mysteries Of Love J\\Julee Cruise - Rocking Back Inside My Heart J\\Julio Iglesias - Mamy Blue (english, french) J\\Julio Iglesias - The Air That I Breathe K K\\Kajagoogoo - Too Shy K\\Kansas - (The Best Of - 01) - Carry On My Wayward Son K\\Kansas - (The Best Of - 02) - Point of No Return K\\Kansas - (The Best Of - 03) - Fight Fire With Fire K\\Kansas - (The Best Of - 04) - Dust in the Wind K\\Kate Bush - Withering Heights K\\KC \u0026amp; The Sunshine Band - Play That Funky Music White Boy (112 kbps) K\\KC \u0026amp; The Sunshine Band - Please Don\u0026#39;t Go K\\KC \u0026amp; The Sunshine Band - Shake Your Booty K\\Keith Jarrett - (Koln, January 24, 1975 - 01) - Part I K\\Keith Jarrett - (Koln, January 24, 1975 - 02) - Part II a K\\Keith Jarrett - (Koln, January 24, 1975 - 03) - Part II b K\\Keith Jarrett - (Koln, January 24, 1975 - 04) - Part II c K\\Kelly Family - Fell In Love With An Alien K\\Kenny Loggins - Danger Zone K\\Kenny Loggins - Footloose K\\Kim Carnes - Bette Davis Eyes K\\Kim Wilde - You Keep Me Hangin On K\\King Crimson - (1969 - In The Court Of The Crimson King - __).gif K\\King Crimson - (1969 - In The Court Of The Crimson King - 01) - 21st Century Schizoid Man K\\King Crimson - (1969 - In The Court Of The Crimson King - 02) - I Talk To The Wind K\\King Crimson - (1969 - In The Court Of The Crimson King - 03) - Epitaph (224 kbps) K\\King Crimson - (1969 - In The Court Of The Crimson King - 04) - Moonchild K\\King Crimson - (1969 - In The Court Of The Crimson King - 05) - The Court Of The Crimson King K\\Kingsmen - Louie Louie K\\Kinks - A Well Respected Man (192 kbps) K\\Kinks - All Day And All Of The Night K\\Kinks - Come Dancing K\\Kinks - Lola K\\Kinks - Sunny Afternoon K\\Kinks - Turning Japanese K\\Kinks - You Really Got Me (192 kbps) K\\Kiss - I Was Made For Loving You K\\Klaus Nomi - Gold Song (Total eclipse of the sun) (live) K\\Klaus Nomi - Gold Song (Total eclipse of the sun) (studio, better version) K\\Klaus Nomi - You Don\u0026#39;t Own Me K\\Knack - My Sharona K\\Knickerbockers - Lies K\\Kool \u0026amp; The Gang - Fresh (3\u0026#39;34\u0026#39;\u0026#39;) K\\Kool \u0026amp; The Gang - Fresh (4\u0026#39;27\u0026#39;\u0026#39;) K\\Kraftwerk - Das Modell K\\Kraftwerk - The Model K\\Krokus - Ballroom Blitz K\\Krokus - Eat the Rich K\\Krokus - Screaming In The Night K\\Krokus - Stayed Awake All Night K\\Krokus - Streamer K\\Krokus - Winning Man K\\Kuhn, Dieter Thomas - Mamy Blue (german) K\\Kylie Minogue - Locomotion (adaptation) L L\\La Bionda - Deserts Of Mars L\\Laid Back - White Horse (Extended, Uncensored Version) L\\Laura Branigan - Gloria (3\u0026#39;53\u0026#39;\u0026#39;) L\\Laura Branigan - Gloria (4\u0026#39;49\u0026#39;\u0026#39;) L\\Laura Branigan - Self Control L\\Led Zeppelin - All Of My Love L\\Led Zeppelin - Dazed and Confused L\\Led Zeppelin - Kashmir L\\Led Zeppelin - Rock And Roll L\\Led Zeppelin - Stairway To Heaven (live) L\\Led Zeppelin - Stairway To Heaven (starts left) L\\Led Zeppelin - Stairway To Heaven (starts right) L\\Lemonheads - Mrs Robinson L\\Leonard Cohen - Everybody Knows L\\Leonard Cohen - First We Take Manhattan L\\Leonard Cohen - I\u0026#39;m Your Man L\\Leonard Cohen - Suzanne (live) L\\Leonard Cohen - Take This Waltz L\\Leonard Cohen - Tower of Songs L\\Leonard Cohen - Who By Fire L\\Leonard Kohen - Dance Me to The End of Love L\\Leonard Kohen - Democracy L\\Leonard Kohen - Hallelujah L\\Leonard Kohen - The Future L\\Lesley Gore - It\u0026#39;s My Party L\\Lesley Gore - You Don\u0026#39;t Own Me L\\Lightning Seeds - You Showed Me L\\Liona Boyd - Spanish Romance (very low volume) L\\Lionel Richie - Hello L\\Lipps Inc. - Funky Town L\\Little Eva - The Locomotion L\\Little Richard - Tutti Frutti L\\Live - I Alone L\\Live - The Dolphins Cry (Album Version) L\\Living Color - Cult Of Personality L\\Liz Phair - Turning Japanese L\\Lords Of Acid - Mixed Emotions L\\Lords Of Acid - You Wanna Suck My Pussy (Dance Mix) L\\Lou Bega - Mambo Number 5 (extended mix) L\\Lou Bega - Mambo Number 5 L\\Lou Reed - Sweet Jane L\\Lou Reed - This Magic Moment L\\Lou Reed - Vicious L\\Lou Reed - Walk on the Wild Side L\\Love - Alone Again Or (\u0026amp; Arthur Lee) L\\Love And Rockets - So Alive L\\Lovin\u0026#39; Spoonful - Summer In The City L\\Lulu \u0026amp; The Luvvers - Shout L\\Lynyrd Skynyrd - Call Me The Breeze L\\Lynyrd Skynyrd - Free Bird L\\Lynyrd Skynyrd - Gimme Three Steps L\\Lynyrd Skynyrd - Saturday Night Special L\\Lynyrd Skynyrd - Simple Man L\\Lynyrd Skynyrd - Sweet Home Alabama L\\Lynyrd Skynyrd - That Smell L\\Lynyrd Skynyrd - Tuesday\u0026#39;s Gone L\\Lynyrd Skynyrd - What\u0026#39;s Your Name M M\\Madison Avenue - Don\u0026#39;t Call Me Baby M\\Madness - House Of Fun M\\Madness - Our House M\\Madonna - Into The Groove M\\Madonna - La Isla Bonita M\\Mahavishnu Orchestra - Meeting Of The Spirits M\\Mahavishnu Orchestra - Noonward Race M\\Mamas \u0026amp; Papas - California Dreamin\u0026#39; (starts left) (2) M\\Mamas \u0026amp; Papas - California Dreamin\u0026#39; (starts left) M\\Mamas \u0026amp; Papas - California Dreamin\u0026#39; (starts right) M\\Mamas \u0026amp; Papas - I Call Your Name M\\Mamas \u0026amp; Papas - Monday Monday M\\Manfred Mann - Blinded By The Light (3\u0026#39;50\u0026#39;\u0026#39;) M\\Manfred Mann - Blinded By The Light (7\u0026#39;07\u0026#39;\u0026#39;) M\\Manfred Mann - Do Wah Diddy Diddy (adaptation) M\\Manfred Mann - Father Of Day, Father Of Night M\\Manfred Mann - In The Beginning Darkness M\\Manfred Mann - Spirits in the Night M\\Manic Street Preachers - A Design for Life M\\Manic Street Preachers - If You Tolerate This Your Children Will Be Next M\\Manic Street Preachers - So Why Sad M\\Manic Street Preachers - The Everlasting M\\Marcy Playground - Sex \u0026amp; Candy M\\Mariah Carey - Without You M\\Marilyn Manson - Sweet Dreams Are Made Of These (112 kbps) M\\Marilyn Monroe - Happy Birthday Mr President M\\Mark \u0026#39;Oh - Fade To Grey (adaptation) M\\Marketts - Out of Limits M\\Mars - Pump Up The Volume M\\Martika - Toy Soldier M\\Marvin Gaye - I Heard It Through The Grapevine (112 kbps) M\\Mary Hopkin - Those Were The Days M\\Massive Attack - Better Things M\\Massive Attack - Heat Miser M\\Massive Attack - Protection M\\Mathew Wilder - Break My Stride M\\Maurice Williams - Stay (Just a Little Bit Longer) M\\Maxx - Get-a-way M\\Mazzy Star - Fade Into You M\\MC Hammer - U Can\u0026#39;t Touch This M\\MC Raw - Fuck the Macarena M\\McAuley Schenker Group - Anytime M\\McAuley Schenker Group - Never Ending Nightmare (160 kbps) M\\McAuley Schenker Group - Never Ending Nightmare M\\Melanie \u0026amp; The Edwin Hawkins Singers - Lay Down (Candles in the Rain) M\\Melanie C - I Turn To You M\\Melanie C. - Ga Ga M\\Men At Work - Down Under M\\Men At Work - Who Can It Be Now M\\Men Without Hats - Pop Goes The World (3\u0026#39;35\u0026#39;\u0026#39;) M\\Men Without Hats - Pop Goes The World (3\u0026#39;45\u0026#39;\u0026#39;) M\\Men Without Hats - Safety Dance M\\Meredith Brooks - Bitch M\\Metallica - Enter Sandman M\\Metallica - Nothing Else Matters M\\Metallica - One (192 kbps) M\\Metallica - The Unforgiven II M\\Metallica - The Unforgiven M\\Miami Sound Machine - Conga M\\Michael Bolton - Sitting on the Dock of the Bay M\\Michael Bolton - When a Man Loves a Woman M\\Michael Jackson - Beat It M\\Michael Jackson - Billie Jean M\\Michael Nyman - (The Piano) - The Promise M\\Michael Nyman - The Heart Asks Pleasure First M\\Michael Sembello - Maniac M\\Midnight Oil - Beds Are Burning M\\Mike and The Mechanics - All I Need Is A Miracle (3\u0026#39;43\u0026#39;\u0026#39;) M\\Mike and The Mechanics - All I Need Is A Miracle (4\u0026#39;09\u0026#39;\u0026#39;) M\\Mike and The Mechanics - Can You Hear Me M\\Mike Bloomfield, Al Kooper, Steve Stills - Albert\u0026#39;s Shuffle M\\Mike Oldfield - (Elements - The Best Of - 13) - Ommadawn (Excerpt) M\\Mike Oldfield - (The Instrumental Section - 08) - Blue Peter M\\Mike Oldfield - (Tubular Bells III - 01) - The Source of Secrets M\\Mike Oldfield - (Tubular Bells III - 10) - Secrets M\\Mike Oldfield - (Tubular Bells III - 11) - Far Above The Clouds M\\Mike Oldfield - Moonlight Shadow (extended) M\\Mike Oldfield - Moonlight Shadow M\\Mike Oldfield - Shadow On The Wall M\\Miles Davis - Donna M\\Miles Davis - My Funny Valentine M\\Miles Davis - Summertime M\\Ming Tea - BBC M\\Minimalistix - Struggle for Pleasure (Radio Edit) M\\Mission UK - Butterfly On A Wheel M\\Mitch Miller \u0026amp; His Orchestra - Bridge Over The River Kwai Movie Theme M\\Moby - (Play - __).jpg M\\Moby - (Play - 01) - Honey M\\Moby - (Play - 02) - Find My Baby M\\Moby - (Play - 03) - Porcelain M\\Moby - (Play - 04) - Why Does My Heart Feel So Bad M\\Moby - (Play - 05) - Southside M\\Moby - (Play - 06) - Rushing M\\Moby - (Play - 07) - Bodyrock M\\Moby - (Play - 08) - Natural Blues M\\Moby - (Play - 09) - Machete NOT M\\Moby - (Play - 09) - Machete M\\Moby - (Play - 10) - 7 M\\Moby - (Play - 11) - Run On M\\Moby - (Play - 12) - Down Slow M\\Moby - (Play - 13) - If Things Were Perfect M\\Moby - (Play - 14) - Everloving M\\Moby - (Play - 15) - Inside M\\Moby - (Play - 16) - Guitar Flute \u0026amp; String M\\Moby - (Play - 17) - The Sky Is Broken M\\Moby - (Play - 18) - My Weakness M\\Moby - A Season in Hell (112 kbps) M\\Moby - Dead City (112 kbps) M\\Moby - Dead Sun (112 kbps) M\\Moby - Degenerate (112 kbps) M\\Moby - Hymn (dance mix) M\\Moby - Hymn M\\Moby - Living M\\Moby - New Dawn Fades M\\Moby - Old M\\Moby - Reject (112 kbps) M\\Moby - That\u0026#39;s When I Reach For My Revolver M\\Moby - Walnut M\\Modern Talking - You\u0026#39;re My Heart, You\u0026#39;re My Soul M\\Moist - Push M\\Mojo - Lady M\\Molly Hatchet - Flirtin With Disaster M\\Monkees - I\u0026#39;m A Believer M\\Monkees - Valleri M\\Monty Python - The Spam Song M\\Moody Blues - I\u0026#39;m Just A Singer In A Rock And Roll Band M\\Moody Blues - Nights in White Satin M\\Moody Blues - Question M\\Moody Blues - Ride My See-Saw M\\Moody Blues - The Story In Your Eyes M\\Moody Blues - The Voice M\\Moody Blues - Tuesday Afternoon (Forever Afternoon) M\\Motley Crue - Dr. Feelgood M\\Motley Crue - Girls Girls Girls M\\Motorhead - Ace Of Spades M\\Motorhead - Killed By Death M\\Mott The Hoople - All The Young Dudes M\\Mozart - Eine Kleine Nachtmusik M\\Mozart - Turkish March M\\Mr Big - Wild World M\\Mr Mister - Broken Wings M\\Mr. President - Coco Jamboo M\\Muddy Waters - 40 Days And 40 Nights M\\Muddy Waters - Baby Please Don\u0026#39;t Go M\\Muddy Waters - Close To You M\\Muddy Waters - Goin\u0026#39; Home M\\Muddy Waters - I Can\u0026#39;t Be Satisfied M\\Muddy Waters - Mannish Boy M\\Muddy Waters - Mean Mistreater M\\Muddy Waters - One More Mile M\\Muddy Waters - Rock Me M\\Muddy Waters - Rollin\u0026#39; And Thumblin\u0026#39; M\\Muddy Waters - Rollin\u0026#39; Stone M\\Muddy Waters - She Moves Me M\\Muddy Waters - Walkin\u0026#39; Thru The Park M\\Muddy Waters - You Can\u0026#39;t Loose What You Ain\u0026#39;t Never Had M\\Mungo Jerry - In The Summertime (192 kbps) M\\Muppet Show - Manamanah M\\Muppet Show - Tonight\u0026#39;s Opening Theme N N\\N\u0026#39;Sync - It\u0026#39;s Gonna Be Me (192 kbps) N\\Nails - 88 Lines About 44 Women N\\Naked Eyes - Something There To Remind Me N\\Nancy Sinatra - These Boots Are Made For Walking N\\Nancy Sinatra \u0026amp; Lee Hazelwood - Summer Wine N\\Natalie Imbruglia - Torn (112 kbps) (minor flaw at 1\u0026#39;10\u0026#39;\u0026#39;) N\\Natalie Imbruglia - Torn (acoustic, unplugged) N\\Natalie Imbruglia - Torn (flaw at 1\u0026#39;10\u0026#39;\u0026#39;) (2) N\\Natalie Imbruglia - Torn (flaw at 1\u0026#39;10\u0026#39;\u0026#39;) (3) N\\Natalie Imbruglia - Torn (flaw at 1\u0026#39;10\u0026#39;\u0026#39;) (4) N\\Natalie Merchant - Carnival N\\Neil Diamond - Girl, You\u0026#39;ll Be a Woman Soon N\\Neil Young - Cinnamon Girl N\\Neil Young - Harvest N\\Neil Young - Heart Of Gold N\\Neil Young - My My, Hey Hey N\\Neil Young - Old Man N\\Neil Young - Rockin\u0026#39; In the Free World (112 kbps) N\\Neil Young - Southern Men N\\Nena - 99 Luft Ballons N\\Nena - 99 Red Balloons N\\New Order - Blue Monday N\\New Order - Confusion N\\New Order - True Faith \u0026#39;94 (4\u0026#39;27\u0026#39;\u0026#39;) N\\New Order - True Faith (5\u0026#39;51\u0026#39;\u0026#39;) N\\Newbeats - Bread and Butter N\\Nick Cave \u0026amp; Current 93 - All the Pretty Little Horsies (mix start) N\\Nick Cave \u0026amp; The Bad Seeds - Do You Love Me N\\Nick Cave \u0026amp; The Bad Seeds - I Let Love In N\\Nick Cave \u0026amp; The Dirty Three - Time Jesum Transuentum Et Non Revertentum N\\Nico Fidenco - A Casa d\u0026#39; Irene N\\Nicoletta - Mamy Blue (french) N\\Nina Simon (Screamin Jay Hawkins) - I Put A Spell On You N\\Nine Inch Nails - Closer N\\Nineteen Ten Fruitgum Company - Simon Says N\\Nirvana - All Apologies (live, acoustic) N\\Nirvana - Come as You Are N\\Nirvana - Drain You N\\Nirvana - Heart Shaped Box N\\Nirvana - In Bloom N\\Nirvana - Lithium N\\Nirvana - Smells Like Teen Spirit N\\No Doubt - Don\u0026#39;t Speak N\\No Doubt - I\u0026#39;m Just A Girl N\\Nuno Bettencourt - Crave O O\\Oasis - Champagne Supernova O\\Oasis - Don\u0026#39;t Look Back In Anger O\\Oasis - Wonder Wall O\\Offspring - Come Out And Play O\\Offspring - Pretty Fly (For a White Guy) O\\Offspring - Self Esteem O\\Olivia Newton John - Hopelessly Devoted To You (Grease Soundtrack - 03) O\\Olivia Newton John - Physical O\\Olivia Newton John - Xanadu O\\OMD - Electricity O\\OMD - Enola Gay O\\Opus - Live is Life (live) O\\Otis Day \u0026amp; The Knights - Shout (adaptation) O\\Otis Redding - Ain\u0026#39;t No Sunshine When She\u0026#39;s Gone O\\Otis Redding - Sitting On The Dock Of The Bay O\\Otis Redding - Too Hard to Handle (192 kbps) O\\Otis Redding - When a Man Loves A Woman O\\Ozzy Osbourne - Bark At The Moon O\\Ozzy Osbourne - Centre of Eternity O\\Ozzy Osbourne - Crazy Train O\\Ozzy Osbourne - Mr. Crowley O\\Ozzy Osbourne - No More Tears O\\Ozzy Osbourne - Over The Mountain O\\Ozzy Osbourne - Rock \u0026#39;N\u0026#39; Roll Rebel O\\Ozzy Osbourne - Shock The Monkey (adaptation) O\\Ozzy Osbourne - Stayin\u0026#39; Alive (adaptation) (bad quality) O\\Ozzy Osbourne \u0026amp; Lita Ford - If I Close My Eyes Forever O\\Ozzy Osbourne \u0026amp; Miss Piggy - Born to be Wild P P\\Pachelbel - Canon in D P\\Pat Benatar - Heartbreaker P\\Pat Benatar - I Need A Lover P\\Pat Boone - Speedy Gonzales P\\Patti LaBelle - Lady Marmalade P\\Patty Pravo - La Bambola P\\Patty Smith - Because the Night P\\Paul Anka - Diana P\\Paul Anka - Put Your Head On My Shoulder P\\Paul Anka - You Are My Destiny P\\Paul Hernandez - Born To Be Alive P\\Paul Johnson - Down Down Down P\\Paul Mauriat - Mamy blue (no other words) P\\Paul Revere \u0026amp; Mark Lindsay - Indian Reservation P\\Paul Weller - You Do Something To Me P\\Paula Abdul - Straight Up P\\Paula Cole - Where Have All the Cowboys Gone P\\Pavlov\u0026#39;s Dog - Julia P\\Pavlov\u0026#39;s Dog - Late November P\\Pavlov\u0026#39;s Dog - Song Dance P\\Pearl Jam - Betterman P\\Pearl Jam - Evenflow P\\Pearl Jam - Release P\\Percy Sledge - When A Man Loves A Woman P\\Peter Frampton - Baby, I Love Your Way (live) P\\Peter Frampton - Show Me The Way (live) P\\Peter Gabriel - Shock the Monkey P\\Peter Gabriel - Sledgehammer P\\Peter Green - Slabo Day P\\Peter Tosh - Legalize It P\\Phil Collins - Another Day In Paradise P\\Phil Collins - Take A Look At Me Now P\\Phil Collins - Take Me Home P\\Phil Oakey \u0026amp; Giorgio Moroder - Together In Electric Dreams P\\Pierre Bachelet \u0026amp; Herve Roy - Emanuelle P\\Pink Floyd - (1967 - The Piper At The Gates of Dawn - __).jpg P\\Pink Floyd - (1967 - The Piper At The Gates of Dawn [remastered] - 01) - Astronomy Domine P\\Pink Floyd - (1967 - The Piper At The Gates of Dawn [remastered] - 02) - Lucifer Sam P\\Pink Floyd - (1967 - The Piper At The Gates of Dawn [remastered] - 03) - Matilda Mother (skip at 0\u0026#39;31\u0026#39;\u0026#39;) P\\Pink Floyd - (1967 - The Piper At The Gates of Dawn [remastered] - 04) - Flaming P\\Pink Floyd - (1967 - The Piper At The Gates of Dawn [remastered] - 05) - Pow R. Toc H. (skip at 0\u0026#39;29\u0026#39;\u0026#39;) P\\Pink Floyd - (1967 - The Piper At The Gates of Dawn [remastered] - 06) - Take Up Thy Stethoscope and Walk P\\Pink Floyd - (1967 - The Piper At The Gates of Dawn [remastered] - 07) - Interstellar Overdrive P\\Pink Floyd - (1967 - The Piper At The Gates of Dawn [remastered] - 08) - The Gnome P\\Pink Floyd - (1967 - The Piper At The Gates of Dawn [remastered] - 09) - Chapter 24 P\\Pink Floyd - (1967 - The Piper At The Gates of Dawn [remastered] - 10) - Scarecrow P\\Pink Floyd - (1967 - The Piper At The Gates of Dawn) - ENTIRE RECORD P\\Pink Floyd - (1968 - A Saucerful Of Secrets - __).jpg P\\Pink Floyd - (1968 - A Saucerful of Secrets - 01) - Let There Be More Light P\\Pink Floyd - (1968 - A Saucerful of Secrets - 02) - Remember A Day P\\Pink Floyd - (1968 - A Saucerful of Secrets - 03) - Set The Controls For The Heart Of The Sun P\\Pink Floyd - (1968 - A Saucerful of Secrets - 04) - Corporal Clegg P\\Pink Floyd - (1968 - A Saucerful of Secrets - 05) - A Saucerful of Secrets P\\Pink Floyd - (1968 - A Saucerful of Secrets - 06) - See - Saw P\\Pink Floyd - (1968 - A Saucerful of Secrets - 07) - Jugband Blues P\\Pink Floyd - (1970 - Atom Heart Mother - __).jpg P\\Pink Floyd - (1970 - Atom Heart Mother - 01) - Atom Heart Mother Suite P\\Pink Floyd - (1970 - Atom Heart Mother - 02) - If P\\Pink Floyd - (1970 - Atom Heart Mother - 03) - Summer Sixty Eight P\\Pink Floyd - (1970 - Atom Heart Mother - 04) - Fat Old Sun P\\Pink Floyd - (1970 - Atom Heart Mother - 05) - Alan\u0026#39;s Psychedelic Breakfast P\\Pink Floyd - (1971 - Meddle - __).jpg P\\Pink Floyd - (1971 - Meddle - 06) - Echoes (not perfect) P\\Pink Floyd - (1971 - Meddle [remastered] - 01) - One of These Days P\\Pink Floyd - (1971 - Meddle [remastered] - 06) - Echoes P\\Pink Floyd - (1971 - Meddle) - ENTIRE RECORD P\\Pink Floyd - (1971 - Relics - __).jpg P\\Pink Floyd - (1971 - Relics - 01) - Arnold Layne (1967) P\\Pink Floyd - (1971 - Relics - 02) - Interstellar Overdrive (1967) P\\Pink Floyd - (1971 - Relics - 03) - See Emily Play (1967) P\\Pink Floyd - (1971 - Relics - 04) - Remeber a Day (1967) P\\Pink Floyd - (1971 - Relics - 05) - Paintbox (1967) (blank at 2\u0026#39;55\u0026#39;\u0026#39;) P\\Pink Floyd - (1971 - Relics - 06) - Julia Dream (1968) P\\Pink Floyd - (1971 - Relics - 07) - Carefull With That Axe, Eugene (1968) P\\Pink Floyd - (1971 - Relics - 08) - Cirrus Minor (1969) P\\Pink Floyd - (1971 - Relics - 09) - The Nile Song (1969) P\\Pink Floyd - (1971 - Relics - 10) - Biding My Time (1969) P\\Pink Floyd - (1971 - Relics - 11) - Bike (1967) P\\Pink Floyd - (1972 - Obscured By Clouds - __).jpg P\\Pink Floyd - (1972 - Obscured by Clouds) - ENTIRE RECORD P\\Pink Floyd - (1973 - The Dark Side of The Moon - __).jpg P\\Pink Floyd - (1973 - The Dark Side of the Moon) - ENTIRE RECORD P\\Pink Floyd - (1975 - Wish You Were Here - __).jpg P\\Pink Floyd - (1975 - Wish You Were Here - 01) - Shine On You Crazy Diamond (Part 1) P\\Pink Floyd - (1975 - Wish You Were Here - 02) - Welcome To The Machine P\\Pink Floyd - (1975 - Wish You Were Here - 03) - Have A Cigar P\\Pink Floyd - (1975 - Wish You Were Here - 04) - Wish You Were Here P\\Pink Floyd - (1975 - Wish You Were Here - 05) - Shine On You Crazy Diamond (Part 2) P\\Pink Floyd - (1975 - Wish You Were Here) - ENTIRE RECORD P\\Pink Floyd - (1977 - Animals - __).jpg P\\Pink Floyd - (1977 - Animals) - ENTIRE RECORD P\\Pink Floyd - (1979 - The Movie The Wall) - ENTIRE RECORD P\\Pink Floyd - (1979 - The Wall - Disc 1 - 01) - In The Flesh [1] P\\Pink Floyd - (1979 - The Wall - Disc 1 - 02) - The Thin Ice P\\Pink Floyd - (1979 - The Wall - Disc 1 - 03) - Another Brick In The Wall Part 1 P\\Pink Floyd - (1979 - The Wall - Disc 1 - 04) - The Happiest Days Of Our Lives P\\Pink Floyd - (1979 - The Wall - Disc 1 - 05) - Another Brick In The Wall Part 2 P\\Pink Floyd - (1979 - The Wall - Disc 1 - 06) - Mother P\\Pink Floyd - (1979 - The Wall - Disc 1 - 07) - Goodbye Blue Sky P\\Pink Floyd - (1979 - The Wall - Disc 1 - 08) - Empty Spaces P\\Pink Floyd - (1979 - The Wall - Disc 1 - 09) - Young Lust P\\Pink Floyd - (1979 - The Wall - Disc 1 - 10) - One Of My Turns P\\Pink Floyd - (1979 - The Wall - Disc 1 - 11) - Don\u0026#39;t Leave Me Now (incomplete) P\\Pink Floyd - (1979 - The Wall - Disc 1 - 12) - Another Brick In The Wall Part 3 P\\Pink Floyd - (1979 - The Wall - Disc 1 - 13) - Goodbye Cruel World P\\Pink Floyd - (1979 - The Wall - Disc 1) - ENTIRE RECORD (corrupt at 29\u0026#39;05\u0026#39;\u0026#39;, incomplete) P\\Pink Floyd - (1979 - The Wall - Disc 1) - ENTIRE RECORD (corrupt at 29\u0026#39;17\u0026#39;\u0026#39;) P\\Pink Floyd - (1979 - The Wall - Disc 2 - 01) - Hey You P\\Pink Floyd - (1979 - The Wall - Disc 2 - 02) - Is There Anybody Out There P\\Pink Floyd - (1979 - The Wall - Disc 2 - 03) - Nobody Home P\\Pink Floyd - (1979 - The Wall - Disc 2 - 04) - Vera P\\Pink Floyd - (1979 - The Wall - Disc 2 - 05) - Bring The Boys Back Home P\\Pink Floyd - (1979 - The Wall - Disc 2 - 06) - Comfortably Numb P\\Pink Floyd - (1979 - The Wall - Disc 2 - 07) - The Show Must Go On P\\Pink Floyd - (1979 - The Wall - Disc 2 - 08) - In The Flesh [2] P\\Pink Floyd - (1979 - The Wall - Disc 2 - 09) - Run Like Hell P\\Pink Floyd - (1979 - The Wall - Disc 2 - 10) - Waiting For The Worms P\\Pink Floyd - (1979 - The Wall - Disc 2 - 11) - Stop P\\Pink Floyd - (1979 - The Wall - Disc 2 - 12) - The Trial P\\Pink Floyd - (1979 - The Wall - Disc 2 - 13) - Outside The Wall P\\Pink Floyd - (1979 - The Wall - Disc 2) - ENTIRE RECORD (spike at 13\u0026#39;47\u0026#39;\u0026#39;) P\\Pink Floyd - (1983 - The Final Cut - 01) - The Post War Dream P\\Pink Floyd - (1983 - The Final Cut - 02) - Your Possible Pasts P\\Pink Floyd - (1983 - The Final Cut - 03) - On Of The Few P\\Pink Floyd - (1983 - The Final Cut - 04) - The Hero\u0026#39;s Return P\\Pink Floyd - (1983 - The Final Cut - 05) - The Gunner\u0026#39;s Dream P\\Pink Floyd - (1983 - The Final Cut - 06) - Paranoid Eyes (2) P\\Pink Floyd - (1983 - The Final Cut - 06) - Paranoid Eyes P\\Pink Floyd - (1983 - The Final Cut - 07) - Get Your Filthy Off My Desert P\\Pink Floyd - (1983 - The Final Cut - 08) - The Fletcher Memorial Home P\\Pink Floyd - (1983 - The Final Cut - 09) - Southampton Dock P\\Pink Floyd - (1983 - The Final Cut - 10) - The Final Cut P\\Pink Floyd - (1983 - The Final Cut - 11) - Not Now John P\\Pink Floyd - (1983 - The Final Cut - 12) - Two Suns In The Sunset P\\Pink Floyd - (1983 - The Final Cut) - ENTIRE RECORD P\\Pink Floyd - (1987 - A Momentary Lapse of Reason - [remastered] - 05) - On The Turning Away P\\Pink Floyd - (1987 - A Momentary Lapse of Reason - 02) - Learning to fly P\\Pink Floyd - (1987 - A Momentary Lapse of Reason - 03) - The Dogs of War P\\Pink Floyd - (1987 - A Momentary Lapse of Reason - 04) - One Slip P\\Pink Floyd - (1987 - A Momentary Lapse of Reason - 05) - On The Turning Away P\\Pink Floyd - (1987 - A Momentary Lapse of Reason - 06) - Yet Another Movie, Round And Round P\\Pink Floyd - (1987 - A Momentary Lapse of Reason - 10) - Sorrow (starts with scratch) P\\Pink Floyd - (1987 - A Momentary Lapse of Reason) - ENTIRE RECORD (spike at 2\u0026#39;45\u0026#39;\u0026#39;, 10\u0026#39;42\u0026#39;\u0026#39;) P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Cover).jpg P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 1 - 01) - Shine On You Crazy Diamond P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 1 - 02) - Learning to Fly P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 1 - 03) - Yet Another Movie P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 1 - 04) - Round And Round P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 1 - 05) - Sorrow P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 1 - 06) - The Dogs of War P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 1 - 07) - On The Turning Away P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 2 - 01) - One Of These Days P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 2 - 02) - Time P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 2 - 03) - Wish You Were Here P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 2 - 04) - Us \u0026amp; Them P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 2 - 05) - Money P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 2 - 06) - Another Brick In The Wall (II) P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 2 - 07) - Comfortably Numb P\\Pink Floyd - (1988 - Delicate Sound Of Thunder - Disc 2 - 08) - Run Like Hell P\\Pink Floyd - (1994 - The Division Bell - __).jpg P\\Pink Floyd - (1994 - The Division Bell - 01) - Cluster One P\\Pink Floyd - (1994 - The Division Bell - 02) - What Do You Want From Me P\\Pink Floyd - (1994 - The Division Bell - 03) - Poles Apart P\\Pink Floyd - (1994 - The Division Bell - 04) - Marooned P\\Pink Floyd - (1994 - The Division Bell - 05) - A Great Day For Freedom P\\Pink Floyd - (1994 - The Division Bell - 06) - Wearing the Inside Out P\\Pink Floyd - (1994 - The Division Bell - 07) - Take It Back P\\Pink Floyd - (1994 - The Division Bell - 08) - Coming Back To Life P\\Pink Floyd - (1994 - The Division Bell - 09) - Keep Talking P\\Pink Floyd - (1994 - The Division Bell - 10) - Lost For Words P\\Pink Floyd - (1994 - The Division Bell - 11) - High Hopes P\\Pink Floyd - (1994 - The Division Bell) - ENTIRE RECORD (corrupt) P\\Pink Floyd - Pigs on the Wing (rare recording) P\\Planet Funk - Chase the Sun (Extended Club Mix) P\\Planet Funk - Chase the Sun (Radio Edit) P\\Planet Funk - Chase the Sun P\\Planet P Project - Why Me (112kbps) P\\Planet P Project - Why Me (192kbps) P\\Plasmatics - (Coup d\u0026#39;Etat) - Country Fairs P\\Platters - Only You P\\Pointer Sisters - I\u0026#39;m So Excited P\\Poison - Every Rose Has Its Thorn P\\Poison - Unskinny Bop P\\Poison - Your Mama Don\u0026#39;t Dance P\\Poison The Well - Torn P\\Police - Can\u0026#39;t Stand Losing You P\\Police - De Do Do Do, De Da Da Da P\\Police - Every Breath You Take P\\Police - Every Little Thing She Does Is Magic P\\Police - Message in a Bottle P\\Police - Roxanne P\\Police - So Lonely P\\Pop Tops - Mamy Blue P\\Porno For Pyros - Pets P\\Portishead - Glory Box P\\Portishead - Roads P\\Pretenders - Back On The Chain Gang P\\Pretenders - Brass In Pocket P\\Pretenders - Creep (adaptation) P\\Pretenders - Don\u0026#39;t Get Me Wrong P\\Prince - When Doves Cry P\\Procol Harum - Conquistador (live) P\\Procol Harum - Whiter Shade Of Pale P\\Prodigy - Firestarter (3\u0026#39;47\u0026#39;\u0026#39;) P\\Prodigy - Firestarter (4\u0026#39;39\u0026#39;\u0026#39;) P\\Prodigy - Molotov P\\Prodigy - No Good (Start The Dance) P\\Prodigy - Smack My Bitch Up P\\Puff Daddy - Come With Me (From Godzilla - The Album - 02) P\\Puressence - Every House on Every Street P\\Puressence - Fire P\\Puressence - I Suppose P\\Puressence - India P\\Puressence - Mr Brown Q Q\\Quarterflash - Harden My Heart Q\\Queen - Another One Bites The Dust Q\\Queen - Bicycle Race Q\\Queen - Bohemian Rhapsody Q\\Queen - I Want it All Q\\Queen - I Want To Break Free Q\\Queen - Innuendo Q\\Queen - Under Pressure (\u0026amp; David Bowie) Q\\Queen - We Are The Champions Q\\Queen - We Will Rock You Q\\Queen - Who Wants To Live Forever (4\u0026#39;55\u0026#39;\u0026#39;) Q\\Queen - Who Wants To Live Forever (5\u0026#39;14\u0026#39;\u0026#39;) Q\\Queensryche - Another Rainy Night Without You Q\\Queensryche - Anybody Listening Q\\Queensryche - Empire Q\\Queensryche - Jet City Woman Q\\Queensryche - Silent Lucidity Q\\Question Mark and the Mysterians - 96 Tears Q\\Quiet Riot - Cum On Feel The Noize (192 kbps) Q\\Quincy Jones - Soul Bossa Nova R R\\Radiohead - Creep R\\Rage Against The Machine - Killing In The Name Of R\\Raiders - Indian Reservation (Lament of the Cherokee Reservation Indian) (192 kbps) R\\Rainbow - Black Sheep Of The Family R\\Rainbow - Catch The Rainbow R\\Rainbow - Fire Dance R\\Rainbow - Kill the King R\\Rainbow - Man On The Silver Mountain R\\Rainbow - Self Portrait R\\Rainbow - Sixteenth Century Greensleeves R\\Rainbow - Stargazer R\\Rainbow - Street Of Dreams R\\Rainbow - The Temple Of The King R\\Ram Jam - Black Betty (3\u0026#39;57\u0026#39;\u0026#39;) (112 kbps) R\\Ram Jam - Black Betty (6\u0026#39;46\u0026#39;\u0026#39;) (112 kbps) R\\Ramones - Blitzkrieg Bop (112 kbps) R\\Ramones - I Dont Want To Grow Up R\\Ramones - I Wanna be Sedated R\\Ramones - KKK Took My Baby Away R\\Ravel - Bolero R\\Ray Charles - Hit the Road Jack R\\Ray Manzarek - The Wheel Of Fortune R\\Ray Parker Jr - Ghostbusters R\\REM - (Don\u0026#39;t Go Back To) Rockville R\\REM - Bang And Blame R\\REM - Drive R\\REM - Everybody Hurts R\\REM - Femme Fatale (adaptation) R\\REM - Harborcoat R\\REM - It\u0026#39;s The End Of The World As We Know It R\\REM - Losing My Religion R\\REM - Man on the Moon R\\REM - Orange Crush R\\REM - Shiny Happy People R\\REM - So. Central Rain R\\REM - Stand (not very good recording) R\\REM - Strange Currencies R\\REM - The One I Love (acoustic) (112 kbps) R\\REM - The One I Love R\\REM - Turn You Inside Out R\\REM - What\u0026#39;s The Frequency, Kenneth R\\REM - White Tornado R\\REM - World Leader Pretend R\\REO Speedwagon - Back On The Road Again R\\REO Speedwagon - Can\u0026#39;t Fight This Feeling R\\REO Speedwagon - Keep On Loving You R\\Real 2 Real - I Like to Move It R\\Real Life - Send Me an Angel R\\Red Hot Chilli Peppers - Around The World R\\Red Hot Chilli Peppers - Californiacation R\\Red Hot Chilli Peppers - Give it Away R\\Red Hot Chilli Peppers - Higher Ground R\\Red Hot Chilli Peppers - Knock Me Down R\\Red Hot Chilli Peppers - Otherside R\\Red Hot Chilli Peppers - Scar Tissue R\\Red Hot Chilli Peppers - Soul to Squeeze R\\Red Hot Chilli Peppers - Under The Bridge R\\Red Ryder - Lunatic Fringe R\\Regina - Day By Day R\\Rene Aubry - Apres La Pluie II R\\Reservior Dogs - Hooked On a Feeling R\\Resonance - OK Chicago R\\Rialto - Monday Morning 5.19 a.m. R\\Richard Cheese - Come Out And Play R\\Richard Cheese - Creep R\\Richard Cheese - Fight For Your Right To Party R\\Richard Cheese - Guerilla Radio R\\Richard Cheese - Only Happy When It Rains R\\Richard Cheese - Rape Me R\\Richard Cheese - Suck My Kiss R\\Richard Cheese - What\u0026#39;s My Age Again R\\Richard Cheese - Wrong Way R\\Richard Marx - Right Here Waiting R\\Richie Valens - Donna R\\Rick James - Super Freak R\\Ricky Martin - Livin\u0026#39; La Vida Loca R\\Ricky Martin - Maria R\\Ricky Shayne - Mamy Blue (german) R\\Righteous Brothers - Unchained Melody R\\Righteous Brothers - When A Man Loves A Woman (needs editing at end) R\\Rivingtons - Papa Ooo Mow Mow R\\Robbie Williams - Millennium R\\Robert Palmer - Bad Case of Loving You (Doctor, Doctor) R\\Robert Palmer - Get It On R\\Robert Palmer - Johnny \u0026amp; Mary R\\Robert Palmer - Simply Irresistible R\\Robert Plant - 29 Palms R\\Robert Plant - Big Log R\\Robert Plant - In The Mood R\\Robert Plant - Reckless Love R\\Robert Plant - Ship Of Fools R\\Robert Plant - Tall Cool One R\\Rockapella - Breakfast at Tiffany\u0026#39;s R\\Rockapella - Stand By Me R\\Roger Miller - King of the Road R\\Rolling Stones - (I Can\u0026#39;t Get No) Satisfaction R\\Rolling Stones - Angie R\\Rolling Stones - Jumpin Jack Flash (spike at 0\u0026#39;56\u0026#39;\u0026#39;) R\\Rolling Stones - Paint It Black R\\Rolling Stones - Sympathy for the Devil (112 kbps) R\\Romantics - Talking In Your Sleep R\\Romantics - What I Like About You R\\Ronnie Jordan - So What R\\Rory Gallagher - Do You Read Me R\\Rory Gallagher - Follow Me R\\Rory Gallagher - Moonchild R\\Rory Gallagher - Philby R\\Rory Gallagher - Shadow Play R\\Roxette - Dangerous R\\Roxette - Listen To Your Heart R\\Roxette - She\u0026#39;s Got the Look R\\Roxy Music - Avalon R\\Roxy Music - Love Is The Drug R\\Roxy Music - More Than This R\\Rush - Freewill R\\Rush - Tom Sawyer R\\Ryan Paris - Dolce Vita S S\\Sabrina - Boys S\\Sade - No Ordinary Love S\\Sade - Smooth Operator S\\Saga - On The Loose S\\Salt N Pepa - Push It S\\Sam Brown - Stop S\\Sam Cooke - Wonderful World (bad quality) (bleep at 0\u0026#39;43\u0026#39;\u0026#39;) S\\Sam Cooke - Wonderful World (bad quality) S\\Sam The Sham - Little Red Riding Hood S\\Sam The Sham \u0026amp; The Pharaohs - Wolly Bully S\\Samantha Fox - Touch Me S\\Sammy Hagar - I Can\u0026#39;t Drive 55 S\\Santana - Black Magic Woman - Gupsy Queen S\\Santana - Black Magic Woman (192 kbps) S\\Santana - Europa S\\Santana - Evil Ways S\\Santana - Luz Amor Y Vida S\\Santana - She\u0026#39;s Not There (adaptation) S\\Sarah Brightman \u0026amp; Michael Crawford - The Phantom of the Opera S\\Sarah Brightman \u0026amp; Steve Harley - The Phantom of the Opera S\\Sash! - Adelante S\\Sash! - Encore Une Fois S\\Sash! - Move Mania (feat. Shannon) S\\Saxon - Denim And Leather S\\Scorpions - Always Somewhere S\\Scorpions - Bad Boys Running Wild S\\Scorpions - Big City Nights S\\Scorpions - Blackout S\\Scorpions - Born To Touch Your Feelings S\\Scorpions - Catch Your Train S\\Scorpions - China White S\\Scorpions - Coast To Coast S\\Scorpions - Coming Home (low volume) S\\Scorpions - Evening Wind S\\Scorpions - He is a Woman - She is a Man S\\Scorpions - Holiday (6\u0026#39;15\u0026#39;\u0026#39;) (192 kbps) 2 S\\Scorpions - Holiday (6\u0026#39;29\u0026#39;\u0026#39;) S\\Scorpions - In Trance S\\Scorpions - In Your Park S\\Scorpions - Is There Anybody There S\\Scorpions - Lady Starlight S\\Scorpions - Life Is Like River S\\Scorpions - Living and Dying S\\Scorpions - Lovedrive S\\Scorpions - Loving You Sunday Morning S\\Scorpions - Make it Real S\\Scorpions - No One Like You S\\Scorpions - Rock You Like A Hurricane S\\Scorpions - Speedy\u0026#39;s Coming (live) S\\Scorpions - The Zoo (minor scratch at 0\u0026#39;11\u0026#39;\u0026#39;) S\\Scorpions - We\u0026#39;ll Burn The Sky S\\Scorpions - When The Smoke Is Going Down S\\Scorpions - Yellow Raven (probably a vinyl rip) S\\Scott Huckabay - (Alchemy - 08) - Daydreams S\\Scott Huckabay - (Alchemy) - Sea Gypsy (96 kbps) S\\Scott Huckabay - (Peace Dance) - Universe S\\Scott Huckabay - Until We Meet Again (incomplete) S\\Scott Joplin - The Entertainer S\\Scott Mckenzie - San Francisco S\\Seal - Crazy (112 kbps) S\\Sergio Endrigo - Se Le Cose Stanno Cosi S\\Sex Pistols - Anarchy In The UK S\\Sex Pistols - God Save the Queen S\\Sex Pistols - Pretty Vacant S\\Sha Na Na - Hound Dog S\\Sha-Na-Na - Blue Moon (Grease Soundtrack - 11) S\\Sha-Na-Na - Hound Dog (Grease Soundtrack - 14) S\\Shadows - Foot Tapper (112 kbps) S\\Shadows - Stars Fell On Stockton S\\Shadows - The Rumble S\\Shadows of Knight - Gloria (check artist) S\\Shannon - Let the Music Play S\\Sheryl Crow - All I Want to Do S\\Sheryl Crow - If It Makes You Happy S\\Sheryl Crow - Strong Enough S\\Sheryl Crow - Sweet Child O\u0026#39; Mine (192 kbps) S\\Shocking Blue - Venus S\\Silver Convention - Fly Robin Fly (4\u0026#39;59\u0026#39;\u0026#39;) (160 kbps) S\\Silver Convention - Fly Robin Fly (5\u0026#39;33\u0026#39;\u0026#39;) S\\Silverchair - Untitled (From Godzilla - The Album - 11) S\\Simon \u0026amp; Garfunkel - A Hazy Shade Of Winter S\\Simon \u0026amp; Garfunkel - Homeward Bound (live) S\\Simon \u0026amp; Garfunkel - Scarborough Fair (112 kbps) S\\Simon \u0026amp; Garfunkel - The Sound of Silence S\\Simple Minds - Don\u0026#39;t You Forget About Me S\\Sinead O\u0026#39;Connor - Drink Before The War S\\Sinead O\u0026#39;Connor - Nothing Compares to You S\\Sister Sledge - We Are Family S\\Sixpence None The Richer - There She Goes S\\Skankin Pickle - Turning Japanese S\\Skid Row - 18 and Life S\\Skid Row - I Remember You S\\Skid Row - Youth Gone Wild S\\Slowdive - Cello S\\Slowdive - Just For A Day - Catch the Breeze S\\Slowdive - Just For A Day - The Sadman S\\Smash Mouth - Walkin\u0026#39; on the Sun S\\Smash Mouth - Waste S\\Smashing Pumpkins - 1979 (Acoustic) S\\Smashing Pumpkins - 1979 S\\Smashing Pumpkins - Bullet With Butterfly Wings S\\Smashing Pumpkins - Cherub Rock S\\Smashing Pumpkins - Disarm S\\Smashing Pumpkins - Landslide S\\Smashing Pumpkins - The End Is The Beginning Is The End (112 kbps) S\\Smashing Pumpkins - The World Is A Vampire S\\Smashing Pumpkins - Today S\\Smashing Pumpkins - Tonight Tonight S\\Smiths - Bigmouth Strikes Again S\\Smiths - Every Day is like Sundays S\\Smiths - How Soon Is Now S\\Smokey Robinson - Stand By Me S\\Smokey Robinson - Tears of a Clown S\\Smokey Robinson \u0026amp; the Miracles - Going to Go-Go S\\Smokie - I\u0026#39;ll Μeet Υou Αt Μidnight S\\Snap - Rhythm Is A Dancer S\\Snap - The Power S\\Sniff \u0026amp; The Tears - Driver\u0026#39;s Seat S\\Soft Cell - Tainted Love S\\Sonique - It Feels So Good S\\Soul Asylum - Runaway Train S\\Soundgarden - Black Hole Sun S\\Soundgarden - Spoonman S\\Sounds From The Ground - Kin S\\Space - Magic Fly S\\Spagetti Surfers - Misirlou S\\Spain - (The Blue Moods of Spain - __).gif S\\Spain - (The Blue Moods of Spain - 01) - It\u0026#39;s So True S\\Spain - (The Blue Moods of Spain - 02) - Ten Nights S\\Spain - (The Blue Moods of Spain - 03) - Dreaming of Love S\\Spain - (The Blue Moods of Spain - 04) - Untitled #1 S\\Spain - (The Blue Moods of Spain - 05) - Her Used-To-Been S\\Spain - (The Blue Moods of Spain - 06) - Ray of Light S\\Spain - (The Blue Moods of Spain - 07) - World Of Blue S\\Spain - (The Blue Moods of Spain - 08) - I Lied S\\Spain - (The Blue Moods of Spain - 09) - Spiritual S\\Spandau Ballet - True S\\Spencer Davis Group - Gimme Some Loving S\\Spice Girls - Viva Forever S\\Spirit - Nature\u0026#39;s Way S\\Starship - Nothing is Gonna Stop Us Now S\\Starship - We Built This City S\\Status Quo - In the Army Now (3\u0026#39;38\u0026#39;\u0026#39;) S\\Status Quo - In the Army Now (4\u0026#39;41\u0026#39;\u0026#39;) S\\Status Quo - Pictures Of Matchstick Men S\\Steam - Na Na Hey Hey Kiss Him Goodbye S\\Steely Dan - Rikki Don\u0026#39;t Lose That Number S\\Steppenwolf - Born To Be Wild S\\Steppenwolf - Magic Carpet Ride S\\Stereo MCs - Connected S\\Steve Miller Band - Abracadabra S\\Steve Miller Band - Fly Like an Eagle S\\Steve Miller Band - Keep On Rockin Me S\\Steve Miller Band - Serenade S\\Steve Miller Band - Take the Money and Run S\\Steve Miller Band - The Joker S\\Steve Miller Band - Winter Time S\\Stevie Ray Vaughn - Crossfire S\\Stevie Ray Vaughn - Scuttlebuttin\u0026#39; S\\Sting - All This Time S\\Sting - Desert Rose S\\Sting - Englishman in New York S\\Sting - Fields of Gold S\\Sting - Fragile S\\Sting - If I Ever Lose My Faith in You S\\Sting - If You Love Somebody Set Them Free S\\Sting - Love Is The Seventh Wave S\\Sting - Mad About You S\\Sting - Moon Over Burboun Street S\\Sting - Russians S\\Sting - They Dance Alone (Cueca Solo) S\\Sting - We\u0026#39;ll Be Together S\\Stone Temple Pilots - Creep (112 kbps) S\\Stone Temple Pilots - Dancing Days S\\Stone Temple Pilots - Interstate Love Song S\\Stone Temple Pilots - Plush (live acoustic) S\\Stone Temple Pilots - Plush S\\Stone Temple Pilots - Vaseline (112 kbps) S\\Stoppa \u0026amp; Nobby - Sweet Lassi Dub S\\Stranglers - (Live in Nijmegen 1983 - 05+06) Midnight Summer Dream + European Female S\\Stranglers - 96 tears S\\Stranglers - All Day And All Of The Night S\\Stranglers - Always The Sun (4\u0026#39;06\u0026#39;\u0026#39;) S\\Stranglers - Duchess (112 kbps) S\\Stranglers - European Female S\\Stranglers - Golden Brown S\\Stranglers - Hanging Around S\\Stranglers - Nice \u0026#39;n\u0026#39; Sleazy S\\Stranglers - No More Heroes S\\Stranglers - Strange Little Girl S\\Stranglers - Walk On By (adaptation) S\\Strawberry Alarm Clock - Incense and Peppermints S\\Stu Hamm, George Lynch, Phil Lewis, Bob Kulick, Vinnie Colaiuta, Derek Sherinian - Billion Dollar Babies S\\Styx - Babe S\\Styx - Blue Collar Man (Long Nights) S\\Styx - Boat On The River (live) S\\Styx - Boat On The River S\\Styx - Come Sail Away S\\Styx - Fooling Yourself (The Angry Young Man) S\\Styx - Mr. Roboto S\\Styx - Sweet Madame Blue S\\Suede - Beautiful Ones S\\Supertramp - Breakfast In America S\\Supertramp - Give A Little Bit S\\Supertramp - School (live) S\\Supertramp - School S\\Supertramp - The Logical Song S\\Surfaris - Wipeout S\\Survivor - Burning Heart S\\Survivor - Eye Of The Tiger S\\Sweet - Love Is Like Oxygen (3\u0026#39;47\u0026#39;\u0026#39;) S\\Sweet - Love Is Like Oxygen (3\u0026#39;58\u0026#39;\u0026#39;) S\\Sweet - Love Is Like Oxygen (6\u0026#39;51\u0026#39;\u0026#39;) S\\Sweet - The Ballroom Blitz S\\Swinging Blue Jeans - Hippy Hippy Shake T T\\T Rex - Bang a Gong (Get It On) T\\Taco - Putting On The Ritz T\\Talking Heads - (Nothing but) Flowers T\\Talking Heads - And She Was T\\Talking Heads - Blind T\\Talking Heads - Burning Down the House T\\Talking Heads - Life During Wartime (live) T\\Talking Heads - Psycho Killer T\\Talking Heads - Road To Nowhere T\\Talking Heads - Take Me To The River T\\Talking Heads - Water Flowing Under (Once in a Lifetime) T\\Talking Heads - Wild Wild Life T\\Tanita Tikaram - Twist In My Sobriety T\\Taylor Dayne - Tell It To My Heart T\\Tears For Fears - Change (3\u0026#39;54\u0026#39;\u0026#39;) T\\Tears For Fears - Change (5\u0026#39;59\u0026#39;\u0026#39;) T\\Tears For Fears - Everybody Wants To Rule The World T\\Tears For Fears - Shout (5\u0026#39;54\u0026#39;\u0026#39;) T\\Tears For Fears - Shout (6\u0026#39;27\u0026#39;\u0026#39;) T\\Tears For Fears - Sowing The Seeds Of Love T\\Ted Nugent - Cat Scratch Fever T\\Temptations - My Girl (112 kbps) T\\Ten cc - Dreadlock Holiday (live) T\\Ten Thousand Maniacs - Because The Night (adaptation) (live) T\\Ten Thousand Maniacs - Because The Night (adaptation) (unplugged) T\\Ten Thousand Maniacs - Every Day Is Like Sunday (adaptation) T\\Ten Years After - I\u0026#39;d Love To Change The World T\\Terence Trent D\u0026#39; Arby - Sign Your Name T\\Terry Jacks - Seasons In The Sun T\\Tesla - Love Song T\\Thirty Eight Special - Hold on Loosely T\\Thompson Twins - Hold Me Now T\\Thompson Twins - In the Name of Love T\\Thompson Twins - Lies T\\Three Degrees - When Will I See You Again T\\Throwing Muses - Bright Yellow Gun T\\Tim Buckley - Phantasmagoria in Two T\\Tim Buckley - Pleasant Street T\\Tina Turner - What\u0026#39;s Love Got To Do With It T\\Tindersticks - Jism (live) T\\Tindersticks - Jism T\\Toad The Wet Sprocket - Crazy Life T\\Tokens - The Lion Sleeps Tonight (check artist) T\\Tom Cochrane - Life Is A Highway T\\Tom Jones - Sexbomb (dance version, I suppose) T\\Tom Jones \u0026amp; Mousse T - Sexbomb (192 kbps) T\\Tom Petty - Free Falling T\\Tom Petty - Won\u0026#39;t Back Down T\\Tom Petty \u0026amp; Stevie Nicks - Stop Draggin\u0026#39; My Heart Around T\\Tom Petty \u0026amp; the Heartbreakers - Refugee T\\Tom Robinson Band - 2-4-6-8 Motorway T\\Tom Robinson Band - Crossing Over The Road T\\Tom Robinson Band - Listen To The Radio, Atmospherics (112 kbps) T\\Tom Robinson Band - Not Ready (112 kbps) T\\Tom Robinson Band - Power In The Darkness (112 kbps) T\\Tom Robinson Band - Rikki Don\u0026#39;t Lose That Number T\\Tom Robinson Band - Too Good To Be True (112 kbps) T\\Tommy James \u0026amp; The Shondells - Crimson And Clover T\\Tommy James \u0026amp; The Shondells - Mony, Mony T\\Tommy Ree - Sugar Sugar T\\Tommy Roe - Dizzy T\\Tone Loc - Funky Cold Medina T\\Tone Loc - Wild Thing T\\Tone Loc \u0026amp; ZZ Top - Funky Cold Madina (160 kbps) T\\Tonic - Open Up Your Eyes T\\Toto - Africa (4\u0026#39;21\u0026#39;\u0026#39;) T\\Toto - Africa (4\u0026#39;58\u0026#39;\u0026#39;) T\\Toto - Hold The Line T\\Toto - Rosanna (4\u0026#39;04\u0026#39;\u0026#39;) T\\Toto - Rosanna (5\u0026#39;33\u0026#39;\u0026#39;) T\\Toto Coelo - I Eat Cannibals T\\Touch \u0026amp; Go - Would You T\\Traffic - Glad T\\Trance Atlantic Air Waves - Addiction Day T\\Trance Atlantic Air Waves - Axel F. T\\Trance Atlantic Air Waves - Chase T\\Trance Atlantic Air Waves - Cream (ATB Remix) T\\Trance Atlantic Air Waves - Crockett\u0026#39;s Theme T\\Trance Atlantic Air Waves - Magic Fly (wonderland mix) T\\Trance Atlantic Air Waves - Pulstar T\\Trance Atlantic Air Waves - The Energy of Sound 01 - Lucifer T\\Trance Atlantic Air Waves - Twelve After Midnight T\\Transvision Vamp - I Want Your Love T\\Trio - Da Da Da T\\Troggs - Love Is All Around T\\Troggs - Wild Thing T\\Troggs - With a Girl Like You T\\Turtles - Elenore T\\Turtles - Happy Together T\\TV Theme - Bonanza - (Original) T\\Two Unlimited - Let the Beat Control Your Body T\\Two Unlimited - Twilight Zone U U\\U2 - Angel Of Harlem U\\U2 - Desire U\\U2 - Gloria U\\U2 - I Still Haven\u0026#39;t Found What I\u0026#39;m Looking For U\\U2 - Mysterious Ways U\\U2 - New Years Day (112 kbps) U\\U2 - Paint it Black U\\U2 - Pride (In The Name Of Love) U\\U2 - Sunday Bloody Sunday U\\U2 - When Love Comes To Town U\\U2 - Where the Streets Have No Name U\\U2 - With Or Without You U\\UB40 - 1 in 10 (112 kbps) U\\UB40 - Can\u0026#39;t Help Falling In Love (112 kbps) U\\UB40 - Here I Am (Come And Take Me) (112 kbps) U\\UB40 - I Got You Babe (112 kbps) U\\UB40 - If It Happens Again (112 kbps) U\\UB40 - Present Arms U\\UB40 - Red Red Wine U\\UFO - Belladonna U\\UFO - Doctor Doctor (live) U\\UFO - Doctor Doctor U\\UFO - Only You Can Rock Me (Live) U\\UFO - Rock Bottom U\\Ugly Kid Joe - Cats In The Cradle (112 kbps) U\\Umberto Tozzi - Ti Amo U\\Unique II - Break My Stride (adaptation) U\\Unknown - Microsoft Sucks - Killing My Software With Windows U\\Urge Overkill - Girl, You\u0026#39;ll Be A Woman Soon U\\Uriah Heep - Easy livin\u0026#39; U\\Uriah Heep - July morning U\\Uriah Heep - Lady In Black U\\Uriah Heep - Look At Yourself U\\Uriah Heep - Sympathy V V\\Van Halen - Jump V\\Van McCoy \u0026amp; The Soul City Symphony - The Hustle V\\Van Morrison - Brown Eyed Girl V\\Vanessa Mae - Hocus Pocus (adaptation) V\\Vangelis - (1971 - Hypothesis - __).jpg V\\Vangelis - (1971 - Hypothesis - 01) - Part I (vinyl rip) V\\Vangelis - (1971 - Hypothesis - 02) - Part II (vinyl rip, 192 kbps) V\\Vangelis - (1973 - Earth - __).jpg V\\Vangelis - (1973 - Earth - 01) - Come On (^thumbs up) V\\Vangelis - (1973 - Earth - 02) - We Were All Uprooted (^so-so) V\\Vangelis - (1973 - Earth - 03) - Sunny Earth (^thumbs down) V\\Vangelis - (1973 - Earth - 04) - He-O (^thumbs down) V\\Vangelis - (1973 - Earth - 05) - Ritual (^thumbs down) V\\Vangelis - (1973 - Earth - 06) - Let It Happen (^so-so) V\\Vangelis - (1973 - Earth - 07) - The City (^thumbs down) V\\Vangelis - (1973 - Earth - 08) - My Face In The Rain (^thumbs down) V\\Vangelis - (1973 - Earth - 09) - Watch Out (^thumbs down) V\\Vangelis - (1973 - Earth - 10) - A Song (^thumbs down) (gaps starting at 1\u0026#39;04\u0026#39;\u0026#39;) V\\Vangelis - (1976 - Albedo 0.39 - __).jpg V\\Vangelis - (1976 - Albedo 0.39 - 01) - Pulstar (^thumbs up) V\\Vangelis - (1976 - Albedo 0.39 - 02) - Freefall (^thumbs down) V\\Vangelis - (1976 - Albedo 0.39 - 03) - Mare Tranquillitatis (^thumbs down) V\\Vangelis - (1976 - Albedo 0.39 - 04) - Main Sequence (^thumbs down) V\\Vangelis - (1976 - Albedo 0.39 - 05) - Sword Of Orion (^thumbs down) V\\Vangelis - (1976 - Albedo 0.39 - 06) - Alpha (^thumbs up) V\\Vangelis - (1976 - Albedo 0.39 - 07) - Nucleogenesis (Part One) (^thumbs up) V\\Vangelis - (1976 - Albedo 0.39 - 08) - Nucleogenesis (Part Two) (^thumbs down) V\\Vangelis - (1976 - Albedo 0.39 - 09) - Albedo 0.39 (^thumbs up) V\\Vangelis - (1976 - La Fete Sauvage - __).jpg V\\Vangelis - (1976 - La Fete Sauvage - ENTIRE RECORD) V\\Vangelis - (1977 - The Spiral - __).jpg V\\Vangelis - (1977 - The Spiral - 01) - Spiral (^thumbs up) V\\Vangelis - (1977 - The Spiral - 02) - Ballad (^thumbs down) V\\Vangelis - (1977 - The Spiral - 03) - Dervish D (^thumbs up) V\\Vangelis - (1977 - The Spiral - 04) - To the Unknown Man (^thumbs down) V\\Vangelis - (1977 - The Spiral - 05) - 3+3 (^thumbs down) V\\Vangelis - (1979 - China - __).jpg V\\Vangelis - (1979 - China - 01) - Chung Kuo (^thumbs down) V\\Vangelis - (1979 - China - 02) - The Long March (96 kbps) (^thumbs down) V\\Vangelis - (1979 - China - 03) - The Dragon (^thumbs down) V\\Vangelis - (1979 - China - 04) - The Plum Blossom (^thumbs down) V\\Vangelis - (1979 - China - 05) - The Tao Of Love (^thumbs down) V\\Vangelis - (1979 - China - 06) - The Little Fete (^thumbs down) V\\Vangelis - (1979 - China - 07) - Yin \u0026amp; Yang (^thumbs down) V\\Vangelis - (1979 - China - 08) - Himalaya (^thumbs down) V\\Vangelis - (1979 - China - 09) - Summit (^thumbs down) V\\Vangelis - (1979 - Opera Sauvage - __).jpg V\\Vangelis - (1979 - Opera Sauvage - 01) - Hymne (thumbs down) V\\Vangelis - (1979 - Opera Sauvage - 02) - Reve (thumbs down) V\\Vangelis - (1979 - Opera Sauvage - 03) - L\u0026#39;enfant (thumbs up) V\\Vangelis - (1979 - Opera Sauvage - 04) - Mouettes (thumbs up) V\\Vangelis - (1979 - Opera Sauvage - 05) - Chromatique (so-so) V\\Vangelis - (1979 - Opera Sauvage - 06) - Irlande (so-so) V\\Vangelis - (1979 - Opera Sauvage - 07) - Flamants Roses (so-so) V\\Vangelis - (1980 - See You Later - __).jpg V\\Vangelis - (1980 - See You Later - 01) - I Can\u0026#39;t Take It Anymore (thumbs down) V\\Vangelis - (1980 - See You Later - 02) - Multi-Track Suggestion (thumbs down) V\\Vangelis - (1980 - See You Later - 03) - Memories of Green (thumbs up) V\\Vangelis - (1980 - See You Later - 04) - Not a bit-all of it (thumbs down) V\\Vangelis - (1980 - See You Later - 05) - Suffocation (thumbs down) V\\Vangelis - (1980 - See You Later - 06) - See You Later (thumbs down) V\\Vangelis - (1983 - Antarctica - __).jpg V\\Vangelis - (1983 - Antarctica - 01) - Theme from Antarctica (^thumbs up) V\\Vangelis - (1983 - Antarctica - 02) - Antarctic Echoes (^thumbs up) V\\Vangelis - (1983 - Antarctica - 03) - Kinematic (^so-so) V\\Vangelis - (1983 - Antarctica - 04) - Song of White (^thumbs up) V\\Vangelis - (1983 - Antarctica - 05) - Life of Antarctica (^thumbs up) V\\Vangelis - (1983 - Antarctica - 06) - Memory of Antarctica (^thumbs up) V\\Vangelis - (1983 - Antarctica - 07) - Other Side Of Antarctica (^so-so) V\\Vangelis - (1983 - Antarctica - 08) - Deliverance (^thumbs down) V\\Vangelis - (1984 - Soil Festivities - __).jpg V\\Vangelis - (1984 - Soil Festivities - 01) - Movement I (so-so) V\\Vangelis - (1984 - Soil Festivities - 02) - Movement II (so-so) V\\Vangelis - (1984 - Soil Festivities - 03) - Movement III (thumbs down) V\\Vangelis - (1984 - Soil Festivities - 04) - Movement IV (so-so) V\\Vangelis - (1984 - Soil Festivities - 05) - Movement V (so-so) V\\Vangelis - (1985 - Mask - __).jpg V\\Vangelis - (1985 - Mask - 01) - Movement I V\\Vangelis - (1985 - Mask - 02) - Movement II V\\Vangelis - (1985 - Mask - 03) - Movement III V\\Vangelis - (1985 - Mask - 04) - Movement IV V\\Vangelis - (1985 - Mask - 05) - Movement V V\\Vangelis - (1985 - Mask - 06) - Movement VI V\\Vangelis - (1988 - Direct - __).jpg V\\Vangelis - (1988 - Direct - 01) - The Motion of Stars (^so-so) V\\Vangelis - (1988 - Direct - 02) - The Will of the Wind (^so-so) V\\Vangelis - (1988 - Direct - 03) - Metallic Rain (thumbs up) V\\Vangelis - (1988 - Direct - 04) - Elsewhere (^so-so) V\\Vangelis - (1988 - Direct - 05) - Dial Out (^thumbs down) V\\Vangelis - (1988 - Direct - 06) - Glorianna (Hymn A La Femme) (^thumbs down) V\\Vangelis - (1988 - Direct - 07) - Rotation\u0026#39;s Logic (^thumbs down) V\\Vangelis - (1988 - Direct - 08) - The Oracle of Apollo (^thumbs down) V\\Vangelis - (1988 - Direct - 09) - Message (^so-so) V\\Vangelis - (1988 - Direct - 10) - Ave (;) (^so-so) V\\Vangelis - (1988 - Direct - 11) - First Aproach (^thumbs down) V\\Vangelis - (1988 - Direct - 12) - Intergalactic Radio Station (^so-so) V\\Vangelis - (1994 - Blade Runner - __).jpg V\\Vangelis - (1994 - Blade Runner - 01) - Main Titles V\\Vangelis - (1994 - Blade Runner - 02) - Blush Response V\\Vangelis - (1994 - Blade Runner - 03) - Wait for Me (spike at 3\u0026#39;10\u0026#39;\u0026#39;) V\\Vangelis - (1994 - Blade Runner - 04) - Rachel\u0026#39;s Song V\\Vangelis - (1994 - Blade Runner - 05) - Love Theme V\\Vangelis - (1994 - Blade Runner - 06) - One More Kiss, Dear V\\Vangelis - (1994 - Blade Runner - 07) - Blade Runner Blues V\\Vangelis - (1994 - Blade Runner - 08) - Memories of Green V\\Vangelis - (1994 - Blade Runner - 09) - Tales Of The Future V\\Vangelis - (1994 - Blade Runner - 10) - Damask Rose V\\Vangelis - (1994 - Blade Runner - 11) - End Titles V\\Vangelis - (1994 - Blade Runner - 12) - Tears In Rain V\\Vangelis - (1995 - Best In Space - __).jpg V\\Vangelis - (1995 - Best In Space - 01) - To The Unknown Man (^thumbs down) V\\Vangelis - (1995 - Best In Space - 03) - Sword Of Orion (^thumbs down) V\\Vangelis - (1995 - Best In Space - 04) - 12 O\u0026#39;Clock (^thumbs up) V\\Vangelis - (1995 - Best In Space - 05) - Spiral (^thumbs up) V\\Vangelis - (1995 - Best In Space - 06) - Alpha (^thumbs up) V\\Vangelis - (1995 - Best In Space - 07) - A Way (^so-so) V\\Vangelis - (1995 - Best In Space - 08) - Theme From TV Series Cosmos (Heaven And Hell third movement)(^thumbs up) V\\Vangelis - (1995 - Best In Space - 09) - So Long Ago So Clear (^thumbs down) V\\Vangelis - (1995 - Best In Space - 10) - Ballad (^thumbs down) V\\Vangelis - (1995 - Best In Space - 11) - Albedo 0,39 (^thumbs up) V\\Vangelis - (1995 - Best In Space - 12) - Beaubourg (excerpt) (^thumbs down) V\\Vangelis - (1995 - Best in Space - 02) - Pulstar (^thumbs up) V\\Vangelis - (1995 - Space Themes - __).gif V\\Vangelis - (1995 - Space Themes - 01) - Baccanale (skip at 0\u0026#39;54\u0026#39;\u0026#39;) (^thumbs down) V\\Vangelis - (1995 - Space Themes - 02) - Aries (^thumbs down) V\\Vangelis - (1995 - Space Themes - 03) - Heaven \u0026amp; Hell 3rd Movement (^thumbs up) V\\Vangelis - (1995 - Space Themes - 04) - So Long Age So Clear (^thumbs down) V\\Vangelis - (1995 - Space Themes - 05) - Pulstar (^thumbs up) V\\Vangelis - (1995 - Space Themes - 06) - Alpha (^thumbs up) V\\Vangelis - (1995 - Space Themes - 07) - To The Unknown Man (8\u0026#39;22\u0026#39;\u0026#39;) (^thumbs down) V\\Vangelis - (1995 - Space Themes - 08) - Spiral (^thumbs down) V\\Vangelis - (1995 - Space Themes - 09) - Ballad (^thumbs down) V\\Vangelis - (1995 - Space Themes - 10) - Albedo 0.39 (^thumbs up) V\\Vangelis - (1995 - Space Themes - 11) - Beaubourg Excerpt (^thumbs down) V\\Vangelis - (1995 - Space Themes - 12) - 12 O\u0026#39;Clock (^thumbs up) V\\Vangelis - (1995 - Space Themes - 13) - A Way (^so-so) V\\Vangelis - (1995 - Voices - __).jpg V\\Vangelis - (1995 - Voices - 01) - Voices (^thumbs down) V\\Vangelis - (1995 - Voices - 02) - Echoes (^thumbs down) V\\Vangelis - (1995 - Voices - 03) - Come To Me (^so-so) V\\Vangelis - (1995 - Voices - 04) - P.S. (^so-so) V\\Vangelis - (1995 - Voices - 05) - Ask The Mountains (^okay) V\\Vangelis - (1995 - Voices - 06) - Prelude (^thumbs up) V\\Vangelis - (1995 - Voices - 07) - Losing Sleep (Still, My heart) (^thumbs down) V\\Vangelis - (1995 - Voices - 08) - Messages (^thumbs down) V\\Vangelis - (1995 - Voices - 09) - Dream in an Open Place (^okay) V\\Vangelis - (1996 - Oceanic - __).jpg V\\Vangelis - (1996 - Oceanic - 01) - Bon Voyage (2\u0026#39;32\u0026#39;\u0026#39;) (^so-so) V\\Vangelis - (1996 - Oceanic - 02) - Sirens\u0026#39; Whispering (^thumbs down) V\\Vangelis - (1996 - Oceanic - 03) - Dreams of Surf (^so-so) V\\Vangelis - (1996 - Oceanic - 04) - Spanish Harbour (^so-so) V\\Vangelis - (1996 - Oceanic - 05) - Islands Of The Orient (^so-so) V\\Vangelis - (1996 - Oceanic - 06) - Fields Of Coral (^thumbs up) V\\Vangelis - (1996 - Oceanic - 07) - Aquatic Dance (^so-so) V\\Vangelis - (1996 - Oceanic - 08) - Memories Of Blue (^so-so) V\\Vangelis - (1996 - Oceanic - 09) - Song Of The Seas (^so-so) V\\Vangelis - (1998 - El Greco - __).jpg V\\Vangelis - (1998 - El Greco - 01) - El Greco I V\\Vangelis - (1998 - El Greco - 02) - El Greco II V\\Vangelis - (1998 - El Greco - 03) - El Greco III V\\Vangelis - (1998 - El Greco - 04) - El Greco IV V\\Vangelis - (1998 - El Greco - 05) - El Greco V V\\Vangelis - (1998 - El Greco - 06) - El Greco VI V\\Vangelis - (1998 - El Greco - 07) - El Greco VII V\\Vangelis - (1998 - El Greco - 08) - El Greco VIII V\\Vangelis - (1998 - El Greco - 09) - El Greco IX V\\Vangelis - (1998 - El Greco - 10) - El Greco X V\\Vangelis - (1999 - Reprise 1990-1999 - __).jpg V\\Vangelis - (1999 - Reprise 1990-1999 - 01) - Bon Voyage (^so-so) V\\Vangelis - (1999 - Reprise 1990-1999 - 02) - Dreams of Surf (^so-so) (spike at 0\u0026#39;45\u0026#39;\u0026#39;) V\\Vangelis - (1999 - Reprise 1990-1999 - 03) - Opening (^so-so) V\\Vangelis - (1999 - Reprise 1990-1999 - 04) - Conquest Of Paradise (^so-so) V\\Vangelis - (1999 - Reprise 1990-1999 - 05) - Monastery of La Rabida (^so-so) V\\Vangelis - (1999 - Reprise 1990-1999 - 06) - Come To Me (^thumbs down) V\\Vangelis - (1999 - Reprise 1990-1999 - 07) - Light And Shadow (^so-so) V\\Vangelis - (1999 - Reprise 1990-1999 - 08) - Fields Of Coral (^thumbs up) V\\Vangelis - (1999 - Reprise 1990-1999 - 09) - Movement V (^thumbs up) (192 kbps) V\\Vangelis - (1999 - Reprise 1990-1999 - 10) - Movement VI (^thumbs up (160 kbps) V\\Vangelis - (1999 - Reprise 1990-1999 - 11) - West Across The Ocean Sea (^thumbs up) V\\Vangelis - (1999 - Reprise 1990-1999 - 12) - Theme From \u0026#39;Bitter Moon\u0026#39; (^so-so) V\\Vangelis - (1999 - Reprise 1990-1999 - 13) - Rachel\u0026#39;s Song (^thumbs up) V\\Vangelis - (1999 - Reprise 1990-1999 - 14) - Movement IV (^so-so) V\\Vangelis - (1999 - Reprise 1990-1999 - 15) - Psalmus Ode (^so-so) V\\Vangelis - (1999 - Reprise 1990-1999 - 16) - Dawn (^so-so) V\\Vangelis - (1999 - Reprise 1990-1999 - 17) - Prelude (^thumbs up) V\\Vangelis - (Antarctica OST - 01) Ice Of Antartica V\\Vangelis - (Antarctica OST - 02) Destination Antartica V\\Vangelis - (Antarctica OST - 04) Land Of Antartica V\\Vangelis - (Antarctica OST - 05) Mountain In Antartica V\\Vangelis - (Antarctica OST - 06) The Ship In The Ice V\\Vangelis - (Antarctica OST - 07) Memory Of Antartica V\\Vangelis - (Antarctica OST - 08 - Escape To Antartica V\\Vangelis - (Antarctica OST - 09 - Living In Antartica V\\Vangelis - (Antarctica OST - 10) Wind Of Antartica V\\Vangelis - (Antarctica OST - 13 - Winter In Antartica V\\Vangelis - (Antarctica OST - 14) Mirage V\\Vangelis - (Antarctica OST - 15) Borealis V\\Vangelis - (Antarctica OST - 16 - Light Of Antartica V\\Vangelis - (Antarctica OST - 17 - Fall V\\Vangelis - (Antarctica OST - 18) Sun Of Antartica V\\Vangelis - (Antarctica OST - 19 - Return To Antartica V\\Vangelis - (Antarctica OST - 22 - Antartic Echoes V\\Vangelis - (Antarctica OST - 23) Back To Antartica V\\Vangelis - (Blade Runner Bootleg - 00) - If I Didn\u0026#39;t Care V\\Vangelis - (Blade Runner Bootleg - 01) - Ladd Company Logo V\\Vangelis - (Blade Runner Bootleg - 02) - Main Titles and Prologue V\\Vangelis - (Blade Runner Bootleg - 03) - Los Angeles November 2019 V\\Vangelis - (Blade Runner Bootleg - 04) - Deckard Meets Rachel V\\Vangelis - (Blade Runner Bootleg - 05) - Bicycle Riders V\\Vangelis - (Blade Runner Bootleg - 06) - Memories Of Green V\\Vangelis - (Blade Runner Bootleg - 07) - Blade Runner Blues (bleep at 7\u0026#39;16\u0026#39;\u0026#39;) V\\Vangelis - (Blade Runner Bootleg - 08) - Deckard\u0026#39;s Dream V\\Vangelis - (Blade Runner Bootleg - 09) - On the trail of Nexus (1\u0026#39;\u0026#39; too long) V\\Vangelis - (Blade Runner Bootleg - 10) - One More Kiss, Dear V\\Vangelis - (Blade Runner Bootleg - 11) - Love Theme V\\Vangelis - (Blade Runner Bootleg - 12) - The Prodigal Son Brings Death V\\Vangelis - (Blade Runner Bootleg - 13) - Japanese Woman\u0026#39;s Blimp Song V\\Vangelis - (Blade Runner Bootleg - 14) - Dangerous Days V\\Vangelis - (Blade Runner Bootleg - 15) - Wounded animals V\\Vangelis - (Blade Runner Bootleg - 16) - Tears in the Rain V\\Vangelis - (Blade Runner Bootleg - 17) - End Titles V\\Vangelis - (Conquest Of Paradise) - Monastery of La Rabida V\\Vangelis - (Light And Shadow - 07) - Star V\\Vangelis - (Mutiny On The Bounty) - Closing Titles V\\Vangelis - 2001 A Space Odessy V\\Vangelis - Djemilla V\\Vangelis - La Petite Fille De La Mer V\\Vangelis - The Year of Living Dangerously (acoustic) (thumbs up) V\\Vanilla Fudge - You Keep Me Hangin\u0026#39; On V\\Vapors - Turning Japanese V\\Velvet Underground - Femme Fatale (\u0026amp; Niko) V\\Velvet Underground - I\u0026#39;m Waiting For The Man V\\Velvet Underground - Pale Blue Eyes V\\Ventures - Hawaii Five-O (192 kbps) V\\Ventures - Misirlou V\\Ventures - Walk, Don\u0026#39;t Run V\\Ventures - Wipeout V\\Veruca Salt - Seether V\\Verve, The - Bitter Sweet Symphony V\\Village People - YMCA V\\Violent Femmes - Add It Up V\\Violent Femmes - Blister In The Sun V\\Violent Femmes - Kiss Off V\\Visage - Fade to Gray V\\Visit Venus - Brooklyn Sky Port (departure) V\\Visit Venus - Harlem Overdrive V\\Visit Venus - Home V\\Visit Venus - Music For Spa V\\Visit Venus - One Step Beyond V\\Visit Venus - Shaft In Space V\\Visit Venus - The Big Tilt V\\Visit Venus - Zoom W W\\Wagner R. - Ride Of The Walkyries W\\Wallflowers - Heroes (From Godzilla - The Album - 01) W\\Walter Murphy - A Fifth Of Beethoven W\\War - Low Rider W\\Warrant - Heaven W\\Warrant - Uncle Toms Cabin W\\Weather Girls - It\u0026#39;s Raining Men (4\u0026#39;54\u0026#39;\u0026#39;) W\\Weather Girls - It\u0026#39;s Raining Men (5\u0026#39;30\u0026#39;\u0026#39;) W\\Weird Al Yankovich - Make My Boobies One More Size W\\Weird Al Yankovich - What if God Smoked Cannabis W\\Weird Al Yankovich - Windows 95 Sucks (Start Me Up).mp2 W\\Wet Wet Wet - Love Is All Around (adaptation) W\\Wham - Careless Whisper (6\u0026#39;32\u0026#39;\u0026#39;) W\\Wham - Everything She Wants W\\Wham - Wake Me Up Before You Go-Go W\\White Lion - When The Children Cry (192 kbps) W\\White Zombie - More Human Than Human W\\Whitesnake - Here I Go Again (3\u0026#39;53\u0026#39;\u0026#39;) (192 kbps) W\\Whitesnake - Here I Go Again (4\u0026#39;37\u0026#39;\u0026#39;) W\\Whitesnake - Still Of The Night W\\Who - 5.15 (112 kbps) W\\Who - Baba O\u0026#39;Riley W\\Who - Behind Blue Eyes (112 kbps) W\\Who - Emminence Front W\\Who - Goin\u0026#39; Mobile W\\Who - I Can See For Miles W\\Who - I Can\u0026#39;t Explain W\\Who - I\u0026#39;m A Boy (112 kbps) W\\Who - Join Together (112 kbps) W\\Who - Magic Bus W\\Who - My Generation W\\Who - Pictures Of Lily (112 kbps) W\\Who - Pinball Wizard W\\Who - Squeeze Box (112 kbps) W\\Who - Summertime Blues (adaptation) (112 kbps) W\\Who - The Seeker W\\Who - We Won\u0026#39;t Get Fooled Again W\\Who - Who Are You (112 kbps) W\\Who - You Better You Bet (112 kbps) W\\Wild Cherry - Play That Funky Music W\\Will To Power - Baby, I Love Your Way Freebird Medley (Free Baby) W\\William Orbit - Water From A Vine Leaf (Xylem Flow Mix) W\\William Orbit - Water From A Vine Leaf W\\Wim Mertens - Humility W\\Wim Mertens - Struggle of Pleasure W\\Wonders - That Thing You Do X X\\XTC - Dear God Y Y\\Yanni - (Live At The Acropolis - 08) Nostalgia Y\\Yanni - (Live At The Acropolis) - Acroyali Standing in Motion Y\\Yanni - (Reflections of Passion - 01) - After The Sunrise Y\\Yanni - (Reflections of Passion - 02) - The Mermaid Y\\Yanni - (Reflections of Passion - 03) - Quiet Man Y\\Yanni - (Reflections of Passion - 04) - Nostalgia Y\\Yanni - (Reflections of Passion - 05) - Almost A Whisper Y\\Yanni - (Reflections of Passion - 06) - The Rain Must Fall Y\\Yanni - (Reflections of Passion - 07) - Acroyali Y\\Yanni - (Reflections of Passion - 08) - Farewell Y\\Yanni - (Reflections of Passion - 09) - Swept Away Y\\Yanni - (Reflections of Passion - 10) - True Nature Y\\Yanni - (Reflections of Passion - 11) - Secret Vows Y\\Yanni - (Reflections of Passion - 12) - Flight Of Fantasy Y\\Yanni - (Reflections of Passion - 13) - A Word in Private Y\\Yanni - (Reflections of Passion - 14) - First Touch Y\\Yanni - (Reflections of Passion - 15) - Reflections Of Passion Y\\Yanni - Adagio In C Minor Y\\Yanni - Aria (Live) Y\\Yanni - Before I go Y\\Yanni - Dance with a Stranger Y\\Yanni - Keys To Imagination Y\\Yanni - Once Upon A Time Y\\Yanni - One Man\u0026#39;s Dream Y\\Yanni - Renegade Y\\Yanni - Santorini Y\\Yanni - So Long My Friend Y\\Yanni - The Face in the Photograph Y\\Yanni - The Mermaid Y\\Yanni - The Rain Must Fall (live) Y\\Yanni - To the One Who Knows Y\\Yanni - Until The Last Moment Y\\Yanni - We\u0026#39;re One (\u0026amp; Niki Nana) Y\\Yanni - Whispers In The Dark Y\\Yanni - Within Attraction Y\\Yardbirds - For Your Love Y\\Yazoo - Bad Connection Y\\Yazoo - Don\u0026#39;t Go Y\\Yazoo - Nobody\u0026#39;s Diary Y\\Yazoo - Only You Y\\Yazoo - Situation Y\\Yello - Oh-Yeah Y\\Yes - Owner Of A Lonely Heart (3\u0026#39;52\u0026#39;\u0026#39;) Y\\Yes - Owner Of A Lonely Heart (4\u0026#39;29\u0026#39;\u0026#39;) (192 kbps) Y\\Yes - Roundabout Y\\Yngwie Malmsteen - Cavallino Rampante Y\\Yngwie Malmsteen - Guitar Solo Y\\Young Rascals - In The Midnight Hour (96 kbps) Z Z\\Zager \u0026amp; Evans - In The Year 2525 Z\\Zombies - She\u0026#39;s Not There Z\\Zombies - Time of the Season Z\\ZZ Top - Bad To The Bone Z\\ZZ Top - Beer Drinkers and Hell Raisers Z\\ZZ Top - Blue Jean Blues 1 Z\\ZZ Top - Blue Jean Blues 2 Z\\ZZ Top - Cheap Sunglasses Z\\ZZ Top - Gimme All Your Lovin\u0026#39; Z\\ZZ Top - I Thank You Z\\ZZ Top - I\u0026#39;m Bad, I\u0026#39;m Nationwide (3\u0026#39;43\u0026#39;\u0026#39;) Z\\ZZ Top - I\u0026#39;m Bad, I\u0026#39;m Nationwide Z\\ZZ Top - La Grange (3\u0026#39;50\u0026#39;\u0026#39;) Z\\ZZ Top - La Grange Unidentified Unidentified\\Unknown - Indiana Jones Theme Unidentified\\Unknown - Italian Goes To Malta Unidentified\\Unknown - Looney Tunes Theme Unidentified\\Unknown - The River Of Permanent Changes part I+II Unidentified\\Unknown - Woody Woodpecker (original) 1 Unidentified\\Unknown - Woody Woodpecker (original) 2 ","date":"2002-10-03T00:00:00Z","permalink":"https://blog.michael.gr/post/2002-10-03-my-international-music-collection/","title":"My international (non-Greek) music collection"},{"content":"This is something that I am very glad to announce. In April of 2000 I quit smoking, and two years later I am still a non-smoker, meaning that I will never pick up that nasty habit again in my life.\nI quit smoking on April of 2000. Some people ask me how I did it. Others don't ask, but I am sure would be curious. So, here is my story.\nFor many years I had been telling myself that I ought to quit smoking, but I was not attempting it of the fear that I would fail to quit. But in fact, to quit smoking for me was not just a desire: there was fear involved, as I was always weary of the stats linking smoking to cancer. Also, I had always been worried about other people's right to breathe air free of smoke, and I would often voluntarily go out of my way to ensure that I was not making people around me feel uncomfortable, so this was, in turn, making my own life difficult. Nonetheless, I was smoking a 25g bag in two to three days, meaning 20 to 25 cigarettes per day, and these roll-your-own cigarettes burn more than two times longer than your usual filter cigarette.\nFirst of all let me say that Mr. Takis Maraletos, president of the company in which I was working at that time, had quit smoking in the beginning of 2000, and his decision to quit did have an instigating effect for my own decision. Also, while working at Mr. Maraletos' company I was fortunate enough to have my own office, which put me in a position of not having to smell other people's smoke. I have a very sensitive nose, so I guess this also counts.\nOn April of 2002 I became sick with pharyngitis, and when I am sick I generally I do not smoke, because cigarettes taste awful. Now, please bear in mind that during the last few years pharyngitis had become a yearly reoccurring phenomenon for me, and I had been told that there is a very strong connection between reoccurring pharyngitis, smoking, and cancer, so during those days I was re-thinking the whole smoking business anyway. Then, what really freaked me out was the realization that even before my sickness was completely over, my desire to smoke made me roll cigarettes, light them up, and try to smoke them, even though they still tasted horrible. I guess that was the final hint I needed. I finished that last bag of tobacco and never bought another one.\nSome people ask how I did it. It is simple: I just stopped paying visits to the tobacco shop. You see, walking up to a tobacco shop, asking the man behind the counter for your particular brand of disease, paying for it, and picking it up, are all parts of a 100% conscious process. It is not possible to successfully perform this sequence of steps without being absolutely aware of what you are doing. So, if that is NOT what you want to be doing, then you can consciously refrain from doing it. I am not going to say it was easy: I did feel nervous, I did pass endless hours under the sensation that something was indeterminably missing from my life, and there were even times when I could not sit my ass on the chair to do the work that I had to do. But I knew that all these symptoms were to be expected, I knew I was not going to die because of lack of nicotine in my veins, and I knew that all I had to do in order to successfully quit was just this one simple thing: refrain from actually buying the damn thing.\nI did not announce my decision to quit smoking to anyone, for two reasons: first, I did not want to play tricks with myself, such as promising everyone that I was quitting, and then forcing myself to keep my promise: besides doubting the effectiveness of such techniques, I think that such techniques are for people who are not 100% sure about their willingness to quit. Second, I was not sure whether I would actually manage to quit, (I was 100% sure about my willingness but not 100% sure about my ability,) so I did not want to say that I had quit smoking and then later find myself in contradiction. People would notice that I was not smoking anymore, and they would ask me if I quit, and I would just say \u0026quot;well, sort of, let's just say I am not smoking during these days.\u0026quot; As you understand, this worked against me, because at any given time I could simply say \u0026quot;hey everyone, well guess what, my non-smoking days are over, and I am back into smoking again\u0026quot;, but believe me, it never even crossed my mind to make use of that option.\nAll of the withdrawal symptoms were over in a matter of two to three months, and after that it was simply a matter of resisting the occasional urge to bump a cigarette off a friend. After the summer (that is, on the fifth month after quitting,) I felt so sure about myself that I even bumped a cigarette off a friend, only to realize that I did not like its taste anymore. That was it. I had successfully quit.\nFinally, let me add that Mr. Takis Maraletos started smoking again a few months after I quit, but that did not affect me at all. (Well, Takis, I did feel somewhat betrayed, but never mind.)\nI hope this was helpful to any smokers out there.\n","date":"2002-04-11T00:00:00Z","permalink":"https://blog.michael.gr/post/2002-04-11-on-smoking/","title":"On Smoking"},{"content":"My web host suddenly decided to enforce a 5 MB quota policy on free accounts like this one, so I had to remove many pictures and apply stronger JPEG compression on the rest in order to stay within quota.\nThis sucks !\n","date":"2002-02-12T00:00:00Z","permalink":"https://blog.michael.gr/post/2002-02-12-web-host-quota/","title":"Had to remove some stuff to meet my web host's quota"},{"content":"It was in my to-do list for a long time, and just now I found the time to do it. I did not even dare, though, to translate to Greek the descriptions of the jobs I held while I was in California. The descriptions of jobs I have held in Greece should be enough in any case.\nMy Resume in English My Resume in Greek ","date":"2001-12-18T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-12-18-my-resume/","title":"My Resume now available in English and in Greek"},{"content":"And, oh, is it a killer machine: 1.7 GHz P4 with 512 MB RDRAM, one 60 GB HDD plus a 20 GB HDD, and a Hercules 3D Prophet Ti 200 video display adapter (AGP 4X with 64 MB DDR RAM at 250 MHz and the NVidia Titanium GeForce 3 chipset at 175 MHz.)\nAnything else?\noh, and a HITACHI CM772 19\u0026quot; monitor.\n","date":"2001-12-11T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-12-11-new-computer/","title":"At last, my new computer is functional"},{"content":" Note in 2025:\nThe pictures were retrieved from my old blog via archive.org.\nPictures: 11 - Summer vacation 2001 part 5: Zakynthos\nPictures: 10 - Summer vacation 2001 part 4: Kefalonia\nPictures: 09 - Summer vacation, August 2001, part 2: Lefkada\nPictures: Summer vacation 2001 part 1: Milos -- this page was not stored by archive.org\nPictures: 07 - More of Milos\nPictures: 06 - Easter vacation, April of 2001, Milos\nPictures: 05 - Back in Greece 1996-1999\nPictures: 04 - Shortly before I left the USA in 1995\nPictures: 03 - Silverlakes, California circa 1994\nPictures: 02 - Trip to San Francisco, thanksgiving of 1991 or 1992\nPictures: 01 - Early\n","date":"2001-10-10T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-10-10-pictures/","title":"Pictures"},{"content":"\rAthens, April 1972 So mean in black leather!\nPicture 1988-12-MeAthens.jpg was not stored by archive.org.\nAthens, December 1988 Back when I used to actually wear color\n","date":"2001-10-10T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-10-10-pictures-01-early/","title":"Pictures: 01 - Early"},{"content":"Picture 1991_-SanFranciscoPiers.jpg was not stored by archive.org.\nSan Francisco 1991 San Francisco Piers\nPicture 1991_-SanFrancisco-Touch1.jpg was not stored by archive.org.\nSan Francisco 1991 The hand-drawn tree says it all\nPicture 1991_-SanFranciscoGoldenGateBridge2.jpg was not stored by archive.org.\nSan Francisco 1991 The Golden Gate Bridge\nPicture 1991_-SanFranciscoGoldenGateBridge1.jpg was not stored by archive.org.\nSan Francisco 1991 The Golden Gate Bridge up close\n","date":"2001-10-10T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-10-10-pictures-02-san-francisco/","title":"Pictures: 02 - Trip to San Francisco, thanksgiving of 1991 or 1992"},{"content":"\rBlack \u0026amp; White\nWhite \u0026amp; Black\nCheap sunglasses\n","date":"2001-10-10T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-10-10-pictures-03-1994-silverlakes-california/","title":"Pictures: 03 - Silverlakes, California circa 1994"},{"content":"\rMe and my red Accura Integra on Route 66, California\nThe Statue of Liberty, New York\nThe World Trade Center, New York\n","date":"2001-10-10T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-10-10-pictures-04-1995-usa/","title":"Pictures: 04 - Shortly before I left the USA in 1995"},{"content":"Picture 1996_-MeAX1-02-Touch1.jpg was not stored archive.org.\nAthens circa 1996 My first bike, a Honda AX-1\nPicture 1996_-MeDelphiCat.jpg was not stored archive.org.\nDelphi, Greece circa 1996 On the sacred grounds of Delphi - with a cat\nPicture 1998_-MeSarakiniko2.jpg was not stored archive.org.\nMilos, Greece circa 1998 Diving into the Aegean sea\nPicture 1999-01-Zappion-Touch2.jpg was not stored archive.org.\nAthens, Greece January 1999 Just don't mess with me.\nPicture 1999-02-MeSteering-Touch1.jpg was not stored archive.org.\nAthens, Greece January 1999 Ah, the sea breeze!\nPicture 1999-02-MeTrimming-Touch1.jpg was not stored archive.org.\nAthens, Greece January 1999 Trimming the sail\nPicture 1999-Greenpeace.jpg was not stored archive.org.\nAthens, Greece 1999? Me, 10 friends, and a kitty\n","date":"2001-10-10T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-10-10-pictures-05-1996-1999-back-in-greece/","title":"Pictures: 05 - Back in Greece 1996-1999"},{"content":"\rThe landmark mountains of Milos\nI guess I am not completely fed up with windows. Yet.\nLooking towards Plaka from Platiena\nSplendid view from a window.\n","date":"2001-10-10T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-10-10-pictures-06-2001-easter-milos/","title":"Pictures: 06 - Easter vacation, April of 2001, Milos"},{"content":"Picture 1998-Erimomilos.jpg was not stored by archive.org.\nErimomilos, Summer of 1998 Shot by my brother, Andrew\n2000-04-26-MilosUtopiaByThemis-Touch1.jpg was not stored by archive.org.\nMilos, April 28, 2000 Shot by Themis from Utopia\n2001-09-Milos-Arkoudes1%20By%20Angelica-Touched.jpg was not stored by archive.org.\nMilos, Summer of 2001 Shot by Angelica\n2001-09-Milos-Arkoudes2%20By%20Angelica-Touched.jpg was not stored by archive.org.\nMilos, Summer of 2001 Shot by Angelica\n2001-09-Milos-Kastanas1%20By%20Angelica.jpg was not stored by archive.org.\nMilos, Summer of 2001 Shot by Angelica\n2001-09-Milos-Kastanas2%20By%20Angelica.jpg was not stored by archive.org.\nMilos, Summer of 2001 Shot by Angelica\n2001-09-Milos-Kastanas3%20By%20Angelica.jpg was not stored by archive.org.\nMilos, Summer of 2001 Shot by Angelica\n2001-09-Milos-Kastanas4%20By%20Angelica.jpg was not stored by archive.org.\nMilos, Summer of 2001 Shot by Angelica\n2001-09-Milos-Kastanas5%20By%20Angelica.jpg was not stored by archive.org.\nMilos, Summer of 2001 Shot by Angelica\n2001-09-Milos-Kastanas6%20By%20Angelica.jpg was not stored by archive.org.\nMilos, Summer of 2001 Shot by Angelica\n2001-09-Milos-Kastanas7%20By%20Angelica.jpg was not stored by archive.org.\nMilos, Summer of 2001 Shot by Angelica\n2001-09-Milos-Platiena%20By%20Angelica-Touched.jpg was not stored by archive.org.\nMilos, Summer of 2001 Shot by Angelica\n","date":"2001-10-10T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-10-10-pictures-07-more-milos/","title":"Pictures: 07 - More of Milos"},{"content":"\rSky, sea, wind, and asphalt\nPicture 2001-08-KefalloniaDontShoot.jpg was not stored by archive.org.\nHunters - Don't Shoot The Cables\nEgremnoi beach from above\n","date":"2001-10-10T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-10-10-pictures-09-2001-part-2-lefkada/","title":"Pictures: 09 - Summer vacation, August 2001, part 2: Lefkada"},{"content":"\rKefalonia, August 2001 Heavy luggage\nKefalonia, August 2001 Shipwreck in a small bay\n","date":"2001-10-10T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-10-10-pictures-10-2001-part-4-kefalonia/","title":"Pictures: 10 - Summer vacation 2001 part 4: Kefalonia"},{"content":"Picture 2001-08-Zakynthos-Navagio.jpg was not stored by archive.org\nZakynthos, August 2001 Near the famous shipwreck, with friends\nPicture 2001-08-MeZakynthosShipWreck.jpg was not stored by archive.org\nZakynthos, August 2001 Overseeing the famous shipwreck\nPicture 2001-08-Zakynthos-Alitzerini.jpg was not stored by archive.org\nZakynthos, August 2001 Alitzerini, a restaurant\nPicture 2001-08-ZakynthosLaganas.jpg was not stored by archive.org\nZakynthos, August 2001 Outside a cafe along Laganas beach.\n","date":"2001-10-10T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-10-10-pictures-11-2001-part-5-zakynthos/","title":"Pictures: 11 - Summer vacation 2001 part 5: Zakynthos"},{"content":"The following quatrain has been circulating on the Net, attributed to Nostradamus:\n_\u0026quot;In the City of Gold there will be a great thunder, Two brothers torn apart by Chaos, while the fortress endures, the great leader will succumb. The third big war will begin when the big city is burning.\u0026quot;\nThis page says that the quatrain is bogus.\nAlso try searching here to see for yourself that no such quatrain exists among Nostradamus' writings.\n","date":"2001-09-12T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-09-12-nostradamus-hoax/","title":"The Nostradamus hoax"},{"content":"\rToday the world is a very different place from what it was yesterday. Today's event was the most significant historical event that has occurred during our lifetimes so far. I hope for the best, but prepare for the worst.\n","date":"2001-09-11T00:00:00Z","permalink":"https://blog.michael.gr/post/2001-09-11-wtc-disaster/","title":"The WTC Disaster"}]