<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="\rAbstract A new method for Automated Software Testing is presented as an alternative to Unit Testing. The new method retains the benefit of Unit Testing, which is Defect Localization, but eliminates white-box testing and mocking, thus greatly lessening the effort of writing and maintaining tests.\n"><title>Incremental Integration Testing</title><link rel=canonical href=https://blog2.michael.gr/post/2022-10-incremental-integration-testing/><link rel=stylesheet href=/scss/style.min.b390e410854a1900b4186a664a0397a2b4ee1b587bc335c2e9befc929a849158.css><meta property='og:title' content="Incremental Integration Testing"><meta property='og:description' content="\rAbstract A new method for Automated Software Testing is presented as an alternative to Unit Testing. The new method retains the benefit of Unit Testing, which is Defect Localization, but eliminates white-box testing and mocking, thus greatly lessening the effort of writing and maintaining tests.\n"><meta property='og:url' content='https://blog2.michael.gr/post/2022-10-incremental-integration-testing/'><meta property='og:site_name' content="Michael's Blog"><meta property='og:type' content='article'><meta property='article:section' content='post'><meta property='article:tag' content='software-architecture'><meta property='article:tag' content='testing'><meta property='article:tag' content='software-engineering'><meta property='article:tag' content='papers'><meta property='article:published_time' content='2021-12-14T09:07:09+00:00'><meta property='article:modified_time' content='2025-10-23T17:47:05+02:00'><meta name=twitter:title content="Incremental Integration Testing"><meta name=twitter:description content="\rAbstract A new method for Automated Software Testing is presented as an alternative to Unit Testing. The new method retains the benefit of Unit Testing, which is Defect Localization, but eliminates white-box testing and mocking, thus greatly lessening the effort of writing and maintaining tests.\n"><link rel="shortcut icon" href=/favicon.svg><script async src="https://www.googletagmanager.com/gtag/js?id=TODO"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","TODO")}</script></head><body class="article-page
article-page"><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><ol class=menu id=main-menu><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol><a href=/><img src=/logo.svg width=210 loading=lazy alt=Logo></a><figure class=site-avatar><a href=/><img src="https://gravatar.com/avatar/8d1c5b5578843f958430afe30e0cbb2fb5092b1712d1933ea37d7bf5cb4305ed?size=400" width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure></header><ul class=menu-social><li><a href=/contact-via-e-mail/ title=e-Mail rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 7a2 2 0 012-2h14a2 2 0 012 2v10a2 2 0 01-2 2H5a2 2 0 01-2-2V7z"/><path d="M3 7l9 6 9-6"/></svg></a></li><li><a href=/contact-via-whatsapp/ title=WhatsApp rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 21l1.65-3.8a9 9 0 113.4 2.9L3 21"/><path d="M9 10a.5.5.0 001 0V9A.5.5.0 009 9v1a5 5 0 005 5h1a.5.5.0 000-1h-1a.5.5.0 000 1"/></svg></a></li><li><a href=https://github.com/mikenakis target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/in/mikenakis target=_blank title=LinkedIn rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M8 11v5"/><path d="M8 8v.01"/><path d="M12 16v-5"/><path d="M16 16v-3a2 2 0 10-4 0"/><path d="M3 7a4 4 0 014-4h10a4 4 0 014 4v10a4 4 0 01-4 4H7a4 4 0 01-4-4z"/></svg></a></li><li><a href=https://stackoverflow.com/users/773113/mike-nakis target=_blank title="Stack Overflow" rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 17v1a2 2 0 002 2h12a2 2 0 002-2v-1"/><path d="M8 16h8"/><path d="M8.322 12.582l7.956.836"/><path d="M8.787 9.168l7.826 1.664"/><path d="M10.096 5.764l7.608 2.472"/></svg></a></li><li><a href=https://twitter.com/mikenakis target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ul><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li class=menu-bottom-section><ol class=menu></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#abstract>Abstract</a></li><li><a href=#summary>Summary</a></li><li><a href=#the-problem>The problem</a></li><li><a href=#the-existing-solution-unit-testing>The existing solution: Unit Testing</a></li><li><a href=#drawbacks-of-unit-testing>Drawbacks of Unit Testing</a></li><li><a href=#a-new-solution-incremental-integration-testing>A new solution: Incremental Integration Testing</a></li><li><a href=#prior-art>Prior Art</a></li><li><a href=#implementing-the-solution-the-poor-mans-approach>Implementing the solution: the poor man's approach</a></li><li><a href=#implementing-the-solution-the-automated-approach>Implementing the solution: the automated approach</a></li><li><a href=#what-if-my-dependencies-are-not-discoverable>What if my dependencies are not discoverable?</a></li><li><a href=#what-about-performance>What about performance?</a></li><li><a href=#benefits-of-incremental-integration-testing>Benefits of Incremental Integration Testing</a></li><li><a href=#arguments-and-counter-arguments>Arguments and counter-arguments</a></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/post/2022-10-incremental-integration-testing/>Incremental Integration Testing</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2021-12-14 Tue 09:07:09 UTC</time></div><div><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M6 20v-2a6 6 0 1112 0v2a1 1 0 01-1 1H7a1 1 0 01-1-1z"/><path d="M6 4v2a6 6 0 1012 0V4a1 1 0 00-1-1H7A1 1 0 006 4z"/></svg>
<time class=article-time--reading>26 minute read</time></div></footer></div></header><section class=article-content><p><img src=/post/2022-10-incremental-integration-testing/media/incremental_integration_testing.svg loading=lazy></p><h3 id=abstract>Abstract</h3><p>A new method for <em><strong>Automated Software Testing</strong></em> is presented as an alternative to <em><strong>Unit Testing</strong></em>. The new method retains the benefit of Unit Testing, which is <em><strong>Defect Localization</strong></em>, but eliminates white-box testing and mocking, thus greatly lessening the effort of writing and maintaining tests.</p><p>(Useful pre-reading: <a href=/post/2022-11-about-these-papers/>About these papers</a>)</p><h3 id=summary>Summary</h3><p>Unit Testing aims to achieve Defect Localization by replacing the collaborators of the Component Under Test with Mocks. As we will show, the use of Mocks is laborious, complicated, over-specified, presumptuous, and constitutes testing against the implementation, not against the interface, thus leading to brittle tests that hinder refactoring rather than facilitating it.</p><p>To avoid these problems, <em><strong>Incremental Integration Testing</strong></em> allows each component to be tested in integration with its collaborators, (or with Fakes thereof,) thus completely abolishing Mocks. Defect Localization is achieved by arranging the order in which tests are executed so that the collaborators of a component get tested before the component gets tested, and stopping as soon as a defect is encountered.</p><p>Thus, when a test discovers a defect, we can be sufficiently confident that the defect lies in the component being tested, and not in any of its collaborators, because by that time, the collaborators have passed their tests.</p><h3 id=the-problem>The problem</h3><p>The goal of automated software testing in general, regardless of what kind of testing it is, is to exercise a software system under various usage scenarios to ensure that it meets its requirements and that it is free from defects. The most simple and straightforward way to achieve this is to set up some input, invoke the system to perform a certain job, and then examine the output to ensure that it is what it is expected to be.</p><p>Unfortunately, this approach only really works in the "sunny day" scenario: if no defects are discovered by the tests, then everything is fine; however, if defects are discovered, we are faced with a problem: the system consists of a large network of collaborating software components, and the test is telling us that there is a defect somewhere, but it is unclear in which component the problem lies. Even if we divide the system into subsystems and try to test each subsystem separately, each subsystem may still consist of many components, so the problem remains.</p><p>What it ultimately boils down to is that each time we test a component, and a defect is discovered, it is unclear whether the defect lies in the component being tested, or in one or more of its collaborators.</p><p>Ideally, we would like each test to be conducted in such a way as to detect defects specifically in the component that is being tested, instead of extraneous defects in its collaborators; in other words, we would like to achieve <em>Defect Localization</em>.</p><h3 id=the-existing-solution-unit-testing>The existing solution: Unit Testing</h3><p><a class=external href=https://en.wikipedia.org/wiki/Unit_testing target=_blank>Unit Testing</a> was invented specifically in order to achieve defect localization. It takes an extremely drastic approach: if the use of collaborators introduces uncertainties, one way to eliminate those uncertainties is to eliminate the collaborators. Thus, Unit Testing aims to test each component in strict isolation. Hence, its name.</p><p>To achieve this remarkably ambitious goal, Unit Testing refrains from supplying the component under test with the actual collaborators that it would normally receive in a production environment; instead, it supplies the component under test with specially crafted <em><strong>substitutes</strong></em> of its collaborators, otherwise known as <em><strong>test doubles</strong></em>. There exist a few different kinds of substitutes, but by far the most widely used kind is <em><strong>Mocks.</strong></em></p><p>Each Mock must be hand-written for every individual test that is performed; it exposes the same interface as the real collaborator that it substitutes, and it expects specific methods of that interface to be invoked by the component-under-test, with specific argument values, sometimes even in a specific order of invocation. If anything goes wrong, such as an unexpected method being invoked, an expected method <em>not</em> being invoked, or a parameter having an unexpected value, the Mock fails the test. When the component-under-test invokes one of the methods that the Mock expects to be invoked, the Mock does nothing of the sort that the real collaborator would do; instead, the Mock is hard-coded to yield a fabricated response which is intended to exactly match the response that the real collaborator would have produced if it was being used, and if it was working exactly according to its specification.</p><p>Or at least, that is the intention.</p><h3 id=drawbacks-of-unit-testing>Drawbacks of Unit Testing</h3><ul><li><strong>Complex and laborious</strong><ul><li>In each test it is not enough to simply set up the input, invoke the component, and examine the output; we also have to anticipate every single call that the component will make to its collaborators, and for each call we have to set up a mock, expecting specific parameter values, and producing a specific response aiming to emulate the real collaborator under the same circumstances. Luckily, mocking frameworks lessen the amount of code necessary to accomplish this, but no matter how terse the mocking code is, the fact still remains that it implements a substantial amount of functionality which represents considerable complexity.</li><li>One of the well-known caveats of software testing at large (regardless of what kind of testing it is) is that a test failure does not necessarily indicate a defect in the production code; it always indicates a defect either in the production code, or in the test itself. The only way to know is to troubleshoot. Thus, the more code we put in tests, and the more complex this code is, the more time we end up wasting in chasing and fixing bugs in the tests themselves rather than in the code that they are meant to test.</li></ul></li><li><strong>Over-specified</strong><ul><li>Unit Testing is concerned not only with what a component accomplishes, but also with every little detail about how the component goes on about accomplishing it. This means that when we engage in Unit Testing we are essentially expressing all of our application logic twice: once with production code expressing the logic in imperative mode, and once more with testing code expressing the same logic in expectational mode. In both cases, we write copious amounts of code describing what should happen in excruciatingly meticulous detail.</li><li>Note that with Unit Testing, over-specification might not even be goal in and of itself in some cases, but it is unavoidable in all cases. This is due to the elimination of the collaborators: the requests that the component under test sends to its collaborators could conceivably be routed into a black hole and ignored, but in order for the component under test to continue working so as to be tested, it still needs to receive a meaningful response to each request; thus, the test has to expect each request in order to produce each needed response, even if the intention of the test was not to know how, or even whether, the request is made.</li></ul></li><li><strong>Presumptuous</strong><ul><li>Each Unit Test claims to have detailed knowledge of not only how the component-under-test invokes its collaborators, but also how each real collaborator would respond to each invocation in a production environment, which is a highly presumptuous thing to do.</li><li>Such presumptuousness might be okay if we are building high-criticality software, where each collaborator is likely to have requirements and specification that are well-defined and unlikely to change; however, in all other software, which is regular, commercial, non-high-criticality software, things are a lot less strict: not only the requirements and specifications change all the time, but also quite often, the requirements, the specification, even the documentation, is the code itself, and the code changes every time a new commit is made to the source code repository. This might not be ideal, but it is pragmatic, and it is established practice. Thus, the only way to know exactly how a component behaves tends to be to actually invoke the latest version of that component and see how it responds, while the mechanism which ensures that these responses are what they are supposed to be is the tests of that component itself, which are unrelated to the tests of components that depend on it.</li><li>As a result of this, Unit Testing often places us in the all too familiar situation where our Unit Tests all pass with flying colors, but our Integration Tests miserably fail because the behavior of the real collaborators turns out to be different from what the mocks assumed it would be.</li></ul></li><li><strong>Fragile</strong><ul><li>During Unit Testing, if the interactions between the component under test and its collaborators deviate even slightly from our expectations, the test fails. However, these interactions may legitimately change as software evolves. This may happen due to the application of a bug-fix, due to refactoring, or due to the fact that whenever new code is added to implement new functionality, preexisting code must almost always be modified to accommodate the new code. With Unit Testing, every time we change the inner workings of production code, we have to go fixing all related tests to expect the new inner workings of that code.</li><li>The original promise of Automated Software Testing was to enable us to continuously evolve software without fear of breaking it. The idea is that whenever you make a modification to the software, you can re-run the tests to ensure that everything still works as before. With Unit Testing this does not work, because every time you change the slightest thing in the production code you have to also change the tests, and you have to do this even for changes that are only internal. The understanding is growing within the software engineering community that Unit Testing with mocks actually hinders refactoring instead of facilitating it.</li></ul></li><li><strong>Non-reusable</strong><ul><li>Unit Testing exercises the implementation of a component rather than its interface. As such, the Unit Test of a certain component can only be used to test that component and nothing else. Thus, with Unit Testing the following things are impossible:<ul><li>Completely rewrite a piece of production code and then reuse the old tests to make sure that the new implementation works exactly as the old one did.</li><li>Reuse the same test to test multiple different components that implement the same interface.</li><li>Use a single test to test multiple different implementations of a certain component, created by independently working development teams taking different approaches to solving the same problem.</li></ul></li></ul></li></ul><p>The above disadvantages of Unit Testing are direct consequences of the fact that it is White-Box Testing by nature. What we need to be doing instead is Black-Box testing, which means that Unit Testing should be avoided, despite the entire Software Industry's addiction to it.</p><p>Note that I am not the only one to voice dissatisfaction with Unit Testing with Mocks. People have been noticing that although tests are intended to facilitate refactoring by ensuring that the code still works after refactoring, tests often end up hindering refactoring, because they are so tied to the implementation that you can't refactor anything without breaking the tests. This problem has been identified by renowned personalities such as Martin Fowler and Ian Cooper, and even by Ken Beck, the inventor of Test-Driven Development (TDD).</p><p>In the video <em>Thoughtworks - TW Hangouts: Is TDD dead?</em> (<a class=external href="https://www.youtube.com/watch?v=z9quxZsLcfo" target=_blank>youtube</a>) at 21':10'' Kent Beck says "My personal practice is I mock almost nothing" and at 23':56'' Martin Fowler says "I'm with Kent, I hardly ever use mocks".</p><p>In the <em>Fragile Test</em> section of his book <em>xUnit Test Patterns: Refactoring Test Code</em> (<a class=external href=https://xunitpatterns.com/ target=_blank>xunitpatterns.com</a>) author Gerard Meszaros states that extensive use of Mock Objects causes overcoupled tests.</p><p>In his presentation <em>TDD, where did it all go wrong?</em> (<a class=external href=https://www.infoq.com/presentations/tdd-original/ target=_blank>InfoQ</a>, <a class=external href="https://www.youtube.com/watch?v=EZ05e7EMOLM" target=_blank>YouTube</a>) at 49':32'' Ian Cooper says "I argue quite heavily against mocks because they are overspecified."</p><p>Note that in an attempt to avoid sounding too blasphemous, none of these people calls for the complete abolition of mocks, they only warn against the excessive use of mocks. Furthermore, do not seem to be isolating the components under test, and yet they seem to have little, if anything, to say about any alternative means of achieving defect localization.</p><h3 id=a-new-solution-incremental-integration-testing>A new solution: Incremental Integration Testing</h3><p>If we were to abandon Unit Testing with mocks, then one might ask what should we be doing instead. Obviously, we must somehow continue testing our software, and it would be nice if we can continue to be enjoying the benefits of defect localization.</p><p>As it turns out, eliminating the collaborators is just one way of achieving defect localization; another, more pragmatic approach is as follows:</p><blockquote><p>Allow each component to be tested in integration with its collaborators, but only after each of the collaborators has undergone its own testing, and has successfully passed it.</p></blockquote><p>Thus, any observed malfunction can be attributed with a high level of confidence to the component being tested, and not to any of its collaborators, because the collaborators have already been tested.</p><p>I call this <em><strong>Incremental Integration Testing</strong></em>.</p><p>An alternative way of arriving at the idea of Incremental Integration Testing begins with the philosophical observation that strictly speaking, there is no such thing as a Unit Test; there always exist collaborators which by established practice we never mock and invariably integrate in Unit Tests without blinking an eye; these are, for example:</p><ul><li>Many of the external libraries that we use.</li><li>Most of the functionality provided by the Runtime Environment in which our software runs.</li><li>Virtually all of the functionality provided by the Runtime Library of the language we are using.</li></ul><p>Nobody mocks standard collections such as array-lists, linked-lists, hash-sets, and hash-maps; very few people bother with mocking filesystems; nobody would mock a math library, a serialization library, and the like; even if one was so paranoid as to mock those, at the extreme end, nobody mocks the MUL and DIV instructions of the CPU; so clearly, there are always some things that we take for granted, and we allow ourselves the luxury of taking these things for granted because we believe that they have been sufficiently tested by their respective creators and can be reasonably assumed to be free of defects.</p><p>So, why not also take our own creations for granted once we have tested them? Are we testing them sufficiently or not?</p><h3 id=prior-art>Prior Art</h3><p>An internet search for "Incremental Integration Testing" does yield some results. An examination of those results reveals that they refer to some strategy for integration testing which is meant to be performed manually by human testers, constitutes an alternative to big-bang integration testing, and requires full Unit Testing of the traditional kind to have already taken place. I am hereby appropriating this term, so from now on it shall mean what I intend it to mean. If a context ever arises where disambiguation is needed, the terms "automated" vs. "manual" can be used.</p><p>The first hints to Incremental Integration Testing can actually be found in the classic 1979 book <em>The Art of Software Testing</em> by Glenford Myers. In chapter 5 "Module (Unit) Testing" the author plants the seeds of what later became white-box testing with mocks by writing:</p><blockquote><p>[â€¦] since module B calls module E, something must be present to receive control when B calls E. A stub module, a special module given the name "E" that must be coded to simulate the function of module E, accomplishes this.</p></blockquote><p>then, the author proceeds to write:</p><blockquote><p>The alternative approach is incremental testing. Rather than testing each module in isolation, the next module to be tested is first combined with the set of modules <em><strong>that have already been tested.</strong></em></p></blockquote><p>(emphasis mine.)</p><p>Back in 1979, Glen Myers envisioned these approaches to testing as being carried out by human testers, manually launching tests and receiving printouts of results to examine. He even envisioned employing multiple human testers to perform multiple tests in parallel. In the last several decades we have much better ways of doing all of that.</p><h3 id=implementing-the-solution-the-poor-mans-approach>Implementing the solution: the poor man's approach</h3><p>As explained earlier, Incremental Integration Testing requires that when we test a component, all of its collaborators must have already been tested. Thus, Incremental Integration Testing necessitates exercising control over the order in which tests are executed.</p><p>Most testing frameworks execute tests in alphanumeric order, so if we want to change the order of execution all we have to do is to appropriately name the tests, and the directories in which they reside.</p><p>For example:</p><p>Let us suppose that we have the following modules:</p><p>com.acme.alpha_depends_on_bravo<br>com.acme.bravo_depends_on_nothing<br>com.acme.charlie_depends_on_alpha</p><p>Note how the modules are listed alphanumerically, but they are not listed in order of dependency.</p><p>Let us also suppose that we have one test suite for each module. By default, the names of the test suites follow the names of the modules that they test, so again, a listing of the test suites in alphanumeric order does not match the order of dependency of the modules that they test:</p><p>com.acme.alpha_depends_on_bravo_<strong>tests</strong><br>com.acme.bravo_depends_on_nothing_<strong>tests</strong><br>com.acme.charlie_depends_on_alpha_<strong>tests</strong></p><p>To achieve Incremental Integration Testing, we add a suitably chosen prefix to the name of each test suite, as follows:</p><p>com.acme.<strong>T02</strong>_alpha_depends_on_bravo_tests<br>com.acme.<strong>T01</strong>_bravo_depends_on_nothing_tests<br>com.acme.<strong>T03</strong>_charlie_depends_on_alpha_tests</p><p>Note how the prefixes have been chosen in such a way as to establish a new alphanumerical order for the tests. Thus, an alphanumeric listing of the test suites now lists them in order of dependency of the modules that they test:</p><p>com.acme.T01_bravo_depends_on_nothing_tests<br>com.acme.T02_alpha_depends_on_bravo_tests<br>com.acme.T03_charlie_depends_on_alpha_tests</p><p>At this point Java developers might object that this is impossible, because in Java, the tests always go in the same module as the production code, directory names must match package names, and test package names always match production package names. Well, I have news for you: they don't have to. The practice of doing things this way is very widespread in the Java world, but there are no rules that require it: the tests do not in fact have to be in the same module, nor in the same package as the production code. The only inviolable rule is that directory names must match package names, but you can call your test packages whatever you like, and your test directories accordingly.</p><p>Java developers tend to place tests in the same module as the production code simply because the tools (maven) have a built-in provision for this, without ever questioning whether there is any actual benefit in doing so. Spoiler: there isn't. As a matter of fact, in the DotNet world there is no such provision, and nobody complains. Furthermore, Java developers tend to place tests in the same package as the production code for no purpose other than to make package-private entities of their production code accessible from their tests, but this is testing against the implementation, not against the interface, and therefore, as I have already explained, it is misguided.</p><p>So, I know that this is a very hard thing to ask from most Java developers, but trust me, if you would only dare to take a tiny step off the beaten path, if you would for once do something in a certain way for reasons other than "everyone else does it this way", you can very well do the renaming necessary to achieve Incremental Integration Testing.</p><p>Now, admittedly, renaming tests in order to achieve a certain order of execution is not an ideal solution. It is awkward, it is thought-intensive since we have to figure out the right order of execution by ourselves, and it is error-prone because there is nothing to guarantee that we will get the order right. That's why I call it "the poor man's approach". Let us now see how all of this could be automated.</p><h3 id=implementing-the-solution-the-automated-approach>Implementing the solution: the automated approach</h3><p>Here is an algorithm to automate Incremental Integration Testing:</p><ol><li><strong>Begin by building a model of the dependency graph of the entire software system.</strong><ul><li>This requires system-wide static analysis to discover all components in our system, and all dependencies of each component. I did not say it was going to be easy.</li><li>The graph should not include external dependencies, since they are presumed to have already been tested by their respective creators.</li></ul></li><li><strong>Test each leaf node in the model.</strong><ul><li>A leaf node in the dependency graph is a node which has no dependencies; at this level, a Unit Test is indistinguishable from an Integration Test, because there are no dependencies to either integrate or mock.</li></ul></li><li><strong>If any malfunction is discovered during step 2, then stop as soon as step 2 is complete.</strong><ul><li>If a certain component fails to pass its test, it is counter-productive to proceed with the tests of components that depend on it. Unit Testing seems to be completely oblivious to this little fact; Incremental Integration Testing fixes this.</li></ul></li><li><strong>Remove the leaf nodes from the model of the dependency graph.</strong><ul><li>Thus removing the nodes that were previously tested in step 2, and obtaining a new, smaller graph, where a different set of nodes are now the leaf nodes.</li><li>The dependencies of the new set of leaf nodes have already been successfully tested, so they are of no interest anymore: they are as good as external dependencies now.</li></ul></li><li><strong>Repeat starting from step 2, until there are no more nodes left in the model.</strong><ul><li>Allowing each component to be tested in integration with its collaborators, since they have already been tested.</li></ul></li></ol><p>No testing framework that I know of (JUnit, MSTest, etc.) is capable of doing the above; for this reason, I have developed a utility which I call <em><strong>Testana</strong></em>, that does exactly that.</p><p>Testana will analyze a system to discover its structure, will analyze modules to discover dependencies and tests, and will run the tests in the right order so as to achieve Incremental Integration Testing. It will also do a few other nice things, like keep track of last successful test runs, and examine timestamps, so as to refrain from running tests whose dependencies have not changed since the last successful test run. For more information, see <a href=/post/2024-10-testana/>Testana: A better way of running tests</a>.</p><h3 id=what-if-my-dependencies-are-not-discoverable>What if my dependencies are not discoverable?</h3><p>Some very trendy practices of our modern day and age include:</p><ul><li>Using scripting languages, where there is no notion of types, and therefore no way of discovering dependencies via static analysis.</li><li>Breaking up systems into disparate source code repositories, so there is no single system on which to perform system-wide static analysis to discover dependencies.</li><li>Incorporating multiple different programming languages in a single system, (following the polyglot craze,) thus hindering system-wide static analysis, since it now needs to be performed on multiple languages and across language barriers.</li><li>Making modules interoperate not via normal programmatic interfaces, but instead via various byzantine mechanisms such as REST, whose modus operandi is binding by name, thus making dependencies undiscoverable.</li></ul><p>If you are following any of the above trendy practices, then you cannot programmatically discover dependencies, so you have no way of automating Incremental Integration Testing, so you will have to manually specify the order in which your tests will run, and you will have to keep maintaining this order manually.</p><p>Sorry, but silly architectural choices do come with consequences.</p><h3 id=what-about-performance>What about performance?</h3><p>One might argue that Incremental Integration Testing does not address one very important issue which is nicely taken care of by Unit Testing with Mocks, and that issue is performance:</p><ul><li>When collaborators are replaced with Mocks, the tests tend to be fast.</li><li>When actual collaborators are integrated, such as file systems, relational database management systems, messaging queues, and what not, the tests can become very slow.</li></ul><p>To address the performance issue I recommend the use of <em><strong>Fakes</strong></em>, not Mocks. For an explanation of what Fakes are, and why they are incontestably preferable over Mocks, please read <a href=/post/2022-10-testing-with-fakes/>Testing with Fakes instead of Mocks</a>.</p><p>By supplying a component under test with a Fake instead of a Mock we benefit from great performance, while utilizing a collaborator which has already been tested by its creators and can be reasonably assumed to be free of defects. In doing so, we continue to avoid White-Box Testing and we keep defects localized.</p><p>Furthermore, nothing prevents us from having our CI/CD server run the test of each component twice:</p><ul><li>Once in integration with Fakes</li><li>Once in integration with the actual collaborators</li></ul><p>This will be slow, but CI/CD servers generally do not mind. The benefit of doing this is that it gives further guarantees that everything works as intended.</p><h3 id=benefits-of-incremental-integration-testing>Benefits of Incremental Integration Testing</h3><ul><li>It greatly reduces the effort of writing and maintaining tests, by eliminating the need for mocking code in each test.</li><li>It allows our tests to engage in Black-Box Testing instead of White-Box Testing. For an in-depth discussion of what is wrong with White-Box Testing, see <a href=/post/2021-12-white-box-vs-black-box-testing/>White-Box vs. Black-Box Testing</a>.</li><li>It makes tests more effective and accurate, by eliminating assumptions about the behavior of the real collaborators.</li><li>It simplifies our testing operations by eliminating the need for two separate testing phases, one for Unit Testing and one for Integration Testing.</li><li>It is unobtrusive, since it does not dictate how to construct the tests, it only dictates the order in which the tests should be executed.</li></ul><h3 id=arguments-and-counter-arguments>Arguments and counter-arguments</h3><ul><li><p><strong>Argument: Incremental Integration Testing assumes that a component which has been tested is free of defects.</strong></p><p>A well-known caveat of software testing is that it cannot actually prove that software is free from defects, because it necessarily only checks for defects that we have anticipated and tested for. As Edsger W. Dijkstra famously put it, "program testing can be used to show the presence of bugs, but never to show their absence!"</p><p>Counter-arguments:</p><ul><li>I am not claiming that once a component has been tested, it has been proven to be free from defects; all I am saying is that it can reasonably be assumed to be free from defects. Incremental Integration Testing is not meant to be a perfect solution; it is meant to be a pragmatic solution.</li><li>The fact that testing cannot prove the absence of bugs does not mean that everything is futile in this vain world, and that we should abandon all hope in despair: testing might be imperfect, but it is what we can do, and it is in fact what we do, and practical, real-world observations show that it is quite effective.</li><li>Most importantly: Any defects in an insufficiently tested component will not magically disappear if we mock that component in the tests of its dependents.<ul><li>In this sense, the practice of mocking collaborators can arguably be likened to <a class=external href=https://en.wikipedia.org/wiki/Ostrich_policy target=_blank><em>Ostrich policy</em></a>.</li><li>On the contrary, continuing to integrate that component in subsequent tests gives us incrementally more opportunities to discover defects in it.</li></ul></li></ul></li><li><p><strong>Argument: Incremental Integration Testing fails to achieve complete defect localization.</strong></p><p>If a certain component has defects which were not detected when it was being tested, these defects may cause tests of collaborators of that component to fail, in which case it will be unclear where the defect lies.</p><p>Counter-arguments:</p><ul><li>It is true that Incremental Integration Testing may fall short of achieving defect localization when collaborators have defects despite having already been tested. It is also true that Unit Testing with Mocks does not suffer from that problem when collaborators have defects; but then again, neither does it detect those defects. For that, it is necessary to always follow a round of Unit Testing with a round of Integration Testing. However, when the malfunction is finally observed during Integration Testing, we are facing the exact same problem that we would have faced if we had done a single round of Incremental Integration Testing instead: a malfunction is being observed which is not due to a defect in the root component of the integration, but instead due to a defect in some unknown collaborator. The difference is that Incremental Integration Testing gets us there faster.</li><li>Let us not forget that the primary goal of software testing is to guarantee that software works as intended, and that defect localization is an important but nonetheless secondary goal. Incremental Integration Testing goes a long way towards achieving defect localization, but it may not achieve it perfectly, in favor of other conveniences, such as making it far more easy to write and maintain tests. So, it all boils down to whether Unit Testing represents overall more or less convenience than Incremental Integration Testing. I assert that Incremental Integration Testing is unquestionably far more convenient than Unit Testing.</li></ul></li><li><p><strong>Argument: Incremental Integration Testing only tests behavior; it does not check what is going on under the hood.</strong></p><p>With Unit Testing, you can ensure that a certain module not only produces the right results, but also that it follows an expected sequence of steps to produce those results. With Incremental Integration Testing you cannot observe the steps, you can only check the results. Thus, the internal workings of a component might be slightly wrong, or less than ideal, and you would never know.</p><p>Counter-arguments:</p><ul><li>This is true, and this is why Incremental Integration Testing might be unsuitable for high-criticality software, where White-Box Testing is the explicit intention, since it is necessary to ensure not only that the software produces correct results, but also that its internals are working exactly according to plan. However, Incremental Integration Testing is not being proposed as a perfect solution, it is being proposed as a pragmatic solution: the vast majority of software being developed in the world is regular, commercial-grade, non-high-criticality software, where Black-Box Testing is appropriate and sufficient, since all that matters is that the requirements be met. Essentially, Incremental Integration Testing represents the realization that in the general case, tests which worry not only about the behavior, but also about the inner workings of a component, constitute over-engineering. For a more in-depth discussion about this, please read <a href=/post/2021-12-white-box-vs-black-box-testing/>White-Box vs. Black-Box Testing</a>.</li><li>In order to make sure that everything is happening as expected under the hood, you do not have to stipulate in excruciating detail what should be happening, you do not have to fail the tests at the slightest sign of deviation from what was expected, and you do not have to go fixing tests each time the expectations change. Another way of ensuring the same thing is to simply:<ul><li>Gain visibility into what is happening under the hood.</li><li>Be notified when something different starts happening.</li><li>Visually examine what is now different.</li><li>Vouch for the differences being as expected.
For more details about this, see <a href=/post/2023-01-06-collaboration-monitoring/>Collaboration Monitoring</a>.</li></ul></li></ul></li><li><p><strong>Argument: Incremental Integration Testing prevents us from picking a single test and running it.</strong></p><p>With Unit Testing, we can pick any individual test and run it. With Incremental Integration Testing, running an individual test of a certain component is meaningless unless we first run the tests of the collaborators of that component.</p><p>Counter-arguments:</p><ul><li>Picking an individual test and running it is meaningless under all scenarios. It is usually done in the interest of saving time, but it is based on the assumption that we know what tests have been affected by the changes we just made to the source code. This is never a safe assumption to make.</li><li>Instead of picking an individual test and running it, we need a way to automatically run all tests that have been affected by the changes we just made, which requires knowledge of the dependency graph of the system.</li><li>If you are unsure as to exactly what you just changed, and exactly what depends on it, then consider using a tool like Testana, which figures all this out for you. See <a href=/post/2024-10-testana/>Testana: A better way of running tests</a>.</li></ul></li><li><p><strong>Argument: Incremental Integration Testing requires additional tools.</strong></p><p>Incremental Integration Testing is not supported by any of the popular testing frameworks, which means that in order to start practicing it, new tools are necessary. Obtaining such tools might be very difficult, if not impossible, and creating such tools might be difficult, because they would have to do advanced stuff like system-wide static analysis to discover the dependency graph of a system.</p><p>Counter-arguments:</p><ul><li>My intention is to show the way; if people see the way, the tools will come.</li><li>I have already built such a tool which is compatible with some combinations of programming languages, build systems, and testing frameworks; see <a href=/post/2024-10-testana/>Testana: A better way of running tests</a>.</li><li>Even in lack of tools, it is possible to start experimenting with Incremental Integration Testing today by following the poor-man's approach, which consists of simply naming the tests, and the directories in which they reside, in such a way that your existing testing framework will run them in the right order. This is described in the "poor man's approach" section of this paper.</li></ul></li></ul><h3 id=conclusion>Conclusion</h3><p>Unit Testing was invented in order to achieve defect localization, but as we have shown, it constitutes White-Box Testing, so it is laborious, over-complicated, over-specified, and presumptuous. Furthermore, it is not even, strictly speaking, necessary. Incremental Integration Testing is a pragmatic approach which achieves almost the same degree of defect localization but without the use of mocks, and in so doing it greatly reduces the effort of developing and maintaining tests.</p><hr><p>Cover image: <em>Incremental Integration Testing</em> by michael.gr</p></section><footer class=article-footer><section class=article-tags><a href=/tags/software-architecture/>Software Architecture</a>
<a href=/tags/testing/>Testing</a>
<a href=/tags/software-engineering/>Software Engineering</a>
<a href=/tags/papers/>Papers</a></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on 2025-10-23 Thu 17:47:05 CEST</span></section></footer></article><script>var idcomments_post_id,idcomments_post_url,idcomments_acct="131c02e0ef20d1a4606aa4e3490711ba"</script><span id=IDCommentsPostTitle style=display:none></span>
<script type=text/javascript src=https://www.intensedebate.com/js/genericCommentWrapperV2.js></script><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/post/2024-03-codecoverage/><div class=article-details><h2 class=article-title>Artificial Code Coverage</h2></div></a></article><article><a href=/post/2023-01-14-mocking/><div class=article-details><h2 class=article-title>If you are using mock objects you are doing it wrong</h2></div></a></article><article><a href=/post/2021-12-white-box-vs-black-box-testing/><div class=article-details><h2 class=article-title>White-Box vs. Black-Box Testing</h2></div></a></article><article><a href=/post/2025-04-the-confusion-about-term-unit-testing/><div class=article-details><h2 class=article-title>The confusion about the term Unit Testing</h2></div></a></article><article><a href=/post/2024-10-testana/><div class=article-details><h2 class=article-title>Testana: A better way of running tests</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy; 2001 - 2026 Michael Belivanakis (a.k.a. Mike Nakis)</section><section class=powerby>Made using <b><a href=https://obsidian.md target=_blank>Obsidian</a></b> and <b><a href=https://gohugo.io/ target=_blank>Hugo</a></b><br>Theme based on <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank data-version=%s>hugo-theme-stack</a></b> by <a href=https://jimmycai.com target=_blank>Jimmy Cai</a><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.c922af694cc257bf1ecc41c0dd7b0430f9114ec280ccf67cd2c6ad55f5316c4e.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>